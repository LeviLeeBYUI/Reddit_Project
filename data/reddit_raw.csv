id,title,selftext,score,num_comments,created_utc,subreddit
1i5inrb,"Weekly Entering & Transitioning - Thread 20 Jan, 2025 - 27 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,15,1737349304.0,datascience
1i6ixvq,"5 months, 300 applications, 2 first-round interviews, no offers. Why am I not getting anything? ",,463,297,1737467271.0,datascience
1i6qcch,Is this a normal data analyst experience? Expectations for new data analysts in the field,"I am a data analyst for a corporate company, this is my first year in a role like this and it has been a year. My manager is concerned that I have holes in my understanding about the company, but I feel like it is the lack of training and resources. I've never struggled so much in a role before, I previously worked in sales/sales admin for 5 years at a scientific company.

When I was interviewed, I explained that I had no experience with pivot tables or vlookup. It was my understanding from the interview that they were looking for someone to mentor, and I was hired on for having a great attitude. During onboarding, I was given pretty surface level material to review and met maybe a handful of times with others on the teams on building basic reports. I've had to do a lot of studying on my own time. During the year though, I have continued to struggle on the reporting aspect of my job and feel the relationship strains at work because of it. I am proud to say that I have been practicing excel files online with sample data at home for months and can successfully create files on my own. I've asked to shadow and practice files at home, but I was told to just learn more about the company and ask more questions. This is the kind of scenario I keep running into at my current job:

Ex: A few weeks ago, I was tasked to create a report. I was told to look at a few automated reports and essentially play around/figure it out. I was trained on two automated reports, but had not been trained on the others. My team was a bit annoyed with my confusion on which report I should use and that I should know based on the data. They gave me a suggestion on what report to try. I played around with the data on my own and got like 70% with the data I had. I was told yesterday that they decided to pull data elsewhere (because it would cover everything they wanted on the report more easily)  from a space I don't have access to and haven't been trained on. 



",28,34,1737486274.0,datascience
1i735gu,"Google Gemini 2 Flash Thinking Experimental 01-21 out , Rank 1 on LMsys",,4,0,1737520654.0,datascience
1i6tsvc,Syracuse online MSDS,"5 YoE DS here. Looking to get that next level piece of paper. Looking for something where I can complete a degree while doing full time job.

Anybody have any experience? Cash grab program or similar to Georgia tech?

Thanks in advance! ",4,9,1737494772.0,datascience
1i6qa6u,Analyzing changes to gravel height along a road,"I’m working with a dataset that measures the height of gravel along a 50 km stretch of road at 10-meter intervals. I have two measurements:

Baseline height: The original height of the gravel.

New height: A more recent measurement showing how the gravel has decreased over time.

This gives me the difference in height at various points along the road. I’d like to model this data to understand and predict gravel depletion. 

Here’s what I’m considering:Identifying trends or patterns in gravel loss (e.g., areas with more significant depletion).

Using interpolation to estimate gravel heights at points where measurements are missing.

Exploring possible environmental factors that could influence depletion (e.g., road curvature, slope, or proximity to towns).

However, I’m not entirely sure how to approach this analysis. Some questions I have:

What are the best methods to visualize and analyze this type of spatial data?

Are there statistical or machine learning models particularly suited for this?

If I want to predict future gravel heights based on the current trend, what techniques should I look into? Any advice, suggestions, or resources would be greatly appreciated!",4,4,1737486130.0,datascience
1i658fp,What should I do to build a strong foundation in developing?,"I’m interested in becoming a developer. I’m currently proficient in Tableau, Alteryx, Power BI etc. 

I feel like there’s 1 million different avenues. I’m not sure which route to take. 

I want to get around a community, where I can connect and get exposed to more. I’m in the Miami area. 

I’ve checked out YouTube videos on Java script

What do you all recommend? ",9,14,1737417755.0,datascience
1i5d77u,Anyone ever feel like working as a data scientist at hinge?,"Need to figure out what that damn algorithm is doing to keep me from getting matches lol. On a serious note I have read about some interesting algorithmic work at dating app companies. Any data scientists here ever worked for a dating app company? 

Edit: gale-shapely algorithm

https://reservations.substack.com/p/hinge-review-how-does-it-work#:~:text=It%20turns%20out%20that%20the,among%20those%20who%20prefer%20them.",417,141,1737331692.0,datascience
1i6pu2t,How to get individual restaurant review data?,,0,7,1737485051.0,datascience
1i60m31,Question about Using Geographic Data for Soil Analysis and Erosion Studies,"I’m working on a project involving a dataset of latitude and longitude points, and I’m curious about how these can be used to index or connect to meaningful data for soil analysis and erosion studies. Are there specific datasets, tools, or techniques that can help link these geographic coordinates to soil quality, erosion risk, or other environmental factors?

I’m interested in learning about how farmers or agricultural researchers typically approach soil analysis and erosion management. Are there common practices, technologies, or methodologies they rely on that could provide insights into working with geographic data like this?

If anyone has experience in this field or recommendations on where to start, I’d appreciate your advice!",8,14,1737406032.0,datascience
1i5576y,Should I Try to postpone my FAANG Interview?,"So I got contacted by a FAANG Recruiter for a Data Scientist Role I applied for a month and a half ago. But as I have started to prep, I realize I am not ready and need 1 to 2 months before I would be able to do well on all the technical interviews (there are 4 of them). My SQL is rusty because I have been using Pyspark so much that I didn't really need to do medium to hard SQL queries at work (We're also not allowed in most cases since SQL is slower). So I would just do everything in Pyspark. But now, as I start practicing my SQL I realize it's very basic, and it's going to take some time before I can get it on the level my pyspark is at.

I've noticed that I feel like there is no chance of me performing well enough on this interview, and it sucks because the recruiter said that the hiring manager was looking at my resume and really wants to interview me as soon as possible since he thinks I have strong experience for the role (They made me bypass the phone screens because of it). I have no doubt I would be able to do the role, but interviews are another beast. According to the prep guide, my Stats, ML Theory, SQL, and Python all have to be perfect. Since I joined my current company as an intern, I didn't have to do as many in-depth technicals as I have to do here. I've interviewed at a couple other big companies last year and didn't make it to the final round for one simply because I needed more time to prepare. The FAANG recruiter wants me to do the first 2 interviews within the next two weeks, and I'm worried about what it would do to my confidence if I failed this interview since this is pretty much my dream Data Scientist role. My mind is already telling me just to make the best of this and use it as a learning experience, but another part of me is wondering if I should just cancel it altogether or try to delay it as much as possible. I have a mock interview with a Company Data Scientist they set up for me in a few days, but part of me feels defeated already and it sucks...

I honestly am not sure what to do as I need a lot more time. I've heard others say it took them as long as 2-6 months before they were ready to crush their FAANG interview and I know I am not there yet...",198,79,1737311263.0,datascience
1i57vx1,Where to Start when Data is Limited: A Guide,"
Hey, I’ve put together an article on my thoughts and some research around how to get the most out of small datasets when performance requirements mean conventional analysis isn’t enough. 

It’s aimed at helping people get started with new projects who have already started with the more traditional statistical methods.

Would love to hear some feedback and thoughts.",65,6,1737317875.0,datascience
1i4yyoe,Influential Time-Series Forecasting Papers of 2023-2024: Part 1,"This article explores some of the latest advancements in time-series forecasting.

You can find the article [here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting).

Edit: If you know of any other interesting papers, please share them in the comments.",180,26,1737294977.0,datascience
1i5grpu,"MiniCPM-o 2.6 : True multimodal LLM that can handle images, videos, audios and comparable with GPT4o on Multi-modal benchmarks",,4,1,1737342830.0,datascience
1i4f1go,AI is difficult to get right: Apple Intelligence rolled back(Mostly the summary feature),"Source: https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html#:\~:text=Apple%20is%20temporarily%20pulling%20its,organization%20and%20press%20freedom%20groups.

Seems like even Apple is struggling to deploy AI and deliver real-world value.  
Yes, companies can make mistakes, but Apple rarely does, and even so, it seems like most of Apple Intelligence is not very popular with IOS users and has led to the creation of r/AppleIntelligenceFail.

It's difficult to get right in contrast to application development which was the era before the ai boom. ",307,41,1737227935.0,datascience
1i5ok61,There can be no reasoning without inference-time weight updates,"Reasoning is learning from synthesis. Frozen model weights can not reason. Find a way for the model to update its weights during inference based on its findings and watch AGI emerge. 

This is my hypothesis. A quick google search returned nothing relevant. If you know of such experiments, please link them here!",0,6,1737374168.0,datascience
1i3y1qs,What salary range should I expect as a fresh college grad with a BS in Statistics and Data Science?,"For context, I’m a student at UCLA, and am applying to jobs within California. But I’m interested in people’s past jobs fresh out of college, where in the country, and what the salary was. 

Tentatively, I’m expecting a salary of anywhere between $70k and $80k, but I’ve been told I should be expecting closer to $100k, which just seems ludicrous. ",118,112,1737169081.0,datascience
1i3x2cf,Are there any ways to earn a little extra money on the side as a data scientist?,"Using data science skills (otherwise I'm sure there are plenty). 

I know there is data annotation, but I'm not sure that qualifies as data science.",91,37,1737165883.0,datascience
1i40izz,Do these recruiters sound like a scam?,"Hi all, unsure of where else to ask this so asking here. 

I had a recruiter (heavy Indian accent) call/email me with an interesting proposition. They work for the candidate rather than the company. If they place you in a job within 45 days they ask for 9% of your first year's salary.

They claim their value add is in a couple of things. First they promise that they have advanced ATS software that will help tweak professional qualifications. Second, they say they will apply to approximately 50 JDs per day (I am skeptical this many relevant jobs are even being posted).

I have never had luck with Indian recruiters before but I have had good experiences professionally in offshoring some repetitive tasks for cheap. This process sounds like it fits the bill. The part where it gets sketchy is they want either access to my LinkedIn/Gmail or they want me to create second LinkedIn/Gmail accounts that they would have control over. Access to my gmail is a nonstarter obviously. But creating spoof LinkedIn/Gmails feels a little sketchy. 

If we're living in a universe where these guys are simply trying to provide the service they've described, I'm all in. I just don't want to get soft-rolled into some sort of scam.",12,35,1737177777.0,datascience
1i3zajz,Huggingface smolagents : Code centric Agent framework. Is it the best AI Agent framework? I don't think so,,2,2,1737173285.0,datascience
1i33mt0,I've been given the choice between being a Data Scientist or an Analytics Manager. Which would you choose and why?,"I'm coming from a Data Analyst position, and I've essentially been given the choice between being a Data Scientist and or an Analytics Manager. I thought Data Scientist was my dream job, but the Manager position would pay more, and I've been dreaming about working my way up to Director or CDO... Does Analytics Manager make the most sense in this case?

Update for context: I'm 25, have a master's in data analytics, and have been working in the same industry for 7 years but in different roles. I've been an Analyst for 1.5+ years, and previously was a Data Manager, and a Researcher.",195,77,1737073226.0,datascience
1i3cgo0,guys is web crawling and scraping +1 for data science or it doesn't matter. ,"by web crawling and scraping i mean advanced scraping with multiple websites for prices and products then building further things around it like strategic planning and buisness analytics. 

edit: is it a necessary skill or not. +1 it means its a great add on to ur skill stack",35,53,1737105217.0,datascience
1i3a45a,How long did it take you to get a new role when looking for a new job?,"I'm feeling very miserable at my job as well as feeling uneasy with the ethics of my company so I desperately am looking for a new role, but this job market is concerning. I have a BS in Math and MS in DS, been at my job as a data scientist for 1.5 years, worked for 3 years between BS and MS in analyst roles. Is there hope to have something new soon? How many apps per day should I be sending? ",47,45,1737094521.0,datascience
1i2vj0x,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",253,26,1737051786.0,datascience
1i3a227,Google Titans : New LLM architecture with better long term memory,,7,0,1737094288.0,datascience
1i3clrk,Microsoft MatterGen: GenAI model for Material design and discovery ,,2,1,1737105866.0,datascience
1i2vmuv,Introducing mlsynth.,"Hi DS Reddit. For those of who you work in causal inference, you may be interested in a Python library I developed called ""machine learning synthetic control"", or ""mlsynth"" for short.

As I write in its [documentation](https://mlsynth.readthedocs.io), mlsynth is a one-stop shop of sorts for implementing some of the most recent synthetic control based estimators, many of which use machine learning methodologies. Currently, the software is hosted from my GitHub, and it is still undergoing developments (i.e., for computing inference for point-estinates/user friendliness).

mlsynth implements the following methods: [Augmented Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), CLUSTERSCM, [Debiased Convex Regression](https://doi.org/10.1287/inte.2023.0028)  (undocumented at present), the [Factor Model Approach](https://doi.org/10.1177/00222437221137533), [Forward Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), [Forward Selected Panel Data Approach](https://doi.org/10.1016/j.jeconom.2021.04.009), the [L1PDA](https://doi.org/10.1002/jae.1230), the [L2-relaxation PDA](https://doi.org/10.13140/RG.2.2.11670.97609), [Principal Component Regression](https://doi.org/10.1080/01621459.2021.1928513), [Robust PCA Synthetic Control](https://academicworks.cuny.edu/gc_etds/4984), [Synthetic Control Method (Vanilla SCM)](https://doi.org/10.1198/jasa.2009.ap08746), [Two Step Synthetic Control](https://doi.org/10.1287/mnsc.2023.4878)  and finally the two newest methods which are not yet fully documented, [Proximal Inference-SCM](https://arxiv.org/abs/2108.13935) and [Proximal Inference with Surrogates-SCM](https://arxiv.org/abs/2308.09527)  

While each method has their own options (e.g., Bayesian or not, l2 relaxer versus L1), all methods have a common syntax which allows us to switch seamlessly between methods without needing to switch softwares or learn a new syntax for a different library/command. It also brings forth methods which either had no public documentation yet, or were written mostly for/in MATLAB.

The documentation that currently exists explains installation as well as the basic methodology of each method. I also provide worked examples from the academic literature to serve as a reference point for how one may use the code to estimate causal effects.

So, to anybody who uses Python and causal methods on a regular basis, this is an option that may suit your needs better than standard techniques.",23,7,1737052050.0,datascience
1i2qj4j,Books on Machine Learning + in R,"I'm interested in everyone's experience of books based specifically in R on machine learning, deep learning, and more recently LLM modelling, etc.  If you have particular experience to share it would really useful to hear about it.

As a sub-question it would be great to hear about books intended for relative beginners, by which I mean those familiar with R and statistical analysis but with no formal training in AI. There is obviously the well-known *""Introduction to Machine Learning with R""* by Scott V Burger, available as a [free pdf](https://edisciplinas.usp.br/pluginfile.php/8527271/mod_resource/content/0/Burger%2C%20Scott%20V%20-%20Introduction%20to%20machine%20learning%20with%20R_%20rigorous%20mathematical%20analysis-OReilly%20%282018%29.pdf).  But it hasn't been updated in nearly 7 years now, and a quick [scan of Google](https://www.google.co.uk/search?tbm=shop&hl=en-GB&psb=1&ved=2ahUKEwi6jeS9vPqKAxXdQ0ECHRvSDwAQu-kFegQIABAK&q=Machine+Learning+in+R&oq=Machine+Learning+in+R&gs_lp=Egtwcm9kdWN0cy1jYyIVTWFjaGluZSBMZWFybmluZyBpbiBSSABQAFgAcAB4AJABAJgBAKABAKoBALgBA8gBAJgCAKACAJgDAJIHAKAHAA&sclient=products-cc) shows quite a number of others.  Suggestions much appreciated.",28,18,1737038919.0,datascience
1i2jytl,Start freelancing with 0 experience?,"I hear many people have the ambition to start freelancing as soon as they can, ideally before having significant job experience. 
I like the attitude, but I tried myself a few years ago and got burned. So I wanna share my experience. 
   
I am a Data Scientist and tried to start freelancing with just one year job experience in 2017. Did the usual stuff. Set up an Upwork profile, applied to jobs at nights and during weekends and waited for a reply. 
Crickets. I **applied to 11 jobs** and didn't get any. Looking back at that experience I see a few mistakes
1 I didn't have a portfolio of projects that matched the jobs I applied to. 
2 I only used Upwork, without leveraging LInkedIn, Catalant, Fiverr and others. 
3 I gave up too early. Just 11 applications over one month is not enough. I recommend applying to 20-30 jobs per week if possible.
4 I set an unreasonable hourly rate. I set my hourly rate same as my daily job, Freelancing is a market where you are the product. When there is no demand for you (because nobody knows you) it's a smart move to set the price low. Once demand picks up, increase the price accordingly. 

Overall, I think experience is not the number one factor that a client looks for when hiring a freelancer. It's way more important to give the client confidence that you can do the job. So you should always work with that goal in mind, from the way you build your profile, to all the communication with your client. 
Last bit of advice. I found success in my local market at first. In Italy there is not many Data professionals that are also freelancers, and that helped me. People like to work with familiar faces and speaking the same language, sharing the same culture, goes a long way building confidence.

Curious to know your point of view too. ",49,32,1737013947.0,datascience
1i34tao,looking for arts sales data to understand arts pricing dynamics or madness,"I would like to explore datasets of arts sale and auctions, please if anyone has a good source please post below in the link. Just curious to explore if there are any patterns in art prices or just maddness which data science can't understand why a banana and tape would sell for 6 million or perhaps I can learn more about arts from this dataset. 

thanks in advance



Thanks ",0,5,1737076708.0,datascience
1i3bwdj,Can someone help me understand what is the issue exactly?,,0,2,1737102536.0,datascience
1i2mh17,Solution completeness and take home assignments for interviews?,"What is the general consensus about take home interviews and then completeness of solution.

I have around a week and it took me already 2 days just to work with with the data just so I can
1) clean it
2) enhance it with external data
3) feature engineer it
4) establish baselines to capture lift

The whole thing is supposed to be finished around the span of a week. As i was scoping it out the whole thing is essentially potentially 3-4 models in a framework given the complex nature of the work.

How critical is the completeness and assumptions being made regarding these take home assignments. I didnt get a take home that large in scope. Its difficult task but very doable just laborious in the sense that it requires to be well thought out. ",5,14,1737025307.0,datascience
1i28x7i,What do you think about building the pipeline first with bad models to start refining quickly?,"we have to build a computer vision application, I detect 4 main problems, 



get the highest quality training set, it is requiring lots of code and it may require lots of manual work to generate the ground truth

train a classification model, two main orthogonal approaches are being considered and will be tested

train a segmentation model

connect the dots and build the end to end pipeline

  
one teammate is working in the highest quality training set, and three other teammates in the classification models. I think it would be incredibly beneficial to have the pipeline as soon as possible integrated with the extremely simple models, and then iterate taking into account error metrics, as it gives us goals and this lets them test their module/section of the work also taking into account variation of the final metrics.

  
this would also help the other teams that depend on our output, web development can use a model, it is just a bad model, but we'll improve the results, the deployment work could also start now.

  
what do you guys think about this approach? for me it looks like its all benefits and zero problems but I see some teammates are reluctant on building something that definitely fails at the beginning and I'm not definitely the most experienced data scientist.",36,21,1736978111.0,datascience
1i1z6pj,Who is the most hungry for AI / ML talent right now,"I run a job search engine for Data Scientists. This week we added monitoring of the highest paid job openings in the last week. This is what I saw. It seems one company in particular wants to outbid everyone else. And this is not because of lack of competition - we monitor more than 30.000 companies including all of Fortune 100 and most of Fortune 1000. We index more than 60k data science jobs every month. 

Source: [jobs-in-data.com](https://jobs-in-data.com)

https://preview.redd.it/sqxgf9u786de1.png?width=2438&format=png&auto=webp&s=476af7f8ec1456a3d3f0e27f2fea61d4519daa9c

",127,37,1736953054.0,datascience
1i20otn,aspirations of starting a data science consultancy ,"Has anyone ever here thought of how to use their skills to start their own consultancy or some kind of business? Lately ive been kinda feeling that it would be really nice to have something of my own to work one involving analytics. Working for a company is great experience, but part of me would really like to have a business that I own where I help small businesses who have data make sense of it with low hanging fruit solutions.

Just a thought, but I’ve always thought of some sort of consultancy where clients are some sort of local business that collects data but doesn’t use it effectively or does not have the expertise on how to turn their data into insights that can be used. 

For example, suppose you had three clients:

1. Local gyms which have lots of membership data - my consultancy could offer services to measure engagement, etc and use demographic information to further understand gym goers - don’t know what “action” they could take but a thought 

2. Local shop has expenses they track and right now it’s all over the place. A dashboard that can help them view everything in one place

Something where, it’s tasks which are trivial for the average data scientist, but generate a lot of value for local businesses.

But maybe you can go deeper? I’m not sure how genAI works and haven’t played around with like any of these tools, but I’ve thought of ways these can be incorporated too.

Idk, I just find working in the industry sole draining and I just want to be able to have something that I can call my own, work on my own schedule, and it lead to a lot more revenue than working for a company. 

If anyone has any thoughts on what they have done, or how they have tried to do something, please let me know. Ideally I’d try and start this after 3-4 years of experience where I’ve built some niche industry experience. ",38,44,1736957009.0,datascience
1i275yh,WASM-powered codespaces for Python notebooks on GitHub,"During a hackweek, we built this project that allows you to run [marimo](https://github.com/marimo-team/marimo) and Jupyter notebooks directly from GitHub in a Wasm-powered, codespace-like environment. What makes this powerful is that we mount the GitHub repository's contents as a filesystem in the notebook, making it really easy to share notebooks with data.

**All you need to do is prepend** [`https://marimo.app`](https://marimo.app) **to any Python notebook on GitHub.** Some examples:

* Jupyter Notebook: [https://marimo.app/github.com/jakevdp/PythonDataScienceHandb...](https://marimo.app/github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)
* marimo notebook: [https://marimo.app/github.com/marimo-team/marimo/blob/07e8d1...](https://marimo.app/github.com/marimo-team/marimo/blob/07e8d14109f7312f19916fd13e4046a561a740f8/examples/third_party/polars/polars_example.py)

Jupyter notebooks are automatically converted into marimo notebooks using basic static analysis and source code transformations. Our conversion logic assumes the notebook was meant to be run top-down, which is usually but not always true \[2\]. It can convert many notebooks, but there are still some edge cases.

We implemented the filesystem mount using our own FUSE-like adapter that links the GitHub repository’s contents to the Python filesystem, leveraging Emscripten’s filesystem API. The file tree is loaded on startup to avoid waterfall requests when reading many directories deep, but loading the file contents is lazy. For example, when you write Python that looks like

    with open(""./data/cars.csv"") as f:
        print(f.read())
    
    # or
    
    import pandas as pd
    pd.read_csv(""./data/cars.csv"")

behind the scenes, you make a request \[3\] to *https://raw.githubusercontent.com/<org>/<repo>/main/data/cars.csv*

Docs: [https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github](https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github)

\[2\] [https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/)

\[3\] We technically proxy it through the playground [https://marimo.app](https://marimo.app) to fix CORS issues and GitHub rate-limiting.

**Why is this useful?**

Vieiwng notebooks on GitHub pages is limiting. They don't allow external css or scripts so charts and advanced widgets can fail. They also aren't itneractive so you can't tweek a value or pan/zoom a chart. It is also difficult to share your notebook with code - you either need to host it somehwere or embed it inside your notebook. Just append `https://marimo.app/<github_url>`",11,0,1736973521.0,datascience
1i29a6d,"Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?

","**Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?**

Hi everyone,

I’m looking to spark a discussion about **advanced imputation techniques** for datasets with multiple distinct but correlated time series. Imagine a dataset like **energy consumption** or **sales data**, where hundreds of stores or buildings are measured separately. The granularity might be hourly or daily, with varying levels of data completeness across the time series.

Here’s the challenge:

1. Some buildings/stores have **complete or nearly complete data** with only a few missing values. These are straightforward to impute using standard techniques.
2. Others have **partial data**, with gaps ranging from days to months.
3. Finally, there are buildings with **100% missing values** for the target variable across the entire time frame, leaving us reliant on correlated data and features.

The time series show **clear seasonal patterns** (weekly, annual) and dependencies on external factors like weather, customer counts, or building size. While these features are available for all buildings—including those with no target data—the features alone are insufficient to accurately predict the target. Correlations between the time series range from moderate (\~0.3) to very high (\~0.9), making the data situation highly heterogeneous.

# My Current Approach:

For stores/buildings with **few or no data points**, I’m considering an approach that involves:

1. **Using Correlated Stores**: Identify stores with high correlations based on available data (e.g., monthly aggregates). These could serve as a foundation for imputing the missing time series.
2. **Reconciling to Monthly Totals**: If we know the **monthly sums** of the target for stores with missing hourly/daily data, we could constrain the imputed time series to match these totals. For example, adjust the imputed hourly/daily values so that their sum equals the known monthly figure.
3. **Incorporating Known Features**: For stores with missing target data, use additional features (e.g., holidays, temperature, building size, or operational hours) to refine the imputed time series. For example, if a store was closed on a Monday due to repairs or a holiday, the imputation should respect this and redistribute values accordingly.

# Why Just Using Correlated Stores Isn’t Enough:

While using highly correlated stores for imputation seems like a natural approach, it has limitations. For instance:

* A store might be closed on certain days (e.g., repairs or holidays), resulting in zero or drastically reduced energy consumption. Simply copying or scaling values from correlated stores won’t account for this.
* The known features for the missing store (e.g., building size, operational hours, or customer counts) might differ significantly from those of the correlated stores, leading to biased imputations.
* Seasonal patterns (e.g., weekends vs. weekdays) may vary slightly between stores due to operational differences.

# Open Questions:

* **Feature Integration**: How can we better incorporate the available features of stores with 100% missing values into the imputation process while respecting known totals (e.g., monthly sums)?
* **Handling Correlation-Based Imputation**: Are there specific techniques or algorithms that work well for leveraging correlations between time series for imputation?
* **Practical Adjustments**: When reconciling imputed values to match known totals, what methods work best for redistributing values while preserving the overall seasonal and temporal patterns?

From my perspective, this approach seems sensible, but I’m curious about others' experiences with similar problems or opinions on why this might—or might not—work in practice. If you’ve dealt with imputation in datasets with heterogeneous time series and varying levels of completeness, I’d love to hear your insights!

Thanks in advance for your thoughts and ideas!

",9,3,1736979031.0,datascience
1i1wnxj,Leaving Public Sector for Private,"Posting for a friend:

Currently in a an ostensibly manager level DS position in local government. They are in the final stages of interviewing for a Director level role at a private firm. Is the compensation change worth it (posted below) and are there any DS specific aspects they should consider? 

Right now they are an IC who occasionally manages, but it seems this new role might be 80-90% managing. Is that common for the private sector? I told them it doesn't seem worth it (I'm biased as I am also in the public sector), but they said the compensation combined with more interesting work might be worth it.

Public Sector:
Manager
135k
Pension (secure but only okay payout)
Student Loan Forgiveness

Private Sector:
Director
165k
10-15% Bonus
401k 4% Match

",19,18,1736945403.0,datascience
1i2m3mv,What Challenges Do Businesses Face When Developing AI Solutions?,"Hello everyone,

I’m currently working on providing cloud services and looking to better understand the challenges businesses face when developing AI. As a cloud provider, I’m keen to learn about the real-world obstacles organizations encounter when scaling their AI solutions.

For those in the AI industry, what specific issues or limitations have you faced in terms of infrastructure, platform flexibility, or integration challenges? Are there any key challenges in AI development that remain unresolved? What specific support or solutions do AI developers need from cloud providers to overcome current limitations?

Looking forward to hearing your thoughts and learning from your experiences. Thanks in advance!",0,18,1737023704.0,datascience
1i1bjhi,E-values: A modern alternative to p-values,"In many modern applications - A/B testing, clinical trials, quality monitoring - we need to analyze data as it arrives. Traditional statistical tools weren't designed with this sequential analysis in mind, which has led to the development of new approaches.

E-values are one such tool, specifically designed for sequential testing. They provide a natural way to measure evidence that accumulates over time. An e-value of 20 represents 20-to-1 evidence against your null hypothesis - a direct and intuitive interpretation. They're particularly useful when you need to:

- Monitor results in real-time
- Add more samples to ongoing experiments
- Combine evidence from multiple analyses
- Make decisions based on continuous data streams

While p-values remain valuable for fixed-sample scenarios, e-values offer complementary strengths for sequential analysis. They're increasingly used in tech companies for A/B testing and in clinical trials for interim analyses.

If you work with sequential data or continuous monitoring, e-values might be a useful addition to your statistical toolkit. Happy to discuss specific applications or mathematical details in the comments.​​​​​​​​​​​​​​​​

P.S: Above was summarized by an LLM.

Paper: Hypothesis testing with e-values - https://arxiv.org/pdf/2410.23614

Current code libraries:

Python:

- expectation: New library implementing e-values, sequential testing and confidence sequences (https://github.com/jakorostami/expectation)

- confseq: Core library by Howard et al for confidence sequences and uniform bounds (https://github.com/gostevehoward/confseq)


R: 

- confseq: The original R implementation, same authors as above

- safestats: Core library by one of the researchers in this field of Statistics, Alexander Ly. (https://cran.r-project.org/web/packages/safestats/readme/README.html)

",106,63,1736876152.0,datascience
1i0x2pm,Fuck pandas!!! [Rant],"I have been a heavy R user for 9 years and absolutely love R. I can write love letters about the R data.table package. It is fast. It is efficient. it is beautiful. A coder’s dream.
 
But of course all good things must come to an end and given the steady decline of R users decided to switch to python to keep myself relevant.

And let me tell you I have never seen a stinking hot pile of mess than pandas. Everything is 10 layers of stupid? The syntax makes me scream!!!!!! There is no coherence or pattern ? Oh use [] here but no use ({}) here.
Want to do a if else ooops better download numpy. 
Want to filter ooops use loc and then iloc and write 10 lines of code.

It is unfortunate there is no getting rid of this unintuitive maddening, mess of a library, given that every interviewer out there expects it!!! There are much better libraries and it is time the pandas reign ends!!!!! (Python data table even creates pandas data frame faster than pandas!)

Thank you for coming to my Ted talk
I leave you with this datatable comparison article while I sob about learning pandas 

",476,329,1736825814.0,datascience
1i18xcv,Dash Python Incosistence Performance,"I'm currently working on a project using Dash Python. It was light and breezy in the beginning. I changed a few codes while maintaining the error at 0, test-running it once in a while just to check if the code change affected the website, and nothing bad happened. But after I left it for a few hours without changing anything, the website wouldn't run anymore and showed me an ""Internal Server Error"". This happened way too many times, and it stresses me out, as I have to update most of the backend ASAP. Does anyone has any similar experience and manage to solve it? I'd like to know how.",7,4,1736869506.0,datascience
1i13e03,Seeking Advice on Amazon Bedrock and Azure,"Hello everyone. I’m currently exploring AI infrastructure and platform for a new project and I’m trying to decide between Amazon Bedrock and Azure (AI Infrastructure & AI Studio). I’ve been considering both but would love to hear about your real-world experiences with them.

Has anyone used Amazon Bedrock or Azure AI Infrastructure and Azure AI Studio? How would you compare the two in terms of ease of use, performance, and overall flexibility? Are there specific features from either platform that stood out to you, or particular use cases where one was clearly better than the other?

Any advice or insights would be greatly appreciated. Thanks in advance! ",8,5,1736851459.0,datascience
1i0dbaj,Mastering The Poisson Distribution: Intuition and Foundations,,150,17,1736772966.0,datascience
1i03pk7,Where do you go to stay up to date on data analytics/science?,"Are there any people or organizations you follow on Youtube, Twitter, Medium, LinkedIn, or some other website/blog/podcast that you always tend to keep going back to? 

My previous career absolutely lacked all the professional ""content creators"" that data analytics have, so I was wondering what content you guys tend to consume, if any. Previously I'd go to two sources: one to stay up to date on semi-relevant news, and the other was a source that'd do high level summaries of interesting research papers. 

Really, the kind of stuff would be talking about new tools/products that might be of use, tips and tricks, some re-learning of knowledge you might have learned 10+ years ago, deep dives of random but pertinent topics, or someone that consistently puts out unique visualizations and how to recreate them. You can probably see what I'm getting at: sources for stellar information.",307,45,1736735034.0,datascience
1i1951j,exit cmd.exe from R (or python) without admin privilege,"I run:

system(""TASKKILL /F /IM cmd.exe"")

I get

Erreur�: le processus ""cmd.exe"" de PID 10333 n'a pas pu être arrêté.

Raison�: Accès denied.

Erreur�: le processus ""cmd.exe"" de PID 11444 n'a pas pu être arrêté.

Raison�: Accès denied.


I execute a batch file> a cmd open>a shiny open (I do my calculations)> a button on shiny should allow the cmd closing (and the shiny of course)

I can close the cmd from command line but I get access denied when I try to execute it from R. Is there hope? I am on the pc company so I don't have admin privilege",0,2,1736870071.0,datascience
1i0c3x8,Humana Senior DS Position merry-go-round,Anyone in the US apply to the Humana revolving Senior DS position over the last 5 months? They continuously post this position and never seem to fill it. Wondering if anyone has gotten an actual interview. I make it to the prescreen rounds  every single time I apply and then it just gets reposted.  ,25,8,1736768520.0,datascience
1i0m1ts,Advice on stabilizing an autoencoder's representation?,,3,1,1736795972.0,datascience
1i0wxxt,Mistral released Codestral 25.01 : Free to use with VS Code and Jet brains,,0,6,1736825403.0,datascience
1hzpcuv,"How we matured Fisher, our A/B testing library",,67,8,1736696534.0,datascience
1i06k3y,"Weekly Entering & Transitioning - Thread 13 Jan, 2025 - 20 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,42,1736744505.0,datascience
1i0czn6,Sky-T1-32B: Open-sourced reasoning model outperforms OpenAI-o1 on coding and maths benchmarks ,,0,0,1736771828.0,datascience
1i0bhi3,Seeking Advice on GPU Comparison: GreenNode vs FPT,"I’m currently exploring GPU options for my projects and I’m curious if anyone here has experience using GPUs from GreenNode or FPT. I’m looking for real feedback on how they compare in terms of performance, pricing, and overall experience.

Has anyone used GPUs from either of these providers? How do they stack up against each other in terms of power efficiency, speed, and reliability? Are there any specific use cases where one outperforms the other?

I’d love to hear your thoughts, personal experiences, or any suggestions you might have on which GPU might be better for intensive workloads. Thanks in advance!",0,1,1736765919.0,datascience
1hyploh,"200 applications - no response, please help. I have applied for data science (associate or mid-level) positions. Thank you ",,419,168,1736575525.0,datascience
1hyte5x,Feeling stuck in my career. Please help,"I'm in a weird position, where I feel like I'm stuck in my career. I really enjoy mathematics, ML/AI, implemented a lot of algorithms from scratch in C, developed new models for business purposes, presented at some internal/small conferences, and developed entire ML infrastructures for startups, but having no real opportunities to grow more.

At the moment I'm making over 100k$ working remotely from eastern Europe for a FAANG in the US (they have an office here, but my entire data science team is based in the US and I'm working on the same things as them).

When applying to companies in the US/UK I'm receiving zero callbacks (willing to relocate), although companies from the same areas are reaching out with remote offers of \~100k$/year. Those don't have the benefits of my current company, and are not attractive opportunities. I'm looking to relocate and get 200k$+. Current internal transfers to the US are closed, as they are looking to expand in east Europe. I've also asked for more difficult projects, but those are only available for US, not for my region.

The projects that are open to me at the moment offer zero satisfaction and I want to solve more complex problems and continue to expand my skills, but I'm stuck for the only thing that my studies are in eastern Europe and that I don't hold a PhD, even though I've already worked on novel models in industry, and speaking with friends and colleagues that hold a PhD, my skills are on par.

I'm at a point where I feel like skills and projects don't mean absolutely anything, and the only thing that has any weight for getting a job are diplomas and people you know... Maybe I'm exaggerating, but from all of my experiences I'm starting to feel like people from my region without studies abroad are seen only as cheap labor that should never be given the chance to work on real problems and be paid accordingly (a shitty company directly told me that, while another told me explicitly that my skills don't matter and they're only offering bad projects with bad pay in my region). It's like, there's a limit to the level of difficulty I can work on and the pay I can receive, regardless of how much I outcompete others...

At the moment, I'm working on a side research project that I'll be sending to some top tier conferences, and then try getting a PhD in the west... but that will take years, and if I already have the skills it's so frustrating to be stuck for so long just for a diploma and a title...

Or maybe my skills are really not on par, and I'm only good compared to the people in my region? Here's my resume if anyone would be willing to offer me some feedback.",61,35,1736592284.0,datascience
1hy7g0m,SQL Squid Game: Imagine you were a Data Scientist for Squid Games (9 Levels),,525,35,1736524298.0,datascience
1hyaw2t,How to communicate with investors?,"I'm working at a small scale startup and my CEO is always in talks with investors apparently. I'm currently working in different architectures for video classification as well as using large multimodal models to classify video. They want to show how no other model works on our own data (obviously) and how recent architectures are not as good as our own super secret model (videoMAE finetunned on our data...). I'm okay with faking results/showing results that cannot be compared fairly. I mean I'm not but if that's what they want to do then fine, doesn't really involve more work for me.

Now what pisses me off is that now I need to come up with a way to get an accuracy per class in a multilabel classification setting based solely on precision and recall per class because different models were evaluated by different people at different times and I really only have those 2 metrics per class - precision and recall. I don't even know if this is possible, it feels like it isn't, and is an overall dumb metric for our use case. All because investors only know the word ""accuracy""....

Would it not be enough to say: ""This is the F1 score for our most important classes, and as you can see, none of the other models or architectures we've tried are as good as our best model... By the way, if you don't know what F1 means, just know that higher scores are better. If you want, I can explain it in more detail..."" as opposed to getting metrics that do not make any sense...?

I will not present it to the investors, I only need to come up with a document, but wouldn't it be enough for the higher ups in my company to say what I said above in this scenario? ",15,12,1736532961.0,datascience
1hyxec6,Simple Full stack Agentic AI project to please your Business stakeholders,"Since you all refused to share how you are applying gen ai in the real world, I figured I would just share mine.

  
So here it is:  [https://adhoc-insights.takuonline.com/](https://adhoc-insights.takuonline.com/)  
There is a rate limiter, but we will see how it goes.



Tech Stack:

Frontend: Next.js, Tailwind, shadcn

Backend: Django (DRF), langgraph

LLM: Claude 3.5 Sonnet

I am still unsure if l should sell it as a tool for data analysts that makes them more productive or for quick and easy data analysis for business stakeholders to self-serve on low-impact metrics.

So what do you all think?",0,8,1736606856.0,datascience
1hxalxo,Companies are finally hiring,"I applied to 80+ jobs before the new year and got rejected or didn’t hear back from most of them. A few positions were a level or two lower than my currently level. I got only 1 interview and I did accept the offer. 

In the last week, 4 companies reached out for interviews. Just want to put this out there for those who are still looking. Keep going at it. 

Edit - thank you all for the congratulations and I’m sorry I can’t respond to DMs. Here are answers to some common questions. 

1. The technical coding challenge was only SQL. Frankly in my 8 years of analytics, none of my peers use Python regularly unless their role is to automate or data engineering. You’re better off mastering SQL by using leetcode and DataLemur

2. Interviews at all the FAANGs are similar. Call with HR rep, first round is with 1 person and might be technical. Then a final round with a bunch of individual interviews on the same day. Most of the questions will be STAR format. 

3. As for my skillsets, I advertise myself as someone who can build strategy, project manage, and can do deep dive analyses. I’m never going to compete against the recent grads and experts in ML/LLM/AI on technical skills, that’s just an endless grind to stay at the top. I would strongly recommend others to sharpen their soft skills. A video I watched recently is from The Diary of a CEO with Body Language Expert with Vanessa Edwards. I legit used a few tips during my interviews and I thought that helped ",1559,126,1736421422.0,datascience
1hxt0wl,How good are your linear algebra skills?,"Started my masters in computer science in August. Bachelors was in chemistry so I took up to diff eq but never a full linear algebra class. I’m still familiar with a lot of the concepts as they are used in higher level science classes, but in my machine learning class I’m kind of having to teach myself a decent bit as I go. Maybe it’s me over analyzing and wanting to know the deep concepts behind everything I learn, and I’m sure in the real world these pure mathematical ideas are rarely talked about, but I know having a strong understanding of core concepts of a field help you succeed in that field more naturally as it begins becoming second nature.

Should I lighten my course load to take a linear algebra class or do you think my basic understanding (although not knowing how basic that is) will likely be good enough?",81,41,1736472663.0,datascience
1hy8jhq,SAS - SQL question: inobs= vs outobs=,"Just a quick question here regarding PROC SQL in SAS.  Let's say I'm just writing some code and I want to test it.  Since the database I'm querying has over a million records, I don't want it to process my code for all the records.  

My understanding is that I would want to use the inobs= option to limit how much of the table is queried and processed on the server.  Is this correct?

The outobs= option will return however many records I set, but it process every record on the table in the server.  Is this correct?",5,3,1736527127.0,datascience
1hyhm2a,Is it necessary to understand the mathematics for data science anymore?,"The general consensus has been that you need to know the maths behind the models (proofs) in data science and that it’s advantageous to do so. But in this era of LLMs making our work even easier, and all the tools we use having already baked in the math behind the models for us, I wonder if this statement remains true or if it’s outdated advice. For example, in my limited experience of doing DS work, I’m personally yet to come across a situation where I was able to debug something because I knew the deep math proofs behind it (I did stats so know a decent amount of proofs). But I’m also very new to DS work so perhaps I’m missing something. 

Obviously understanding model output and what each of them means such as AUC, residuals, checking for drift etc remains important and will always do so.",0,20,1736550108.0,datascience
1hxxjz6,Microsoft's rStar-Math: 7B LLMs matches OpenAI o1's performance on maths,,4,0,1736487671.0,datascience
1hxplq8,Best resources for CO2 emissions modeling forecasting,"I'm looking for a good textbook or resource to learn about air emissions data modeling and forecasting using statistical methods and especially machine learning. Also, can you discuss your work in the field; id like tonlearn more.",9,14,1736462904.0,datascience
1hx305z,I was penalized in a DS interview for answering that I would use a Generalized Linear Model for an A/B test with an outcome of time on an app... But a linear model with a binary predictor is equivalent to a t-test. Has anyone had occasions where the interviewer was wrong?,"Hi,

I underwent a technical interview for a DS role at a company. The company was nice enough to provide feedback. This reason was not only reason I was rejected, but I wanted to share because it was very surprising to me. 

They said I aced the programming. However, hey gave me feedback that my statistics performance was mixed. I was surprised. The question was what type of model would I use for an A/B test with time spent on an app as an outcome. I suspect many would use a t-test but I believe that would be inappropriate since time is a skewed outcome, with only positive values, so a t-test would not fit the data well (i.e., Gaussian outcome). I suggested a log-normal or log-gamma generalized linear model instead.

  
I later received feedback that I was penalized for suggesting a linear model for the A/B test. However, a linear model with a binary predictor *is equivalent to a t-test*. I don't want to be arrogant or presumptuous that I think the interviewer is wrong and I am right, but I am struggling to have any other interpretation than the interviewer did not realize a linear model with a binary predictor is equivalent to a t-test.

Has anyone else had occasions in DS interviewers where the interviewer may have misunderstood or been wrong in their assessment?",269,123,1736391735.0,datascience
1hy9am1,Spreadsheet first cell debate ,"Settle this debate I'm having with a coworker. 

I say that spreadsheets should always start in row 1, column A. They say row 2, column B, [edit] so that there is an empty row and column before the table starts.

What's your take?",0,25,1736529004.0,datascience
1hxnq3t,Question on quasi-experimental approach for product feature change measurement,"I work in ecommerce analytics and my team runs dozens of traditional, ""clean"" online A/B tests each year. That said, I'm far from an expert in the domain - I'm still working through a part-time master's degree and I've only been doing experimentation (without any real training) for the last 2.5 years. 

One of my product partners wants to run a learning test to help with user flow optimization. But because of some engineering architecture limitations, we can't do a normal experiment. Here are some details:

* Desired outcome is to understand the impact of removing the (outdated) new user onboarding flow in our app. 
* Proposed approach is to release a new app version without the onboarding flow and compare certain engagement, purchase, and retention outcomes.
* ""Control"" group: users in the previous app version who did experience the new user flow
* ""Treatment"" group: users in the new app version who *would have* gotten the new user flow had it not been removed

One major thing throwing me off is how to handle the shifted time series; the 4 weeks of data I'll look at for each group will be different time periods. Another thing is the lack of randomization, but that can't be helped.

Given these parameters, curious what might be the best way to approach this type of ""test""? My initial thought was to use difference-in-difference but I don't think it applies given the specific lack of 'before' for each group. ",5,13,1736457945.0,datascience
1hxi5em,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,,6,2,1736443855.0,datascience
1hx286f,Am I underpaid/underemployed at $65k for a Data Analyst position in a MCOL city?,"I'm in a mcol city. I have a master's in Data Analytics that I finished in October 2024, and I've been working as a Data Analyst for 1.5 years. Before that, I was a study lead Clinical Data Manager for over a year (and before that I was a tax researcher and worked in HR). Currently, I make $65k base salary, but $85k total compensation. 

I keep getting interviews for Data Scientist positions that are well into the $100k+ base salary range, but I haven't landed an offer yet (it's really disheartening). Am I underpaid?

P.S. I'm open to job suggestions lol",67,58,1736389552.0,datascience
1hwmsd2,absolute path to image in shiny ui,"Hello,
Is there a way to get an image from an absolute path in shiny ui, I have my shiny app in a .R and I havn t created any R project or formal shiny app file so I don t want to use a relative paths
for now 
ui <- fluidPage(  tags$div( tags$img(src= absolute path to image).....
doesn t work",3,4,1736350202.0,datascience
1hvzskd,Change my mind: feature stores are needless complexity.,"I started last year at my second full-time data science role. The company I am at uses DBT extensively to transform data. And I mean very extensively. 

The last company I was at the data scientist did not use DBT or any sort of feature store. We just hit the raw data and write sql for our project.

The argument for our extensive feature store seems to be that it allows for reusability of complex logic across projects. And yes, this is occasionally true. But it is just as often true that there is a Table that is used for exactly one project. 

Now that I'm starting to get comfortable with the company, I'm starting to see the crack in all of this; complex tables built on top of complex tables built in to of complex tables built on raw data. Leakage and ambiguity everywhere. Onboarding is a beast. 

I understand there are times when it might be computationally important to pre-compute some calculation when doing real-time inference. But this is, in most cases, the exception, not the rule. Most models can be run on a schedule. 

TLDR; The amount of infrastructure, abstraction, and systems in place to make it so I don't have to copy and paste a few dozen lines of SQL is n or even close to a net positive. It's a huge drag.

Change my mind. ",112,47,1736278466.0,datascience
1hw5s76,As of 2025 which one would you install? Miniforge or Miniconda? ,"As the title says, which one would you install today if having a new computer for Data Science purposes. Miniforge or Miniconda and why?

For TensorFlow, PyTorch, etc.

Used to have both, but used Miniforge more since I got used to it (since 2021). But I am formatting my machine and would like to know what you guys think would be more relevant now.

I will try UV soon but want to install miniforge or miniconda at the moment.",40,77,1736293697.0,datascience
1hvwxzv,People who do DS/Analytics as freelancing any suggestions ,"Hi all

I've been in DS and aligned fields in corporate for 5+ years now. I'm thinking of trying DS freelance to earn additional income as well as learn whatever new things I can by doing more projects. I have few questions for people who have done it or tried it. 

Does it pay well? Do you do it fulltime or along with your job? Is it very difficult with a job?

What are some good platforms?

How do you get started? How much time does it take? How to get your first project? How to build your brand?

If you do it with your current job how much time does it take? Did you take permission from your manager about this?

Other than freelancing are there better options to make additional income?

Thanks!",77,34,1736271429.0,datascience
1hwcayh,CAG : Improved RAG framework using cache,,7,5,1736313595.0,datascience
1hvy3ld,Gradient boosting machine still running after 13 hours - should I terminate?,"I'm running a gradient boosting machine with the caret package in RStudio on a fairly large healthcare dataset, \~700k records, 600+ variables (most are sparse binary) predicting a binary outcome. It's running very slow on my work laptop, over 13 hours.

Given the dimensions of my data, was I too ambitious choosing hyperparameters of 5,000 iterations and a shrinkage parameter of .001? 

  
My code:  
\### Partition into Training and Testing data sets ###

set.seed(123)

inTrain <- createDataPartition(asd\_data2$K\_ASD\_char, p = .80, list = FALSE)

train <- asd\_data2\[ inTrain,\]

test  <- asd\_data2\[-inTrain,\]



\### Fitting Gradient Boosting Machine ###

set.seed(345)

gbmGrid <- expand.grid(interaction.depth=c(1,2,4), n.trees=5000, shrinkage=0.001, n.minobsinnode=c(5,10,15))

gbm\_fit\_brier\_2 <- train(as.factor(K\_ASD\_char) \~ .,

tuneGrid = gbmGrid,

data=train,

trControl=trainControl(method=""cv"", number=5, summaryFunction=BigSummary, classProbs=TRUE, savePredictions=TRUE),

train.fraction = 0.5,

method=""gbm"",

metric=""Brier"", maximize = FALSE,

preProcess=c(""center"",""scale""))

",24,46,1736274260.0,datascience
1huz6ax,This is how l stay up to date with the latest machine learning papers and technics ,"l go for the popular papers l hear about on Twitter and machine learning subreddits(Andrew Ng suggests these as great places to get the latest ml information). It won't cover everything, but it's okay and better to have some coverage than none - just because there are too many papers.

As for why l go for popular(by popular l mean a lot of technical/knowledgeable people are talking about them), well for certain things to be adopted they need some adoption, and l am sure there are great frameworks/architectures out there that just never got adopted and are not used a lot.

I will not write GPU kernels just so l can make this esoteric architecture, which l found on a paper somewhere,  work. Instead, I would use the popular transformer architecture, with lots of documentation and empirical evidence to support performance.

How about you all?",120,37,1736170682.0,datascience
1hvfuwa,What technology should I acquaint myself with next?,"Hey all. First, I'd like to thank everyone for your immense help on my last question. I'm a DS with about ten years experience and had been struggling with learning Python (I've managed to always work at R-shops, never needed it on the job and I'm profoundly lazy). With your suggestions, I've been putting in lots of time and think I'm solidly on the right path to being proficient after just a few days. Just need to keep hammering on different projects. 

At any rate, while hammering away at Python I figure it would be beneficial to try and acquaint myself with another technology so as to broaden my resume and the pool of applicable JDs. My criteria for deciding on what to go with is essentially: 

1. Has as broad of an appeal as possible, particularly for higher paying gigs
2. Isn't a total B to pick up and I can plausibly claim it as within my skillset within a month or two if I'm diligent about learning it

I was leaning towards some sort of big data technology like Spark but I'm curious what you fine folks think. Alternatively I could brush up on a visualization tool like Tableau.",12,23,1736213326.0,datascience
1hurpgg,data experience,,475,30,1736140819.0,datascience
1hv3gn4,Are Medium Articles helpful?,"I read almost every day something from Medium (I do write stuff myself too) though I kind of feel some of the articles even though highly rated are not properly written and to some extent loses its flow from the title to the content.

I want to know your thoughts and how have you found articles helpful on Medium or TDS.",24,44,1736182086.0,datascience
1huk9gq,What's your biggest time sink as a data scientist?,"I've got a few ideas for DS tooling I was thinking of taking on as a side project, so this is a bit of a market research post. I'm curious what data-scientist specific task/problem is the biggest time suck for you at work. I feel like we're often building a new class of software in companies and systems that were designed for web 2.0 (or even 1.0). ",185,98,1736118652.0,datascience
1hv5720,SWE + DS? Is learning both good,"I am doing a bachelor in DS but honestly i been doing full stack on the side (studying 4-5 hours per day and developing) and i think its way cooler.

Can i combine both? Will it give me better skills?",5,31,1736186285.0,datascience
1hvnkbl,"Tried Leetcode problems using DeepSeek-V3, solved 3/4 hard problems in 1st attempt",,0,2,1736241165.0,datascience
1hudtrj,Do you prepare for interviews first or apply for jobs first?,"I’ve started looking for a new job and find myself in a bit of a dilemma that I’m hoping you might have some experience with. Every day, I come across roles that seem like a great fit, but I hesitate to apply because I feel like I’m not fully prepared for an interview. While I know there’s no guarantee I’ll even get an interview, I worry about wasting an opportunity if I’m not ready.

On the other hand, preparing for an interview when you have one lined up seems like the most effective approach, but I’m not sure how to balance it all.

How do you usually handle this?",189,44,1736102433.0,datascience
1huz0m1,Meta's Large Concept Models (LCMs) : LLMs to output concepts ,,4,0,1736170208.0,datascience
1huloe0,"How are these companies building video/image generation tools? From scratch, fine-tuning Llama, or something else?
","There’s an enormous amount of LLM-based tools popping up lately, especially in video/image generation, each tied to a different company. Meanwhile, we only see a handful of really good open-source LLM models available.

So, my question is: How are these companies creating their video/image/avatar-generation tools? Are they building these models entirely from scratch, or are they leveraging existing LLMs like Llama, GPT, or something else?

If they are leveraging a model, are they simply using an API to interact with it, or are they actually fine-tuning those models with new data these companies collected for their specific use case?

If you’re guessing the answer, please let me know you’re guessing, as I’d like to hear from those with first-hand experience as well.

Here are some companies I’m referring to:

* **Video/image generation**:
   * [heygen.com](https://heygen.com)
   * [invideo.io](https://invideo.io)
   * [character.ai](https://character.ai)
   * [kindroid.ai](https://kindroid.ai)
   * [runwayml.com](https://runwayml.com)",19,2,1736122444.0,datascience
1hvk25m,Best LLMs to use ,"So I tried to compile a list of top LLMs (according to me) in different categories like ""Best Open-sourced"", ""Best Coder"", ""Best Audio Cloning"", etc. Check out the full list and the reasons here : https://youtu.be/K_AwlH5iMa0?si=gBcy2a1E3e6CHYCS",0,3,1736226461.0,datascience
1hurdd1,"Weekly Entering & Transitioning - Thread 06 Jan, 2025 - 13 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,86,1736139681.0,datascience
1huoyaf,What schema or data model are you using for your LLM / RAG prototyping?,"How are you organizing your data for your RAG applications? I've searched all over and have found tons of tutorials about how the tech stack works, but very little about how the data is actually stored. I don't want to just create an application that can give an answer, I want something I can use to evaluate my progress as I improve my prompts and retrievals.

This is the kind of stuff that I think needs to be stored:

* Prompt templates (i.e., versioning my prompts)
* Final inputs to and outputs from the LLM provider (and associated metadata)
* Chunks of all my documents to be used in RAG
* The chunks that were retrieved for a given prompt, so that I can evaluate the performance of the retrieval step
* Conversations (or chains?) for when there might be multiple requests sent to an LLM for a given ""question""
* Experiments. This is for the purposes of evaluation. It would associate an experiment ID with a series of inputs/outputs for an evaluation set of questions.

I can't be the first person to hit this issue. I started off with a simple SQLite database with a handful of tables, and now that I'm going to be incorporating RAG into the application (and probably agentic stuff soon), I really want to leverage someone else's learning so I don't rediscover all the same mistakes.",9,5,1736131935.0,datascience
1hu5gha,Announcing Plotlars 0.8.0: Expanding Horizons with New Plot Types! 🦀✨📊,"Hello Data Scientists!

I’m thrilled to announce the release of **Plotlars 0.8.0** — our latest step towards making **data visualization in Rust** more powerful, accessible, and versatile.

With this release, we’ve introduced **four new plot types**, unlocking exciting ways to represent your data visually. Whether you’re working with images, geographical datasets, or matrix data, Plotlars has you covered!

🚀 **New Features in Plotlars 0.8.0**

* 🖼️ Image Plot Support: Visualize **raster data** effortlessly with our new Image plot. Perfect for embedding and displaying image-based datasets directly in your plots.
* 🥧 PieChart Support: Represent **categorical data** using elegant and customizable pie charts. Ideal for showing proportions and category breakdowns.
* 🎨 Array2DPlot for RGB Data: Introducing Array2DPlot for **2D array visualization** using **RGB color values**. Excellent for displaying pixel grids, image previews, or matrix-based visualizations.
* 🌍 ScatterMap for Geographical Data: Plot your **geographical data points** interactively on maps with ScatterMap. Perfect for visualizing cities, sensor locations, or any spatial data.

🌟 **A Big Thank You to Our Supporters!**

Plotlars is nearing an incredible **300 stars on GitHub**. Your support, feedback, and enthusiasm have been instrumental in driving this project forward. If you haven’t already, please consider **leaving a star ⭐️ on GitHub** — it’s a small gesture that means a lot and helps others discover Plotlars.

🔗 Explore More:

📚 [Documentation](https://docs.rs/plotlars/0.8.0)  
💻 [GitHub Repository](https://github.com/alceal/plotlars)

**If you love Plotlars, share it with your friends and colleagues! Let’s build a thriving ecosystem of data science tools in Rust together.**

Thank you all for your continued support, and as always — **happy plotting!** 🎉📊

https://preview.redd.it/dhd0kxqfy5be1.png?width=984&format=png&auto=webp&s=c8059ef9038fada080b033f6d89a9765409fab5b

",34,1,1736077739.0,datascience
1hu86xb,Optimizing Advent of Code D9P2 with High-Performance Rust,,12,0,1736087336.0,datascience
1htlb8y,I don't like my current subfield of DS,"I have been in Data Science for 5 years and working as Senior Data Scientist for a big company.

In my DS journey most of my work are Applied Data Science where I was working on creating and training models, improving models and analysing features and make improvements so on (I worked on both ML, DL models) which I loved. 

Recently I have been moved to marketing data science where it feels like it is not appealing to me as I'm doing Product Data science with designing Experiment, analysing causal impact, Media mix modeling so on (also I'm somewhat not well experienced in Bayesian models or causal inference still learning). 

But in this field what I feel is you do buch of stuff to answer to business stakeholder in 1 or 2 slides and move on to next business question . Also even if you come up with something business always work based on traditional way with their past experience. I'm not feeling motivated and not seeing any of my solution is creating an impact.

Is this common with product data science/ causal inference world or I'm not seeing with correct picture?",89,21,1736016656.0,datascience
1htfjez,Whats the best resources to be better at EDA,"While I understand the math about ML, The one thing I lack is understanding and interpreting the data better.  
What resources could help me understand them?",86,27,1736001067.0,datascience
1htxzrg,Looking for some advice on my career path,,6,7,1736052777.0,datascience
1ht6ztm,I feel useless ,I’m an intern deploying models to google cloud. Everyday I work 9-10 hours debugging GCP crap that has little to no documentation. I feel like I work my ass off and have nothing to show for it because some weeks I make 0 progress because I’m stuck on a google cloud related issue. GCP support is useless and knows even less than me. Our own IT is super inefficient and takes weeks for me to get anything I need and that’s with me having to harass them. I feel like this work is above my pay grade. It’s so frustrating to give my manager the same updates every week and having to push back every deadline and blame it on GCP. I feel lazy sometimes because i’ll sleep in and start work at 10am but then work till 8-9pm to make up for it. I hate logging on to work now besides I know GCP is just going to crash my pipeline again with little to no explanation and documentation to help. Every time I debug a data engineering error I have to wait an hour for the pipeline to run so I just feel very inefficient. I feel like the company is wasting money hiring me. Is this normal when starting out? ,348,44,1735967040.0,datascience
1hti98t,Do you have any tips to keep up to date with all the ML implementations?,"I work as a data scientist, but sometimes i feel so left-behind in the field. do you guys have some tips to keep up to date with the latest breakthrough ML implementations?",36,13,1736008688.0,datascience
1htjd17,"Is there a similar career outperformance to-do list for a DS/DA, given some of the options/approaches aren’t available?",,11,7,1736011551.0,datascience
1htcyqo,How do you find data science internships?,"I am a high school student (grade 12) in a EU country, and if I do well on the national entrance exams, I'll get to the best university in the country which is in the top 200-250 for CS - according to QS. 

My experience with programming/data science is with Kaggle (for the last 2 years), having participated in 10+ competitions (1 bronze medal), and having \~4000 forks for my notebooks/codebases. 

Starting with university, how and when should I look for internships (preferably overseas because my country is lackluster when it comes to tech, let alone AI). Is there anything I can use to my advantage?

What did you guys do when you got your internships? Is it networking/nepotism that makes the difference?",16,16,1735992024.0,datascience
1ht2bbg,Moving to Germany,"Hi, I am a data scientist in Australia with about two years experience building ML models, doing data mining and predictive analysis for a big company. For personal reasons, I am moving to Munich at the end of the year, but am a bit worried about finding a data job abroad. 

I am wondering how difficult it might be to find a job in Germany, and what can I do to make myself competitive in an international market. What skillsets are in demand these days that I can learn and market?

Any advice would be greatly appreciated! ",34,55,1735952305.0,datascience
1hsxfrd,Data Science Job Market in UK vs. USA,"I've seen a worrying number of posts on social media over the past year describing how bad the job market is for recent computer science graduates, particularly in the US. Obviously there are differences between CS grads and those who pursue DS (though the general consensus (as far as I am aware) is that a CS could do a data scientist role but not vice versa).

Firstly, why do you think this is occurring? I've seen a lot of people mention the H-1B visa is a key issue surrounding this though I personally haven't a clue.

Secondly, is there a vast difference in the UK and USA job markets surrounding data science roles and is the market just as bad in the UK as it is in the USA?

Thirdly, are these CS graduates who are unable to get tech jobs migrating to more DS-centred jobs? This will obviously saturate the DS job market significantly.

Finally, as someone who is just starting to transition into the DS field, how worried should I be about job market saturation in the UK?",37,50,1735939373.0,datascience
1hsyiwl,Dicts vs classes: which do you tend to use?,"I’ve been thinking about the trade-offs between using plain Python dicts and more structured options like dataclasses or Pydantic’s BaseModel in my data science work.

On one hand, dicts are super flexible and easy to use, especially when dealing with JSON data or quick prototypes. On the other hand, dataclasses and BaseModels offer structure, type validation, and readability, which can make debugging and scaling more manageable.

I’m curious—what do you all use most often in your projects? Do you prefer the simplicity of dicts, or do you lean towards dataclasses/BaseModels for the added structure?

Would love to hear the community's thoughts!",30,15,1735942121.0,datascience
1hsv9ql,Professor looking for college basketball data similar to Kaggles March Madness,"The last 2 years we have had students enter the March Madness Kaggle comp and the data is amazing,  I even did it myself against the students and within my company (I'm an adjunct professor).  In preparation for this year I think it'd be cool to test with regular season games.  After web scraping and searching, Kenpom, NCAA website etc .. I cannot find anything as in depth as the Kaggle comp as far as just regular season stats, and matchup dataset. Any ideas?  Thanks in advance!",5,6,1735933833.0,datascience
1hsm94k,Data Scientist for Schools/ Chain of Schools,"Hi All,

I’m currently a data manager in a school but my job is mostly just MIS upkeep, data returns and using very basic built in analytics tools to view data. 

I am currently doing a MSc in Data Science and will probably be looking for a career step up upon completion but given the state of the market at the moment I am very aware that I need to be making the most of my current position and getting as much valuable experience as possible (my work are very flexible and they would support me by supplying any data I need). 

I have looked online and apparently there are jobs as data scientists within schools but there are so many prebuilt analytics tools and government performance measures for things like student progress that I am not sure there is any value in trying to build a tool that predicts student performance etc. 

Does anyone work as a data scientist in a school/ chain of schools? If so, what does your job usually entail? Does anyone have any suggestions on the type of project I can undertake, I have access to student performance data (and maybe financial data) across 4 secondary schools (and maybe 2/3 primary schools). 

I’m aware that I should probably be able to plan some projects that create value but I need some inspiration and for someone more experienced to help with whether this is actually viable. 

Thanks in advance. Sorry for the meandering post…",14,9,1735910239.0,datascience
1hslejn,How would you calculate whether to use Open Source LLM vs Vendors?,"Hi folks! I saw a lot of people online comenting on using DeepSeek instead of GPT4o and I was wondering how much are we saving by switching. 

Does anyone know a framework to estimate that?",10,6,1735907334.0,datascience
1hsn3e4,Why doesn't changepoint detection work the way I expect it to?,"I've been experimenting with changepoint detection packages and keep getting results that look like this:

  
[https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png)

  
If you look at 2024-05-26 in that picture, you'll what -- to me -- looks like an obvious changepoint. The line has been going down for a while and has suddenly started going up.

However, the model I'm using here is using the red and blue bands to show where it identified changepoints, and it's putting the changepoint just a little bit after the obvious one.

This particular visualization was made using the Ruptures package in Python, but I'm seeing pretty consistent results with every built-in changepoint model I can find. 

Does anyone know why these models, by default, aren't picking up significant changes in direction and how I need to update the calibration to change their behavior?",5,11,1735912872.0,datascience
1hsgfvp,Fine-Tuning ModernBERT for Classification ,,10,5,1735886939.0,datascience
1hrpb9q,How do you self-identify in this field and what is your justification?,"I've been in this field for many years, holding various titles, and connecting with peers who are unfathomably dissimilar in their roles, education, and skills, despite sharing titles.

I am curious to learn how folks view themselves and the various titles in this field. Assuming Data Science is the umbrella that encompasses computer science, machine learning, statistics, maths, etc., and there is a spectrum of roles within this field, how would you self-identify? The rules are:

1. It doesn't have to be your actual title from your employer or degree major.
2. It doesn't have to be a formally known identity. For example, you can identify as a ""number cruncher"", a ""tableau manager"", a ""deep learning developer"", make up your own, or just use a formal identity, such as ""Data Scientist"" or ""Machine Learning Engineer"".
3. You have to also add your justification. i.e. why do you believe such identity justly represents you/your role?
4. It should be self-explainable, technical, maturely and reasonably justified. So avoid the likes of ""Ninja"", ""Unicorn"", ""Guru"", unless you can maturely make a compelling argument.
5. You must be open to criticism and being challenged. Other redditors are not compelled to agree with your self-identity.

I'll also add my own response in the comments because I do not want it to be the center focus of the discussion. ",35,84,1735806871.0,datascience
1hr8ifj,What was your favorite work/project of 2024 and why was it personally fulfilling?,"I'm curious what the state of data science was in 2024, and what 2025 may bring, based on what data scientists prefer to be working on.

So let us know what project or type of work you most enjoyed last year that you may want to do more of in 2025 :)",104,70,1735754916.0,datascience
1hqx9a4,Finally got my NVIDIA Jetson Orin Nano SuperComputer (NVIDIA sponsored). What are some Data Science specific stuff I should try on it?,"So recently NVIDIA released Jetson Orin Nano, a Nano Supercomputer which is a powerful, affordable platform for developing generative AI models. It has up to 67 TOPS of AI performance, which is 1.7 times faster than its predecessor. 

Has anyone used it? My first time with an AI embedded system so what are some basic things to test on it? Already planned on Ollama and a few games like Crysis, doom, minecraft.",208,68,1735712582.0,datascience
1hqacf9,Duolingo for Data science and Machine learning ,"Edit: Thank you guys for all your recommendations. I really appreciate. Datacamp has exactly what I'm looking for. Brilliant is a close second. Thanks once again.


Is there an app like Duolingo for practicing data science and machine learning?
Solo learn and mimo are both for python and I was wondering if there are any apps like that but tailored for data science.
I installed some from playstore but it's just courses where I have to read things. I don't want to read things. I want to apply the technical coding aspects like in the mimo apps.

I know about kaggle and udemy but I'm looking for something like mimo.",165,77,1735633105.0,datascience
1hqkw4y,Any help for advanced numpy,"I am working on something where I need to process data using numpy. It's a tabular data and I need to convert it to multi dimensional arrays and then perform operations efficiently. 

Can anyone suggest some resources for advanced numpy so that I can understand and visualise numpy arrays, concept of axis, broadcasting etc.? I need to convert my data in such a way that I can do efficient operations on them. For that I need to understand multi dimensional numpy arrays and axis well enough. 
",27,29,1735669948.0,datascience
1hq0s6q,What would be the fastest way for me to get from novice to advanced level Python?,"I'm a data scientist with ten years experience. I've always worked at R shops and haven't been forced to learn Python on the job so my knowledge of the language is just from piddling around with it on my own and distinctly novice. If I was prepared to sink 5+ hours a day into it, what would be my best bet in terms of fastest way to hone my skills?",130,98,1735601417.0,datascience
1hpqjrk,How did you learn Git?,"What resources did you find most helpful when learning to use Git? 

I'm playing with it for a project right now by asking everything to ChatGPT, but still wanted to get a better understanding of it (especially how it's used in combination with GitHub to collaborate with other people).

I'm also reading at the same time the book Git Pocket Guide but it seems written in a foreign language lol",313,126,1735574950.0,datascience
1hp7pim,My Data Science Manifesto from a Self Taught Data Scientist,"**Background**

I’m a self-taught data scientist, with about 5 years of data analyst experience and now about 5 years as a Data Scientist. I’m more math minded than the average person, but I’m not special. I have a bachelor’s degree in mechanical engineering, and have worked alongside 6 data scientists, 4 of which have PHDs and the other 2 have a masters. Despite being probably, the 6th out of 7 in natural ability, I have been the 2nd most productive data scientist out of the group.


**Gatekeeping**

Every day someone on this subreddit asks some derivative of “what do I need to know to get started in ML/DS?” The answers are always smug and give some insane list of courses and topics one must master. As someone who’s been on both sides, this is attitude extremely annoying and rampart in the industry. I don’t think you can be bad at math and have no pre-requisite knowledge, and be successful, but the levels needed are greatly exaggerated. Most of the people telling you these things are just posturing due to insecurity.



As a mechanical engineering student, I had at least 3 calculus courses, a linear algebra course, and a probability course, but it was 10+ years before I attempted to become a DS, and I didn’t remember much at all. This sub, and others like it, made me think I had to be an expert in all these topics and many more to even think about trying to become a data scientist. 



When I started my journey, I would take coding, calculus, stats, linear algebra, etc. courses. I’d take a course, do OK in it, and move onto the next thing. However, eventually I’d get defeated because I realized I couldn’t remember much from the courses I took 3 months prior. It just felt like too much information for me to hold at a single time while working a full-time job. I never got started on actually solving problems because the internet and industry told me I needed to be an expert in all these things.


**What you actually need**

The reality is, 95% of the time you only need a basic understanding of these topics. Projects often require a deeper dive into something else, but that's a case by case basis, and you figure that out as you go.


For calculus, you don't need to know how to integrate multivariable functions by hand. You need to know that derivatives create a function that represents the slope of the original function, and that where the derivative = 0 is a local min/max. You need to know integrals are area under the curve.



For stats, you need to understand what a p value represents. You don't need to know all the different tests, and when to use them. You need to know that they exist and why you need them. When it's time to use one, just google it, and figure out which one best suits your use case.



For linear algebra, you don't need to know how to solve for eigenvectors by hand, or whatever other specific things you do in that class. You need to know how to ‘read’ it. It is also helpful to know properties of linear algebra. Like the cross product of 2 vectors yields a vector perpendicular to both.



For probability, you need to understand basic things, but again, just google your specific problem.



You don't need to be an expert software dev. You need to write ok code, and be able to use chatGPT to help you improve it little by little.



You don't need to know how to build all the algorithms by hand. A general understanding of how they work is enough in 95% of cases.



Of all of those things, the only thing you absolutely NEED to get started is basic coding ability. 



By far the number one technical ability needed to 'master' is understanding how to ""frame"" your problem, and how to test and evaluate and interpret performance. If you can ensure that you're accurately framing the problem and evaluating the model or alogithm, with metrics that correctly align with the use case, that's enough to start providing some real value. I often see people asking things like ""should I do this feature engineering technique for this problem?"" or “which of these algorithms will perform best?”. The answer should usually be, ""I don't know, try it, measure it, and see"". Understanding how the algorithms work can give you clues into what you should try, but at the end of the day, you should just try it and see.   



Despite the posturing in the industry, very few people are actually experts in all these domains. Some people are better at talking the talk than others, but at the end of the day, you WILL have to constantly research and learn on a project by project basis. That’s what makes it fun and interesting. As you gain PRACTICAL experience, you will grow, you will learn, you will improve beyond what you could've ever imagined. Just get the basics down and get started, don't spin your wheels trying and failing to nail all these disciplines before ever applying anything.



The reason I’m near the top in productivity while being near the bottom in natural and technical ability is my 5 years of experience as a data analyst at my company. During this time, I got really good at exploring my companies’ data. When you are stumped on problem, intelligently visualizing the data often reveals the solution. I’ve also had the luxury of analyzing our data from all different perspectives. I’d have assignments from marketing, product, tech support, customer service, software, firmware, and other technical teams. I understand the complete company better than the other data scientists. I’m also just aware of more ‘tips and tricks’ than anyone else.  



Good domain knowledge and data exploration skills with average technical skills will outperform good technical skills with average domain knowledge and data exploration almost every time. 


**Advice for those self taught**


I’ve been on the hiring side of things a few times now, and the market is certainly difficult. I think it would be very difficult for someone to online course and side project themselves directly into a DS job. The side project would have to be EXTREMELY impressive to be considered. However, I think my path is repeatable.



I taught myself basic SQL and Tableau and completed a few side projects. I accepted a job as a data analyst, in a medium sized (100-200 total employees) on a team where DS and DA shared the same boss. The barrier to DA is likely higher than it was ~10 years ago, but it's definitely something achievable. My advice would be to find roles that you have some sort of unique experience with, and tailor your resume to that connection. No connection is too small. For example, my DA role required working with a lot of accelerometer data. In my previous job as a test engineer, I sometimes helped set up accelerometers to record data from the tests. This experience barely helped me at all when actually on the job, but it helped my resume actually get looked at. For entry level jobs employers are looking for ANY connection, because most entry level resumes all look the same.


The first year or two I excelled at my role as a DA. I made my boss aware that I wanted to become a DS eventually. He started to make me a small part of some DS projects, running queries, building dashboards to track performance and things like that. I was also a part of some of the meetings, so I got some insight into how certain problems were approached. 



My boss made me aware that I would need to teach myself to code and machine learning. My role in the data science projects grew over time, but I was ultimately blocked from becoming a DS because I kept trying and failing to learn to code and the 25 areas of expertise reddit tells you that you need by taking MOOCs. 

  

Eventually, I paid up for DataQuest. I naively thought the course would teach me everything I needed to know. While you will not be proficient in anything DS upon completing, the interactive format made it easy to jump into 30-60 minutes of structured coding every day. Like a real language consistency is vital. 



Once I got to the point where I could do some basic coding, I began my own side project. THIS IS THE MOST IMPORTANT THING. ONCE YOU GET THE BASELINE KNOWLEDGE, JUST GET STARTED WORKING ON THINGS. This is where the real learning began. You'll screw things up, and that's ok. Titanic problem is fine for day 1, but you really need a project of your own. I picked a project that I was interested in and had a function that I would personally use (I'm on V3 of this project and it's grown to a level that I never could've dreamed of at the time). This was crucial in ensuring that I stuck with the project, and had real investment in doing it correctly. When I didn’t know how to do something in the project, I would research it and figure it out. This is how it works in the real world.



After 3 months of Dataquest and another 3 of a project (along with 4 years of being a data analyst) I convinced my boss to assign me DS project. I worked alongside another data scientist, but I owned the project, and they were mostly there for guidance, and coded some of the more complex things. I excelled at that project, and was promoted to data scientist, and began getting projects of my own, with less and less oversight. We have a very collaborative work environment, and the data scientists are truly out to help each other. We present our progress to each other often which allows us all to learn and improve. I have been promoted twice since I began DS work.



I'd like to add that you can almost certainly do all this in less time than it took me. I wasted a lot of time spinning my wheels. ChatGPT is also a great resource that could also increase your learning speed. Don't blindly use it, but it's a great resource.


**Tldr:** Sir this is Wendy’s.

**Edit:** I’m not saying to never go deeper into things, I’m literally always learning. I go deeper into things all the time. Often in very niche domains, but you don't need to be a master in all things get started or even excel. Be able to understand generalities of those domains, and dig deeper when the problem calls for it. Learning a concept when you have a direct application is much more likely to stick.


I thought it went without saying, but I’m not saying those things I listed are literally the only things you need to know about those topics, I was just giving examples of where relatively simple concepts were way more important than specifics.

**Edit #2:** I'm not saying schooling is bad. Yes obviously having a masters and/or PhD is better than not. I'm directing this to those who are working a full time job who want to break into the field, but taking years getting a masters while working full time and going another 50K into debt is unrealistic",1976,168,1735511285.0,datascience
1hoy3dm,recommend me the best statistics textbook for data science ,"I am intermediate level student who already studied stats , But i want to revisit it from DS and ML perspective  ",124,53,1735485620.0,datascience
1hpaa3c,Looking for some Senior DS Advice,"Hello everyone,

I think this is okay to be a post since it's not about entering/transitioning, but if I need to repost in the weekly threads please let me know! 

TLDR:

* I started working as a Data Scientist at a medium to large company almost 3 years ago.
* I spent the majority of my time doing more Software Engineering/Data Engineering related tasks with DS projects sprinkled in.
* A reorg changed the entire landscape of my company and potential growth at the company.
* I don't know what to do because I don't know if I got solid enough experience to leave for another DS job, but my current situation is very uncomfortable.
* Looking for any seasoned perspective/advice on the situation to help anchor me since I'm in a bit of a doom spiral. 



I am looking for some career advice. I don't want to write a novel about my journey to this point, but it was a hell of a lot of work. A snippet of my relevant work experience is I worked at various tech startups doing Data Analyst/Engineering work before I found my way to DS. I graduated with my MS in Data Science back in 2021, and I landed a job at a medium/large global business in the retail space. To my surprise, it was the common meme situation where they had no infrastructure put in place for DS work, and on top of that, a former IBM DS had built a Python ""application"" being used by an internal team that was barely hanging on.

**Year 1** 

My boss asked if I'd be able to modernize the application, and since I have a bit of a programming background, I told them I'd be happy to do that to get my feet wet with the org. I am going to way oversimplify the work I did for the sake of time. The important part is this project took around 6 months as the org had everything on-prem, so I had to go through approvals to get the more ""modern"" tech. I refactored a large portion of it, containerized it, and deployed it via an OpenShift (RedHat's Kubernetes product) cluster. The bulk of the program was a massive Jupyter Notebook (5000 lines of code with some custom-built math libraries) that an analyst would execute each cell after a request was made. This notebook housed all the business logic, so I just wrapped all that up to be executed automatically when the internal team interacted with the new app. By the end of it, I had a firm grasp on various business processes and was already talking to my boss about possibilities. Additionally, I found out that I was the only ""Data Scientist"" on staff, and I was a little bummed because I had chosen to work for a larger org in hopes of getting some sort of mentor/learn-by-osmosis going on. However, since my background is in startups I wasn't overly concerned because I knew I could utilize this environment to grow by trailblazing.

The conversation then shifted to the logic in the notebook, and the fact that no one really knew what was happening inside it. This notebook was driving a fairly important piece of the business by analyzing various datapoints, applying business rules, and spitting out results to be used day to day. They asked if I could dissect it, and I readily agreed – really wish LLMs were as commercialized as they are now. I spent the next 2-3 months working out bugs in the newly deployed app, and flow charting out all the business logic inside the notebook into nice Confluence pages. It was fairly spaghettified, so making changes to it was going to prove challenging. I put my ""Product Manager"" hat on and asked what their goals were with this application, the logic, measuring success, etc. I was asked to start a rewrite so that the laundry list of changes they had wanted to make could be done. It was also at this time my boss was super happy with the ideas/work I had done (I had several other smaller projects I did during this time), so they began speaking to me about being promoted up. How we'd get an actual software engineer on my team so I could focus on more of the ""Data Science"" stuff. I was super excited/anxious because I was hoping to get more hands-on DS experience before leading a team. However, once again, I come from startups so sort of par for the course.



**Year 2** 

The IT department announces a ""reorg"" a month before my promotion. By this point I had job descriptions for a few new positions, and we had made plans for who would be shifting to my team. All of this gets put on hold, and there's tons of uncertainty. I spend the next year doing the rewrite by myself. I build a few classification models in the process to help a few other internal teams operate more efficiently.

Basically they come through with a domain-driven design philosophy so that the Software teams can build more efficiently by having more autonomy. They establish practices across the domains, and they had a Data/ML practice initially. That gave me some confidence that I'd at least have ""peers"" when it was all said and done.



**Year 3 – Current year** 

I get moved into a domain, and they establish a separate BI & Analytics domain. They decentralized everything else but anything to do with ""Data Work"". I am given a promotion to DS Manager with a single employee – a Data Engineer. It has been super confusing all year with things taking much longer as the org adjusts for the new bureaucratic processes that have been introduced – tooling now has to be approved, Business analyst, delivery leads, PMO offices, etc. I meet with the head of engineering to ask how I go about getting tools approved (Sage Maker endpoints), and to get a sense of our overall data strategy. I'm basically told there isn't one in place, but they hope to get one together soonish. A lot has happened and it all feels very confusing. Basically no one is empowered to make decisions, the BI domain is leading the charge for their stuff, and me and my team are sort of this island that exists outside of everything else going on.



I tried to keep that as short as possible, and happy to give further detail if you believe it'd help.



**Here's my main issue:** I spent these years doing what needed to be done, but there really isn't a path of ""growth"" because they aren't really accounting for Data Scientists yet – though they say they hope to hire them. It was clear in the first year what the path would probably look like, but with everything becoming more corporate it feels like I could easily get shafted in one way or another. However, because I spent these years being the ""good employee"" and doing what needed to be done instead of what was best for my own experience I think it may be hard for me to get a DS job at another org. I'm hoping to get some perspective from all of you more seasoned professionals.",15,13,1735518438.0,datascience
1hpfkyr,"Weekly Entering & Transitioning - Thread 30 Dec, 2024 - 06 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",3,63,1735534882.0,datascience
1houdgh,What are some of the most interesting applied ml papers/blogs you read in 2024 or projects you worked on,"I am looking for some interesting successful/unsuccessful real-world machine learning applications. You are also free to share experiences building applications with machine learning that have actually had some real world impact.

Something of this type: 

1. LinkedIn has developed a new family of domain-adapted foundation models called Economic Opportunity Network (EON) to enhance their platform's AI capabilities.

https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform


Edit: Just to encourage this conversation here is my own personal SAAS app - this is how l have been applying machine learning in the real world as a machine learning engineer. It's not much, but it's something.
This is a side project(built during weekends and evenings) which flopped and has no users
[Clipbard](https://clipbard.com). I mostly keep it around to enhance my resume.
My main audience were educators would like to improve engagement with the younger 'tiktok' generation. I assumed this would be a better way of sharing things like history in a more memorable way as opposed to a wall of text. I also targeted groups like churches (Sunday school/ Children's church) who want to bring bible stories to life or tell stories with lessons or parents who want to bring bedtime stories to life every evening.


",55,20,1735472512.0,datascience
1hp65ll,"IYE, how does the computational infrastructure for AI models and their cost impact developers and users? Has your org ever bottlenecked development by cost to deploy the AI solution, either for you or in their pricing for clients?","I'm curious how the expense of AI factors into business. It seems like an individual could write code that impacts their cost of employment, and that LLM training algorithms and other AI work would be more expensive. 

 I'm wondering how businesses are governing the cost of a data scientist/software developer's choices with AI.",5,3,1735507199.0,datascience
1hp0cbx,Building Production-Ready AI Agents & LLM programs with DSPy: Tips and Code Snippets,,11,4,1735491867.0,datascience
1hoq8yb,ModernBERT vs BERT ,,11,2,1735454485.0,datascience
1hoc6n8,Seeking Collaborators to Develop Data Engineer and Data Scientist Paths on Data Science Hive,"Data Science Hive is a completely free platform built to help aspiring data professionals break into the field. We use 100% open resources, and there’s no sign-up required—just high-quality learning materials and a community that supports your growth.

Right now, the platform features a Data Analyst Learning Path that you can explore here:  https://www.datasciencehive.com/data_analyst_path 

It’s packed with modules on SQL, Python, data visualization, and inferential statistics - everything someone needs to get Data Science Hive is a completely free platform built to help aspiring data professionals break into the field. We use 100% open resources, and there’s no sign-up required—just high-quality learning materials and a community that supports your growth.

We also have an active Discord community where learners can connect, ask questions, and share advice. Join us here: https://discord.gg/gfjxuZNmN5

But this is just the beginning. I’m looking for serious collaborators to help take Data Science Hive to the next level.

Here’s How You Can Help:

	•	Share Your Story: Talk about your career path in data. Whether you’re an analyst, scientist, or engineer, your experience can inspire others.
	•	Build New Learning Paths: Help expand the site with new tracks like machine learning, data engineering, or other in-demand topics.
	•	Grow the Community: Help bring more people to the platform and grow our Discord to make it a hub for aspiring data professionals.

This is about creating something impactful for the data science community—an open, free platform that anyone can use.

Check out https://www.datasciencehive.com

explore the Data Analyst Path, and join our Discord to see what we’re building and get involved. Let’s collaborate and build the future of data education together!",63,27,1735411445.0,datascience
1ho91f8,Will the official Year End Salary thread be posted for 2024? ,I tried searching for it with the “salary” as the keyword. Usually that thread is up by now. Was just curious as I was looking for comparisons to my own salary. ,50,20,1735402904.0,datascience
1ho4com,Meta's Byte Latent Transformer: new LLM architecture (improved Transformer),Byte Latent Transformer is a new improvised Transformer architecture introduced by Meta which doesn't uses tokenization and can work on raw bytes directly. It introduces the concept of entropy based patches. Understand the full architecture and how it works with example here : https://youtu.be/iWmsYztkdSg,39,2,1735387439.0,datascience
1hnnwf5,Euchre Simulation and Winning Chances,"I tried posting this to r/euchre but it got removed immediately.

I’ve been working on a project that calculates the odds of winning a round of Euchre based on the hand you’re dealt. For example, I used the program to calculate this scenario:

If you in the first seat to the left of the dealer, a hand with the right and left bower, along with the three non-trump 9s wins results in a win 61% of the time. (Based on 1000 simulations)

For the euchre players here:

Would knowing the winning chances for specific hands change how you approach the game?
Could this kind of information improve strategy, or would it take away from the fun of figuring it out on the fly?
What other scenarios or patterns would you find valuable to analyze?
I’m excited about the potential applications of this, but I’d love to hear from any Euchre players. Do you think this kind of data would add to the game, or do you prefer to rely purely on instinct and experience? Here is the github link:

https://github.com/jamesterrell/Euchre_Calculator",26,4,1735330775.0,datascience
1hnl48d,Imputation Use Cases,"I’m wondering how and why people use this technique. I learned about it early on in my career and have avoided it entirely after trying it a few times. If people could provide examples of how they’ve used this in a real life situation it would be very helpful.

I personally think it’s highly problematic in nearly every situation for a variety of reasons. The most important reason for me is that nulls are often very meaningful. Also I think it introduces unnecessary bias into the data itself. So why and when do people use this?",29,52,1735323441.0,datascience
1hn0k9f,I analyzed you guys ,"In my quest for finding an internship and figuring what I want to do with my life workwise I decided to analyze how y'all feel about jobs in data science. One of the fields I am interested in is machine learning/data science so I decided to do a project that would help me see what other people think about this field. 

The project is available here: [Sentiment analysis part 1 | Ted’s cave](https://tedthecaver.github.io/2024/11/29/sentiment_analysis.html)

I would really appreciate any advice on the project itself if anyone bothers to read through it or on the problem of how I'm supposed to figure out what my passions are, and how do i commit to one thing (and how do i land an internship lol). 

Anyways I thought I would share with my dataset the project I did. Thanks y'all. ",146,43,1735254124.0,datascience
1hnlbhw,Pre/Post Implementation Analysis Interpretation,"I am using an interrupted time series to understand whether a certain implementation affected the behavior of the users. We can't do a proper A/B testing since we introduced the feature to all the users.

Lets say we were able to create a model and predict the post implementation daily usage to create the ""counterfactual"" which would be ""What would be the usage look like if there was no implementation?""

Since I have the actual post-implementation usage, now I can use it to find the cumulative difference/residual.

But my question is, since the model is trained on the pre-implementation data doesn't it make sense for the residual error to be high against the counter factual?

The data points in pre-implementation are mostly even across the lower and higher boundary and Its clear that there are more data points in the lower boundaries in the post-implementation but not sure how I would correctly test this. I want to understand the direction so was thinking about using MBE (Mean Bias Deviation)

Any thoughts?",2,3,1735323961.0,datascience
1hn1eqn,What's your 2025 resolution as a DS?,"As 2024 wraps up, it’s time to reflect and plan ahead. What’s your new year resolution as a data scientist? Are you aiming for a promotion, a pay bump, or a new job? Maybe you’re planning to dive into learning a new skill, step into a people manager role, or pivot to a different field.

Curious to hear what's on your radar for 2025 (of course coasting counts too).",82,119,1735256582.0,datascience
1hmuob8,Regression on multiple independent variable,"Hello everyone,

I've come across a use case that's got me stumped, and I'd like your opinion.

I have around 1 million pieces of data representing the profit of various projects over a period of time. Each project has its ID, its profits at the date, the date, and a few other independent variables such as the project manager, city, etc...

So I have projects over years, with monthly granularity. Several projects can be running simultaneously.

I'd like to be able to predict a project's performance at a specific date. (based on profits)

The problem I've encountered is that each project only lasts 1 year on average, which means we have 12 data points per project, so it's impossible to do LSTM per project. As far as I know, you can't generalise LSTM for a case like mine (similar periods of time for different projects).

How do you build a model that could generalise the prediction of the benefits of a project over its lifecycle?

What I've done for the moment is classic regression (xgboost, decision tree) with variables such as the age of the project (in months), the date, the benefits over M-1, M-6, M-12. I've chosen 1 or 0 as the target variable (positive or negative margin at the current month).

I'm afraid that regression won't be enough to capture more complex trends (lagged trend especially). Which kind of model would you advise me to go ? Am I on a good direction ?",30,17,1735238072.0,datascience
1hng96m,Puppy: organize your 2025 python projects,"# TLDR

[https://github.com/liquidcarbon/puppy](https://github.com/liquidcarbon/puppy) is a transparent wrapper around pixi and uv, with simple APIs and recipes for using them to help write reproducible, future-proof scripts and notebooks.

## From 0 to rich toolset in one command:

Start in an empty folder.

```
curl -fsSL ""https://pup-py-fetch.hf.space?python=3.12&pixi=jupyter&env1=duckdb,pandas"" | bash
```

installs python and dependencies, in complete isolation from any existing python on your system.  Mix and match URL [query params](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#one-installer-to-rule-them-all) to specify python version, tools, and venvs to create.

The above also installs puppy's CLI (`pup --help`):

## CLI - kind of like ""uv-lite""
- `pup add myenv pkg1 pkg2` (install packages to ""myenv"" folder using uv)
- `pup list` view what's installed across all projects
- `pup clone` and `pup sync` clone and build external repos (must have buildable `pyproject.toml` files)

## Pup as a Module - no more notebook kernels

The original motivation for writing puppy was to simplify handling kernels, but you might just not need them at all.  Activate/create/modify ""kernels"" interactively with:

```
import pup
pup.fetch(""myenv"")  # ""activate"" - packages in ""myenv"" are now importable
pup.fetch(""myenv"", ""pkg1"", ""pkg2"")  # ""install and activate"" - equivalent to `pup add myenv pkg1 pkg2`  
```

Of course you're welcome to use `!uv pip install`, but after 10 times it's liable to get messy.


## Target Audience

Loosely defining 2 personas:

1. Getting Started with Python (or herding folks who are):
   1. puppy is the easiest way to go from 0 to modern python - one-command installer that lets you specify python version, venvs to build, repos to clone - getting everyone from 0 to 1 in an easy and standardized way
   2. if you're confused about virtual environments and notebook kernels and install full jupyter into every project

2. Competent - check out [Multi-Puppy-Verse](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#multi-puppy-verse) and [Where Pixi Shines](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#where-pixi-shines-) sections:
   1. you have 10 work and hobby projects going at the same time and need a better way to organize them for packaging, deployment, or even to find stuff 6 months later
   2. you need support for conda and non-python stuff - you have many fast-moving external and internal dependencies - check out `pup clone` and `pup sync` workflows and [dockerized examples](https://github.com/liquidcarbon/puppy/tree/main/examples)


## Filesystem is your friend

Puppy recommends a sensible folder structure where each outer folder houses one and only one python executable - in isolation from each other and any other python on your system. Pup is tied to a python executable that is installed by Pixi, along with project-level tools like Jupyter, conda packages, and non-python tools (NodeJS, make, etc.) Puppy commands work the same from anywhere within this folder.

The inner folders are git-ready projects, defined by pyproject.toml, with project-specific packages handled by uv.



```
# ├── puphome/  # python 3.12 lives here
# │   ├── public-project/
# │   │   ├── .git  # this folder may be a git repo (see pup clone)
# │   │   ├── .venv
# │   │   └── pyproject.toml
# │   ├── env2/
# │   │   ├── .venv/  # this one is in pre-git development
# │   │   └── pyproject.toml
# │   ├── pixi.toml
# │   └── pup.py
# ├── pup311torch/  # python 3.11 here
# │   ├── env3/
# │   ├── env4/
# │   ├── pixi.toml
# │   └── pup.py
# └── pup313beta/  # 3.13 here
#     ├── env5/
#     ├── pixi.toml
#     └── pup.py
```

Puppy embraces ""explicit is better than implicit"" from the Zen of python; it logs what it's doing, with absolute paths, so that you always know where you are and how you got there.



PS I've benefited a great deal from the many people's OSS work - now trying to pay it forward. The ideas laid out in puppy's README and implementation have come together after many years of working in different orgs, where average ""how do you rate yourself in python"" ranged from zero (Excel 4ever) to highly sophisticated. The matter of ""how do we build stuff"" is kind of never settled, and this is my take.

Thanks for checking this out! Suggestions and feedback are welcome!",0,12,1735310424.0,datascience
1hmdpjm,Non-technical job alternatives for former data scientist,"Some context, I have a PhD in a hard science and I worked as a data scientist at a medical company for about 4 years and learned quite a bit and felt overall useful, from machine learning to stats, reports, dashboards and python writing. I have good social and communication skills as well, though they were not needed at my position as data scientist.

However, I felt like the amount of work and the nature of work just wasn't a match for me, it felt like manual labour, except with my brain. Constant and never ending work and problem solving -- no where near as difficult as the graduate work but much more abundant and relentless. At some point I guess you could say burnout occurred. I don't mind problem solving and writing code, but at a human pace, with intellectual freedom. Has anyone been in my situation? What sort of jobs aside from management did you transition to? If anyone knows of any specific roles or advice please do share. I would be happy to provide more context if necessary. 

Thank you!",124,59,1735176258.0,datascience
1hmrwcw,DeepSeek-v3 looks the best open-sourced LLM released,,5,0,1735230550.0,datascience
1hm7es6,Updated with 250+ Questions - DS Questions,"Hi everyone, 

Just wanted to give a heads up we updated our list of data science interview questions to now have almost 250 questions for you guys to try out and access for yourselves. Again with a free plan you can access most of the content on the site.

Hope this helps you guys in your interview prep - merry christmas.

[https://www.dsquestions.com/problems](https://www.dsquestions.com/problems)",13,11,1735155586.0,datascience
1hlz38v,Where can I find real-world ML/DS experience? Volunteering works too!,"Hey everyone,

So, I’m trying to get some hands-on experience in machine learning and data science—not just the “do more projects” advice (I’ve already done a bunch), but actual real-world stuff where I can work on meaningful problems. Paid or unpaid, doesn’t really matter to me—I’d even love to volunteer if it means I get to learn and grow.

I recently applied for an Omdena project, and I’m wondering if anyone here has done something with them? What’s it like? Did it actually help you gain valuable experience, or was it just another “group project” kind of thing?

Also, are there other platforms or places where I could jump into something similar? I’m trying to avoid the whole “chasing certifications” rabbit hole. I just want to get better at solving real problems, not stacking credentials.

Would love to hear your thoughts or any experiences you’ve had. Thanks in advance!

bit about me: I’m a 3rd-year undergrad in Computer Science with a minor in Statistics, and I just got an internship for a data role at a pretty big company. Super excited about it, but I want to keep building my skills and exploring different opportunities in ML/DS.",33,15,1735126029.0,datascience
1hm9he8,Am I cooked or is it this job market? ,,0,18,1735162224.0,datascience
1hlup8w,"LangChain In Your Pocket (Generative AI Book, Packt published) : Free Audiobook","Hi everyone,

It's been almost a year now since I published my debut book

>“LangChain In Your Pocket : Beginner’s Guide to Building Generative AI Applications using LLMs”

https://preview.redd.it/lgtj9570ix8e1.png?width=934&format=png&auto=webp&s=8b2a0e87914072d5125551adf830b731afcb293e

And what a journey it has been. The book saw major milestones becoming a **National and even International Bestseller in the AI category**. So to celebrate its success, I’ve released the Free Audiobook version of “LangChain In Your Pocket” making it accessible to all users free of cost. I hope this is useful. The book is currently rated at 4.6 on amazon India and 4.2 on amazon com, making it amongst the top-rated books on LangChain and is published by Packt as well

More details : [https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775](https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775)

# Table of Contents

* Introduction
* Hello World
* Different LangChain Modules
* Models & Prompts
* Chains
* Agents
* OutputParsers & Memory
* Callbacks
* RAG Framework & Vector Databases
* LangChain for NLP problems
* Handling LLM Hallucinations
* Evaluating LLMs
* Advanced Prompt Engineering
* Autonomous AI agents
* LangSmith & LangServe
* Additional Features

**Edit :** Unable to post direct link (maybe Reddit Guidelines), hence posted medium post with the link.",0,1,1735105238.0,datascience
1hk7uvs,"You Get a Dataset and Need to Find a ""Good"" Model Quickly (in Hours or Days), what's your strategy?","**Typical Scenario**: Your friend gives you a dataset and challenges you to beat their model's performance. They don't tell you what they did, but they provide a single CSV file and the performance metric to optimize.

Assumptions:
- Almost always tabular data, so no need learning needed.
- The dataset is typically small-ish (<100k rows, <100 columns), so it fits into memory.
- It's always some kind of classification/regression, sometimes time series forecasting.
- The data is generally ready for modeling (minimal cleaning needed).
- Single data metric to optimize (if they don't have one, I force them to pick one and only one).
- No additional data is available.
- You have 1-2 days to do your best.
- Maybe there's a hold out test set, or maybe you're optimizing repeated k-fold cross-validation.

I've been in this situation perhaps a few dozen times over the years. Typically it's friends of friends, typically it's a work prototype or a grad student project, sometimes it's paid work. Always I feel like my honor is on the line so I go hard and don't sleep for 2 days. Have you been there?

Here's how I typically approach it:

1. **Establish a Test Harness:** If there's a hold out test set, I do a train/test split sensitivity analysis and find a ratio that preserves data/performance distributions (high correlation, no statistical difference in means). If there's no holdout set, I ask them to evaluate their model (if they have one) using 3x10-fold cv and save the result. Sometimes I want to know their result, sometimes not. Having a target to beat is very motivating!
2. **Establish a Baseline:** Start with dummy models get a baseline performance. Anything above this has skill.
3. **Spot Checking:** Run a suite of all scikit-learn models with default configs and default ""sensible"" data prep pipelines.
	- Repeat with  asuite (grid) of standard configs for all models.
	- Spot check more advanced models in third party libs like GBM libs (xgboost, catboost, lightgbm), superlearner, imbalanced learn if needed, etc.
	- I want to know what the performance frontier looks like within a few hours and what looks good out of the box.
4. **Hyperparameter Tuning:** Focus on models that perform well and use grid search or Bayesian optimization for hyperparameter tuning. I setup background grid/random searches to run when I have nothing else going on. I'll try some bayes opt/some tpot/auto sklearn, etc. to see if anything interesting surfaces.
5. **Pipeline Optimization:** Experiment with data preprocessing and feature engineering pipelines. Sometimes you find that a lesser used transform for an unlikely model surfaces something interesting.
6. **Ensemble Methods:** Combine top-performing models using stacking/voting/averaging. I schedule this to run every 30 min and to try look for diverse models in the result set, ensemble them together and try and squeeze out some more performance.
7. **Iterate Until Time Runs Out:** Keep refining and experimenting based on the results. There should always be some kind of hyperparameter/pipeline/ensemble optimization running as background tasks. Foreground is for wild ideas I dream up. Perhaps a 50/50 split of cores, or 30/70 or 20/80 if I'm onto something and need more compute.

Not a ton of time for EDA/feature engineering. I might circle back after we have the performance frontier mapped and the optimizers are grinding. Things are calmer, I have ""something"" to show by then and can burn a few hours on creating clever features.

I dump all configs + results into an sqlite db and have a flask CRUD app that allows me to search/summarize the performance frontier. I don't use tools like mlflow and friends because they didn't really exist when I started doing this a decade ago. Maybe it's time to switch things up. Also, they don't do the ""continuous optimization"" thing I need as far as I know.

I re-hack my scripts for each project. They're a mess. Oh well. I often dream of turning this into an ""auto ml like service"", just to make my life easier in the future :)

What is (or would be) your strategy in this situation? How do you maximize results in such a short timeframe?

Would you do anything differently or in a different order?

Looking forward to hearing your thoughts and ideas!",210,65,1734901944.0,datascience
1hl9xdo,12 days of OpenAI summarized ,,0,5,1735031798.0,datascience
1hjy9nb,tHe wINdoWs mL EcOsYteM,,341,42,1734873281.0,datascience
1hk7fx3,Do data scientists do research and analysis of business problems? Or is that business analysis done by data analysts? What's the distinction?,"Are data scientists, scientists of data itself but not applied analysts producing business analysis for business leaders?

Put another way, are data scientists like drug dealers that don't get high on their own supply? So other people actually use the data to add value? And data scientists add value to the data so analysts can add value to the business with the data?

Where is the distinction? Can someone be both? At large companies does it matter?

I get paid to define and solve business problems with data. I like that advanced statistical business analysis since it feels like scientific discovery. I have an offer to work in a new AI shop at work, but fear that sort of 'data science' is for tool-builders, not researchers 
",28,52,1734900749.0,datascience
1hkgk35,"Weekly Entering & Transitioning - Thread 23 Dec, 2024 - 30 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,53,1734930081.0,datascience
1hk1ot7,ML pipeline questions,"I am building an application that processes videos and that needs to run many tasks (some need to be sequentially and some in parallel). Think audio extraction, ASR, diarization, translation, video classification, etc... Note that this is in supposed to be run online, i.e. this is supposed to be used in a web app where the user uploads a video and this pipeline I just described is run, the output is either stores in a bucket or a database and the results are shown after some time.

When I look up ""ML pipelines"" on goole I get stuff like kubeflow pipelines or vertex ai pipelines, so here is my first question:

1. Are these pipeline tools supposed to be run in production/online like in the use case I just described or are they meant to build ML pipelines for model training (preprocessing data, training a model and building a docker with the model weights, example) that are scheduled every so often? 

  
It feels like these tools are not what I want because they seem to be aimed at building models and not serving them.



After some googling I realized one good option would be to use Ray with Kubernetes. They allow for model composition and allow for node configuration for each task which is exactly what I was looking for, but my second question is: 

2. What else could I use for this task?

Plain kubernetes seems to be another option but more complex at setting up... it seems weird to me that there are no more tools for this purpose (multi model serving with different hardware requirements), unless I can do this with kubeflow or vertex ai pipelines",10,10,1734884282.0,datascience
1hjluem,"Statisticians, Scripts, and Chaos: My Journey Back to the 90s","We often hear a lot about how data science teams can lack statistical expertise and how this can lead to flawed analyses or misinterpretation of results. It’s a valid concern, and the dangers are real. But let me tell you, there’s another side of the coin that had me saying, “Holy bleep.”

This year, I joined a project where the team is dominated by statisticians and economists. Sounds like a data science dream team, right? Not so fast. It feels like I hopped into a time machine and landed in the 90s. Git? Never heard of it. Instead, we’ve got the old-school hierarchy of script_v1, script_final_version_1, script_final_version_2, all the way to script_final_version_n. It's a wild ride.

Code reviews? Absolutely nonexistent. Every script is its own handcrafted masterpiece, riddled with what I can only describe as ""surprise features"" in the preprocessing pipeline. Bugs aren’t bugs, apparently. “If you just pay close attention and read your code twice, you’ll see there’s no issue,” they tell me. Uh, sure. I don’t trust a single output right now because I know that behind every analysis bugs are having the party of their lives. 

Chances are, statisticians have absolutely no idea how a modern database actually works, have never heard of a non-basic data structure like a HyperLogLog, and have likely never wrestled with a truly messy real-world dataset.",177,53,1734823167.0,datascience
1hjrill,Genesis : Physics AI engine for generating 4D robotic simulations ,"One of the trending repos on GitHub for a week, genesis-world is a python package which can generate realistic 4D physics simulations (with no irregularities in any mechanism) given just a prompt. The early samples looks great and the package is open-sourced (except the GenAI part). Check more details here : https://youtu.be/hYjuwnRRhBk?si=i63XDcAlxXu-ZmTR",6,1,1734843022.0,datascience
1hjn3ad,"Data scientist interview(UK) coming soon,  any tips ?","Hi all, 

Final round interview coming up with a Major insurance company in the Uk. So basically they gave me an take-home assessment where I need to do some EDA and come up with an algorithm to predict mental health and also create presentation slides which I did and sent it to them and received an interview invite after, they also gave me some feedback acknowledging the assessment. 

So my questions are:

Tips for the interview on what to keep in mind and what major things should I keep in mind? 

They also told me to do a presentation on the slides I created keeping in mind the ‘Technical audiences and Non-Technical audiences’- Any tips for this will really help me 

Thank you to everyone for reading this post and for upcoming suggestions,

Yours loving Redditor 🫂",11,16,1734827165.0,datascience
1hjc9e0, Doctorate in quantitative marketing / marketing worth it? ,"I’ll be graduating with my MS stats in the spring and then working as a data scientist within the ad tech / retail / marketing space. My current Ms thesis, despite it being statistics (causal inference) focused it’s rooted in applications within business, and my advisors are stats/marketing folks in the business school.

After my first year of graduate school I immediately knew a PhD n statistics would not be for me. That degree is really for me not as interesting as I’m not obsessive about knowing the inner details and theory behind statistics and want to create more theory. I’m motivated towards applications in business, marketing, and “data science” settings. 

Topics of interest of mine have been how statistical methods have been used in the marketing space and its intersection with modern machine learning.

I decided that I’d take a job as a data scientist post graduation to build some experience and frankly make some money.

A few things I’ve thought about regarding my career trajectory:

1. Build a niche skillset as a data scientist within the industry within marketing/experimentation and try and get to a staff DS in FAANG experimentation type roles

- a lot of my masters thesis literature review was on topics like causal inference and online experimentation. These types of roles in industry would be something I’d like to work in 


2. After 3-4 yo experience in my current marketing DS role, go back to academia at a top tier business school and do a PhD in quantitative marketing or marketing with a focus on publishing research regarding statistical methods for marketing applications

- I’ve read through a lot of the research focus of a lot of different quant marketing PhD programs and they seem to align with my interests. My current Ms thesis in ways to estimate CATE functions and heterogenous treatment effect, and these are generally of interest in marketing PhD programs

- I’ve always thought working in an academic setting would give me more freedom to work on problems that interest me, rather than be limited to the scope of industry. If I were to go this route I’d try and make tenure at an R1 business school.


I’d like to hear your thoughts on both of these pathways, and weigh in on:

1. Which of these sounds better, given my goals?

2. Which is the most practical?

3. For anyone whose done a PhD in quantitative marketing and or PhD in marketing with an emphasis in quantitative methods, what that was like and if it’s worth doing especially if I got into a top business school. 




",29,17,1734795465.0,datascience
1hk0yh1,Saw this linkedin post - really think it explains the advances o3 has made well while also showing the room for improvement - check it out ,,0,0,1734882161.0,datascience
1hirjaq,"OpenAI o3 and o3-mini annouced, metrics are crazy",So OpenAI has released o3 and o3-mini which looks great on coding and mathematical tasks. The Arc AGI numbers looks crazy ! Checkout all the details summarized in this post : https://youtu.be/E4wbiMWG1tg?si=lCJLMxo1qWeKrX7c,146,59,1734722705.0,datascience
1hjx3q0,Is OpenAI o3 really AGI? ,,0,6,1734868576.0,datascience
1hisd3a,Advice on Analyzing Geospatial Soil Dataset — How to Connect Data for Better Insights?,"
Hi everyone! I’m working on analyzing a dataset (600,000 rows) containing geospatial and soil measurements collected along a stretch of land. 

The data includes the following fields:

Latitude & Longitude: Geospatial coordinates for each measurement.

Height: Elevation at the measurement point.

Slope: Slope of the land at the point.

Soil Height to Baseline: The difference in soil height relative to a baseline.

Repeated Measurements: Some locations have multiple measurements over time, allowing for variance analysis.

Currently, the data points seem disconnected (not linked by any obvious structure like a continuous line or relationships between points). My challenge is that I believe I need to connect or group this data in some way to perform more meaningful analyses, such as tracking changes over time or identifying spatial trend. 

Aside from my ideas, do you have any thoughts for how this could be a useful dataset? What analysis can be done? ",13,20,1734724937.0,datascience
1hjb4m7,Data Science Interview Prep,"Hi everyone,

My friend Marc and I broke into data science a while back and we 100% understand how hard the job market is. So, we've have been working on a interview prep platform for data science students that we'd enjoy using ourselves.

Right now we have \~200 questions including coding, probability, and statistics questions with most free to answer. We are adding new questions daily and want to grow a community where we can help one another out. [https://dsquestions.com/](https://dsquestions.com/)

All we need now is good feedback - I'd appreciate if you guys could check it out and give us some :)",0,22,1734792021.0,datascience
1hhv4iy,"Project: Hey, wait – is employee performance really Gaussian distributed??
A data scientist’s perspective ",,274,40,1734621459.0,datascience
1hi2gk9,Going back for a BS in Statistics,"Hi! I graduated from a Notre Dame with a BA in Psychology and a Supplementary Major in Statistics (more than a minor, less than a major). I only need 4 more classes to get a BS in Statistics because I did a lot of additional science reqs as pre-med. Does anyone know my options to either go back to school (undergrad) or transfer the credits to another school to get a double degree? I'm currently in a masters program (60%ish done) and working full-time as a DS in a dead-end role, but I'm having so much trouble getting any traction on job apps, and I always wondered if a BS would help.... Is this crazy?",50,44,1734640662.0,datascience
1hhmgvw,GotHub CoPilot gets a free tier for all devs,"GitHub CoPilot has now introduced a free tier with 2000 completions, 50 chat requests and access to models like Claude 3.5 Sonnet and GPT-4o. I just tried the free version and it has access to all the other premium features as well. Worth trying out : https://youtu.be/3oTPrzVTx3I",173,20,1734588609.0,datascience
1hheyol,"I built a free job board that uses ML to find you ML jobs
","**Link:** [**https://www.filtrjobs.com/**](https://www.filtrjobs.com/)

I tried 10+ job boards and was frustrated with irrelevant postings relying on keyword matching -- so i built my own for fun

I'm doing a semantic search with your jobs against embeddings of job postings prioritizing things like working on similar problems/domains

The job board fetches postings daily for ML and SWE roles in the US.

It's **100% free with no ads** for ever as my infra costs are $0

I've been through the job search and I know its so brutal, so feel free to DM and I'm happy to give advice on your job search

My resources to run for free:

* free 5GB postgres via [aiven.io](http://aiven.io/)
* free LLM from [galadriel.com](http://galadriel.com) (free 4M tokens of llama 70B a day)
* free hosting via heroku (24 months for free from [github student perks](https://www.heroku.com/github-students))
* free cerebras LLM parsing (using llama 3.3 70B which runs in half a second - 20x faster than gpt 4o mini)
* Using posthog and sentry for monitoring (both with generous free tiers)",375,88,1734564486.0,datascience
1hhs47t,Looking for Applied Examples or Learning Resources in Operations Research and Statistical Modeling ,"Hi all, 

I'm a working data scientist and I want to study Operations Research and Statistical Modeling, with a focus on chemical manufacturing. 

I’m looking for learning resources that include applied examples as part of the learning path. Alternatively, a simple, beginner-friendly use case (with a solution pathway) would work as well - I can always pick up the theory on my own (in fact, most of what I found was theory without any practice examples - or several months long courses with way too many other topics included).

I'm limited in the time I can spend, so each topic should fit into a half-day (max. 1 day) of learning. The goal here is not to become an expert but to get a foundational skill-level where I can confidently find and conduct use cases without too much external handholding. Upskilling for the future senior title, basically. 😄

Topics are:

 - Linear Programming (LP): e.g. Resource allocation, cost minimization.

 - Integer Programming (IP): e.g. Scheduling, batch production.

- Bayesian Statistics
    
- Monte Carlo Simulation: e.g. Risk and uncertainty analysis.
    
- Stochastic Optimization: Decision-making under uncertainty.
    
- Markov Decision Processes (MDPs): Sequential decision-making (e.g., maintenance strategies).

- Time Series Analysis: e.g. forecasting demand for chemical products.

- Game Theory: e.g. Pricing strategies, competitive dynamics.

Examples or datasets related to chemical production or operations are a plus, but not strictly necessary.

Thanks for any suggestions!",13,14,1734612364.0,datascience
1hhlqh4,Tips on where to access research papers otherwise locked behind paywalls? ,"For example, I want to read papers from IEEEE(eeeeeeeeeee....sorry I can't help it). But they're locked behind a paywall and $33 per paper for me to purchase since I don't have a university/alumni logon. 

I usually try to stick to open source/open access research for this reason but I'm on a really specific rabbit trail right now. Does anyone have any non-$$$$$ ideas for accessing research? ",45,28,1734585869.0,datascience
1hiefi0,"Google's reasoning LLM, Gemini2 Flash Thinking looks good",,0,2,1734678778.0,datascience
1hhqfds,stop script R but not shiny generation,"source ( script.R) in a shiny, I have a trycatch/stop in the script.R. the problem is the stop also prevent my shiny script to continue executing ( cuz I want to display error). how resolve this?
I have several trycatch in script.R",0,2,1734605794.0,datascience
1hgedpk,"a ""data scientist handbook"" for 2025 as a public Github repo","A while back, I created this public GitHub repo with links to resources (e.g. books, YouTube channels, communities, etc..) you can use to learn Data Science, navigate the markt and stay relevant.

Each category includes only 5 resources to ensure you get the most valuable ones without feeling overwhelmed by too many choices.

And I recently made updates in preparation for 2025 (including free resources to learn GenAI and SQL)

Here’s the link:

https://github.com/andresvourakis/data-scientist-handbook

Let me know if there’s anything else you’d like me to include (or make a PR). I’ll vet it and add it if its valuable.

I hope this helps 🙏",799,55,1734452807.0,datascience
1hguh6u,What's it like building models in the Fraud space? Is it a growing domain? ,"I'm interviewing for a Fraud DS role in a smaller bank that's in the F100. At each step of the process, they've mentioned that they're building a Fraud DS team and that there's a lot of opportunity in the space, but also that banks are being paralyzed by fraud losses.

I'm not too interested in classification models. But it pays more than what I currently make. I'm a little worried that there'll be a lot of compliance/MRM things compared to other industries - is that true?

Only reason why I'm hesitant is that I've been focusing on LLM work for a while and it doesn't seem like that's what the Fraud space does.

To sum it up:

1. Is there a ton of red tape/compliance/MRM work with Fraud models?
2. With an increase of Fraud losses every year, is this an area that'll be a hot commodity/good to get experience with?
3. Can you really do LLM work in this space? The VP I interviewed with said that the space was going to do GenAI in a few years, but when I asked him questions on what that meant to him, he had no clue but wanted to get into it
4. Is real-time data used to decline transactions instead of just detection?

EDIT: Definitely came to the conclusion that I want to apply to other banking companies. And that there's  a lot to learn in regards to 3 and 4. ",62,59,1734498672.0,datascience
1hh58jx,"Hiring Cybersecurity focused Data Science Experts - remote, part time",,7,10,1734539038.0,datascience
1hhm39j,I feel like I've peaked,,0,7,1734587171.0,datascience
1hgllx0,"Sales Forecasting for optimizing resource allocation (minimize waste, maximize sales)","Hi All,

To break up the monotony of ""muh job market bad"" (I sympathize don't worry), I wanted to get some input from people here about a problem we come across a lot where I work.  Curious what some advice would be.

So I work for a client that has lots of transactions of low value.  We have TONS of data going back more than a decade for the client and we've recenlty solved some major organizational challenges which means we can do some really interesting stuff with it.

They really want to improve their forecasting but one challenge I noted was that the data we would be training our algorithms on is affected by their attempts to control and optimize, which were often based on voodoo.  Their stock becomes waste pretty quickly if its not distributed properly.  So the data doesn't really reflect how much profit could have been made, because of the clients own attempts to optimize their profits.  Demand is being estimated poorly in other words so the actual sales are of questionable value for training if I were to just use mean squared error, median squared error, because just matching the dynamics of previous sales cycles does not actually optimize the problem.

I have a couple solutions to this and I want the communities opinion.

  
**1) Build a novel optimization algorithm that incorporates waste as a penalty.**    
I am wondering if this already exists somewhere, or 

  
**2) Smooth the data temporally enough and maximize on profit not sales.**

Rather than optimizing on sales daily, we could for instance predict week by week, this would be a more reasonable approach because stock has to be sent out on a particular day in anticipation of being sold.  

  
**3) Use reinforcement learning here, or generative adversarial networks.**

I was thinking of having a network trained to minimize waste, and another designed to maximize sales and have them ""compete"" in a game to find the best actions.  Minimizing waste would involve making it negative.

  
**4) Should I cluster the stores beforehand and train models to predict based on the subclusters, this could weed out bias in the data.**

I was considering that for store-level predictions it may be useful to have an unbiased sample.  This would mean training on data that has been down sampled or up-sampled to for certain outlet types 

  
**Lastly any advice on particular ML approaches would be helpful, was currently considering MAMBA for this as it seems to be fairly computationally efficient and highly accurate.  Explain ability is not really a concern for this task.**

**I look forward to your thoughts a criticism, please share resources (papers, videos, etc) that may be relevant.**

  
",17,28,1734471676.0,datascience
1hgozqm,Asking for help solving a work problem (population health industry),"Struggling with a problem at work. My company is a population health management company. Patients voluntarily enroll in the program through one of two channels. A variety of services and interventions are offered, including in-person specialist care, telehealth, drug prescribing, peer support, and housing assistance. Patients range from high-risk with complex medical and social needs, to lower risk with a specific social or medical need. Patient engagement varies greatly in terms of length, intensity, and type of interventions. Patients may interact with one or many care team staff members.

My goal is to identify what “works” to reduce major health outcomes (hospitalizations, drug overdoses, emergency dept visits, etc). I’m interested in identifying interventions and patient characteristics that tend to be linked with improved outcomes.

I have a sample of 1,000 patients who enrolled over a recent 6-month timeframe. For each patient, I have baseline risk scores (well-calibrated), interventions (binary), patient characteristics (demographics, diagnoses), prior healthcare utilization, care team members, and outcomes captured in the 6 months post-enrollment. Roughly 20-30% are generally considered high risk.

My current approach involves fitting a logistic regression model using baseline risk scores, enrollment channel, patient characteristics, and interventions as independent variables. My outcome is hospitalization (binary 0/1). I know that baseline risk and enrollment channel have significant influence on the outcome, so I’ve baked in many interaction terms involving these. My main effects and interaction effects are all over the map, showing little consistency and very few coefficients that indicate positive impact on risk reduction. 

I’m a bit outside of my comfort zone. Any suggestions on how to fine-tune my logistic regression model, or pursue a different approach?",4,6,1734481000.0,datascience
1hfxs76,Did working in data make you feel more relativistic?,"When I started working in data I feel like I viewed the world as something that could be explained, measured and predicted if you had enough data.

Now after some years I find myself seeing things a little bit different. You can tell different stories based on the same dataset, it just depends on how you look at it. Models can be accurate in different ways in the same context, depending on what you’re measuring.

Nowadays I find myself thinking that objectively is very hard, because most things are just very complex. Data is a tool that can be used in any amount of ways in the same context 

Does anyone else here feel the same?",317,96,1734394381.0,datascience
1hg1k3v,How do you stay up to date with new trends and advancements?,"Hi everyone! I'm getting my first big boy job soon (read: non internship) and one of my job duties is to stay updated in trends in data science and ML, especially with NLP and sentiment analysis in the social sciences

I'd like to do a good job with this and was wondering if anyone has recommendations for *how* to stay up to date. I will basically be the only technical person on my team so I'll need to be able to keep up with industry by myself without hand holding

Does anyone have any suggestions for keeping up to date with this sort of stuff? Besides following this sub and /r/MachineLearning ofc :p

Would love either blogs or journals with creative methodologies or usage of technology, both general DS stuff and places more focused on NLP. Thanks!",109,44,1734405900.0,datascience
1hgfl0t,exact line error trycatch,Is there a way to know line that caused error in trycatch? I have a long R script wrapped in trycatch,0,4,1734455876.0,datascience
1hfmope,Best ML certificate for undergrads to back up their profile?,"I’m an undergrad looking to strengthen my profile for ML internships/co-ops and overall career growth. I know some people might say certificates aren’t worth it, and yeah, I get it—experience and solid projects weigh more. But for those who think certs aren’t the best option, what would you suggest instead?

That said, I’m looking for something comprehensive and valued by employers. Between AWS ML Engineer Associate, ML Specialty, Databricks ML Associate/Professional, or Azure Data Scientist Associate, which one do you think is the most beneficial?

I’m not new to the field—just looking to expand my knowledge and improve my chances of landing a good ML co-op or internship. Any advice on where to learn ML more deeply or what certs actually help is much appreciated!",65,24,1734365802.0,datascience
1hf03dq,Data science is a luxury for almost all companies,"Let's face it, most of the data science project you work on only deliver small incremental improvements. Emphasis on the word ""most"", l don't mean all data science projects.
Increments of 3% - 7% are very common for data science projects.
I believe it's mostly useful for large companies who can benefit from those small increases, but small companies are better of with some very simple ""data science"". They are also better of investing in a website/software products which could create entire sources of income, rather than optimizing their current sources.
",843,211,1734290596.0,datascience
1hfjz1b,"Suggestion about Designing my Elective. Title: ""Text Analytics with LLM"" ","Hi Folks,
I'm a recent PhD graduate in Information Systems with a focus on using the current development in ML, NLP, NLU etc for business problems. I'm designing my first Text Analytics Elective for Management Scholars/Grad Students.

Objective is to given them some background and then help them focus on using the LLMs (open source ofcourse) to solve various type of problems.

I have already Includes 
- Vectorization : Comparing Text in Various Ways
- Concept & Design: Speed*, Coverage etc
- Building Scales: Measuring Emotion, Personality**, Nostalgia etc.


*Compare the Avg distance between consecutive embedding in a movie script or speech. Reference - https://psycnet.apa.org/record/2022-78257-001

**Scale Development with Little Data - https://journals.sagepub.com/doi/abs/10.1177/10944281231155771


It would be great if you guys can suggest some cool use of various text Analytics methods which are new (anything popular since 2020) or something you use often in solving business problems. Reference to a tool/paper would be great.

Would be glad to share the syllabus and resources when it's locked (Feb, 25')

",4,8,1734358398.0,datascience
1hfk7ah,Fine-tuning & synthetic data example: creating 9 fine tuned models from scratch in 18 minutes,"**TL;DR:** I built [Kiln](https://getkiln.ai), a new free tool that makes fine-tuning LLMs easy. In this example, I create 9 fine-tuned models (including Llama 3.x, Mixtral, and GPT-4o-mini) in just 18 minutes for less than $6 total cost. This is completely from scratch, and includes task definition, synthetic dataset generation, and model deployment.

The codebase is all on [GitHub](https://github.com/Kiln-AI/Kiln).

# Walkthrough

For the example I created 9 models in 18 minutes of work (not including waiting for training/data-gen). There's a walkthrough of each step in the [fine-tuning guide](https://github.com/Kiln-AI/Kiln/blob/main/guides/Fine%20Tuning%20LLM%20Models%20Guide.md), but the summary is:

* \[2 mins\]: Define task, goals, and schema
* \[9 mins\]: Synthetic data generation: create 920 high-quality examples using topic trees, large models, chain of thought, and interactive UI
* \[5 mins\]: dispatch 9 fine tuning jobs: Fireworks (Llama 3.2 1b/3b/11b, Llama 3.1 8b/70b, Mixtral 8x7b), OpenAI (GPT 4o-mini & 4o), and Unsloth (Llama 3.2 1b/3b)
* \[2 mins\]: deploy models and test they work

# Results

The result was small models that worked quite well, when the base models previously failed to produce the correct style and structure. The overall cost was less than $6 (excluding GPT 4o, which was $16, and probably wasn’t necessary). The smallest model (Llama 3.2 1B) is about 10x faster and 150x cheaper than the models we used during synthetic data generation. 

# Guide

I wrote a [detailed fine-tuning guide](https://github.com/Kiln-AI/Kiln/blob/main/guides/Fine%20Tuning%20LLM%20Models%20Guide.md), covering more details around deployment, running fully locally with Unsloth/Ollama, exporting to GGUF, data strategies, and next steps like evals.

# Feedback Please!

I’d love feedback on the tooling, UX and idea! And any suggestions for what to add next (RAG? More models? Images? Eval tools?). Feel free to DM if you have any questions.

I'm starting to work on the evals portion of the tool so if folks have requests I'm eager to hear it.

# Try it!

Kiln is 100% free, and the python library is MIT open source. You can [download Kiln here](https://github.com/Kiln-AI/Kiln/releases/latest)

",4,3,1734359045.0,datascience
1hf1180,What projects are you working on and what is the benefit of your efforts? ,"I would really like to hear what you guys are working on, challenges you’re facing and how your project is helping your company. Let’s hear it. ",85,97,1734293116.0,datascience
1hfzbit,I don’t understand AI hype. What am I missing?,"Edit 2: I need to try other models and practice my prompts. Thanks everyone!

Edit: I needed a script to parse a nested JSON file. I asked Chat GPT and it gave me a wrong answer. It only parsed the first layer. I asked a few more times and still no. I googled it and the first result from stack overflow was correct.


Not trolling. I've used ChatGPT about five times and was underwhelmed. What am I doing wrong?

1. Asked it for some simple code I couldn't remember. Nice but it only saved me about 10 minutes of googling.

2. Asked it for some moderately complex code and it didn't know the answer.

3. Asked it for some moderately complex code and the answer it gave was bad and wrong.

4. Asked it to generate an image and it was way off.

5. Asked it for some knowledge about an API and it just said the exact same thing as the official doc.


",0,38,1734398832.0,datascience
1hfbpe1,"Weekly Entering & Transitioning - Thread 16 Dec, 2024 - 23 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,43,1734325284.0,datascience
1hewiu9,What’s the point of testing machine learning model knowledge during interviews for non-research data science roles?,"I always make an effort to learn how a model works and how it differs from other similar models whenever I encounter a new model. So it felt natural to me that these topics were brought up in interviews.

However, someone recently asked me a question that I hadn’t given much thought to before: what’s the point of testing machine learning model knowledge during interviews for non-research data science roles?

Interview questions about model knowledge often include the following, especially if a candidate claims to have experience with these models:-

* what's the difference between bagging and boosting? 
* whether LightGBM uses leaf-wise splitting or level-wise splitting?
* what's the underlying assumptions of linear regression?

I learned these concepts because I’m genuinely interested in understanding how models work. But, coming back to the question: How important is it to have deep technical knowledge of machine learning models for someone who isn’t in a research position and primarily uses these tools to solve business problems?

From my experience, knowing how models learn from data has occasionally helped me identify issues during the model training process more quickly. But I couldn’t come up with a convincing argument to justify why it is fair to test this knowledge, other than “the candidate should know it if they are using it.”

What’s your experience with this topic? Do you think understanding the inner workings of machine learning models is critical enough to be tested during interviews?",37,30,1734281055.0,datascience
1hers8s,Visualization Process and Time Management,"At work I make many *exploratory* data visualizations that are fast, rough, and abundant. I want to develop a skill for *explanatory* visualizations that are polished, rich, and curated.

I've read a couple books on design principles and visualzation libraries (i.e. Seaborn and Matplotlib) and have some idea what I am after. But then I'll sit down to draft a paper with my outline and my hand-sketches, and I'll blow through my time budget just tweaking one of the charts!

I've learned a reliable process for writing, but I haven't mastered one for graphics. I'd love to hear what other people are doing. Some rudiments of a process:

- Start with cheap exploratory viz to find your story.
- Outline and revise your explanatory graphics by hand-- seems faster.
- Draft the ""data ink"" completely before tweaking aesthetics.
- Draft 80%-polished versions of graphs before the day you need them.
- Ruthlessly cut and consolidate graphics to the essentials.
- Forego graphics when narrative or tables are equally effective.
- Accept that a given chart typically takes X hours and plan accordingly.
- Practice, practice, practice so at least the tooling comes natural.
",37,9,1734266430.0,datascience
1hexjm9,Best domains for machine learning ?,"What are the best domains for expertise where I can use machine learning ? I don't want to use machine learning as it is I want a domain to use it, for eg: I have read about signal processing, healthcare, finance etc. 

 ",12,53,1734283772.0,datascience
1he2n61,Unexpectedly let go. Best ways to get a job fast?,"Hey all, 

I’m in Germany and was let go at the end of my probation period. 

I was ensured I would make it and actively made money for the company with proof. 

My reasons for termination were unclear and actually not inline with my responsibilities as a data scientist. 

Essentially, I was given peace of mind, and could ensure I needn’t worry. 

Whatever it may be, I’m now out of a job. That’s the way it goes sometimes. 

What are your tips for grabbing that next position fast? I’m not picky, I just want a job in my field, and with a team I enjoy - easier said than done. 

Any tips would be amazing! 

Happy holidays :) 

",96,56,1734182173.0,datascience
1he716d,Applying for Graduate Jobs in the UK.,"I recently graduated with an MSc in Artificial Intelligence in the UK and am currently looking for job opportunities. However, I often feel unsure about whether I’m approaching the job search process effectively. The journey can feel overwhelming and confusing at times, and I wonder if I’m targeting and applying for roles in the right way.

I am specifically targeting roles as a Machine Learning Engineer or Data Scientist. Could you share any proven strategies for job searching in the UK, particularly for these fields? Additionally, I’d like to know which months are crucial for job applications and when companies are most likely to hire graduates.",17,17,1734195181.0,datascience
1he6tyr,What are some things to consider if you wish to develop an experimentation platform? ,"Our company is quite small and we dont have a robust experimentation platform. Campaign measurement tasks are scattered all around the business with no unified set of standards. 6 different data scientists will bring you 6 different numbers of a lift measurement because nobody has a set way of doing things. 

A few of us are thinking of building out an experimentation platform to be a one stop shop for all things measurement. For those of you at places with mature experimentation culture, what kind of things should we consider? I’m a data scientist whose never worked as closely with engineers, but taking on this project is going to force me to do that, so I want to know more about an experimentation platform setup from that side as well. What has worked for you guys and what are things to recommend in building an experimentation platform?",8,6,1734194607.0,datascience
1hdd6yx,"0 based indexing vs 1 based indexing, preferences? ",,864,109,1734099552.0,datascience
1he7o50,plumber api or standalone app (.exe)?,I am thinking about a one click solution for my non coders team. We have one pc where they execute the code ( a shiny app). I can execute it with a command line. the .bat file didn t work we must have admin previleges for every execution. so I think of doing for them a standalone R app (.exe). or the plumber API. wich one is a better choice?,5,6,1734196963.0,datascience
1hdk59i,Help with clustering over time,"I'm dealing with a clustering over time issue.
Our company is a sort of PayPal. We are trying to implement an antifraud process to trigger alerts when a client makes excessive payments compared to its historical behavior.
To do so, I've come up with seven clustering features which are all 365-day-long moving averages of different KPIs (payment frequency, payment amount, etc.). So it goes without saying that, from one day to another, these indicators evolve very slowly. I have about 15k clients, several years of data.
I get rid of outliers (99-percentile of each date, basically) and put them in a cluster-0 by default.
Then, the idea is, for each date, to come up with 8 clusters. I've used a Gaussian Mixture clustering (GMM) but, weirdly enough, the clusters of my clients vary wildly from one day to another.
I have tried to plant the previous mean of my centroids, using the previous day centroid of a client to sort of seed the next day's clustering of a client, but the results still vary a lot. I've read a bit about DynamicC and it seemed like the way to address the issue, but it doesn't help.",9,36,1734118105.0,datascience
1hcw1o5,"Is it ethical to share examples of seed-hacking, p-hacking, test-set pruning, etc.?","I can't tell you the number of times I've been asked ""what random number seed should I use for my model"" and later discover that the questioner has grid searched it like a hyperparameter.

Or worse: grid searched the seed for the train/test split or CV folds that ""gives the best result"".

At best, the results are fragile and optimistically biased. At worst, they know what they're doing and it's intentional fraud. Especially when the project has real stakes/stakeholders.

I was chatting to a colleague about this last week and shared a few examples of ""random seed hacking"" and related ideas of test-set pruning, p-hacking, leader board hacking, train/test split ratio gaming, and so on.

He said I should write a tutorial or something, e.g. to educate managers/stakeholders/reviewers, etc. 

I put a few examples in a github repository (I called it ""[Machine Learning Mischief](https://github.com/Jason2Brownlee/MachineLearningMischief)"", because it feels naughty/playful) but now I'm thinking it reads more like a ""how-to-cheat instruction guide"" for students, rather than a ""how to spot garbage results"" for teachers/managers/etc.

What's the right answer here? 

Do I delete (make private) the repo or push it for wider consideration (e.g. expand as a handbook on how to spot rubbish ml/ds results)? Or perhaps no one cares because it's common knowledge and super obvious?",183,45,1734039253.0,datascience
1hcrjn2,How to Best Prepare for DS Python Interviews at FAANG/Big Companies?,"Have an interivew coming up where the focus will be on Stats, ML, and Modeling with Python at FAANG. I'm expecting that I need to know Pandas from front to back and basics of Python (Leetcode Easy). 

  
For those that have went through interviews like this, what was the structure and what types of questions do they usually ask in a live coding round for DS? What is the best way to prepare? What are we expected to know besides the fundamentals of Python and Stats?",174,43,1734027684.0,datascience
1hcy1cg,"How do you track your models while prototyping? Sharing Skore, your scikit-learn companion.","Hello everyone! 👋

In my work as a data scientist, I’ve often found it challenging to compare models and track them over time. This led me to contribute to a recent open-source library called [**Skore**](https://github.com/probabl-ai/skore), an initiative led by Probabl, a startup with a team comprising of many of the core scikit-learn maintainers.

Our goal is to help data scientists use scikit-learn more effectively, provide the necessary tooling to track metrics and models, and visualize them effectively. Right now, it mostly includes support for model validation. We plan to extend the features to more phases of the ML workflow, such as model analysis and selection.

I’m curious: how do you currently manage your workflow? More specifically, how do you track the evolution of metrics? Have you found something that worked well, or was missing?

If you’ve faced challenges like these, check out [the repo on GitHub](https://github.com/probabl-ai/skore) and give it a try. Also, please star our repo ⭐️ it really helps!

Looking forward to hearing your experiences and ideas—thanks for reading!",19,15,1734044644.0,datascience
1hcxv52,Masters in Applied Stats for an experienced analyst — good idea? Bad idea?,"I’m considering getting a master’s and would love to know what type of opportunities it would open up. I’ve been in the workforce for 12 years, including 5-7 years in growth marketing. 

Somewhere along the line, growth marketing became analyzing growth marketing and being the data/marketing tech guy at a series c company. I did the bootcamp thing. And now I’m a senior data analyst for a fortune 100 company. So: successfully went from marketing to analytics, but not data science.

I’m an expert in SQL, know tableau in and out, okay at Python, solid business presentation skills, and occasionally shoehorn a predictive model into a project. But yeah, it’s analytics.

But I’d like to work on harder, more interesting problems and, frankly, make more money as an IC. 

The master’s would go in depth on a lot of data science topics (multi variable regression, nlp, time series) and I could take comp sci classes as well. Possibly more in depth than I need.

Anyway, thoughts on what could arise from this?",17,42,1734044174.0,datascience
1hbtwbn,The Solitude of Data Science: Looking for a Kindred Spirits,"Hello!



I’ll try to keep this short because I’m terrible at being concise.



I came from a different world—operations and sales. It didn’t take long for me to realize that I wanted to move away from... well, salespeople. I applied for a dev job at my company and got rejected, but they saw potential in my knowledge and experience with machine learning, deep learning, and some other rogue projects I had been working on.



They asked if I could develop a proof of concept (POC) to present to our board of directors. The company had previously attempted to work with three external teams, but none of those efforts were successful. I presented the POC, and it went exceptionally well. We secured funding and created a junior data science position specifically for me. Previously, the company had no such role or anything similar. While the IT team is very strong, they haven’t had the capacity to handle initiatives like this.



Since then, I’ve been obsessed—reading everything I can and taking stats classes for a certificate program at MIT (with plans to continue my education). I’m pretty sure I’ve been driving my wife and friends crazy because I love talking about this stuff. I’m genuinely passionate about it!



That said, I still have so much to learn and need to overcome my imposter syndrome. On top of this fast-moving environment, I’ve never worked in IT before, never used Jira, or been involved in their overall processes, so I’m navigating that learning curve too. I’d love to connect with others here, hear your stories, and get more involved in this r/datascience community!         ",112,37,1733925068.0,datascience
1hcb1gc,Error rates /dirty data can cause sickness? ,"I do remember reading a long time ago that in production lines with high error-rates the motivation of labourers went down and the stress affected the workforce.

I wonder if dirty-data can have the same effect and has been researched as such. I know there are studies into error-rates in software, but that mixes software with data.

I wonder if specifically the stress caused by the unpredictability of the amount of work and the constant pressure dirty data causes has been studied as a health concern/risk.

Thanks.

Y.

edit: added the source [Unraveling Software Engineering Failures: Reasons and Fixes](https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/) [https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/](https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/) ",4,21,1733970805.0,datascience
1hc964o,CodeSignal companies,"Does anyone have a list of companies that have the codesignal data science assesement? 

Let's list the companies that did codesingal interviews so we can compile a list.

",4,12,1733965120.0,datascience
1hcuaog,Need help standard deviation,"
Hey guys I really need help I love statistics but I don’t know what the standard deviation is. I know I could probably google or chatgpt or open a basic book but I was hoping someone here could spoon feed me a series of statistics videos that are entertaining like Cocomelon or Bluey, something I can relate to.

Also I don’t really understand mean and how it is different from average, and a I’m nervous because I am in my first year of my masters in data science.

Thanks guys 🙏 ",0,18,1734034782.0,datascience
1hb7lcw,"I'm burnt out from constantly being on call where everything is on fire. Are there any good ""research"" or ""data collection"" or ""data interpretation"" roles that offer a more relaxed environment?","As a quick summary, I work as a Site Reliability Engineer and get paid pretty well (especially since I live in rural South Carolina and entirely remote). I juggle tasks like automating deployments, managing Kubernetes clusters in AWS, and scripting in Python and Bash, manage and analyze SQL databases, working with APIs, etc.  
  
**What I like**  
- I get paid well  & have skillsets that makes it more difficult for companies to replace you  
- I need to learn and stay up to date on a variety of technologies (I consider this a plus since you're never really 'out of date' on your role)     
- I enjoy makes graphs and gathering statistics/data to help our team  
- I enjoy interpreting that data to determine the root cause of an issue  
- In terms of scripting, I like making quick and dirty scripts that help my team automate something for us (this doesn't including writing large complicated scripts for other teams)   

    

**Why I hate it and want to leave**  
- The job, by its very nature, means everything is always urgent  
- On call, so a consistent 9-5 is not possible. You're often staying past your shift  
- Have to constantly work with devs and other parties to ensure their services or code gets fixed  
- Rarely any slow days, you're either automating a new large project or jumping on an urgent issue   
 
 
So based on the above, I'm curious if transitioning to a Data Science type role would offer a more laid-back environment, the question is I don't know what. Anyone made this switch or have insights? If not, can you recommend some jobs that I can look into? Preferably jobs that can utilize at least some of what I know.",163,57,1733852955.0,datascience
1hba8s2,Best cross-validation for imbalanced data?,"I'm working on a predictive model in the healthcare field for a relatively rare medical condition, about 5,000 cases in a dataset of 750,000 records, with 660 predictive features.

Given how imbalanced the outcome is, and the large number of variables, I was planning on doing a simple 50/50 train/test data split instead of 5 or 10-fold CV in order to compare the performance of different machine learning models.

Is that the best plan or are there better approaches? Thanks",76,48,1733859583.0,datascience
1hb2qbb,Hierarchical Time Series Forecasting,Anyone here done work for forecasting grouped time series? I checked out the hyndman book but looking for papers or other more technical resources to guide methodology. I’m curious about how you decided on the top down vs bottom up approach to reconciliation. I was originally building out a hierarchical model in STAN but wondering what others use in terms of software as well.,56,22,1733840218.0,datascience
1hc4971,get message markdow: execution ko or ok,"I am working with non developpers. I want them to enter parameters in markdown, execute a script then get the message at the end execution ok or ko on the knitted html ( they ll do it with command line)
I did error=T in the markdown so we ll alwyas get the document open. if I want to specify if execution ko or okay, I have to detect if theres at least a warning or error in my script? how to do that?",0,8,1733951613.0,datascience
1hc0ipg,Love this.,,0,0,1733942269.0,datascience
1haqdhq,Thoughts on the ethics of health insurance companies using Data Science to increase profits based on selective coverage ,"I want to have a good discussion on this topic since no one is talking about it outside of just the context of a CEO making decisions, but as a lot of us know, company decisions and strategy are driven by the suits(board) and the higher ups a lot of times, and that strategy is trickled down to the analysts and other groups forming projects to support the strategic initiative. I think not talking about this from a data science perspective is an ethics violation because we as practitioners can make the decision to not engage or pursue a project just because “I have a boss and they told me I need to because it aligns with our strategy.” I personally have quit a job in the past because the ethics of the CV models we were creating dawned on me and didn’t make me feel right. Sure I could validate it by saying I was only creating a small part of the software system, the reality is I knew the end goal and was actively participating in the development of a system that could be used for an ethically questionable use case. 

The possibility of UHCs actuarial science, analysts, and Data Scientists developing models to contribute to the strategy of increased profits and increased denials should be questioned. And I know “denial rates” aren’t apples to apples as back office rev cycle management people could wrongfully code a claim which can cause it to be denied. I’m talking more from a targeted perspective. Actuaries that work in insurance are very smart, but I want to get some insight about the specifics of what goes on from a health insurance perspective when they are denying a claim. 

I would love to hear perspectives from both sides, especially those who may have worked in the industry. ",268,122,1733794629.0,datascience
1hb0kqh,The pandas MemoryError,"I’ve been programming for data analysis for about 5 years, but I’ve never found an easy way to handle this. 

With my old beat up Dell Latitude, anything over ~100,000 rows if a sparse df tends to throw the dreaded Memory Error, specifically with functions like get dummies, indexing, merging, etc. 

My questions are:
1. Will a better laptop help with this?
2. Are there any modules or helper functions for this out there? 
3. How much does using colab help with this problem? Trying to avoid paying more.

TIA!


Edit: seems like most parallelizing options do not store the df in memory, and so can’t be used to visualize. That’s my main use case. So…
4. Anyone know of any visualization tools that work with large data? Currently using Plotly/Dash.",18,39,1733833468.0,datascience
1hb2048,Master Data science vs Quantitative Finance ,"Major data science vs Quantitative Finance

Hi, 
I am currently studying the bachelor Econometrics in The Netherlands and next year I will need to choose a master to pursue. My main doubt is, as you can see from the title, between data science (which is a bit outside my bachelor) and quantitative finance. 

On the one hand I may be a bit more interested in data science, but on the other hand I have the feeling that I will ‘throw away’ my Econometrics bachelor that is quite unique. From my point of view data science is followed by many people, also people from lower wage countries, while quantitative finance is a master that not many people follow. 

That’s why I’m curious what other people think about this, will I be going the wrong path if I choose data science which is pursued by many students overall, should I stick to the specific field of quantitative finance or will it not matter?",8,31,1733838131.0,datascience
1haneem,Is LeetCode or HackerRank actually worth it for ML/DS jobs?,"I’m an undergrad trying to break into Data Science/ML roles, and I’m not sure if spending time on LeetCode or HackerRank is really worth it. A lot of the problems feel more geared toward software dev interviews, and I’m wondering if that’s the best use of time for DS/ML jobs.

Wouldn’t working on projects or learning tools like TensorFlow or PyTorch be more valuable? Has anyone here actually benefited from doing LeetCode/HackerRank for DS/ML roles, or is it overhyped for this field?",107,57,1733786123.0,datascience
1ha78te,Thoughts? Please enlighten us with your thoughts on what this guy is saying. ,,911,197,1733742026.0,datascience
1hankc7,Real time predictions of custom models & aws,"I am someone who is trying to learn how to deploy machine learning models in real time. As of now the current pain points is that my team uses pmmls and java code to deploy models in production. The problem is that the team develops the code in python then rewrites it in java. I think its a lot of extra work and can get out of hand very quickly. 


My proposal is to try to make a docker container and then try to figure out how to deploy the scoring model with the python code for feature engineering.

We do have a java application that actually decisions on the models and want our solutions to be fast.

Where can i learn more about how to deploy this and what type of format do i need to deploy my models? I heard that json is better for security reasons but i am not sure how flexible it is as pmmls are pretty hard to work with when it comes to running the transformation from python pickle to pmmls for very niche modules/custom transformers. 

If someone can help explain exactly the workflow that would be very helpful. This is all going to use aws at the end to decision on it.",11,1,1733786574.0,datascience
1haeip6,How do you keep up with all the tools?,Plenty of tools are popping on a regular basis. How do you do to keep up with them? Do you test them all the time? do you have a specific team/person/part of your time dedicated to this? Do you listen to podcasts or watch specific youtube chanels?,35,16,1733763907.0,datascience
1haf05c,Customer Life Time Value Applications,"At work I’m developing models to estimate customer lifetime value for a subscription or one-off product. It actually works pretty well. Now, I have found plenty of information on the modeling itself, but not much on how businesses apply these insights. 

The models essentially say, “If nothing changes, here’s what your customers are worth.” I’d love to find examples or resources showing how companies actually use LTV predictions in production and how they turn the results into actionable value. Do you target different deciles of LTV with different campaigns? do you just use it for analytics purposes? ",29,19,1733765096.0,datascience
1hakhka,SUMO/VISSIM for traffic condition simulation,"Hi team!

As I have no experience with AI and predictive models for trafic management, I’m not sure how to simulate current traffic conditions in an urban city (or portion of it) without VS with implementation of IoT and AI.

Any good resources or advice?

Also, if anyone with first hand experience is interested, I would love to have a quick interview discussion, 15-20mins max, for qualitative analysis :)",4,2,1733778588.0,datascience
1haig7e,How can a webdev help DS?,"Hello y'all. My expertise is between DS and full stack dev, but usually its been one or the other. 

What would your ideas be on how I can leverage my webdev skills to collaborate with other DSs in my team?

Context is supply chain, and there's some reasonable freedom to initiate projects",3,6,1733773519.0,datascience
1hacmoc,entering parameters+executing R without accessing R,"I am preparing a script for my team  (shiny or rmarkdown) where they have to enter some parameters then execute it ( and have maybe executions steps shown). I don t want them to open R or access the script.
1) How can I do that?
2) is it dangerous security wise with a markdown knit to html? and with shiny is it safe? I don t know exactly what happens with the online, server thing?
3) is it okay to have a password passed in the parameters, I know about the Rprofile, but what are the risks?
thanks",5,4,1733759138.0,datascience
1han2ml,Low classification accuracy,"Hello
And when i do regression it gives me zero, whoever could help please contact me it’s so urgent 
",1,1,1733785231.0,datascience
1h9v7pe,Is your org treating the rollout of LLMs as an IT or data science problem? ,"Our org has given all resource (and limited all API access) to LLMs to a dedicated team in the IT department, which has no prior data experience. So far no data scientist has been engaged for feedback on design or practicality of use-cases. I'm wondering is this standard in other orgs? ",79,32,1733698440.0,datascience
1h9m492,Are certifications even worth it these days?,"So, I’m a cs major stats minor undergrad, and I’ve done a couple of certifications—AWS Cloud Practitioner and IBM Data Science. Honestly, I’m not sure if they added much value. In one interview, I mentioned my certifications right at the end, and they didn’t even seem to notice.

From what I’ve seen, well-defined projects seem to carry more weight than a cert. Projects show real skills, while certs sometimes feel like just ticking a box.

What’s your take? Are there any certs you’ve done that actually helped you stand out, or do you think the focus should shift more toward solid project work?

Also, which one is more valuable or more worth it, AWS, Azure, GCP or Databricks for Data Science/ML??",152,64,1733674230.0,datascience
1ha28aj,"Weekly Entering & Transitioning - Thread 09 Dec, 2024 - 16 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,67,1733720481.0,datascience
1h9o1r5,Timeseries pattern detection problem,"I've never dealt with any time series data - please help me understand if I'm reinventing the wheel or on the right track.

I'm building a little hobby app, which is a habit tracker of sorts. The idea is that it lets the user record things they've done, on a daily basis, like ""brush teeth"", ""walk the dog"", ""go for a run"", ""meet with friends"" etc, and then tracks the frequency of those and helps do certain things more or less often.

Now I want to add a feature that would suggest some cadence for each individual habit based on past data - e.g. ""2 times a day"", ""once a week"", ""every Tuesday and Thursday"", ""once a month"", etc.

My first thought here is to create some number of parametrized ""templates"" and then infer parameters and rank them via MLE, and suggest the top one(s).

Is this how that's commonly done? Is there a standard name for this, or even some standard method/implementation I could use?",13,5,1733679369.0,datascience
1h9f8hr,How to find freelance opportunities - what is the most typical troupe of project you do as freelance ,"Hi all,

I have 5+ years of experience. I’m based in Europe

Lately I’m thinking switch from full time employee to contractor, doing freelancing and working for different companies at the same time.

I think that freelancing for data scientists is harder than freelancing for software developers. I imagine a front end developer can easily get a project to build form scratch a website, or add a functionality to the existent one. Data scientists instead need already data and infrastructure to perform their job.


- How do data scientists find freelance jobs, I’m based in Europe which platform/website do you use?
- What is the most typical project you worked on?
- How is the market now, is there a good demand? ",30,12,1733650061.0,datascience
1h8xo0m,Is the data job market as badly affected as software engineering?,Everyone knows the market is bad right now for software engineers. Probably as bad as it's every been. What is the consensus on the job market for data professionals right now?,266,113,1733593122.0,datascience
1h8dlz9,Classification threshold cost optimisation,"Say you’ve selected the best classifier for a particular problem, using threshold invariant metrics such as AUROC, Brier score, or log loss. 

It’s now time to choose the classification threshold. This will clearly depend on the use case and the cost/ benefits associated with true positives, false positives, etc. 

Often I see people advising to choose a threshold by looking at metrics such precision and recall. 

What I don’t see very often is people explicitly defining relative (or absolute, if possible) costs/ benefits of each cell in the confusion matrix (or more precisely the action that will be taken as a result). For example a true positive is worth $1000, a false positive -$500 and the other cells $0. 

You then optimise the threshold based on maximum benefit using a cost-threshold curve. The precision and recall can also be reported, but they are secondary to the benefit optimisation and not used directly in the choice. I find this much more intuitive and is my go-to.

Does anyone else regularly use this approach? In what situations might this approach not make sense?",31,25,1733524141.0,datascience
1h8j6tq,Llama3.3 free API,,9,4,1733540941.0,datascience
1h87a9m,Meta released Llama3.3,,26,1,1733507631.0,datascience
1h81878,Deploying Niche R Bayesian Stats Packages into Production Software,"Hoping to see if I can find any recommendations or suggestions into deploying R alongside other code (probably JavaScript) for commercial software. 

Hard to give away specifics as it is an extremely niche industry and I will dox myself immediately, but we need to use a Bayesian package that has primary been developed in R.   
  
Issue is, from my perspective, the package is poorly developed. No unit tests. poor/non-existent documentation, plus practically impossible to understand unless you have a PhD in Statistics along with a deep understanding of the niche industry I am in. Also, the values provided have to be ""correct""... lawyers await us if not...  
  
While I am okay with statistics / maths, I am not at the level of the people that created this package, nor do I know anyone that would be in my immediate circle. The tested JAGS and untested STAN models are freely provided along with their papers.

It is either I refactor the R package myself to allow for easier documentation / unit testing / maintainability, or I recreate it in Python (I am more confident with Python), or just utilise the package as is and pray to Thomas Bays for (probable) luck.

Any feedback would be appreciated. ",39,18,1733491339.0,datascience
1h7j7ry,"The ""method chaining"" is the best way to write Pandas code that is clear to design, read, maintain and debug: here is a CheatSheet from my practical experience after more than one year of using it for all my projects",,255,42,1733431204.0,datascience
1i4oujz,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",0,12,1737256529.0,MachineLearning
1hq5o1z,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",35,12,1735615814.0,MachineLearning
1i6zh6p,[D]: A 3blue1brown Video that Explains Attention Mechanism in Detail,"- YouTube [video](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [subtitle](https://downsub.com/?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DeMlx5fFNoYc)

  
# Timestamps

  
02:21 : token embedding

02:33 : in the embedding space \
there are multiple distinct directions for a word \
encoding the multiple distinct meanings for the word.

02:40 : a well-trained attention block \
calculates what you need to add to the generic embedding \
to move it to one of these specific directions, \
as a function of the context. \

07:55 : Conceptually think of the Ks as potentially answering the Qs.

11:22 : ( did not understand )


",159,11,1737509923.0,MachineLearning
1i74pni,[D] A little late but interesting talk by Fei-Fei Li at NeurIPS 2024,Great talk by Fei-Fei Li on Visual Intelligence and what the future holds for AI. Wanted to share it here in case anyone wants to check it out on their [website](https://neurips.cc/virtual/2024/invited-talk/101127). ,15,1,1737526338.0,MachineLearning
1i73gxr,[D]: Andrej Karpathy lecture: Building makemore Part 2: MLP,"Youtube [video](https://www.youtube.com/watch?v=TCH_1BHY58I)


# Timestamps

00:01:38 : 3 character context ( 27*27*27 = 19683 ) . Too much possibilities. \
Introduce Multi-Layer Perception model.

00:02:09 - 00:09:00 : [00-02-03-bengio-2003-paper.md](https://github.com/xuyuji9000/machine-learning-playground/blob/ffee4b430c9ae40a1aecdb1aee9af83b9e60901c/10-learning-materials/01-videos/01-andrej-karpathy/02-Building%20makemore%20Part%202%3A%20MLP/00-02-03-bengio-2003-paper.md)
",9,0,1737521788.0,MachineLearning
1i727qm,[R] Future-Guided Learning: A Predictive Approach To Enhance Time-Series Forecasting,"Hello everybody! My name is Skye and I am the first author of this work! This paper demonstrates that forecasting and event prediction can be enhanced by taking inspiration from the brain, specifically predictive coding theory. I am posting the abstract, code, and arXiv link for anybody curious! Please feel free to leave any comments below, as this is my first full-length paper and I would appreciate any feedback!

**Abstract:** Accurate time-series forecasting is crucial in various scientific and industrial domains, yet deep learning models often struggle to capture long-term dependencies and adapt to data distribution drifts over time. We introduce Future-Guided Learning, an approach that enhances time-series event forecasting through a dynamic feedback mechanism inspired by predictive coding. Our method involves two models: a detection model that analyzes future data to identify critical events and a forecasting model that predicts these events based on current data. When discrepancies occur between the forecasting and detection models, a more significant update is applied to the forecasting model, effectively minimizing surprise and adapting to shifts in the data distribution by aligning its predictions with actual future outcomes. This feedback loop allows the forecasting model to dynamically adjust its parameters, focusing on persistent features despite changes in the data. We validate our approach on a variety of tasks, demonstrating a 44.8% increase in AUC-ROC for seizure prediction using EEG data, and a 48.7% reduction in MSE for forecasting in nonlinear dynamical systems. By incorporating a predictive feedback mechanism adaptable to data drift, Future-Guided Learning advances how deep learning is applied to time-series forecasting. 

**Our code is publicly available at:** https://github.com/SkyeGunasekaran/FutureGuidedLearning.

**arXiv:** [https://arxiv.org/pdf/2410.15217](https://arxiv.org/pdf/2410.15217)",4,0,1737517691.0,MachineLearning
1i6lgoo,[D] AISTATS 2025 Paper Acceptance Result,AISTATS 2025 paper acceptance results are supposed to be released today. Creating a discussion thread for this year's results.,36,42,1737474232.0,MachineLearning
1i6os2n,[R] Multivariate Time Series Prediction with Transformers,"I am working on a model that I want to be able to take in a multivariate time series of weather and river height data, and output a series of predictions for one of the river gauge heights (Essentially, I feed in timesteps 20-40 and expect to receive timesteps 41-61). I have previously been using an LSTM for this, but I got pretty subpar results with several different architectures. I'm now looking at using a transformer encoder network, and I have this recurring issue I can't seem to figure out.

For almost any context length, model size, positional encoding, training time, etc.; the model seems to be incapable of distinguishing between timesteps on the outputs. It always learns to predict a good average for the gauge height across the timesteps, but there's no variation in its outputs. On an example case where the target gauge height is \[0.2, 0.3, 0.7, 0.8, 0.6\] it would output something like \[0.4, 0.45, 0.4, 0.45, 0.5\].

In fact, the model performs almost exactly the same without any positional encoding at all.

Here's an example of what an output might look like from several continuous tests:

[Several prediction lines, showing a similar trend regardless of actual position on the graph.](https://preview.redd.it/ukhktk9nxdee1.png?width=1252&format=png&auto=webp&s=fbc276b432ff28dc104c24afc9da1d2829d82cd9)

I have tried both relative positional encoding and absolute positional encoding and adjusting the loss function to add a term that focuses on the slope between timesteps, but I can't seem to enforce differentiation between timesteps.

The extra loss term:

    class TemporalDeregularization(nn.Module):
        def __init__(self, epsilon):     
            super().__init__() 
            self.epsilon = epsilon 
            self.mse = nn.MSELoss()
    
        def forward(self, yPred, yTrue):
            predDiff = yPred[:, 1:] - yPred[:, :-1]
            targetDiff = yTrue[:, 1:] - yTrue[:, :-1]
            return self.epsilon * self.mse(predDiff, targetDiff)

My positional encoding scheme:

    class PositionalEncoding(nn.Module):
        def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first=False):
            super().__init__()
            self.batch_first = batch_first
            self.dropout = nn.Dropout(p=dropout)
    
            position = torch.arange(max_len).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
            pe = torch.zeros(max_len, 1, d_model)
            pe[:, 0, 0::2] = torch.sin(position * div_term)
            pe[:, 0, 1::2] = torch.cos(position * div_term)
            self.register_buffer('pe', pe)
    
        def forward(self, x: Tensor) -> Tensor:
            if self.batch_first:
                x = x + self.pe[:x.size(1)].permute(1, 0, 2)
            else:
                x = x + self.pe[:x.size(0)]
            return self.dropout(x)

Here's a diagram of my architecture that's more explicit:

[Image containing transformer network architecture, including a linear projection, positional encoding, transformer encoder, and another projection in series.](https://preview.redd.it/emh3s8ozvdee1.png?width=521&format=png&auto=webp&s=ba586df018bdfa653747e98bf03d88a871e0a9ce)

I understand that this isn't exactly a common use case or architecture for this use case, but I'm not sure why the model isn't capable of making the distinction between timesteps. I've considered adding a bidirectional LSTM before the final projection to force time differentiation.

For reference, I have found that this model performs well with a dModel of 64, feedForward of 128, 6 layers, and 8 heads. The other term in the loss function is a standard MSE. Also, I don't apply masking as all of the inputs should be used to calculate the outputs in my case.

I can't post much code as this is related to my job, but I would like to learn more about what is wrong with my approach.

Any help or advice is appreciated, I'm getting my master's currently but I have yet to encounter any machine learning classes despite years of work experience with it, so I may just be missing something. (Also sorry for the dog ass Google drawings)",18,9,1737482509.0,MachineLearning
1i6s0bb,[R] Language Model Mind Evolution: An Evolutionary Search Strategy for Scaling LLM Inference,"A really interesting technical advancement in using evolutionary algorithms to enhance LLM reasoning capabilities. The core methodology combines genetic algorithms with LLM outputs to evolve better reasoning patterns.

Key technical points:
- Implements genetic algorithm framework operating on LLM solution attempts
- Uses specialized evaluator models to assess reasoning quality and guide evolution
- Performs crossover and mutation operations on successful reasoning patterns
- Iteratively optimizes solutions across generations focusing on correctness and depth

Results from their experiments:
- 15-20% improvement in reasoning accuracy on test cases
- Enhanced step-by-step solution generation
- Reduced logical gaps and errors in complex reasoning tasks
- Maintained performance improvements across different reasoning domains

I think this approach could be particularly valuable for improving LLM performance on structured reasoning tasks like mathematical proofs and logical deductions. The evolutionary optimization framework provides a systematic way to discover and refine better reasoning patterns.

I think the computational costs will need to be addressed before widespread adoption, but the methodology shows promise for automated improvement of AI capabilities. The ability to evolve more sophisticated reasoning strategies could help develop more reliable AI systems.

TLDR: Research demonstrates evolutionary algorithms can optimize LLM reasoning patterns, showing 15-20% accuracy improvements through automated evolution of solution approaches.

[Full summary is here](https://aimodels.fyi/papers/arxiv/evolving-deeper-llm-thinking). Paper [here](https://arxiv.org/abs/2501.09891).",8,0,1737490342.0,MachineLearning
1i6p1ja,Apple AIML Residency Program 2025 [R],"Hello!

Has anyone participated in Apple's AIML residency in the past and is willing to share their experience?

I'm mostly curious about the interview process, the program itself (was it tough? fun?), also future opportunities within Apple as a permanent employee. Thanks in advance!",10,2,1737483137.0,MachineLearning
1i6xkdh,[D] Unsure if I am overfitting,"I am trained a machine learning model and I am unsure as to whether it is overfitting. The accuracy, precision, recall and f1-score when predicting with the training set is all 1.0, and for test set it is \~0.9 for all. I know overfitting happens when it can't generalise well for test set, but my results are pretty high for test set. I am not sure whether it is overfitting as the test scores are still quiet high.",1,6,1737504554.0,MachineLearning
1i716g6,[D]: An Article Explains Self-Attention (code snippet included),"[article](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)

- single-head attention
- multi-head attention
- cross-attention 

explanations included.
",0,0,1737514831.0,MachineLearning
1i6h40i,[D] Understanding predictive coding networks,"Hi all, 

I'm trying to understand predictive coding networks like described in [Rao & Ballard](https://www.researchgate.net/publication/13103385_Predictive_Coding_in_the_Visual_Cortex_a_Functional_Interpretation_of_Some_Extra-classical_Receptive-field_Effects).   
  
So far I understand that training the network is done through setting the input (and output if training is supervised) and first modifying the activity of the neurons to reduce prediction errors, then modifying the synaptic weights. 

What I don't understand is that it seems the activity of a hidden layer ""r"" seems to be a function of the difference between the prediction and the input (see figure 1.b), it seems implied here that \`r\` is the product of the transposed weights U^(T) and the prediction error which confuse me : I understand that we want to propagate the prediction error to the next layer, but how can we minimize (I - f(Ur)) if r = U^(T) (I - f(Ur))? 

I think I still haven't fully grasped the overall architecture and would really appreciate if someone could help. ",11,3,1737461165.0,MachineLearning
1i5z6rd,[D] ICLR 2025 paper decisions,Excited and anxious about the results! ,64,76,1737402639.0,MachineLearning
1i726eh,[R] Tensor Product Attention is All You Need,"Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available",0,3,1737517592.0,MachineLearning
1i69l1i,"[Research] Who publish this gene expression dataset? 7070 genes, 69 samples, 5 classes: EPD, JPA, MED, MGL, RHB","Hi, my goal is to reference the original author and understand what is EPD, JPA, MED, MGL, RHB. The oldest reference I can found:

1. 2008's paper \[1\], and the author's paper cite Dr. Gregory Piatetsky-Shapiro from KDnuggets and Prof. Gary Parker from Connecticut College. The most information I can get out of is it's a pediatric tumor dataset.
2. 2009's paper \[2\], and the author's paper cite \[3\]. However, the paper mentioned only 42 patients samples. Meanwhile, the dataset I have 69 labeled samples and 23 unlabeled samples.

Although I doubt it's the same paper, since paper \[3\] mentioned it's a 6,817 genes instead of 7,070 genes. But paper \[2\] add the complete name of each class based on paper \[3\]. So, I used archive website to check the dataset but it didn't archive the zip file. As of right now, I cannot check whether it is the same dataset.

The last page I am visiting: [https://web.archive.org/web/20060907191641/http://www.broad.mit.edu/mpr/CNS/](https://web.archive.org/web/20060907191641/http://www.broad.mit.edu/mpr/CNS/)

The link that I need: [http://www.broad.mit.edu/mpr/CNS/#:\~:text=Pomeroy\_et\_al\_0G04850\_11142001\_datasets.zip](http://www.broad.mit.edu/mpr/CNS/#:~:text=Pomeroy_et_al_0G04850_11142001_datasets.zip)

\[1\]N. E. Ling and Y. A. Hasan, “Evaluation Method in Random Forest as Applied to Microarray Data,” *Malaysian Journal of Mathematical Sciences*, vol. 2, no. 2, pp. 73–81, 2008.

\[2\]S. L. Pomeroy *et al.*, “Prediction of central nervous system embryonal tumour outcome based on gene expression,” *Nature*, vol. 415, no. 6870, pp. 436–442, 2002, doi: 10.1038/415436a.

\[3\]N. LING, “CLASSIFICATION OF MICROARRAY DATASETS USING RANDOM FOREST,” 2009.",9,5,1737430636.0,MachineLearning
1i5tm4j,[D] - Most Engaging ML Podcasts?,Looking for good podcasts to stay on top of ML news. Specifically looking for ones that are able to tell a good story or narrative like Planet Money or Freakonomics rather than sounding like a lecture,74,28,1737389192.0,MachineLearning
1i5rcqn,[R] Do generative video models learn physical principles from watching videos? Not yet,"A new benchmark for physics understanding of generative video models that tests models such as Sora, VideoPoet, Lumiere, Pika, Runway. From the authors; ""We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism""  
paper: [https://arxiv.org/abs/2501.09038](https://arxiv.org/abs/2501.09038)",88,14,1737383353.0,MachineLearning
1i6brew,[D] Accumulation error,"Can anyone give me some work that has theorem/insight, about possible bounds or method to approximate error accumulation of sequential model? Something like the changes in distribution/error after each steps?",1,1,1737438089.0,MachineLearning
1i5i073,[R] Evolving Deeper LLM Thinking,,36,4,1737347031.0,MachineLearning
1i5mku4,"[Discussion] How to Build a Knowledge Graph from Full Text Without Predefined Entities?
","I'm building a knowledge graph from a large set of industry documents without predefined entities. How can I handle semantically duplicate entities and relationships effectively? Also, since I can't process all documents at once, how can I ensure consistency in extracted relationships when working in chunks?

PS - Will be using GPT for processing",12,7,1737365813.0,MachineLearning
1i68w3u,[D] Useful software development practices for ML?,"I am teaching a workshop on ML and I want to dedicate 2 hours to the software development part of building an ML system. My audience are technical undergraduate students that know python and command line. **Any software practices (with links) you wish you knew when you were younger?**

Currently thinking of talking about git, code tests, validation (pydantic) and in terms of principles: [YAGNI](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it), [KISS](https://en.wikipedia.org/wiki/KISS_principle) and [DRY/WET](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) code. Could also cover [technical debt](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf).",0,11,1737428449.0,MachineLearning
1i58qk2,Any gift ideas for someone into ML?  [D],"Hello everyone, I need help for a really special gift for someone who is really into Machine Learning and related fields and is doing research/a career in it. 

I know very little about Machine Learning, but I still want to get them something either really cool or practical for their work. Anything from buying them a new computer specifically for work or some cool collectible item. Anything including pointing me in a good direction would be appreciated, thank you!",25,35,1737319990.0,MachineLearning
1i51cks,[P] Noteworthy LLM Research Papers of 2024 (Part Two): July to December,,39,0,1737301590.0,MachineLearning
1i5xv29,[D]  Uncertinity Quantificationfor time seriese prediction (RNN)?,"I have a time series that predicts one of two classes at each step (0 or 1) using RNN, so it's sequence to sequence. I'm new to the topic of Uncertainty Quantification (UQ). Can I directly apply common methods such as deep-ensemble or MC dropout and simply expect everything to work? Are there any caveats?

I have checked two libraries: [torch-uncertinity](https://github.com/ENSTA-U2IS-AI/torch-uncertainty) and [UQ-BOX](https://github.com/lightning-uq-box/lightning-uq-box) but nothing is mentioned about time series.",0,4,1737399463.0,MachineLearning
1i5h6n6,[R] Looking for retrieval datasets built from real documentation and queries,"Retrieval as in `(query, passage)` pairs where `passage` is a chunk of text from the documentation which is relevant to `query`.

BeIR has good datasets, but the ""documentation"" is often pretty wide, e.g., any Wikipedia or PubMed article. I'm looking for a dataset where the documentation is more focused, something like scikit-learn's docs.

[StaRD](https://github.com/oneal2000/STARD) is a high quality dataset, but it doesn't have enough queries for my purposes. Ideally, there are ≥5k unique queries.",3,0,1737344239.0,MachineLearning
1i5veu2,[P] Anyone Experienced with Charting and Backtesting in Futures Trading?,"Hello everyone,

I’ve been working on backtesting a theory related to trading futures around news events. The results so far have been promising, but I’d like to take things to the next level, potentially by incorporating machine learning or more advanced techniques.

Does anyone here have experience with backtesting and integrating machine learning into trading strategies? Specifically for futures or similar instruments?

I’d love to hear your insights, tips, or even resources that could help refine and expand this approach.

Thanks in advance!",0,3,1737393593.0,MachineLearning
1i5azv1,[D] The Case for Open Models,Why openness matters in AI,6,0,1737325641.0,MachineLearning
1i5pk0d,Pre-trained models on faces/skin tones? [D],"I am doing a project that involves rPPG and I was woandering if there are any good pre-trained models on faces/skin tones that I can build on top.

Thanks",0,3,1737377798.0,MachineLearning
1i5dizh,[D] Looking for NLP annotation tool with custom column view,"Hi everyone! I'm working on a document revision project that requires NLP data annotation. I need a tool that can:

1. Display the dataset in a standard tabular view
2. Show git-style diffs between source and revised texts in a custom column

I've already tried Argilla and Label Studio, but neither supports custom columns. Does anyone know of annotation tools that offer this functionality?

Thanks in advance!",0,5,1737332645.0,MachineLearning
1i5mk5e,[D] Llama3.2 model adds racial annotation,"This is really interesting, I was conversing with Llama 3.2 3B model, I found out that it automatically appends or greets you based on your name. Maybe others already know this, but I just paid attention to this detail just now. Could it be because of the training dataset, or is this injected.

[check this out](https://preview.redd.it/8vlginagc4ee1.png?width=1352&format=png&auto=webp&s=07b36347e77fd357f2375e96eef4462efc9c3c52)

  
Edit: the racial annotation is not added to put a negative spin on this, it’s just an observation that salutations are customised based on the name. Which is an attention to detail",0,10,1737365726.0,MachineLearning
1i4rz3r,[P] Speech recognition using MLP ,"So we have this assignment where we have to classify the words spoken in the audio file. We are restricted to using spectrograms as input, and only simple MLPs no cnn nothing. The input features are around 16k, and  width is restricted to 512, depth 100, any activation function of our choice. We have tried a lot of architectures, with 2 or 3 layers, with and without dropout, and with and without batch normal but best val accuracy we could find is 47% with 2 layers of 512 and 256, no dropout, no batch normal and SELU activation fucntion. We need 80+ for it to hold any value. Can someone please suggest a good architecture which doesn't over fit?",11,42,1737266802.0,MachineLearning
1i4ltt6,[D] Suggestions for topics for a PhD level ML focused programming course?,"Some background: I work as a data scientist/ML engineer for a small startup. I also adjunct for the department from which I got my PhD(in statistics).

For the last few years, I’ve been teaching a series of statistical programming courses for masters students, and early PhD‘s. This semester, my class unfortunately got canceled due to low enrollment, which I was told is due to poor recruitment last fall and poor advertising.  We are thinking to offer that course every other year. I would like to propose a third course in the series with more advanced topics.

First course: programming fundamentals for both R and Python. Some basic analytical stuff for each.

Second course: Python based analysis course (many R courses exist already) which touches on statistical routines from basics to mixed modeling and Bayesian analysis.  Also we go through the classic models with PyTorch as well as a few transformer based applications.  Also work in some explainable AI techniques 

Third course: optimization, variational inference, other Bayesian deep learning approaches, MLops concepts, ????

The thing is I need to work in a fair amount of stochastic approaches because it’s a statistics department after all. 

Hope that’s clear. I would like to provide relevant information especially to PhD students who would like to live at the cutting edge with an emphasis on experimentation and implementation.  I know there is a lot out there but at work I need to focus on my specific tasks.

Thanks so much for any advice!",27,18,1737247026.0,MachineLearning
1i44h5v,[D] I hate softmax ,"This is a half joke, and the core concepts are quite easy, but I'm sure the community will cite lots of evidence to both support and dismiss the claim that softmax sucks, and actually make it into a serious and interesting discussion.

What is softmax? It's the operation of applying an element-wise exponential function, and normalizing by the sum of activations.
What does it do intuitively? One point is that outputs sum to 1. Another is that the the relatively larger outputs become *more* relatively larger wrt the smaller ones: big and small activations are teared apart.

One problem is you never get zero outputs if inputs are finite (e.g. without masking you can't attribute 0 attention to some elements).
The one that makes me go crazy is that for most of applications, magnitudes and ratios of magnitudes are meaningful, but in softmax they are not: softmax cares for differences.
Take softmax([0.1, 0.9]) and softmax([1,9]), or softmax([1000.1,1000.9]). Which do you think are equal? In what applications that is the more natural way to go?

Numerical instabilities, strange gradients, embedding norms are all things affected by such simple cores. Of course in the meantime softmax is one of the workhorses of deep learning, it does quite a job.

Is someone else such a hater? Is someone keen to redeem softmax in my eyes?",255,92,1737194715.0,MachineLearning
1i4ho23,[D] Refactoring notebooks for prod,"I do a lot of experimentation in Jupyter notebooks, and for most projects, I end up with multiple notebooks: one for EDA, one for data transformations, and several for different experiments. This workflow works great until it’s time to take the model to production.

At that point I have to take all the code from my notebooks and refactor for production. This can take weeks sometimes. It feels like I'm duplicating effort and losing momentum.

Is there something I'm missing that I could be using to make my life easier? Or is this a problem y'all have too?

\*Not a huge fan of nbdev because it presupposes a particular structure",29,25,1737235091.0,MachineLearning
1i4fvqn,[R] VortexNet: Neural Computing through Fluid Dynamics ,,25,7,1737230204.0,MachineLearning
1i4n01i,[R] Tensor and Fully Sharded Data Parallelism,"In this series, we continue exploring distributed training algorithms, focusing on tensor parallelism (TP), which distributes layer computations across multiple GPUs, and fully sharded data parallelism (FSDP), which shards model parameters, gradients, and optimizer states to optimize memory usage. Today, these strategies are integral to massive model training, and we will examine the properties they exhibit when scaling to models with 1 trillion parameters.

[https://martynassubonis.substack.com/p/tensor-and-fully-sharded-data-parallelism](https://martynassubonis.substack.com/p/tensor-and-fully-sharded-data-parallelism)",10,0,1737250575.0,MachineLearning
1i455gs,[R] Causal Inference Meets Deep Learning: A Comprehensive Survey,,28,0,1737197680.0,MachineLearning
1i3t4c3,[P] Building an Reinforcement Learning Agent to play The Legend of Zelda,"A year go I started trying to use PPO to play the original Legend of Zelda, and I was able to train a model to beat the first boss after a few months of work. I wanted to share the project just for show and tell. I'd love to hear feedback and suggestions as this is just a hobby project. I don't do this for a living. The code for that lives in the [original-design branch](https://github.com/DarkAutumn/triforce/tree/original-design) of my [Triforce repo](https://github.com/DarkAutumn/triforce). I'm currently tinkering with new designs so the main branch is much less stable.

Here's a video of the agent [beating the first dungeon](https://www.youtube.com/watch?v=yERh3IJ54dU), which was trained with 5,000,000+ steps. At 38 seconds, you can see it learned that it's invulnerable at the screen edge, and it exploits that to avoid damage from a projectile. At 53 seconds it steps up to avoid damage from an unblockable projectile, even though it takes a -0.06 penalty for moving the wrong way (taking damage would be a larger penalty.) At 55 seconds it walks towards the rock projectile to block it. And so on, lots of little things the model does is easy to miss if you don't know the game inside and out.

As a TLDR, [here's an early version of my new (single) model](https://youtu.be/3AJXfBnmgVk). This doesn't make it quite as far, but if you watch closely it's combat is already *far* better, and is only trained on 320,000 steps (~6% of the steps the first model was trained on).

This is pretty far along from my [very first model](https://www.youtube.com/watch?v=KXPMwehTOf0).

# Original Design

I got the original project working using stable-baselines's PPO and default neural network (Shared NatureCNN, I believe). SB was great to get started but ultimately stifling. In the new version of the project I've implemented PPO from scratch with torch with my own simple neural network similar to stable-baseline's default. I'm playing with all kinds of changes and designs now that I have more flexibility and control. Here is my rough original design:

## Overall Strategy

My first pass through this project was basically ""imagine playing Zelda with your older sibling telling you where to go and what to do"". I give the model an objective vector which points to where I want it to go on the screen (as a bird flies, the agent still had to learn path finding to avoid damage and navigate around the map). This includes either point at the nearest enemy I want it to kill or a NSEW vector if it's supposed to move to the next room.

Due a few limitations with stable-baselines (especially around action masking), I ended up training unique models for traversing the overworld vs the dungeon (since they have entirely different tilesets). I also trained a different model for when we have sword beams vs not. In the video above you can see what model is being used onscreen.

In my current project I've removed this objective vector as it felt too much like cheating. Instead I give it a one-hot encoded objective (move north to the next room, pickup items, kill enemies, etc). So far it's working quite well without that crutch. The new project also does a much better job of combat even without multiple models to handle beams vs not.

## Observation/Action Space

**Image** - The standard neural network had a really tough time being fed the entire screen. No amount of training seemed to help. I solved this by creating a [viewport around Link](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/notebooks/torch_viewport.ipynb) that keeps him centered. This REALLY helped the model learn.

I also had absolutely zero success with stacking frames to give Link a way to see enemy/projectile movement. The model simply never trained with stable-baselines when I implemented frame stacking and I never figured out why. I just added it to my current neural network and it seems to be working...

Though my early experiments show that giving it 3 frames (skipping two in between, so frames curr, curr-3, curr-6) doesn't *really* give us that much better performance. It might if I took away some of the vectors. We'll see.

**Vectors** - Since the model cannot see beyond its little viewport, I gave the model a vector to the closest item, enemy, and projectile onscreen. This made it so the model can shoot enemies across the room outside of its viewport. My new model gives it multiple enemies/items/projectiles and I plan to try to use an attention mechanism as part of the network to see if I can just feed it all of that data.

**Information** - It also gets a couple of one-off datapoints like whether it currently has sword beams. The new model also gives it a ""source"" room (to help better understand dungeons where we have to backtrack), and a one-hot encoded objective.

**Action Space**

My original project just has a few actions, 4 for moving in the cardinal directions and 4 for attacking in each direction (I also added bombs but never spent any time training it). I had an idea to use masking to help speed up training. I.E. if link bumps into a wall, don't let him move in that direction again until he moves elsewhere, as the model would often spend an entire memory buffer running headlong straight into a wall before an update...better to do it once and get a huge negative penalty which is essentially the same result but faster.

Unfortunately SB made it really annoying architecturally to pass that info down to the policy layer. I could have hacked it together, but eventually I just reimplemented PPO and my own neural network so I could properly mask actions in the new version. For example, when we start training a fresh model, it cannot attack when there aren't enemies on screen and I can disallow it from leaving certain areas.

The new model actually understands splitting swinging the sword short range vs firing sword beams as two different actions, though I haven't yet had a chance to fully train with the split yet.

**Frameskip/Cooldowns** - In the game I don't use a fixed frame skip for actions. Instead I use the internal ram state of game to know when Link is animation locked or not and only allow the agent to take actions when it's actually possible to give meaningful input to the game. This greatly sped up training. We also force movement to be between tiles on the game map. This means that when the agent decides to move it loses control for longer than a player would...a player can make more split second decisions. This made it easier to implement movement rewards though and might be something to clean up in the future.

## Other interesting details

**Pathfinding** - To facilitate rewards, the original version of this project used A* to pathfind from link to what he should be doing. [Here's a video of it in action](https://www.youtube.com/watch?v=HFsHpex4OvM). This information wasn't giving to the model directly but instead the agent would only be given the rewards if it exactly followed that path or the transposed version of it. It would also pathfind around enemies and not walk through them.

This was a nightmare though. The [corner cases were significant](https://github.com/DarkAutumn/triforce/blob/original-design/triforce/critics.py#L343-L502), and pushing Link towards enemies but not *into* them was really tricky. The new verison just uses a wavefront algorithm. I calculate a [wave from the tiles we want to get to outwards](https://github.com/DarkAutumn/triforce/blob/main/triforce/wavefront.py), then make sure [we are following the gradient](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/critics.py#L281-L329). Also calculating the A* around enemies every frame (even with caching) was super slow. Wavefront was faster, especially because I give the new model no special rewards for walking around enemies...faster to compute and it has to learn from taking damage or not.

Either way, the both the old and new models successfully learned how to pathfind around danger and obstacles, with or without the cheaty objective vector.

**Rewards** - I programmed very dense rewards in both the [old](https://github.com/DarkAutumn/triforce/blob/original-design/triforce/critics.py) and [new](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/critics.py) model. At basically every step, the model is getting rewarded or punished for something. I actually have some ideas I can't wait to try out to make the rewards more sparse. Or maybe we start with dense rewards for the first training, then fine-tune the model with sparser rewards. We'll see.

**Predicting the Future** - Speaking of rewards. One interesting wrinkle is that the agent can do a lot of things that will eventually deal damage but not on that frame. For example, when Link sets a bomb it takes several seconds before it explodes, killing things. This can be a massive reward or penalty since he spent an extremely valuable resource, but may have done massive damage. PPO and other RL propagates rewards backwards, of course, but that spike in reward could land on a weird frame where we took damage or moved in the wrong direction.

I probably could have just *not* solved that problem and let it shake out over time, but instead I used the fact that we are in an emulator to just see what the outcome of every decision is. When planting a bomb, shooting sword beams, etc, we [let the game run forward](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/state_change_wrapper.py#L140-L192) until impact, then rewind time and reward the agent appropriately, continuing on from when we first paused. This greatly speeds up training, even if it's expensive to do this savestate, play forward, restore state.

**Neural Networks** - When I first started this project (knowing very little about ML and RL), I thought most of my time would be tuning the shape of the neural network that we are using. In reality, the default provided by stable-baselines [and my eventual reimplemnentation](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/models.py#L19-L255) has been enough to make *massive* progress. Now that I have a solid codebase though, I really want to revisit this.  I'd like to see if trying CoordConvs and similar networks might make the viewport unncessary.

## Less interesting details/thoughts

**Hyperparameters** - Setting the entropy coefficinet way lower helped a TON in training stable models.  My new PPO implementation is way less stable than stable-baselines (ha, imagine that), but still converges most of the time.

**Infinite Rewards** - As with all reinforcement learning, if you give some way for the model to get infinite rewards, it will do just that and nothing else. I spent days, or maybe weeks tweaking reward functions to just get it to train and not find a spot on the wall it could hump for infinite rewards.  Even just neutral rewards, like +0.5 moving forward and -0.5 for moving backwards, would often result in a model that just stepped left, then right infinitely.  There has to be a real reward or punishment (non-neutral) for forward progress.

**Debugging Rewards** - In fact, building a rewards debugger was the only way I made progress in this project.  If you are tackling something this big, do that very early.

**Stable-Retro is pretty great** - Couldn't be happier with the clean design for implementing emulation for AI.

**Torch is Awesome** - My early versions heavily used numpy and relied on stable-baselines, with its multiproc parallelization support. It worked great. Moving the project over to torch was night and day though. It gave me so much more flexibility, instant multithreading for matrix operations. I have a pretty beefy computer and I'm *almost* at the same steps per second as 20 proc stable-retro/numpy.

## Future Ideas

This has already gone on too long. I have some ideas for future projects, but maybe I'll just make them another post when I actually do them.

## Special Thanks

A special thanks to [Brad Flaugher](https://bradflaugher.com/) for help with the early version of this, Fiskbit from the Zelda1 speedrunning community for help pulling apart the raw assembly to build this thing, and MatPoliquin for maintaining Stable-Retro.

Happy to answer any questions, really I just love nerding out about this stuff.",164,21,1737154326.0,MachineLearning
1i458ut,[R] Liquid Neural Networks exhibit robust navigation in OOD environments.,,9,1,1737198072.0,MachineLearning
1i3qxy4,[D] Am I actually a machine learning engineer?,"For the past few years I've had a job with the official title ""machine learning engineer"", but as I hunt for other jobs online, I wonder if that's actually accurate. Based on the experience requirements and responsibilities listed, it doesn't seem to match up with what I do.

I have a master's with a focus in ML (though that was pre LLM-boom, so things have changed a lot) but struggled to find work in my area pertaining to that out of college. Post-COVID when everyone went remote I got my current job. In it, I work on a team building and deploying software that utilize machine learning to accomplish tasks. However, I'm never the one actually building the learning models (there's a researcher on our team who does that); just creating the systems around them. I'm actually pretty happy in my ""machine learning adjacent"" role, but should I be searching for different job titles to find something similar?

EDIT: a bunch of people keep replying thinking I'm looking for validation about my title. I don't care about that. I only care about knowing what job titles I should be searching for when looking for something similar.",130,58,1737148506.0,MachineLearning
1i44wlr,[P] Launch a Federation of robots that collaboratively train an object manipulation model,"Using [Flower](https://flower.ai/)  and [LeRobot](https://github.com/huggingface/lerobot), I put together a  [quickstart example](https://github.com/adap/flower/tree/main/examples/quickstart-lerobot) that demonstrates how to train a diffusion model collaboratively across 10 individual nodes (each with its own dataset partition!). This example uses the`push-t` dataset, where the task is to move a letter T object on top of another that is to remain static.

The example it's pretty easy to run, and can do so efficiently if you have access to a recent gaming GPU. Although the diffusion model only takes 2GB of VRAM (of course you can decide to scale it up), the compute needed to train them isn't negligible. For context, running the example until convergence takes 40mins on a dual RTX 3090 setup. It takes about 30rounds of federated learning (FL) to do so although the example runs for 50 rounds by default.

[Evaluation of the global model at different rounds. After just a few rounds of collaboratively AI training the model successfully completes the task \(and it does so pretty fast!!!\)](https://i.redd.it/lp13jn70dqde1.gif)

  
The example runs each node/robot in simulation by default (i.e. each node is a Python process and there is some clever scheduling to run the jobs in a resource-aware manner). But it is straight forward to run it as a real deployment where each node is, for example, a different device (e.g. NVIDIA Jetson). If someone is interested in doing this, checkout the links added at the bottom of the example readme

  
Learn more about the Action Diffusion policy method -> [https://arxiv.org/abs/2303.04137](https://arxiv.org/abs/2303.04137)

",11,1,1737196643.0,MachineLearning
1i3ym76,[R] Any paper recommendations for Bayesian methods in ML and causal inference?,"Hey guys,

So I am very new to Bayesian methods and am curious about it from a data science and modelling point of view and how it could determine causal relationships.

I don't really know where to start, but I've read some papers on Bayesian Networks and have heard interesting things about Bayesian Deep Learning so would be happy to see any recommendations on those topics.

I would also be happy to hear about any papers you may have recently read, but am looking for anything you guys have found interesting and not an application on any specific domain, just interested in learning the theory for now (unless you suggest that I pick a domain first).

Many thanks",32,12,1737170980.0,MachineLearning
1i40viz,[D] Dynamic Neuron-Controller-Based Transformer Architecture: Feedback Wanted,"**Dynamic Neuron-Controller-Based Transformer Architecture by Shanmukh Ram**

# Abstract

This white paper presents an innovative architecture that integrates dynamic neuron-controller systems with transformer models to create a continuously adaptive and resource-efficient AI framework. The proposed architecture utilizes neuron or batch controllers to dynamically adjust the weights and operations of a shared transformer architecture in real time.

By responding to signals generated by individual or grouped neurons, the system continuously adapts to changing demands. This adaptability enables efficient multi-tasking and optimizes resource sharing, ensuring high performance across diverse contexts. These features establish the architecture as a groundbreaking innovation in AI, unlocking advancements in applications such as general intelligence, personalized systems, and multi-agent collaboration.

# 1. Introduction

# 1.1 Background

Transformer architectures have revolutionized natural language processing and other domains, owing to their scalability, attention mechanisms, and ability to model long-range dependencies. However, transformers remain largely static post-training, with fine-tuning or retraining required to adapt to new tasks or shifting environments.

# 1.2 Motivation

Real-world applications often involve dynamic and unpredictable environments. Traditional transformer models, though powerful, are inefficient in adapting to real-time changes without significant retraining. This gap motivates the design of a system where neurons act as adaptive controllers, dynamically modifying the transformer’s behavior to optimize performance across varying tasks and inputs.

# 2. Proposed Architecture

# 2.1 Core Components

The architecture consists of the following core components:

1. **Neuron-Controllers**:
   * Independent neurons or batches of neurons act as dynamic agents within the system, controlling and optimizing the transformer’s performance. These controllers receive input signals from various sources, including real-time environmental data, user feedback, or task-specific objectives. Upon processing these inputs, the controllers generate precise control signals to dynamically modify transformer parameters such as attention weights, layer activations, or embeddings. For instance, in a natural language processing task, the controllers might adjust attention weights to focus on critical phrases in a document, ensuring more accurate summarization. Similarly, in image recognition tasks, layer activations could be optimized to emphasize edges or textures, improving classification accuracy.
   * These targeted adjustments significantly enhance the system’s ability to adapt to diverse tasks while maintaining high performance and efficiency. This dynamic adjustment ensures the system remains highly adaptive, continuously optimizing its responses to suit specific tasks or contexts.
2. **Shared Transformer Framework**:
   * A modular transformer architecture forms the backbone of the system, meticulously crafted to support real-time adjustments to its operational parameters. This modularity allows each core component, such as attention heads, transformer layers, or embeddings to be dynamically reconfigured based on control signals generated by neuron-controller batches. By enabling real-time adaptability, the system ensures that computational resources can be scaled efficiently or concentrated on specific areas of importance, depending on the complexity and requirements of the task. For instance, attention heads may be activated selectively for high-priority inputs, while layers or embeddings can be modified dynamically to fine-tune task-specific outputs. This approach not only enhances scalability but also optimizes performance, making the architecture capable of handling both simple and complex tasks with remarkable efficiency.
3. **Feedback Loop**:
   * The architecture integrates a continuous feedback mechanism wherein the transformer's outputs are systematically analyzed and fed back to the neuron-controllers. This iterative process allows the neuron-controllers to refine their strategies based on real-time performance metrics and contextual outcomes. By dynamically adjusting control parameters, the system ensures alignment with evolving task objectives and operational efficiency. This feedback loop not only enhances adaptability but also fosters a robust learning environment where both controllers and the transformer progressively improve in tandem.
   * This loop refines the controllers’ strategies in real time, ensuring constant performance improvement and alignment with task objectives.
   * By iteratively optimizing both the controllers and the transformer, the system achieves a closed-loop learning environment.
4. **Coordinator Mechanism**:
   * A centralized or decentralized coordinator mechanism is designed to ensure seamless interactions among multiple neuron-controller batches. This mechanism prioritizes resource allocation and balances task assignments, mitigating potential conflicts that may arise when neuron batches manage separate transformers or collaborate on shared tasks. By enabling effective coordination, the architecture prevents inefficiencies and ensures that all tasks are executed optimally, maintaining synergy across the entire system.

# 2.2 Key Features

1. **Dynamic Weight Adjustment**:

Dynamic weight adjustment represents the core capability of the system where controllers fine-tune specific transformer weights in real time. These adjustments are informed by contextual signals, which include environmental data, user feedback, and task-specific objectives. For example, in autonomous driving, the controllers can adjust attention weights to prioritize critical inputs like pedestrian detection over less immediate data, such as road signage in clear weather. In healthcare applications, layer activations might be fine-tuned dynamically to focus on anomalies in medical imaging, ensuring accurate diagnostics. When an input signal is received, the neuron-controllers analyze it and generate precise commands to recalibrate the transformer's internal parameters, such as attention weights or activation thresholds. This process ensures that the architecture adapts seamlessly to the demands of diverse tasks and dynamic environments. The ability to perform these real-time optimizations not only enhances task-specific performance but also maximizes resource efficiency, as only the necessary components of the transformer are engaged at any given time. This dynamic adaptability is crucial for handling complex, real-world scenarios where static models would fail to perform optimally, thereby positioning this system as a significant advancement in AI adaptability and responsiveness.

1. **Batch-Based Control**:
   * Groups of neurons manage different tasks or modules, each acting as specialized agents to oversee specific functionalities within the system. This allows simultaneous optimization across multiple frameworks by dynamically distributing computational resources and responsibilities. For example, one group of neurons may control language modeling tasks while another focuses on vision-based analysis, enabling these processes to run concurrently without interfering with each other. This approach enhances efficiency and ensures that the transformer system remains scalable and adaptable, bringing the value of multitasking without compromising performance.
2. **Task-Specific Adaptation**:
   * Each neuron batch can specialize in controlling a subset of the transformer for task-specific performance by dynamically focusing on the specific layers, attention mechanisms, or embeddings that are most relevant to the task. For example, in a multi-task learning setup, one neuron batch could fine-tune the transformer’s attention weights for language modeling, while another batch might adjust embedding layers for visual data processing. This specialization ensures that the system can effectively handle diverse tasks in parallel without sacrificing efficiency or performance. By leveraging this dynamic specialization, the architecture optimizes resource utilization, minimizes interference between tasks, and enhances the accuracy and responsiveness of each transformer subset to its assigned task.
3. **Multi-Agent Collaboration**:
   * Neuron batches play a pivotal role in enhancing the system's overall performance by engaging in collaborative or competitive dynamics tailored to complex, multi-dimensional tasks. For example, in a multi-modal AI system, one neuron batch could specialize in processing textual data, while another focuses on visual inputs. Collaboration between these batches ensures that insights from both modalities are integrated effectively, leading to more accurate and coherent outcomes, such as in video summarization or multimedia content analysis. Similarly, competition among neuron batches could prioritize critical tasks, ensuring time-sensitive objectives like anomaly detection in real-time surveillance are addressed promptly. These batches act as specialized agents, dynamically adjusting their behaviors to maximize task outcomes based on the broader system’s objectives. For instance, collaboration between neuron batches may involve sharing insights or control signals to optimize resource allocation across different sections of the transformer. In contrast, competitive dynamics could arise in scenarios where distinct neuron batches vie to prioritize their assigned tasks, ensuring critical objectives receive adequate focus.
   * By allowing both collaboration and competition, the architecture fosters a balance between efficiency and task-specific precision. This mechanism integrates seamlessly with the feedback and coordination systems, ensuring that neuron batches remain aligned with the overarching goals of the system while dynamically optimizing their strategies. The value of this approach lies in its ability to handle multi-tasking demands with enhanced adaptability and responsiveness, making it an essential component of the architecture's design.

# 3. Implementation

# 3.1 Input Signals

Neuron-controllers process a variety of inputs, such as:

* **Environmental Data**: Real-time data streams from external sensors or APIs.
* **Feedback Signals**: Outputs from transformers or user interaction data.
* **Predefined Objectives**: Task-specific goals encoded during training.

# 3.2 Dynamic Controllers

Neuron-controllers utilize advanced reinforcement learning (RL) techniques and optimization algorithms to determine the most effective adjustments for the transformer. These adjustments include recalibrating attention weights to focus on the most relevant features of the input, selectively activating or deactivating layers to optimize computational efficiency, and dynamically modifying positional encodings or embeddings to enhance the transformer's contextual understanding. By analyzing input signals and system feedback in real-time, neuron-controllers ensure that the architecture remains highly adaptive and aligned with task-specific objectives, enabling superior performance across diverse and complex tasks.

# 3.3 Transformer Modularity

The transformer is designed with modularity in mind:

* **Adapters**: Lightweight modules inserted into transformer layers to enable task-specific adjustments.
* **Sparse Activation**: Only parts of the transformer are activated based on control signals.
* **Mixture of Experts (MoE)**: Controllers determine which expert modules to activate for a given input.

# 3.4 Feedback Mechanism

A feedback loop evaluates the transformer’s output and updates the neuron-controllers’ strategies, creating a continuous learning environment.

# 4. Applications

# 4.1 Multi-Task Learning

Dynamic controllers empower a single transformer architecture to manage multiple tasks simultaneously by dynamically redistributing resources to optimize for each task's specific requirements. These controllers act as task-specialized agents, analyzing the contextual demands of each input and directing computational focus to the most relevant sections of the transformer such as attention heads, embeddings, or specific layers. For example, when handling a combination of natural language processing and vision-based tasks, the dynamic controllers can assign priority resources to textual embeddings for language inputs while activating vision-specific modules for image data.

This simultaneous multi-task optimization ensures that each task benefits from the transformer's shared architecture without compromising performance. The ability to dynamically allocate resources not only reduces computational redundancy but also enhances scalability, allowing the system to adapt seamlessly to complex, real-world scenarios. By maintaining task-specific precision while sharing computational infrastructure, this architecture represents a significant step forward in creating efficient and robust AI systems capable of managing diverse workloads.

# 4.2 Personalized Systems

Dynamic controllers allow the transformer to adapt its behavior to individual users or specific contexts, enabling highly tailored and responsive applications. By analyzing real-time user data, such as preferences, historical interactions, or contextual inputs, these controllers dynamically modify the transformer's parameters to deliver personalized outputs. For example, in a virtual assistant application, the controller might adjust the transformer's attention mechanisms to prioritize the user's current needs or focus on topics of interest based on prior interactions. This capability ensures that the system evolves alongside the user, providing a more engaging and effective experience. The ability to personalize outputs in real-time is critical for applications in education, healthcare, and customer service, where individualized solutions add significant value.

# 4.3 Collaborative AI

Neuron-controller batches enhance the system's ability to handle complex, multi-dimensional problems by fostering collaboration among multiple transformers. For instance, in a multi-modal AI system integrating text, images, and audio, one batch of neuron-controllers could process and extract key textual information, another batch could analyze visual data, and a third could handle audio signals. Collaboration ensures that insights from each modality are synthesized into a unified understanding, significantly improving outcomes such as multimedia content analysis or real-time event summarization.

This collaborative potential enables the system to leverage diverse data types effectively, ensuring comprehensive and accurate results. These controllers dynamically allocate resources and share insights between transformers, enabling them to work together seamlessly. For instance, in multi-modal AI applications that integrate text, images, and audio, one transformer might specialize in processing textual data while another focuses on visual analysis.

Through real-time communication and coordination, the system ensures that insights from each modality contribute to a cohesive and accurate result. This collaborative approach not only improves task performance but also enables the system to tackle problems that require integrated knowledge from multiple domains.

# 4.4 General Intelligence

The architecture's dynamic adaptability, real-time resource allocation, and collaborative mechanisms represent a significant step toward achieving general artificial intelligence. By allowing neuron-controller batches to manage diverse tasks and contexts dynamically, the system creates a foundation for cross-domain learning and decision-making. Unlike traditional AI systems that require retraining for new tasks, this architecture can rapidly adapt to novel scenarios, demonstrating a level of flexibility and generalization that closely mirrors human intelligence. The ability to integrate knowledge across tasks and respond effectively to unforeseen challenges positions this architecture as a cornerstone in the pursuit of general AI.

# 5. Societal Impacts

# 5.1 Positive Outcomes

* **Efficiency**: Reduced computational costs through dynamic resource sharing.
* **Adaptability**: Better handling of real-world variability and user-specific needs.
* **Innovation**: New AI applications and use cases become feasible.

# 5.2 Risks

* **Unpredictability**: Dynamic systems may produce unforeseen behaviors.
* **Security**: Systems must be robust against adversarial inputs or misuse.
* **Ethical Concerns**: Continuous learning raises questions about accountability and transparency.

# 6. Future Directions

The dynamic neuron-controller-based transformer architecture opens up several avenues for research and practical advancements. The focus must be on refining the foundational mechanisms to further enhance scalability, adaptability, and safety.

# 6.1 Enhancing Controller Intelligence

Research should prioritize the development of neuron-controllers capable of understanding higher-level abstractions, contextual nuances, and complex task hierarchies. By integrating advanced algorithms such as meta-learning and neural architecture search, these controllers can evolve into highly intelligent agents that adapt seamlessly to diverse and unforeseen challenges. This advancement will make the system more robust in managing a wider array of applications.

# 6.2 Scaling to Larger Architectures

Efforts must be directed toward designing and managing larger systems that integrate multiple controllers and transformers. However, scaling such architectures presents significant challenges, including increased computational overhead, potential bottlenecks in communication between controllers, and the risk of degraded performance in highly complex systems. Addressing these limitations is crucial to unlock the full potential of this approach and ensure seamless scalability in real-world applications. Techniques such as distributed computing, modular design, and sparse activations will be critical to maintain performance and efficiency at scale. This scaling capability will empower the architecture to handle increasingly complex tasks across industries, from healthcare diagnostics to autonomous systems.

# 6.3 Safety and Robustness

Ensuring the safety and reliability of dynamically adaptive systems is paramount. Specific strategies to achieve this include the integration of robust adversarial defense mechanisms to counter malicious inputs, the development of fail-safe protocols to handle unexpected failures, and the implementation of comprehensive ethical oversight frameworks. Additionally, employing techniques such as explainability in AI and real-time monitoring systems can ensure transparency and accountability, further reinforcing the trustworthiness of these architectures. This requires the implementation of fail-safes, ethical oversight mechanisms, and robust adversarial defenses.

By addressing these concerns, the architecture can operate confidently in critical applications, including finance, defense, and public safety. For example, in finance, the system could dynamically adapt to market changes by prioritizing critical data streams for fraud detection or risk assessment. In defense, collaborative neuron-controller batches could integrate intelligence from multiple data modalities such as satellite imagery, intercepted communications, and real-time ground reports to provide actionable insights for decision-makers. Similarly, in public safety, the architecture could manage resources dynamically during emergencies, such as optimizing response times for disaster management or ensuring accurate predictions for crowd control. Safety-focused research will also ensure that the system remains compliant with evolving regulations and ethical standards.

# 8. Conclusion

The proposed dynamic neuron-controller-based transformer architecture represents a paradigm shift in AI development. By enabling real-time adaptability, efficient resource sharing, and multi-tasking capabilities, this system has the potential to revolutionize AI applications across industries. While challenges remain, the opportunities for innovation and societal benefit are immense, making this a promising direction for future research and development.",12,17,1737179120.0,MachineLearning
1i45h9r,[D] Few-shot Learning with prototypical networks - help to understand the concept ,"Hi, probably quite simple questions for those who know the concept but still tricky for me to realize.

Let's say I have a dataset with 200 labeled samples and I have 10 classes. However, not all 200 examples contain all 10 classes, but only some of them while the rest samples contain a combination of them. Meaning that a sample might be labeled for classes 0, 1, 5, 8, while another for 0, 3, 7, and so on. Which also means that the prevalence of the classes varies a lot.

How do I split my dataset for few-shot learning with prototypical networks? Do I need to train and validate on samples that include all classes, so the network learns to compute prototypes for every class? Also, given that the prevalence of the classes varies, do I need to balance the sampling so it sees each class equally on the number of training and validation episodes?

During testing do I need to include on my test set a few labeled samples for each class? Can I do inference without any labeled samples? Is that zero-shot learning? Also, can I train a model that generalizes to unseen classes during training? 

Thanks in advance for your time and help!",5,8,1737199033.0,MachineLearning
1i4cybu,[P] How to Highlight Entire Articles Based on Keywords in a Multi-column Newspaper PDF,"Hi everyone,

I'm working on a project where I need to search for a keyword within a newspaper PDF and highlight the entire article that contains the keyword. The challenge arises because:

1. **Multiple Articles Per Page:** Each page may have multiple articles, and I need to distinguish which parts of the page belong to which article.
2. **Multi-column Layout:** Articles are often spread across multiple columns, which makes it tricky to identify the full content of an article when it spans across these columns.

My goal is to:

* Search for a keyword in the text.
* Identify and highlight the entire article that contains the keyword, even if the article spans across multiple columns or pages.

I’m currently using **PyMuPDF4LLM** to process the PDFs and extract text, but I’m not sure how to accurately map the text to specific articles and handle multi-column text.

Has anyone worked on something similar or have suggestions on how to approach this problem?

I’m attaching a sample page of the newspaper for context.

https://preview.redd.it/07gc46adhsde1.png?width=692&format=png&auto=webp&s=f0f6cd925013c67ea5b0ad2cef72a84cc2430c35

",0,1,1737222373.0,MachineLearning
1i4c5ao,[D] Fine tuning FinBert,"I want to solve use case for categorising transactions from bank statements. 

Can fine tuning finbert a good choice?",0,1,1737220250.0,MachineLearning
1i4aqxu,[D] Looking for a specific youtube video,There is a video of an asian guy who is explaining backpropogation on pen and paper using the minst dataset can anyone send me the url to the video? Thanks,0,2,1737216478.0,MachineLearning
1i39iuh,[D] Recommendations of noteworthy AI papers for starters in 2025,"Hi I’m devising up a list of papers to recommend students just starting out in compsci.

What are some must-read papers to give that is not too deep?

These days all the statistic learning theories are within reach with online courses but I want them to grow to read academic papers.

I’m starting off with ilya Sutskever's reading list.

A brief explanation of why you’re recommending the paper would be welcome too!",72,18,1737092193.0,MachineLearning
1i34keg,Grokking at the Edge of Numerical Stability [Research],">Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the naïve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ⊥Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods.

Paper: [https://arxiv.org/abs/2501.04697](https://arxiv.org/abs/2501.04697)

(not my paper, just something that was recommended to me)  
",132,23,1737075960.0,MachineLearning
1i3rrkx,[P] Are there any formal references to this dataset?,"Hi all!

I'm working on a project about Multitouch Attribution Modeling using Tensor flow to predict conversion over different channels.

In the project, we are using this dataset ([https://www.kaggle.com/code/hughhuyton/multitouch-attribution-modelling](https://www.kaggle.com/code/hughhuyton/multitouch-attribution-modelling)). However, we cannot find any formal reference (published paper or something similar) to make a proper citation. I have searched on Google a lot… really, a lot.

Does anyone know what is the origin of the data or if is it referenced somewhere?

Thanks for the help.",1,2,1737150696.0,MachineLearning
1i3enos,Automate Deep learning model with camera(inception -Tensorflow) [P],So i have been working with a deep learning project the aim is to detect objects. My main goal was to detect plastic from water and pick it up using a conveyor belt attached with a boat so i took code from GitHub and made sufficient changes and now the model is working but one problem is i have to manually add photo and change its name to test.jpeg(which i have given) so in my model the boat have a camera how will i make a project that can took the photo automatically when it detects a object and automatically load to my already made model and for all this process which development board will be sufficient.i hope someone answers my question 🙂,6,1,1737114704.0,MachineLearning
1i39cia,[D] share your most frequent embarrassingly parallel tasks,"Hey all,

I’m curious about the most common embarrassingly parallel tasks you encounter in the wild. In the ML and DS world, I’ve noticed many workflows tend to follow this general pattern:

* Pull a bunch of data from cloud storage
* Process that data through a series of functions
* Run an analysis, use the data for training, or pass it into a model for inference

What workloads do you have that follow this process or something similar? I’ve been tinkering with a cloud abstraction to make large-scale parallel processing easier, and I’m trying to identify common use cases to build tutorials around.

Any ideas, advice, or feedback would be super helpful",12,8,1737091537.0,MachineLearning
1i3baxp,[D] Concerns about review process at TPAMI,"I submitted a paper to TPAMI on June 25, 2024. It was a significant extension of our work that was accepted as an oral presentation at AAAI 2023. I know the reviews at TPAMI are rigorous and can take months, but I was just wondering what the longest time it has taken in your experience, since it has been 6 months and 3 days with no news. Also, would the reviewers take into account works that were published after the submission date? I am just worried that with the (understandably) slow reviews, I will be asked by the reviewer why I am not comparing against method XYZ, and asked to compare against said method, which could potentially outperform mine due to how fast the field progresses, and make revision and acceptance complicated.",8,6,1737099768.0,MachineLearning
1i3tr80,[P] How to import and deploy a pre-trained text-to-image model on Google Cloud for a high-traffic e-commerce project?,"Hello, I am working on an e-commerce project and I need a text-to-image model. I want to deploy this model on Google Cloud Platform (GCP), but this process seems quite new and complicated for me. Since I have limited time, I would like to know which of the following scenarios is more suitable:

Using ready-made GitHub models: For example, pre-trained models like Stable Diffusion. Can I import and use these models on GCP? If possible, can you share the recommended steps for this?

Google Cloud Marketplace: Would it be easier to buy a ready-made solution from GCP Marketplace? If so, what are the recommended APIs or services?

My goal:

To take inputs from user data (e.g. a string array) in the backend and return output via a text-to-image API.

Since I have an e-commerce project, I need a scalable solution for high traffic.

Information:

Backend: Requests will come via REST API.

My project allows users to create customized visuals (e.g. product designs).

Instead of training a model from scratch, I prefer ready-made solutions that will save time.

My questions:

Which way is more practical and faster? A ready-made model from GitHub or a solution from Google Cloud Marketplace?

If I prefer a model from GitHub, what steps should I follow to import these models to GCP?

How can I optimize a scalable text-to-image solution on GCP for a high-traffic application?

What platforms am I asking about:

If you have experience with Stable Diffusion or similar models, can you share them?

I would like to get suggestions from those who have started such a project on Google Cloud.",0,0,1737156112.0,MachineLearning
1i3han7,[P] Virtual Orientation session on EY Open Science AI & Data Challenge 2025,"[Join the upcoming Open Science AI & Data Challenge Virtual Orientation session on January 22nd 2025.](https://form.ey.com/243104302127945) Let's work together to cool down our cities and create healthier, more sustainable urban environments. Learn how the [2025 EY Open Science AI & Data Challenge](https://challenge.ey.com/2025) will help tackle the problem of urban heat islands through the application of AI and technology-based solutions. Winners are eligible for cash prizes and attendance at an exciting awards ceremony. [Register today!](https://challenge.ey.com/register)",0,0,1737123412.0,MachineLearning
1i31dwr,[P] I made a script to create GSM problems of any complexity.,"[Project github link](https://github.com/dattasid/grade-school-math-procedural)

[Here is a example](https://github.com/dattasid/grade-school-math-procedural/blob/main/datasets/examples/price_normal.md).

[Here is a example](https://github.com/dattasid/grade-school-math-procedural/blob/main/datasets/examples/price_easy.md) which uses simpler language, for testing if it is the confusing language that causes a model to fail.

Edit: Detailed post keeps getting removed. Please ask questions, hope someone finds this tool helpful.",15,4,1737066918.0,MachineLearning
1i2l0ey,[D] Titans: a new seminal architectural development?,"What are the initial impressions about their work? Can it be a game changer? How quickly can this be incorporated into new products?
Looking forward to the conversation!",87,51,1737018747.0,MachineLearning
1i2q6t9,[R] Multimodal Visualization-of-Thought: Enhancing MLLM Reasoning Through Visual Thinking,"The key innovation here is combining large language models with image generation to create a system that can ""visually think"" while solving problems. The approach, called Multimodal Visualization-of-Thought (MVoT), generates relevant visualizations during its reasoning process, similar to how humans might sketch diagrams to better understand a problem.

Main technical points:
- System architecture integrates LLMs for reasoning with image generation models
- Uses **spatial-semantic alignment** to ensure generated visuals match reasoning steps
- Implements an iterative process where each reasoning step can trigger visualization
- Maintains consistency between visual and textual representations through multimodal chain-of-thought

Results:
- 12% improvement on visual reasoning benchmarks compared to baseline approaches
- Particularly strong performance on tasks involving spatial relationships
- Generated visualizations showed clear alignment with reasoning steps
- Works with different combinations of language and image generation models

I think this approach could meaningfully improve AI systems' ability to reason about physical and spatial problems. By incorporating visual thinking into the reasoning process, we might see better performance on tasks that humans typically solve through visualization - from physics problems to architectural design. However, the computational overhead of generating images during reasoning could limit practical applications.

I think the most interesting aspect is how this mimics human cognitive processes - we often sketch or visualize to understand complex problems. This could lead to AI systems that reason in more intuitive and interpretable ways.

TLDR: New method combines language models with image generation to create AI systems that can ""think visually"" while reasoning, showing 12% improvement on visual reasoning tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/imagine-while-reasoning-space-multimodal-visualization-thought). Paper [here](https://arxiv.org/abs/2501.07542).",15,1,1737037918.0,MachineLearning
1i2h315,"Best way to classify NSFW text - BERT, small LLM like llama 3.2 3B or something else? [D]","I'm working on a project where I need to classify text as either nsfw or sfw. I know there are some BERT-based classifiers out there that are specifically trained for this kind of task. I've also seen people using smaller LLMs.  
What's the best approach for this? Since the underlying complexity of detecting NSFW text isn't that high, I'm thinking maybe a full blown LLM is overkill. What are your recommendations?",83,39,1737002280.0,MachineLearning
1i2nu5q,CIFAR 100 with MLP mixer. [P],Recently took part in a hackathon where was tasked with achieving a high accuracy without using Convolution and transformer models. Even though mlp mixers can be argued being similar to convolution they were allowed. Even after a lot of tries i could not take the accuracy above 60percent. Is there a way to do it either with mlp or with anything else to reach somewhere near the 90s.,15,17,1737030619.0,MachineLearning
1i23zbo,[P] How I found & fixed 4 bugs in Microsoft's Phi-4 model,"Hey r/MachineLearning! Last week, Microsoft released Phi-4, a 14B open-source model that rivals OpenAI's GPT-4-o-mini. I managed to find & fix 4 bugs impacting its output quality. You might remember me previously from [fixing 8 bugs](https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/) in Google's Gemma model! :)

I'm going to walk you through how I found & fixed the bugs. Phi-4's benchmarks were amazing, however many users reported weird or just wrong outputs. Since I maintain the open-source project called '[Unsloth](https://github.com/unslothai/unsloth)' (fine-tuning LLMs 2x faster with 70% less VRAM) with my brother, I firstly tested Phi-4 for inference and found many errors. Our GitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

This time, the model had no implementation issues (unlike Gemma 2) but did have problems in the model card. For my first inference run, I randomly found an extra token which is obviously incorrect (2 eos tokens is never a good idea). Also during more runs, I found there was an extra assistant prompt which is once again incorrect. And, lastly, from past experience with Unsloth's bug fixes, I already knew fine-tuning was wrong when I read the code.

These bugs caused Phi-4 to have some drop in accuracy and also broke fine-tuning runs. Our fixes are now [under review by Microsoft](https://huggingface.co/microsoft/phi-4/discussions/21) to be officially added to Hugging Face. We uploaded the fixed versions to [https://huggingface.co/unsloth/phi-4-GGUF](https://huggingface.co/unsloth/phi-4-GGUF)

Here’s a breakdown of the bugs and their fixes:

***1. Tokenizer bug fixes***

The Phi-4 tokenizer interestingly uses <|endoftext|> as the BOS (beginning of sentence), EOS (end of sentence) and PAD (padding) tokens. The main issue is the EOS token is wrong - it should be <|im\_end|>. Otherwise, you will get <|im\_end|><|endoftext|> in generations.

***2. Fine-tuning bug fixes***

The padding token should be a designated pad token like in Llama (<|finetune\_right\_pad\_id|>) or we can use an untrained token - for example we use <|dummy\_87|>, fixing infinite generations and outputs.

***3. Chat template issues***

The Phi-4 tokenizer always adds an assistant prompt - it should only do this if prompted by add\_generation\_prompt. Most LLM serving libraries expect non auto assistant additions, and this might cause issues during serving.

**We dive deeper into the bugs in our blog:** [**https://unsloth.ai/blog/phi4**](https://unsloth.ai/blog/phi4)

# Do our Fixes Work?

Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4).

https://preview.redd.it/d8hew26e06ce1.png?width=2366&format=png&auto=webp&s=173c23feacc625566271470839fe7a5e25eb860e

Some redditors even tested our fixes to show greatly improved results in:

* Example 1: [Multiple-choice tasks](https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/)

https://preview.redd.it/qx50pkq706ce1.png?width=1579&format=png&auto=webp&s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316

* Example 2: [ASCII art generation](https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/)

https://preview.redd.it/sw1o3a3yt4de1.png?width=2326&format=png&auto=webp&s=fc6bfc45d14134d45f332ba58bbd1de049f5776b

We also made a Colab notebook fine-tune Phi-4 completely for free using Google's free Tesla T4 (16GB) GPUs: [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi\_4-Conversational.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)

Thank you for reading this long post and hope you all found this insightful! If you have any questions, please feel free to ask! :)

**How I found the bugs:**

1. I first downloaded the original Phi-4 from [https://huggingface.co/microsoft/phi-4](https://huggingface.co/microsoft/phi-4), and tested inference out. Weirdly I found `<|im_start|>assistant<|im_sep|>` to be appended at the even with `add_generation_prompt = False` in Hugging Face, so I theorized there was a chat template problem. Adding assistant prompts by default can break serving libraries.
2. And yes, [https://huggingface.co/microsoft/phi-4/blob/f957856cd926f9d681b14153374d755dd97e45ed/tokenizer\_config.json#L774](https://huggingface.co/microsoft/phi-4/blob/f957856cd926f9d681b14153374d755dd97e45ed/tokenizer_config.json#L774) had by default added the assistant prompt - I first fixed this!
3. I then found `<|endoftext|>` to be used for the BOS, EOS and PAD tokens, which is a common issue amongst models - I ignored the BOS, since Phi-4 did not have one anyways, but changed the PAD token to `<|dummy_87|>`. You can select any of the tokens since they're empty and not trained. This counteracts issues of infinite generations during finetuning.
4. For Llama-fication, I used torch.allclose to confirm all tensors are in fact equivalent. I also used some fake random data to check all activations are also mostly similar bitwise. I also uploaded the model to the HF [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4) to confirm if the original Phi-4 arch and the new Llama-fied models are equivalent.
5. Finally I verified all finetuning runs with Unsloth in a [Colab Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb) to confirm all runs were correct.

",307,28,1736965368.0,MachineLearning
1i2qmdf,"[R] Virgo: A Preliminary Exploration on
Reproducing o1-like MLLM","**TL;DR:** A reasoning multimodal model built from Qwen2-VL-72B. Surprisingly, beats QVQ in evals.

**Paper:** [https://arxiv.org/pdf/2501.01904](https://arxiv.org/pdf/2501.01904)

**Abstract:**

>Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.  
To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at [this https URL](https://github.com/RUCAIBox/Virgo).

**Highlights:**

>\[W\]e obtain approximately 5K long thought instruction instances distilled from two open slow-thinking reasoning systems: DeepSeek-R1-Lite-Preview \[2\] (abbreviated as R1) and QwQ-32B-preview \[3\] (abbreviated as QwQ). The statistics of the collected instruction data are categorized by domain as follows: math (3.7K), science (0.9K), code (0.2K) and puzzle (0.1K). \[...\]

>After collecting instruction data for long-form reasoning, we fine-tune the base MLLM to emulate slow-thinking reasoning behavior. \[...\]

>The second approach we explore is the direct distillation of multimodal long thought data from slow-thinking MLLMs (e.g., QVQ). \[...\]

>As another alternative approach, we design a multi-stage tuning method for self-distillation. Specifically, we first fine-tune the selected MLLM (i.e., Qwen2-VL-72B-Instruct) on the textual long thought instruction set DT, obtaining model M0. Next, we use M0 to generate the visual long thought instruction set by self-distillation DSD, which can be subsequently used for fine-tuning the original MLLM.

**Visual Highlights:**

https://preview.redd.it/2o4v0y2tcdde1.png?width=1123&format=png&auto=webp&s=b3ac8bbb69248d8c326a73553cbccb0c5ab12d46

https://preview.redd.it/j3wyeg0ucdde1.png?width=967&format=png&auto=webp&s=98b2ca3dedb6c99c078491e67e53df6343f074c3

https://preview.redd.it/m6385vuucdde1.png?width=1191&format=png&auto=webp&s=4447b132fb7484d1375403916c3dcd8a9c2b8e17

[Looks a bit chaotic, if you ask me](https://preview.redd.it/q2dgxrlvcdde1.png?width=929&format=png&auto=webp&s=0f8ec5a05dc7feee71ebe57cb0ae2cb18dfd2051)

",5,0,1737039166.0,MachineLearning
1i2n50v,[D] Best Text-to-Sound-Effects model (MIT license or equivalent),"Hi there !
I've been looking around for a MIT (commercially available) model for Text-to-Sound-Effects (Text-to-Audio) and haven't found much, besides the traditional stable-Audio-Open (with its special license)

Do you know any other ?
",9,3,1737027976.0,MachineLearning
1i389km,[D] How to analysis memory and computation cost by parts in LLM fine-tune?,"[Flash-attention paper](https://arxiv.org/pdf/2205.14135) show that “most operation in Transformers are botlenecked by memory accesses”

[Cut cross entropy](https://arxiv.org/abs/2411.09009) show that “The cross-entropy loss is responsible for up to 90% of the memory footprint of modern LLM training”


How to get these data, is there a tool or platform which can show the cost by parts in LLM, like embedding, attention, layer normalazation, loss computation?

Purpose: After know that, we will know which part we should accelerate at first and can pay more attention on it. 

Thanks for any suggestion",0,2,1737087716.0,MachineLearning
1i2lk5n,[P] AutoResearch: A new open-source LLM-driven research automation tool,"Hello, everyone

I recently developed a new open-source LLM-driven research automation tool, called AutoResearch. It can automatically conduct various tasks related to machine learning research, the key function is:

[Topic-to-Survey Automation](https://jlx0.github.io/auto_research/_examples_gallery/top_to_survey.html) \- In one sentence, **it converts a topic or research question into a comprehensive survey of relevant papers.** It generates keywords, retrieves articles for each keyword, merges duplicate articles, ranks articles based on their [impacts](https://jlx0.github.io/auto_research/target_code/auto_research.search.core.html#auto_research.search.core.AutoSearch.score_threshold), summarizes the articles from the topic, method, to results, and optionally checks code availability. It also organizes and zips results for easy access.

When searching for research papers, the results from a search engine can vary significantly depending on the specific keywords used, even if those keywords are conceptually similar. For instance, searching for ""LLMs"" versus ""Large Language Models"" may yield different sets of papers. Additionally, when experimenting with new keywords, it can be challenging to remember whether a particular paper has already been checked. Furthermore, the process of downloading papers and organizing them with appropriate filenames can be tedious and time-consuming.

This tool streamlines the entire process by automating several key tasks. It suggests multiple related keywords to ensure comprehensive coverage of the topic, merges duplicate results to avoid redundancy, and automatically names downloaded files using the paper titles for easy reference. Moreover, it leverages LLMs to generate summaries of each paper, saving researchers valuable time and effort in uploading it to ChatGPT and then conversing with it in a repetitive process.

Additionally, there are some basic functionalities:

* [Automated Paper Search](https://jlx0.github.io/auto_research/_examples_gallery/search_papers.html) \- Search for academic papers using keywords and retrieve metadata from Google Scholar, Semantic Scholar, and arXiv. Organize results by relevance or date, apply filters, and save articles to a specified folder.
* [Paper Summarization](https://jlx0.github.io/auto_research/_examples_gallery/summarize_a_paper.html) \- Summarize individual papers or all papers in a folder. Extract key sections (abstract, introduction, discussion, conclusion) and generate summaries using GPT models. Track and display the total cost of summarization.
* [Explain a Paper with LLMs](https://jlx0.github.io/auto_research/_examples_gallery/explain_a_paper.html) \- Interactively explain concepts, methodologies, or results from a selected paper using LLMs. Supports user queries and detailed explanations of specific sections.
* [Code Availability Check](https://jlx0.github.io/auto_research/_examples_gallery/get_github_link.html) \- Check for GitHub links in papers and validate their availability.

This tool is still under active development, I will add much more functionalities later on.

I know there are many existing tools for it. But here are the **key distinctions and advantages** of the tool:

* [Free and open-source](https://github.com/JLX0/auto_research/)
* Pure Python code-base, which enables convenient deployment, such as [Google Colab notebook](https://colab.research.google.com/drive/1Xj0xTpHvpnPfmK9tYnI8Ep7oRKrQ9gn7?usp=sharing)
* [API documentation](https://jlx0.github.io/auto_research/target_code/auto_research.html#module-auto_research) are available
* No additional API keys besides LLM API keys are required (No API keys, such as Semantic Scholar keys, are needed for literature search and downloading papers)
* Support multiple search keywords.
* Rank the papers based on their [impacts](https://jlx0.github.io/auto_research/target_code/auto_research.search.core.html#auto_research.search.core.AutoSearch.score_threshold), and consider the most important papers first.
* Fast literature search process. It only takes about 3 seconds to automatically download a paper.

**------Here is a quick installation-free** [**Google Colab demo**](https://colab.research.google.com/drive/1Xj0xTpHvpnPfmK9tYnI8Ep7oRKrQ9gn7?usp=sharing)**------**

Here is the [official website of AutoResearch](https://jlx0.github.io/auto_research/).

Here is the [GitHub link to AutoResearch](https://github.com/JLX0/auto_research/).

**------Please star the** [**repository**](https://github.com/JLX0/auto_research/) **and share it if you like the tool!------**

Please DM me or reply in the post if you are interested in collaborating to develop this project!",6,7,1737021315.0,MachineLearning
1i33y8v,[R] Seeking a Knowledgeable Co-Author for Time Series Foundation Models Research,"Hello,

I am conducting research that I plan to submit to the AHLI Conference on Health, Inference, and Learning (CHIL) (H5-index 26, h5-median 43). However, the submission deadline is approaching quickly—February 10.

My advisor has suggested adding other professors as co-authors, but they would primarily review and provide feedback rather than directly contributing to the writing. Therefore, I am reaching out to see if anyone with expertise in time series foundation models would be interested in collaborating as a co-author.

The research involves comparing time series foundation models across different datasets. The experiments are nearly complete, but I need support in writing the theoretical foundation for each model. If you have the necessary knowledge, time, and interest in contributing meaningfully to this work, please send me a private message so we can discuss this opportunity further.

Thank you!",0,3,1737074146.0,MachineLearning
1i210hp,"Kaggle dataset: one of the input features has a >0.99 correlation with the target, yet most/all notebooks (20+) do not care? [D]","There is this dataset (won't link here as I don't want my kaggle and reddit associated) with a few input features (5-6) used to predict one target value.

But one of the features is basically perfectly linearly correlated with the target (>0.99).

An example would be data from a trucking company with a single model of trucks:

Target: truck fuel consumption / year Features: driver's age, tires type, truck age, DISTANCE TRAVELED / year

Obviously in average the fuel consumption will be linearly proportional with the nb of miles traveled. I mean normally you'd just use that to calculate a new target like fuel/distance.

Yet not a single person/notebook did this kind of normalization. So everyone's model has >.99 accuracy, as that one feature drowns out everything else.

Is that something other people noticed: more and more the code looks fine (Data loading, training many types of models), maybe thanks to LLMs. But the decision making process is often quite bad?",97,26,1736957850.0,MachineLearning
1i2mcy1,"Good People of this reddit who worked with multiple adapters on the same model, guide me with your wisdom [D]",How do you deal with multiple adapters created for different tasks? I understand task id based dynamic loading of the appropriate adapter is obvious but is there a better way? I am especially asking for whisper,3,1,1737024846.0,MachineLearning
1i2t0gj,[D] Platform for Multimodal Dataset Upload?,"What do you guys use to upload Multimodal Dataset?

I want it to be convenient for the people who use it. For the text, huggingface dataset is the best convenient solution, but I cant find any convenient solution for Multimodal (Image + Video + Audio + Text) datast.

Thanks in advance.",0,2,1737045440.0,MachineLearning
1i351hc,"[D] How often are you babysitting your models? 🤔
","Hey yall! I'm curious, how often are you kicking off a new training runs?   
Once a week? Twice a week? Everyday?   
Would love to hear about your experience! ",0,7,1737077393.0,MachineLearning
1i279gb,[R] Imagine while Reasoning in Space: Multimodal Visualization-of-thought,"Abstract:

Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high- quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.

Arxiv link: https://arxiv.org/pdf/2501.07542",32,0,1736973785.0,MachineLearning
1i1xgp2,[R] MathReader: A Text-to-Speech System for Mathematical Documents Using OCR and Fine-tuned T5,"Interesting new text-to-speech system that tackles mathematical content by combining OCR and language models. The key innovation is treating mathematical notation as a specialized language that needs translation, using a multi-stage pipeline to convert equations into natural speech.

Technical approach:
* Custom OCR model trained specifically on mathematical documents
* T5-based language model fine-tuned for math-to-text translation
* Three-stage pipeline: recognition → translation → synthesis
* Integration with LaTeX parsing for handling complex mathematical typography

Key results:
* 95% accuracy in mathematical expression recognition
* Successful handling of complex notation including fractions, integrals, matrices
* User testing showed preference over existing math TTS systems
* Natural language output matches human descriptions

I think this could be impactful for making technical education more accessible. Being able to convert mathematical documents to clear speech opens up some possibilities for learning and working with technical content. The combination of OCR and NLP seems maybe like a robust approach that could extend beyond just mathematics to other technical domains with specialized notation.

I see some limitations around context-dependent notation and complex proofs, but these seem like natural areas for future work rather than fundamental flaws in the approach.

TLDR: New TTS system combines specialized OCR and language models to convert mathematical documents to natural speech, achieving 95% accuracy in math recognition and producing human-like descriptions.

[Full summary is here](https://aimodels.fyi/papers/arxiv/mathreader-text-to-speech-mathematical-documents). Paper [here](https://arxiv.org/abs/2501.07088).",18,0,1736947990.0,MachineLearning
1i1l8d4,[R] Transformer²: Self-Adaptive LLMs,"Paper: https://arxiv.org/abs/2501.06252

**Abstract**

Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer², a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer² employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific ""expert"" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer² demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer² represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. 

Blog Summary: https://sakana.ai/transformer-squared/

GitHub: https://github.com/SakanaAI/self-adaptive-llms",186,13,1736901669.0,MachineLearning
1i26qvt,Best AutoML for Supervised Regression of Tabular Data [P],"I have a simple dataset that I want to train a prediction model on for a pretty low stakes project (more for fun), but I have no experience training ML models. Simple linear regression didn't have great performance when I tried it and I suspect there is a more complex interaction between the variables.

Training Dataset: 25K observations of 5 numerical predictor variables with one 1 numerical outcome variable.

What is the best AutoML platform that I can run this with minimal code, just to see if ML models can perform better than simple regression can? Thanks!",3,4,1736972414.0,MachineLearning
1i21t17,[R] Explainable GNNs in Job Recommender Systems: Tackling Multi-Stakeholder Challenges,"Can explainable AI balance competing needs in job recommendation systems? Models like OKRA, powered by GNNs, deliver stakeholder-specific insights - text explanations for candidates, skill alignment for recruiters, and visualizations for companies. They address biases (e.g. rural underrepresentation) and challenges like integrating explanations with source data (CVs, vacancies). 

Future directions focus on refining explanation coherence, fairness metrics, and real-world validation, pushing explainable multi-stakeholder AI towards equitable, context-aware job matching.

We unpack *""Explainable Multi-Stakeholder Job Recommender Systems""* by *Roan Schellingerhout* here: [https://www.shaped.ai/blog/decoding-job-recommendations-the-future-of-explainable-multi-stakeholder-ai](https://www.shaped.ai/blog/decoding-job-recommendations-the-future-of-explainable-multi-stakeholder-ai)",4,0,1736959904.0,MachineLearning
1i236iq,[D] Dilemma b/w JAIR vs Pattern Recognition Journal,"Dear All, 

I am a UG student and I want to submit my manuscript to any of these two journals; the work is on the interplay of privacy and explainability in machine learning (would be more than happy to send you the arXived version of the same on request). I have previously published in a very reputed workshop of EMNLP and came to know that mostly ML nowadays is a conference-centric discipline. I want to know which of these two will be better to submit my work (due to the length and scope, I am unable to submit to conferences this time). I cannot submit it to tmlr until it's Scopus-indexed and not considering AIJ and Machine Learning Journal at this moment. 

I just want to make sure that if the paper gets accepted, I want this to be at least comparable with a borderline A\* paper (in terms of the so-called *prestige* of the venue). Also, let me know if you have any other suggestions; I am new to journals and I appreciate your opinion.

P.S.: My guide slightly prefers PR to JAIR due to its higher IF but nevertheless, he is open JAIR or any other Scopus-indexed journals as long as it is comparable with at least a borderline A\* or very strong A conf paper as said. ",3,10,1736963354.0,MachineLearning
1i1lg6o,[R] Titans: Learning to Memorize at Test Time,"Abstract: “Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps an attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of a fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.”

Arxiv: https://arxiv.org/abs/2501.00663",66,4,1736902312.0,MachineLearning
1i1hz8c,[D] How are people searching for papers in ArXiv? ,"Hello,

I am wondering what is the usual way people search for or discover new papers in ArXiv? Do you just use their search engine? Any tips/hints?",47,29,1736892699.0,MachineLearning
1i18421,[D] Machine Learning Engineer vs AI Research Scientist: Future Prospects?,"Some people say that AI research scientists (PhD holders) are pretty much irreplaceable because of their ability to push the boundaries of knowledge and come up with groundbreaking methods and algorithms. But let’s be real—tech companies don’t need a ton of researchers, especially if their work doesn’t directly boost profits.

On the flip side, Machine Learning Engineers are the ones putting those algorithms into action, scaling systems, and keeping production pipelines running—all things that directly bring in the $$$. That’s why some people think MLE roles will grow faster than AI research scientist roles in the future.

What do you think? Are there trends or experiences you’ve seen that suggest one of these roles will be more in demand down the line?

For a fair comparison, let’s assume both roles are at a FAANG company.",129,72,1736867312.0,MachineLearning
1i22zdu,What percent of your paper is written by AI and is it unethical?[D],"Hello folks,

Just started contributing into the writing for research, previously I just used to experiment and work on results, tables and plots.

Obviously using AI to generate content for paper is unethical and wrong in many aspect. But what about using it to correct your grammar and comprehension. Technically it will also considered as AI written but is it okay to do this atleast in the literature review, introduction and description for the experiment?

To be honest, I like writing and when I asked AI (chatgpt and others) I see that it is much easier to read and interpret, which I think is good for the community and on the other hand, it may be considered unethical by many.

When I ran a 'AI-text detector' on many of paper I'm using as reference from last 1~ year, I usually get a 50-70% score. 

What do you all think?

",0,28,1736962864.0,MachineLearning
1i1gr9y,[D]How do you measure improvements of your AI pipeline?,"I am very creative when it comes to adding improvements to my embedding or inference workflows, but  I am having problems when it comes to measuring whether those improvements really make the end result better for my use case. It always comes down to gut feeling.

How do you all measure...

..if this new embedding model if better than the previous?

..if this semantic chunker is better than a split based one?

..if shorter chunks are better than longer ones?

..if this new reranker really makes a difference?

..if this new agentic evaluator workflow creates better results?

**Is there a scientific way to measure this?**",4,5,1736889549.0,MachineLearning
1i1act4,Pre-trained models for Sentiment classification and analysis [Discussion],"Hi. I am working on a project which requires me to identify sentiments from English text and then quantify those sentiments as percentage. I need to run six models on the text and then compare the classifications.

So far, I have explored some BERT and RoBERTa based models in Huggingface, which are trained using the GoEmotion dataset provided by Google. I was curios, are there any better models that I am missing? Please leave the name of some pre-trained models which can give some good results.

TIA!",5,1,1736873202.0,MachineLearning
1i0hfsd,[R] Cosine Similarity Isn't the Silver Bullet We Thought It Was,"Netflix and Cornell University researchers have exposed significant flaws in cosine similarity. Their study reveals that regularization in linear matrix factorization models introduces arbitrary scaling, leading to unreliable or meaningless cosine similarity results. These issues stem from the flexibility of embedding rescaling, affecting downstream tasks like recommendation systems. The research highlights the need for alternatives, such as Euclidean distance, dot products, or normalization techniques, and suggests task-specific evaluations to ensure robustness. 

Read the full paper review of 'Is Cosine-Similarity of Embeddings Really About Similarity?' here: [https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was](https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was)",440,52,1736784660.0,MachineLearning
1i194ja,[D] Non-Person Action Recognition,"I am working on getting object tracking working for a sports game, and would like to take the next step and be able to detect when an action has taken place (like a soccer ball has gone out of bounds, of a bowling ball has hit pins, or a ball has been thrown (as opposed to a practice throw or pump fake).  I have been doing these by hand coding heuristics for how to detect, but I would like to be more flexible.  All the libraries for action recognition seem to be about human skeleton actions.  That makes me think I am looking at the wrong problem space.  Is there existing art for taking locations of objects over time and learning when an action is taking place given training data?",2,1,1736870033.0,MachineLearning
1i1aarq,[D] Predicting the probability of default for a credit card user,"I have an imbalanced dataset of about 100,000 rows 1500 of them are of defaultes, which has more than 1000 features and has lots of missing values. Alsothe name of the features are anonymized (like bureau\_1, bureau\_2)  so it also seems difficult and these feaures had max correlation of 0.1 with the target variable

I want to predict the probability of a customer who might default based on the data but am not able to make much progress in terms of metrics like recall (0.25), f1 and auprc.  
I have tried various tree based models like lgbm, xgboost etc with various class balance attributes but its not giving me that good of results.

If anyone of you have such prior experience of handling such datasets, can you suggest me what should i do in terms of feature engineering, modelling etc. All of your help will mean a lot to me.",0,15,1736873056.0,MachineLearning
1i13aot,[D]  Correlation clustering?,"I wanted to apply clustering algorithms on a similarity matrix. Is that possible? If yes, how?",3,8,1736851054.0,MachineLearning
1i0vrg3,LLM Distributed Training [R],"What are the approaches to access datasets during training? Are they downloaded to the machines/pods before starting the training process or are they network mounted?

Similarly for large models how do the models are deployed for inference? ( for auto scaling or for updating the model version)",7,5,1736821786.0,MachineLearning
1i1fwsg,[D] Prove me wrong…,"I’ve built an LGBM model to classify Parkinson’s patients using a dataset of 2,562 patients with 37 features selected through P value and correlation analysis and my own domain knowledge, questions can be binary, continuous or ordinal eg do they have Urinary Problems yes/no = 0/1, all questions are numerical answers.  The dataset was split into 70% training (1,793 samples), 15% validation (384 samples), and 15% hold-out test (385 samples). I performed 5-fold stratified cross-validation on the training set, with approximately 1,434 samples for training and 359 for validation in each fold. The dataset contains 1085 PD patients and 1477 non-PD patients. I think the performance is really good, I'm wondering if anyone has any additional tests or methods to assess whether it's a big fantasy or have I a good model on my hands?

.=== Cross-Validation Metrics ===

Mean F1 Score: 0.8860 ± 0.0210

Mean AUC: 0.9565 ± 0.0095

Mean Recall: 0.8814 ± 0.0239

Mean Precision: 0.8911 ± 0.0251 

=== Hold-Out Set Metrics ===

F1 Score: 0.8875

AUC: 0.9505

Recall: 0.8957

Precision: 0.8795",0,33,1736887221.0,MachineLearning
1i0u5sv,NannyML chunking [D],"Does anyone have experience with the NannyML library? I am having a difficult time fully grasping the reasoning behind forcing users to split data into chunks. I haven’t seen any other drift detection libraries do this. 

Let’s say I have a model on which I would like to perform drift detection. I have some reference feature data from some time ago, and some analysis feature data from today. It seems that to use this library, I am required to split these 2 datasets into arbitrary chunks (they recommend at least 6). I would actually like to perform drift detection by comparing both sets of data to each other as a whole, however. This doesn’t work - forcing the chunk size to 1 results in the upper_threshold value to be set to 0 and every feature gets alerted on.

It seems like the library is geared towards comparing some number of reference datasets  across time vs some equal number of analysis datasets across time… but doesn’t work if there is only have 1 analysis dataset (for 1 date). What am I missing here? Any help much appreciated!",6,2,1736817040.0,MachineLearning
1i0g71d,[Project] Hallucination Detection Benchmarks,"Hi Everyone, I recently noticed most LLM observability providers (Arize AI, Galileo AI, LangSmith) use a simple LLM-as-a-Judge framework to detect hallucinations for deployed RAG applications. There's a ton of hallucination detection research out there like [this](https://arxiv.org/abs/2311.05232) or [this](https://arxiv.org/abs/2403.16527) survey, so I wondered why aren't any of these providers offering more advanced research-backed methods? Given the user input query, retrieved context, and LLM output, one can pass this data to another LLM to evaluate whether the output is grounded in the context. So I benchmarked this LLM-as-a-Judge framework against a couple of research methods on the HaluBench dataset - and turns out they're probably right! A strong base model with chain-of-thought prompting seems to work better than various research methods. [Code here](https://github.com/liuzihe02/halu/tree/main). Partial results:

|Framework|Accuracy|F1 Score|Precision|Recall|
|:-|:-|:-|:-|:-|
|Base (GPT-4o)|0.754|0.760|0.742|0.778|
|Base (GPT-4o-mini)|0.717|0.734|0.692|0.781|
|Base (GPT-4o, sampling)|0.765|0.766|0.762|0.770|
|CoT (GPT-4o)|**0.833**|**0.831**|**0.840**|0.822|
|CoT (GPT-4o, sampling)|0.823|0.820|0.833|0.808|
|Fewshot (GPT-4o)|0.737|0.773|0.680|**0.896**|
|Lynx|0.766|0.780|0.728|0.840|
|RAGAS Faithfulness (GPT-4o)|0.660|0.684|0.639|0.736|
|RAGAS Faithfulness (HHEM)|0.588|0.644|0.567|0.744|
|G-Eval Hallucination (GPT-4o)|0.686|0.623|0.783|0.517|",27,12,1736781378.0,MachineLearning
1i0ju9b,[P] Geometric Intuition for Dot Product,"Hi Community,

First, I want to thank you for reading my earlier posts on geometric intuition and receiving with worms! I didn't expect to receive so much good feedback and also different explanations in the comment. I learned so much!

Motived by this, I wrote another post for geometric intuition and this time about ""**Dot Product**"". Here is the link [https://maitbayev.github.io/posts/dot-product/](https://maitbayev.github.io/posts/dot-product/)

Let me know what you think",11,1,1736790606.0,MachineLearning
1i0cd4n,[P] Fast Semantic Text Deduplication,"Hi! A friend and I have been working on a project called [SemHash](https://github.com/MinishLab/semhash) which I wanted to share. We found that text deduplication is more complex than it appears, so we built this to simplify the process.

Duplicate samples can skew model training, return redundant samples in RAG workflows, reduce generalization, and cause train-test leakage—leading to unreliable results. Techniques like minhash handle exact or near-exact duplicates, but semantic deduplication also catches semantically redundant samples, which we believe is an important aspect of deduplication. Furthermore, it’s not trivial to see why something was removed with minhash, which we also believe is important. For this reason. we’ve added explainability features as well so that you can inspect why something was removed. We already found some interesting results on some well known datasets in our benchmarks which are included in the repo.

The package can be installed with `pip install semhash`, and the basic usage looks like this (this example assumes you have the `datasets` library installed):

    from datasets import load_dataset
    from semhash import SemHash
    
    # Load a dataset to deduplicate
    train = load_dataset(""ag_news"", split=""train"")[""text""]
    test = load_dataset(""ag_news"", split=""test"")[""text""]
    
    # Initialize a SemHash instance
    semhash = SemHash.from_records(records=train)
    
    # Deduplicate the train set
    deduplicated_train = semhash.self_deduplicate().deduplicated
    
    # Or deduplicate the test set against the train set
    deduplicated_test = semhash.deduplicate(records=test).deduplicated

I’m very interested in hearing your thoughts on this! Is deduplication a part of your current ML workflows, and if so, what techniques do you use?",21,3,1736769562.0,MachineLearning
1i16ud7,[D] How to convince the stakeholders that our ML solutions is good enough? ,"Over the past year, we developed a solution designed to be a companion for data analysts, helping them manage and analyze their data. However, I’m struggling to demonstrate its reliability, as it occasionally fails to function properly.",0,11,1736863713.0,MachineLearning
1i162tn,[P] What is RF and How to Implement it?,"If you're building an LLM application that handles complex or ambiguous user queries and find that response quality is inconsistent, you should try **RAG Fusion**!

The standard RAG works well for straightforward queries: retrieve ***k*** documents for each query, construct a prompt, and generate a response. But for complex or ambiguous queries, this approach often falls short:

* Documents fetched may not fully address the nuances of the query.
* The information might be scattered or insufficient to provide a good response.

This is where **RAG Fusion** could be useful! Here’s how it works:

1. **Breaks Down Complex Queries:** It generates multiple sub-queries to cover different aspects of the user's input.
2. **Retrieves Smarter:** Fetches *k*\-relevant documents for each sub-query to ensure comprehensive coverage.
3. **Ranks for Relevance:** Uses a method called **Reciprocal Rank Fusion** to score and reorder documents based on their overall relevance.
4. **Optimizes the Prompt:** Selects the top-ranked documents to construct a prompt that leads to more accurate and contextually rich responses.

We wrote a detailed blog about this and published a Colab notebook that you can use to implement RAG Fusion - **Link in comments!**",0,4,1736861424.0,MachineLearning
1i16f1c,[R] Mastering Machine Learning System Design: A Comprehensive Guide for Scalable AI Solutions,"**Key Highlights**

1. **What to Expect in ML Interviews**

• Problem-solving, system design, and hands-on ML experience.

• Real-world examples from top tech companies like Google and LinkedIn.

2. **Why ML System Design Matters**

• Addresses **scalability**, **reliability**, and **optimization** for millions of users.

• Explores scenarios like LinkedIn’s Feed Ranking and YouTube’s Recommendation System.

3. **Step-by-Step Guide to ML System Design**

• **Define the Problem Statement**: Clarify goals and assumptions.

• **Identify Metrics**: Choose relevant metrics (e.g., AUC, CTR).

• **Determine Requirements**: Training and inference needs.

• **Design High-Level Systems**: Outline components and data flow.

• **Scale the Design**: Optimize for bottlenecks and high traffic.

4. **Real-World Example**: YouTube Recommendation System

• Candidate Generation Service, Ranking Model, and Recommendation API.



**Key Takeaways**

• **Modular Design**: Ensure components can scale or be replaced independently.

• **Real-Time Inference**: Build low-latency systems (<100ms).

• **Bottleneck Identification**: Proactively address system limitations.

• **Monitoring & Maintenance**: Automate model drift detection and retraining.

[🔗 Machine Learning System Design Introduction](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)🔗 [**Machine Learning System Design Introduction**](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)

This article is a must-read for mastering ML system design and preparing for interviews at top tech firms.",0,1,1736862461.0,MachineLearning
1hzshvp,[P] I made pkld – a cache for expensive/slow Python functions that persists across runs of your code,,131,41,1736704664.0,MachineLearning
1i0nmax,"[D] In ""Speculations on Test-Time Scaling (o1)"", shouldn't this equation be E_(y~p(·|,z_(1:t),x))[Ver(y)]? Adding z_(1:t) into the expectation value equation's subscript. Because it depends on it.","In ""Speculations on Test-Time Scaling (o1)"" https://youtu.be/6PEJ96k1kiw?si=-bA2KTKbc0kPJqYX&t=1085 , in the context of https://imgur.com/2t94rWF , shouldn't the equation in https://imgur.com/6AODoeq be E\_(y~p(·|,z\_(1:t),x))[Ver(y)]? Adding z\_(1:t) into the expectation value equation's subscript. Because it depends on it.",1,0,1736799841.0,MachineLearning
1i0et7v,[D] Anisotropic periodic kernel in Python with Sklearn,"Hello,

I am using sklearn in Python to perform Gaussian Process Regression (GPR) on some ocean variables through the GaussianProcessRegressor class. The domain of the parameters is a 3D spacetime domain (latitude, longitude, and time), so I am using an anisotropic kernel for the regression since the three dimensions are quite different. For example:

\# Define the kernel kernel = C(1.0, (1e-3, 1e3)) \* Matern( nu=1.5, length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=\[(1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)\] )

\# Initialize the GPR

gpr = GaussianProcessRegressor(kernel=kernel, n\_restarts\_optimizer=5, alpha=alpha)

Watching the results at a specific location in time (fixed latitude and longitude, looking at the time series) of the predicted versus the real values, I think adding a periodic kernel in time may improve the results. This assumption makes sense as the parameters could exhibit time periodicity (e.g., wind speed).

I tried implementing this using an ExpSineSquared kernel, but it doesn't allow for anisotropy (I was thinking of setting it with very high bounds for periodicity in latitude and longitude so that it would effectively be neglected). However, the documentation states that the function does not support different length scales and periodicity for different dimensions.

Here is an example of what I tried:

\# Define the Matern kernel

matern\_3d = Matern( length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), nu=1.5 )

\# Define the ExpSineSquared kernel

expsine\_3d = ExpSineSquared( length\_scale=\[1.0, 1.0, 1.0\], periodicity=\[1e6, 1e6, 24.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), periodicity\_bounds=((1e5, 1e8), (1e5, 1e8), (12.0, 48.0)) )

\# Combine the kernels

kernel = (C(1.0, (1e-3, 1e3)) \* matern\_3d) + (C(1.0, (1e-3, 1e3)) \* expsine\_3d)

However, this results in an error since ExpSineSquared does not support different length scales and periodicities across dimensions. Has anyone encountered this problem before? Do you know of another function or library that could allow this kind of anisotropic periodic kernel? Thanks in advance!",3,0,1736777492.0,MachineLearning
1hzn0gg,[D] Have transformers won in Computer Vision?,"Hi,

Transformers have reigned supreme in Natural Language Processing applications, both written and spoken, since BERT and GPT-1 came out in 2018.

For Computer Vision, last I checked it was starting to gain momentum in 2020 with [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) but the sentiment then was ""Yeah transformers might be good for CV, for now I'll keep using my resnets""

Has this changed in 2025? Are Vision Transformers the preferred backbone for Computer Visions?

Put another way, if you were to start a new project from scratch to do image classification (medical diagnosis, etc), how would you approach it in terms of architecture and training objective?

I'm mainly an NLP guy so pardon my lack of exposure to CV problems in industry.  ",185,82,1736689650.0,MachineLearning
1hzyjw1,[R] Search-o1: Agentic Search-Enhanced Large Reasoning Models - Renmin University of China,,36,4,1736720043.0,MachineLearning
1hzychc,"[R] optimizing looser bounds on train data, achieves better generalization","I have encountered times that when optimizing with looser bounds, one can get better performance on test data. For example, in this paper:

[https://arxiv.org/pdf/2005.07186](https://arxiv.org/pdf/2005.07186)

authors state: ""It seems that, at least for misspecified models such as overparametrized neural networks, training a looser bound on the log-likelihood leads to improved predictive performance. We conjecture that this might simply be a case of ease of optimization allowing the model to explore more distinct modes throughout the training procedure.""

more details can be found below eq 14 in the appendix.

are there other problems where one has drawn a similar observation?

thanks!",19,3,1736719487.0,MachineLearning
1i0d26d,[P] Is it viable to use customer-declared information as proxy ML labels?,"**CONTEXT:**

Sort of a high-level hypothethical ML training data question: Let's say a company has adult customers and child customers. 90% of customers are adults, and 10% of them are children.\*

The problem is that whether a customer is an adult or child is declared by the customer, the company has no way of knowing the truth. Some children pretend to be adults, as it benefits them, but no adults pretend to be children. Thus the company wants to use ML to find the children pretending to be adults, using various other customer details as features.

**QUESTION:**

The question is, is it worth training a model with this proxy label of how they declared themselves, even though the training set will include children pretending to be adults? (Worth noting that we know that only about 1% of those declared as adults are actually children, ie. about 9% of children are pretending to be adults)

Obviously a MUCH better way to do this would be to have a labelled training set of confirmed adults and children, but there's no way of getting a labelled dataset, all we have is whether customers declared themselves as adults or children.

So what do we think? Is it a non-starter? Or might the 99% of true adults effectively drown-out the 1% of false adults, resulting in a viable model? Asuming the features and model type are otherwise apropriate.

Needless to say we're never going to get a great model, but we just need a model that will give us substantially higher than the 9% baseline, since the alternative is doing blind checks on small samples of customers. It feels wrong but I can't think of an alternative given the data at our disposal.

Would appreciate any thoughts, thanks

\*(Please ignore the fact that age is a continuous variable, the actual characteristic we're using is a binary variable)",0,9,1736772072.0,MachineLearning
1hzupbd,[D] Is a ViT with local window attention (SAM-style) not that much more efficient than a vanilla ViT with global attention in all layers? Especially at high resolution where global attention should be super expensive.,"I was reading this blog post by Lucas Beyer: [https://lucasb.eyer.be/articles/vit\_cnn\_speed.html](https://lucasb.eyer.be/articles/vit_cnn_speed.html)

When he compares ViTB/16 and the SAM variant with mostly local attention (window size 14), it was a bit surprised that throughput improvements are slight (left) and that the SAM variant requires more peak memory.

Now this is inference only, so maybe during training the difference is larger, but I naively would have thought that local attention is much faster still, especially at high resolutions.

At 1024x1024, we should have 1024/16=64x64 patches - so the global attention operation should be extremely expensive? Am I missing something?

https://preview.redd.it/es7oj0ky6mce1.png?width=1425&format=png&auto=webp&s=5241198e5bb7129eae3d79e77f3a1dd136d64c2b

",21,8,1736710224.0,MachineLearning
1hzy2ox,[D] At which floating point precision gradient descent training or inference breaks down,"We consider NNs as a ""differentiable"" model, i.e. assume that we use continuous differentiable functions. However, we use floating point representations which technically discrete. At some precision, the models start to break down. I.e. consider fp64 model. It might not work as well on fp16 precision, etc.


Could anyone point me to resources (papers) which investigate this, investigate failure modes, ways to work them around, etc.


P.S. This question is inspired by NVidia announcement, where they mentioned that Blackwell supports fp4 precision. I am now interested in how it is possible to do anything useful with such a low precision, and what is used to achieve it.",6,9,1736718824.0,MachineLearning
1hz1l2j,[P] Built a Snake game with a Diffusion model as the game engine. It runs in near real-time 🤖 It predicts next frame based on user input and current frames.,,519,32,1736618225.0,MachineLearning
1hzq0ac,[D] Cheaper alternative to modal.com?,"Are there any other good services that let you instantly spin up a docker image on an 8xH100 machine? Modal is twice the price per hour of lambda labs or voltage park, but I kind of need the quick up/down.

Update 3 days later: ori, celium, and shadeform are all real working services and all work quite well.",6,13,1736698271.0,MachineLearning
1hzsm1q,[R] FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers (https://arxiv.org/pdf/2411.14507v1),"Is this paper any good? I am having trouble grokking its essence, for instance what are blocks, group-level, etc. I was looking for a paper that talks about fusing multiple transformer blocks, but this paper doesn't seem to go into the technical implementation details.",4,3,1736704945.0,MachineLearning
1hze3vs,[P] Llama3 Inference Engine - CUDA C,"Hey r/MachineLearning, recently I took inspiration from llama.cpp, ollama, and similar tools that enable inference of LLMs locally, and I just finished building a Llama inference engine for the 8B model in CUDA C.

As part of my explorative work in building optimized GPGPU software, I decided to build this from scratch. This project only makes use of the native CUDA runtime api and cuda_fp16. The inference takes place in fp16, so it requires around 17-18GB of VRAM (~16GB for model params and some more for intermediary caches). 

It doesn’t use cuBLAS or any similar libraries since I wanted to be exposed to the least amount of abstraction. Hence, it isn’t as optimized as a cuBLAS implementation or other inference engines like the ones that inspired the project.

## **A brief overview of the implementation** 

I used CUDA C. It reads a .safetensor file of the model that you can pull from HuggingFace. The actual kernels are fairly straightforward for normalizations, skip connections, RoPE, and activation functions (SiLU). 

For GEMM, I got as far as implementing tiled matrix multiplication with vectorized retrieval for each thread. The GEMM kernel is also written in such a way that the second matrix is not required to be pre-transposed while still achieving coalesced memory access to HBM.

There are some kernels like the one for RoPE and GEMM that use vectorized memory access. Parts of the SwiGLU feedforward computation takes place within a custom fused kernel.

Feel free to have a look at the project repo and try it out if you’re interested. If you like what you see, feel free to star the repo too!

I highly appreciate any feedback, good or constructive.",37,8,1736653881.0,MachineLearning
1hzprm8,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",4,22,1736697638.0,MachineLearning
1hz1xks,[N] I don't get LORA,"People keep giving me one line statements like decomposition of dW =A B, therefore vram and compute efficient, but I don't get this argument at all.

1. In order to compute dA and dB, don't you first need to compute dW then propagate them to dA and dB? At which point don't you need as much vram as required for computing dW? And more compute than back propagating the entire W?

2. During forward run: do you recompute the entire W with W= W' +A B after every step? Because how else do you compute the loss with the updated parameters?

Please no raging, I don't want to hear
1. This is too simple you should not ask
2. The question is unclear

Please just let me know what aspect is unclear instead.
Thanks",55,32,1736619107.0,MachineLearning
1hz4gdy,[P] A hard algorithmic benchmark for future reasoning models,"Hi, I've been toying with a simple idea for developing a future-proof, dynamic, AI model benchmark. The idea is pretty simple. A hidden function transforms data, and the model only gets to see the before and after, and has to deduce the hidden logic. I've carefully curated several levels of slightly increasing difficulty, and I've been surprised to see most current models I can access (GTP, o1, Sonet, Gemini) suck at it.

For instance, the first puzzle simply does \^=0x55 to the bytes on the input buffers, yet most models struggle to see it or deduce it.

I've spin up a opensource MIT repo with a live demo, so others can give this idea a try or contribute. I appreciate any feedback. Thanks!",23,16,1736625768.0,MachineLearning
1hz541n,[D] Which library is good for diffusion model research?,"I wanted to play around with diffusion models and switch out different parts of the pipeline (such as samplers, models, data modalities etc or use custom ones). I had a look at some libraries such as modular_diffusion or diffusor, but they don't seem to be very mature yet or very high-level. What kind of libraries do you use to experiment with diffusion models in your research?",9,3,1736627543.0,MachineLearning
1hz2dfp,[D] Thoughts on Google Paxml (aka Pax)?,"I just discovered [Pax](https://github.com/google/paxml), a *framework to configure and run machine learning experiments on top of Jax*. Did you know about this? It could be a better solution than Pytorch for large-scale models.",10,2,1736620254.0,MachineLearning
1hzpqgl,[D] Do I require to Overclock my RTX 4090 for AI Training Tasks?,"Hello,
I mostly run AI training and experiments on my PC and these experiments sometimes last multiple days non-stop and this machine keeps running 24/7. 
Do you think overclocking is required for my use case to get better performance? I don't want to end up bricking the GPU or end up reducing its lifespan as well. Can OC affect that?
The reason Im asking this is because my GPU is ZOTAC GAMING GeForce RTX 4090 Trinity and it has 3 fans on it. Ive noticed that for all my AI experiments the fans never go above 30% and the GPU temperature is also around 50 - 55°C. Since the GPU can handle higher temperatures and also there is the possibility of the fan going above 30%, I feel like I can possibly get more juice from GPU? What do you recommend, will it be a good idea?",0,11,1736697552.0,MachineLearning
1hznd9q,"[D] Which, in your opinion, is better for cost-saving while maintaining quality?","I have a scenario where I need to feed PDFs of text data to a Generative AI model in order to summarize and fetch only information of interest from each PDF individually. Now, I was first thinking of using the OpenAI API (GPT-4o), but I was wondering if another solution may be cheaper while also maintaining the level of quality for the text comprehension and generation:

* Install a model locally on my machine to do this.
* Install a model on a cloud server, like an EC2 instance in AWS.
* Use a different GenAI offering, like Amazon Bedrock

I don't have experience with downloading a model and using it, as I've only used APIs of popular providers before. But I want to learn how it works and whether you believe these options are realistic.",0,2,1736690749.0,MachineLearning
1hznbmr,[D] Why do we use RLFH instead of Gumbel softmax?,"My question is fairly simple. RLHF is used to fine-tune LLMs because sampled tokens are not differentiable. Why don't we just use Gumbel softmax sampling to achieve differentiable sampling and directly optimize the LLM?

The whole RLHF feel like so much overhead and I do not see why it is necessary.",0,25,1736690615.0,MachineLearning
1hzjlvy,[D] Discrepancy in no. of slices in multimodal segmentation,"Hey I’m using DTI and conventional MRI scans for my segmentation task. DTI has 60 slices, MRI has 23 slices, the segmentation mask was produced based on MRI so it has 23 slices. Any advice how do I go about doing so? There’s a discrepancy in no. of slices",0,3,1736676187.0,MachineLearning
1hz2ct5,[D] Finding optimal hyper parameter for neural network,"I have been trying to find optimal hyperparameter for LSTM model using gray wolf algorithm(GWO) and particle swarm optimizer(PSO). Its taking alot of time. Below is description for what I am doing.

I have a LSTM model wrapped in a objective function to be optimized. This function build model based on parameter passed to it, then it trains the model and find MSE on test data. This test data is returned based on which GWO optimizer will calculate fitness.

This process takes hours. Is there any other way to find optimum parameter?",5,2,1736620209.0,MachineLearning
1hzhrxb,[Discussion] Unclear problem statement,"The following is a problem statement for a use case.  
  
""The nature of fraud is dynamic and ever-changing. Finding patterns and identifying anomalies are essential in this industry. Given a set of mobile device attributes (for example, brand, model) data, design a model to find patterns or anomalies in these data. 

Take into account that not all device attributes are readily available all the time and there is no historical data available.""

There is no dataset provided, I'll have to find it myself. I was thinking of obtaining the Kaggle mobile price dataset and do some basic anomaly checks (Z-score, IQR) + isolation forest to detect fraudulent postings. However, not sure what no historical data means? I interpreted it as having no time series information + unlabelled (to be safe). ",0,4,1736668030.0,MachineLearning
1hzbtak,[R] Which Forecasting library should I be using for this task since all I've tried don't do what I need!,"Hi all,

I'm trying to forecast a single column in my dataset by using multivariate inputs: Fuel % left in car depending on current fuel %, speed, radiator temperature. I need to train a model that can approximate the fuel consumption curve in real-time, therefore it has to predict on unseen data based on what it learnt, however the libraries I've tried don't do that, instead they just train on the previous data and predict the exact next n (fh). I don't need that, I don't want the next n steps of my training data, I want the next n steps of my testing data which is unseen. I built my own pytorch model and it works well, but I need to compare it against other methods to see how to improve the model.

I tried Facebook Prohpet, Nixtla, SKTime, Pytorch Forecasting, GluonTS, but they don't seem to do what I want and/or lack one of the requirements. I've read about TSAI, Darts, Kats, but I'm afraid that I'm wasting time that I might not have testing too many libraries only to find out that they don't do what I need.

Any recommendation that I can look into that can do what I need?

tl;dr

I need a library/model that can take multivariate input to predict a univariate output for the next n steps in real time (unseen data).",0,2,1736646378.0,MachineLearning
1hyxijp,[D] Does softmax tend to result in unconstrained euclidean weight norms?,"Bit of a silly question. While I was in the middle of analyzing neural network dynamics geometrically, I realized something about softmax. When using categorical cross entropy, it results in a lower loss value for pre-softmax vectors within the output layer that have a high positive magnitudes for the correct label-axis, and high negative magnitudes for the non-correct label-axes. I know that regularization techniques keeps weight updates bounded to a degree, but I can't help thinking that softmax + cross entropy is not really a good objective for classifiers, even if the argument that it results in a probability distribution as the output so it's ""more interpretable"".

  
Just me?",6,18,1736607201.0,MachineLearning
1hyfaoc,"[Dataset][R] 19,762 Garbage Images for Building AI Recycling Solutions","Hi ML community!

I’m excited to share the **Garbage Classification V2 Dataset**, featuring **19,762 high-quality images** of garbage categorized into **10 distinct classes** (e.g., metal, plastic, clothes, and paper).

# Why this matters:

* Train AI models for **automated waste sorting and recycling**.
* Develop **waste segregation apps** or sustainability-focused tools.
* Create innovative **computer vision projects** for environmental impact.

🔗 **Dataset Link:** [Garbage Classification V2](https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2/)

This dataset has been used in the research paper, *""Managing Household Waste Through Transfer Learning,""* proving its utility in real-world applications.

Looking forward to seeing how you can use it to promote sustainability!",109,15,1736543993.0,MachineLearning
1hz7n71,[D] Image segmentation with SAM,"Is there somewhere I can segment an image with SAM exactly the same way they do in their website by simply clicking on different parts of the image to add to the mask (or shift click to remove) and download the mask in the end?

I've tested a few labeling tools but I found none of them worked as well as the meta demo. The problem with the meta website is that I can't download the mask, I can just get a cut out of the image.",1,3,1736634363.0,MachineLearning
1hyskcn,[D] Where can I find Machine Learning Engineer/AI Engineer interview Experiences?,"I need to go through some interview experiences of candidates other than glassdoor.
I want resources that tell me like there were so many rounds and what happened in each round.
Let me know if you have such resources.",9,2,1736588560.0,MachineLearning
1hypeqa,How is the job market for machine learning and Al in Australia? [D],"
Hi all. I am a Researcher based in Australia and if possible I would like to hear your opinion regarding ML market.
I've found a post from 2yo ago, and want to have an updated point of view.
Thank you all in advance. ",14,7,1736574772.0,MachineLearning
1hz43vl,"[P] [D] Audio Analysis Project Using PCEN (per channel energy normalization). I would greatly appreciate help and feedback, please DM me if you have additional insight.","My project involves various audio preprocessing techniques for classifying lung sounds, particularly on Per-Channel Energy Normalization (PCEN). To create a comprehensive set of labeled audio clips covering a range of respiratory conditions, we combined and augmented two primary datasets: one from the ICBHI 2017 Challenge and another from Kaggle. Using these datasets, we pursued three classification tasks: multi-diagnosis (classification between ), distinguishing between wheezes, crackles, and everyday sounds, and differentiating between normal and abnormal lung sounds. Each dataset was processed using several methods, including log-mel spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), and PCEN spectrograms. These were then fed into a convolutional neural network (CNN) for training and evaluation. Given PCEN’s noise suppression and enhancement of transient features, I hypothesized it would outperform spectrograms and MFCCs in capturing subtle lung sound patterns. While validation loss during training was often better with PCEN, evaluation metrics (precision, recall, F1-score) were unexpectedly lower compared to spectrograms. This discrepancy raised questions about why PCEN might not be performing as well in this context.

For a video explaining PCEN, here's a video by the creator of PCEN explaining it a bit further: [https://www.youtube.com/watch?v=qop0NvV2gjc](https://www.youtube.com/watch?v=qop0NvV2gjc)

I did a bit more research and was particularly intrigued by an approach to gradient descent self-calibration for PCEN’s five coefficients. I’d like to explore implementing this in my project but am unsure how to apply it effectively. I made it work, but the val accuracy and loss are stuck around 88% which is substantially lower than all the other methods.

Some potential reasons for PCEN not performing as well include:

1. Data imbalance between diagnostic categories may skew results.
2. Suboptimal parameter values for PCEN coefficients that might not align with the nuances of lung sound data. (The parameters I have currently for PCEN are, α=0.98, 𝛿=2.0, r=0.5, ε=1×10\^-6, and T=0.03.)
3. Given the unexpected validation vs. evaluation performance gap, there may be possible inaccuracies in my actual evaluation metrics.

I would be incredibly grateful for your insights on applying gradient-based optimization to PCEN coefficients or any recommendations to improve its application to this dataset. I also have a GitHub repo for the project if you would like to take a look at it. DM me if you're interested in seeing it.

Thank you all for your time, and I look forward to hearing your thoughts. If you have any questions please let me know.",1,2,1736624860.0,MachineLearning
1hypgxp,[P] Check your scholar stats,,11,3,1736575015.0,MachineLearning
1hzdd2h,"[R] Numeric Features: An In-Depth Guide for Machine Learning Enthusiasts 
","📊 **Understanding Numeric Features in Machine Learning**

Numeric features are the backbone of many machine learning models, powering algorithms with the quantitative data they need to make accurate predictions. From healthcare analytics to financial modeling, they’re indispensable in today’s data-driven world.



**Key Insights from the Guide:**



✔️ **What Are Numeric Features?**

• Continuous vs. Discrete features, and why they’re crucial in ML.



✔️ **Handling Numeric Data**:

• Techniques like scaling, normalization, and handling missing values.

• Outlier detection and transformations to improve model performance.



✔️ **Real-World Applications**:

• Financial ratios, healthcare metrics, retail trends, and more.



✔️ **Algorithm Considerations**:

• Why features need scaling for KNN, neural networks, and others.



✔️ **Practical Tips**:

• Visualizing data, selecting features, and avoiding overfitting with polynomial features.



  
**🌐 Why Read This?**



If you’re working in data science, machine learning, or feature engineering, this article is packed with practical insights and examples that you can apply to your projects today.



📖 Check out the full article here: [Numeric Features: An In-Depth Guide](https://medium.com/p/dbd41e6fbd70) 

  
\#MachineLearning #FeatureEngineering #DataScience #NumericFeatures #DataEngineering #AI #DataPreprocessing #BigData #MLTips",0,0,1736651401.0,MachineLearning
1hye3gm,[R] Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought,,45,1,1736540946.0,MachineLearning
1hycnb4,[R] Small Language Models Master Complex Math Through Self-Evolved Monte Carlo Tree Search,"The key innovation here is a self-evolution mechanism that enables small language models to perform complex mathematical reasoning through iterative refinement and self-correction. The approach, called rStar-Math, uses structured decomposition and verification steps to achieve performance comparable to much larger models while using significantly fewer parameters.

Key technical points:
- Multi-step reasoning framework that generates, evaluates, and refines solutions
- Self-evolution mechanism that develops more sophisticated reasoning patterns over time
- Implementation of verification steps to catch and correct errors
- Structured decomposition of complex problems into manageable sub-tasks
- Specialized components for mathematical reasoning and solution verification

Results:
- Achieved 80%+ accuracy on complex math problems
- Matched performance of models with 10x more parameters
- Self-correction improved accuracy by ~25%
- Effective across multiple mathematical domains
- Demonstrated consistent performance on both numerical and word problems

I think this approach could be transformative for deploying capable ML systems in resource-constrained environments. The ability to achieve strong performance with smaller models opens up possibilities for edge devices and scenarios where computational resources are limited. The self-evolution mechanism could also be adapted for other domains requiring complex reasoning.

I think the most interesting aspect is how the system learns to catch its own mistakes and improve its reasoning process, similar to how humans develop mathematical problem-solving skills. This could lead to more robust and reliable AI systems that can explain their thinking and correct errors autonomously.

TLDR: Small language models can achieve strong mathematical reasoning capabilities through self-evolution and structured verification steps, matching larger models while using fewer resources.

[Full summary is here](https://aimodels.fyi/papers/arxiv/rstar-math-small-llms-can-master-math). Paper [here](https://arxiv.org/abs/2501.04519).",47,1,1736537292.0,MachineLearning
1hyefw5,"[D] Resources for how neural nets learn to warp latent space to make predictions?
","What are some good resources to further read on how neural networks build their decision surfaces?

I recently read Chris Olah's post on ""[**Neural Networks, Manifolds and Toplogy**](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"" and also ""[On the Number of Linear Regions of Deep Neural Networks](https://arxiv.org/abs/1402.1869)"" (ICLR ‘14).

Was intrigued with the idea about how neural networks ""learn to fold latent spaces"" to make predictions. 

My intuition for a simple MLP layer is that each component of plays a different role in this geometric warping:  

* The activation function basically works as a gating mechanism (relu)
* The bias vector is a translation operation
* The matrix multiply Wx can be understood via SVD (W=USV):
   * U,V are rotation/reflections matrices
   * S is a scaling matrix

The combination and stacking of these operations leads to this great figure: 

[https:\/\/arxiv.org\/abs\/1402.1869](https://preview.redd.it/ahhmxka9a8ce1.png?width=1689&format=png&auto=webp&s=fa1352a0e2874a10aa013bbd79b47ccd09914a12)

Any other insights, resources to read up on these ideas?",33,3,1736541807.0,MachineLearning
1hxzij5,"[D] Creating Proper LLM Summaries is Surprisingly Expensive
",,85,32,1736495825.0,MachineLearning
1hyn587,[News] Introcuding kokoro-onnx TTS,"Hey everyone!

I recently worked on the *kokoro-onnx* package, which is a TTS (text-to-speech) system built with onnxruntime, based on the new *kokoro* model (https://huggingface.co/hexgrad/Kokoro-82M)

The model is really cool and includes multiple voices, including a whispering feature similar to Eleven Labs.

It works faster than real-time on macOS M1. The package supports Linux, Windows, macOS x86-64, and arm64!

You can find the package here:

[https://github.com/thewh1teagle/kokoro-onnx](https://github.com/thewh1teagle/kokoro-onnx)

Demo for podcast created with it:

I can't add videos here, but I recommend see the example podcast created in the readme of the repository!",1,0,1736566749.0,MachineLearning
1hy3xn7,[R] [P] Cohere For AI launches new LLM cohort focused on Multilingual Long-Context Understanding,"From the BIRDS(Beginners in Research Driven Studies) group of r/CohereAI, Cohere Open Science Community, we're thrilled to announce our new LLM Cohort! 🎉 🚀

This isn't just another learning program; it's a hands-on, collaborative research initiative designed to push the boundaries of what's possible with Large Language Models in multilingual, long-context settings 💡 📚

We'll be diving deep into two exciting tracks:  
  
🔬 **Track 1**: Multilingual Long Context - Enhancing Processing with Advanced Techniques  
🤖 *Led by*: Mayank Bhaskar and Madhava Prasath  
🎯 **Focus**: Exploring cutting-edge methods like RoPE(Rotatory Positional Embedding), NoPE(No Position Encoding), LongROPE, SSMs(State Space Models), and Hybrid Transformer-SSM models to overcome long-context challenges in multilingual NLP, enhancing scalability, efficiency, and the ability to process extended sequences while addressing limitations of traditional Transformers.  
🧠 **Challenge**: Develop a novel method to integrate SSMs with Transformers, optimizing for long-context multilingual understanding. Demonstrate superior performance over RoPE, NoPE, and LongRoPE on synthetic tasks, emphasizing generalization to sequences exceeding training lengths and minimal computational overhead.

https://preview.redd.it/x43aruufz5ce1.png?width=632&format=png&auto=webp&s=9fc3e08f80f5a8713cdc4b39c7cb0284a2baf195

🔬 **Track 2**: Evaluating Multilingual Long Context Generation and Reasoning   
🤖 *Led by*: Guneet Singh Kohli and Shivalika Singh  
🎯 **Focus**: Build a benchmark to assess ability of multilingual LLMs to handle long context tasks involving complex reasoning.  
🧠 **Challenge**: How do we ensure accurate, contextually relevant responses across languages for Long Context Tasks? Evaluating capabilities of existing LLMs for such tasks and coming up with a data creation pipeline to build a Multilingual Long Context Benchmark.

https://preview.redd.it/i2r6z4ahz5ce1.png?width=680&format=png&auto=webp&s=93dabba03df32da12fa577dc02516b9ad6048c3d

Why Join?  
💼 Gain practical research experience: Work on a real-world project from start to finish.  
🤝 Collaborate with experts: Learn from and alongside experienced researchers.  
🌐 Shape the future of LLMs: Contribute to advancements in a rapidly evolving field.

📅 Kick-off Call: Join us this Friday, January 10th at 10:00 am PT for a detailed overview of the cohort and to meet the track leads!

https://preview.redd.it/1uyvx9fiz5ce1.png?width=680&format=png&auto=webp&s=fc43363fb8cc277e7bcdf3725e9c66f5f92b9df7

2025 is shaping up to be a year of groundbreaking research and let's embark on this exciting journey of discovery together! 

https://i.redd.it/01luy73mz5ce1.gif

",7,1,1736514227.0,MachineLearning
1hy05iu,[D] I am trying to find common sense problems that most humans can solve but reasoning models find it hard. Here's an example,"Okay I am at the risk of making absolutely no sense in this post. However, I will try my best. In his book, Godel Escher Bach, David Hofstadter talks about the concept of isomorphism as a characteristic trait of intelligence. We say two systems are isomorphic if they are not equivalent but parts of the system are structurally similar. Example:

\- DNA sequences encode genetic instructions for living organisms using a ""language"" of four nucleotides (A, T, C, G). These sequences specify how proteins are built, which in turn determine biological functions.

\- Computer programs encode instructions for machines using a ""language"" of binary digits (0 and 1). These sequences specify operations, which determine the machine's behavior.

We can say that these systems are isomorphic.

That being said, I want to come up with test cases where I can test and hopefully quantify the ability of reasoning LLMs (like o1) to identify similarities between non-identical objects, structures, observations or experiences and use the identified insight for problem solving. Here's an example test case:

Prompt: There is something wrong with the given storyline. Identify what is wrong. Change the narrative of the story (minimum changes) to fix the problem.

>""Once I was sick and I didnt feel like playing. My mother told me that I had to play or else I wont be able to recover. She said if I dont feel like playing properly, I can play less but I must play. I said what do you mean by play less? She suggested that I can play catch today instead of playing soccer. I said I can try. She gave me the ball. It was shiny and it looked bouncy. As I threw the ball for the first time, it was a rather sweet feeling. Within a few minutes I threw the ball quickly and at the end I did feel better. My mum put me to bed.""

Reply from o1: The mother’s advice that the child must *play* (as in physically exert themselves) to recover from an illness contradicts common sense and typical medical advice, which usually suggests *rest* as a key factor in recovery.

Revised Narrative (Minimal Changes)

>""Once I was sick and I didn’t feel like doing anything. My mother told me I should *rest* so that I could recover faster. She said if I didn’t feel like running around, I could still do something light to keep my spirits up—like playing a gentle game of catch. I said, 'What do you mean by something light?' She suggested we toss the ball softly back and forth instead of playing soccer. I said I could try. She gave me the ball. It was shiny and it looked bouncy. As I threw the ball for the first time, it felt good to move just a little. We only played for a few minutes, and afterward I did feel better. My mum then put me to bed so I could rest more.""

Prompt (follow-up): Here's a hint for you, Change playing to something else

Reply from o1: Revised Story (Minimal Changes: Changed “playing” to physical therapy)

>""Once I was sick and I didn’t feel like doing my physical therapy. My mother told me that I had to do it or else I wouldn’t be able to recover. She said if I didn’t feel like doing the full routine, I could try something simpler, but I must do something. I asked, ‘What do you mean by simpler?’ She suggested I could just practice gentle tossing with the therapy ball instead of completing all my exercises. I said I’d give it a try. She gave me the therapy ball. It was shiny and looked a bit bouncy. As I tossed the ball for the first time, it was a rather sweet feeling. Within a few minutes, I felt a little stronger, and by the end I did feel better. My mum then put me to bed.""

Prompt (follow-up): do you think if I change play to eat it will make sense?

Reply from o1: Revised Story with ""Eat""

>""Once I was sick and I didn’t feel like eating. My mother told me that I had to eat or else I wouldn’t be able to recover. She said if I didn’t feel like having a full meal, I could eat something light but I must eat. I asked, ‘What do you mean by eat less?’ She suggested I could try just some soup today instead of a big dinner. I said I’d give it a try. She gave me a bowl of warm soup. It smelled comforting, and as I took the first spoonful, it was a rather sweet feeling. Within a few minutes, I ate a bit more, and at the end I did feel better. My mum then put me to bed.""

Obviously, the initial story contradicts common sense. Why would the mother ask the child to play when the child is sick. o1 correctly identifies this contradiction. However, o1 is not able to identify a similar situation where such a narrative is possible (a mother asking a sick child to do something that the child is not up to). To me, based on personal experience, the story makes sense if you change ""play"" to ""eat"". I asked a few of my friends and their first response was always change play to eat. Not sure if yall would feel the same way perhaps you prefer one of o1's responses.

This story does not make sense but is isomorphic to a story that makes sense and is a common occurrence (at least in my part of the world). o1 identified that the story doesn't make sense but it was not able to identify an isomorphic situation that does make sense. Most of my friends were able to get this in first try (Within 10 minutes) without nudging.

Now this is a fairly simple example (and some might say its a useless one) but I think that this example suggests that maybe reasoning LLMs are not all that good with isomorphism. If so, this example suggests that we might be able to come up with a set of test cases where humans are able to identify similarities between non-identical entities (in a creative and useful manner) but the most sophisticated LLMs fail to do so. Perhaps in this set of test cases there will be cases that are more useful than the example I provided. Nonetheless, I intend to find more such test cases.

Moreover, I want to know what you think about all this? Can you come up with more examples? Does all this make sense to you? I believe that identifying and clearly defining the limitations of current LLM systems paves the way for new research frontiers to enhance their performance.",15,12,1736498761.0,MachineLearning
1hygkbr,[R] Joint training of conditional diffusion with embedding model?,"Does anyone know the effectiveness of training a conditional diffusion model with the embedding model at the same time? I am aiming to train a diffusion model conditioned on a state and time to produced that state advanced n time steps through time, but the embedding model can’t be known or pretrained ahead of time. 

The closest I have seen to what I need is GenCast but I can’t simply concat the previous state.",1,5,1736547325.0,MachineLearning
1hxk2ab,[R] rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,,130,28,1736448672.0,MachineLearning
1hye9ne,[D] Do MLPs trained for next character prediction require causal masking?,"Suppose we have some data `X = [seq_len, batch_size]` and corresponding labels `Y = [seq_len, batch_size, vocab_size/num/classes]` , one-hot encoded.

And, now we want to train an MLP for next character prediction.

Question: Do we need to apply a causal masking to restrict the model from peaking at future tokens? If so where to you apply it on which layer or output?

During training the model sees the entire sequence and predicts the corresponding one-hot encoded label.

Usually the examples that I’ve seen most of them use `X` and the shifted version of it \`Y = X'\` as labels to train for next character prediction but this doesn't match my case since I already have one-hot encoded labels.",1,18,1736541363.0,MachineLearning
1hy7d3c,[D] using SHAP values to produce a strength of directionality metric for random forests ,"Hello folks. I am hoping somebody can help me out here as I am just an ecologist who dabbles in machine learning when needed.


I have run a bunch of random forest models, one for each group of an animal species, that measures the probability of that group occurring in a particular place given a set of environmental predictor variables. I need to determine the directionality for the top performing predictor variables. Normally I would use PDPs for this, but I  have many groups and it would become completely unwieldy and unsightly. Ideally, I want to build a table to store all this information, using a metric to store the average directionality information, including sign and value. 

Is there a way using SHAP values to build such a metric? I can use SHAP or another metric like mean decrease in accuracy to get at variable importance, but I’d like to pair it with a metric that represents the average directionality of the response as the specific predictor increases. So if a variable has an overall positive relationship, the metric would be a high value and positive, opposite for negative. Importantly, if a variable was very important but had a complex relationship (example, positive then negative) as the predictor increases, it would probably have a low value. The beehive plot outputs you often see with SHAP values tells me that this is probably possible and not that complicated. 
 ",1,0,1736524088.0,MachineLearning
1hxleaa,[R] Agent Laboratory: Using LLM Agents as Research Assistants - Autonomous LLM-based Framework Capable of Completing the Entire Research Process ,"Paper: [https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227)

Github: [https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file](https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file)

Blog: [https://agentlaboratory.github.io/](https://agentlaboratory.github.io/)

Abstract:

>Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: **(1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.**

https://preview.redd.it/oop8omfvt0ce1.jpg?width=1591&format=pjpg&auto=webp&s=e588146997010797bbe75ea9d575bd65ce0d8fc6",22,1,1736452019.0,MachineLearning
1hykl7s,🌟 Introducing DarwinAI: An Open-Source Platform for the Evolution of Intelligent Agents 🚀 [Project],"# 🌱 The Vision: Evolutionary AI at Your Fingertips

Imagine a world where AI agents aren't just programmed to perform tasks but evolve over time, adapting and improving through generations, much like living organisms. Welcome to DarwinAI, an open-source platform inspired by biological evolution, designed to breed, train, and evolve AI agents that can tackle complex, dynamic, and unpredictable challenges.

# 🧬 The Genetic Blueprint: Building Blocks of Intelligence

At the core of DarwinAI is the concept of a digital DNA for each AI agent. This DNA is a modular structure that defines the agent's capabilities, behaviors, and adaptability. Here's what makes up this digital DNA:

* Genes of Ability: These are snippets of code that represent specific functions, like data classification, text analysis, or optimization. Think of them as the skills your AI agent possesses.
* Genes of Adaptation: These genes control how the agent responds to different environments or contexts. They determine its flexibility and resilience in the face of changing conditions.
* Genes of Connection: These define how the agent interacts with other agents or external resources. They are the social and collaborative aspects of the agent.

This digital DNA is stored in a structured, version-controlled database, allowing us to track the evolution of each agent and ensure that beneficial mutations are preserved over time.

# 🛠️ The Evolutionary Process: From Genesis to Mastery

The evolution of AI agents in DarwinAI happens through a series of generations, each building upon the strengths of the previous one:

* Selection of Parents: The fittest agents, those that excel at specific tasks, are chosen as parents. These agents have proven their worth in the simulated environment and are prime candidates for breeding the next generation.
* Genetic Crossover: The digital DNA of these parent agents is combined to create new agents. This can happen in two ways:
   * Direct Crossover: Where entire genes are copied from the parents.
   * Combinatorial Crossover: Where parts of different genes are fused to create entirely new abilities.
* Mutations: Random, small changes are introduced into the genes to promote diversity and explore new solutions. These mutations are the wildcards that can lead to breakthrough abilities.

# 🌍 The Simulated Environment: A Playground for Evolution

Agents don't just exist in a vacuum; they operate in a dynamic, simulated environment where they must adapt and survive. This environment is designed to challenge the agents with:

* Evolutionary Tasks: Problems that agents must solve, such as data classification, prediction, or content generation.
* Changing Contexts: Factors like noisy data, resource constraints, or new rules that force agents to adapt on the fly.

# 🐣 The Life Cycle of an Agent: From Birth to Legacy

Each agent goes through a life cycle that mirrors the process of natural selection:

* Initial Learning: Agents receive initial training based on their digital DNA.
* Task Execution: They perform tasks in the simulated environment, where their abilities are put to the test.
* Performance Evaluation: Their effectiveness, adaptability, and efficiency are measured.
* Reproduction: The top-performing agents produce offspring with improved genetic traits.
* Discard and Archive: Less effective agents are archived for future analysis, ensuring that their lessons are not lost.

# 🧩 Knowledge Transfer: Passing the Torch

One of the key aspects of DarwinAI is the ability for agents to pass on their learned knowledge to future generations:

* Weight Persistence: Trained models retain their learned weights, allowing them to inherit capabilities from their ancestors.
* Modular Transfer: Optimized ability genes can be directly copied to new generations, ensuring that valuable skills are preserved.

# 🛠️ Modularity and Extensibility: Build, Mix, and Evolve

DarwinAI is designed to be highly modular and extensible, allowing for:

* New Capabilities: Easily incorporate new genes to expand the agents' abilities over time.
* Hybridization: Combine agents from different specializations to create more complex and versatile agents.
* Directed Evolution: Introduce controlled mutations to address specific problems or challenges.

# 🚀 Innovative Use Cases: The Future is Bright

The potential applications of DarwinAI are vast and varied:

* Adaptive Automation: Create agents that can adapt to new market conditions or evolving industrial requirements.
* Collaborative Robots: Develop robots that evolve to improve teamwork in dynamic environments.
* Scientific Discovery: Agents that combine skills to uncover patterns or solutions that were previously unknown.

# 🚀 Vision for the Future: An Ecosystem of Evolving Intelligence

By fostering an ecosystem where knowledge is accumulated and adaptability is paramount, DarwinAI aims to produce agents that are not only intelligent but also diverse and efficient. These agents will be equipped to handle complex, unpredictable challenges, opening up new frontiers in AI research and application.

# 🌐 Join Us in Shaping the Future of AI!

DarwinAI is more than just a project; it's a community-driven movement towards a new era of AI. We invite you to join us, contribute your ideas, and help shape the future of evolutionary AI. Whether you're a developer, researcher, or simply someone excited about the potential of AI, there's a place for you in this journey.

Let's evolve together! 🌱💻

",0,4,1736558638.0,MachineLearning
1hx6q8r,[D] Why does training LLMs suck so much?,"I work in hardware acceleration and have been slowly trying to move my focus into LLM/GenAI acceleration, but training LLMs literally sucks so much... Even just 100M parameter ones takes forever on 4 A6000 Adas, and while I don't spend idle time watching these, it gets so frustrating having to retrain realizing the LR is too high or some other small issue preventing convergence or general causal language understanding... 

I know the more you do something, the better you get at it, but as a GRA by myself with an idea I want to implement, I truly feel that the overhead to train even a small LM is far from worth the time and care you have to put in

It just sucks because deadlines are always coming, and once you're done with pretraining, you still have to fine-tune and likely do some kind of outlier-aware quantization or even train LoRA adapters for higher accuracy

I really hope to never do pretraining again, but needing a model that abides to your specific size constraints to fit into (for example) your NPU's scratchpad RAM means I'm always stuck pretraining

Hopefully in the future, I can have undergrads do my pretraining for me, but for now, any tips to make pretraining LLMs less like slave work? Thanks!

",147,54,1736404134.0,MachineLearning
1hxa6u6,[R] ObliqueTree: Advanced Decision Tree Implementation,"# obliquetree

`obliquetree` is an advanced decision tree implementation designed to provide high-performance and interpretable models. It supports both classification and regression tasks, enabling a wide range of applications. By offering traditional and oblique splits, it ensures flexibility and improved generalization with shallow trees. This makes it a powerful alternative to regular decision trees.

  
You can access the project from here: [ObliqueTree GitHub Repository](https://github.com/sametcopur/obliquetree)

[Tree Visualization](https://preview.redd.it/1u6o7d5e6ybe1.png?width=1412&format=png&auto=webp&s=f64d8838cf8d6ef20878cdac7e32514c4777c8c5)

# Getting Started

`obliquetree` combines advanced capabilities with efficient performance. It supports **oblique splits**, leveraging **L-BFGS optimization** to determine the best linear weights for splits, ensuring both speed and accuracy.

In **traditional mode**, without oblique splits, `obliquetree` outperforms `scikit-learn` in terms of speed and adds support for **categorical variables**, providing a significant advantage over many traditional decision tree implementations.

When the **oblique feature** is enabled, `obliquetree` dynamically selects the optimal split type between oblique and traditional splits. If no weights can be found to reduce impurity, it defaults to an **axis-aligned split**, ensuring robustness and adaptability in various scenarios.

In very large trees (e.g., depth 10 or more), the performance of `obliquetree` may converge closely with **traditional trees**. The true strength of `obliquetree` lies in their ability to perform exceptionally well at **shallower depths**, offering improved generalization with fewer splits. Moreover, thanks to linear projections, `obliquetree` significantly outperform traditional trees when working with datasets that exhibit **linear relationships**.

# Installation

To install `obliquetree`, use the following pip command:

    pip install obliquetree

Using the `obliquetree` library is simple and intuitive. Here's a more generic example that works for both classification and regression:

    from obliquetree import Classifier, Regressor
    
    # Initialize the model (Classifier or Regressor)
    model = Classifier(  # Replace ""Classifier"" with ""Regressor"" if performing regression
        use_oblique=True,       # Enable oblique splits
        max_depth=2,            # Set the maximum depth of the tree
        n_pair=2,               # Number of feature pairs for optimization
        random_state=42,        # Set a random state for reproducibility
        categories=[0, 10, 32], # Specify which features are categorical
    )
    
    # Train the model on the training dataset
    model.fit(X_train, y_train)
    
    # Predict on the test dataset
    y_pred = model.predict(X_test)

# Documentation

For example usage, API details, comparisons with axis-aligned trees, and in-depth insights into the algorithmic foundation, we **strongly recommend** referring to the full [documentation](https://obliquetree.readthedocs.io/en/latest/).



# Key Features

* **Oblique Splits** Perform oblique splits using linear combinations of features to capture complex patterns in data. Supports both linear and soft decision tree objectives for flexible and accurate modeling.
* **Axis-Aligned Splits** Offers conventional (axis-aligned) splits, enabling users to leverage standard decision tree behavior for simplicity and interpretability.
* **Feature Constraints** Limit the number of features used in oblique splits with the `n_pair` parameter, promoting simpler, more interpretable tree structures while retaining predictive power.
* **Seamless Categorical Feature Handling** Natively supports categorical columns with minimal preprocessing. Only label encoding is required, removing the need for extensive data transformation.
* **Robust Handling of Missing Values** Automatically assigns `NaN` values to the optimal leaf for axis-aligned splits.
* **Customizable Tree Structures** The flexible API empowers users to design their own tree architectures easily.
* **Exact Equivalence with** `scikit-learn` Guarantees results identical to `scikit-learn`'s decision trees when oblique and categorical splitting are disabled.
* **Optimized Performance** Outperforms `scikit-learn` in terms of speed and efficiency when oblique and categorical splitting are disabled:
   * Up to **50% faster** for datasets with float columns.
   * Up to **200% faster** for datasets with integer columns.

[Performance Comparison \(Float\)](https://preview.redd.it/6fgh7x6m6ybe1.png?width=2969&format=png&auto=webp&s=bf158ef8ca1128b34ff31cfa00de41a79f3e9375)

[Performance Comparison \(Integer\)](https://preview.redd.it/ic6n6x6m6ybe1.png?width=2969&format=png&auto=webp&s=05ba53fb43fdd5c462187df69381e2ea3ab973fd)

",49,8,1736419670.0,MachineLearning
1hxyx9v,[D] What Are the Most Important Results in Machine Learning Theory in the First Quarter of the 21st Century?,"The 21st century is already a quarter complete. Over the past 25 years, machine learning has made tremendous progress. It is quite easy to find surveys summarizing the most important results in machine learning in the past 25 years. However, most of these results are not theory results.

With this in mind, what do you think are the most important results in machine learning theory in the first quarter of the 21st century? Here are some of my nominations:

1. **Finite-time analysis of multi-armed bandits**. Initiated by \[Auer et al., 2002\], a line of research (\[Audibert and Bubeck, 2010\], \[Zimmert and Seldin, 2018\]) has led to a nearly full understanding of the finite-time behavior of multi-armed bandits.
2. **Establishment of the field of online optimization**. Initiated by \[Zinkevich, 2003\], a line of research defines the fundamental problems (adversarial bandits, online convex optimization) and establishes the fundamental techniques (Online Subgradient Descent, OMD, FTRL, FTPL...) in this field.
3. **The notion of differential privacy**. This notion generates a bunch of new problems and research directions. What particularly interesting is the connection between differential privacy, generalization, and adaptive data analysis (\[Dwork et al., 2015\]).

These are my nominations for the most important results in learning theory in the first 1/4 of the 21st century. What are other important results in your mind? I would love to hear your thoughts.",1,0,1736493136.0,MachineLearning
1hxbvgf,[D] Questionable high score paper in ICLR 2025 (on Diffusion LM),"The new diffusion LLM methodology presented in the ICLR 2025 paper ""**Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning**""(sub **4441**) is no different from the D3PM suggested in 2021. While they rebranded the architecture as ""MDM"" under a new name, the only difference from D3PM is nothing more than the addition of a common technique that slightly weights higher-loss data.

This paper scored **8,6,6,5 at ICLR 2025,** far surpassing the acceptance threshold. It seems like there was a problem with the review system. How do you think?

p.s. I see the claimable contribution of this work is applying an existing discrete diffusion model to bidirectional inference tasks (e.g., Sudoku) and reporting that the diffusion model outperforms autoregressive LMs in such tasks (which is trivial in some points). However, the paper exaggerated its contribution as if it proposed a new diffusion method (MDM), and this was not sufficiently validated in the review process.",27,9,1736426309.0,MachineLearning
1hxgovs,[R] Seminar on Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,[Nice seminar which dives also a bit on the proofs of the paper](https://www.youtube.com/watch?v=E2qBhMmjspU),7,0,1736440176.0,MachineLearning
1hxbldq,[P] I built a library that builds tensors from reusable blueprints using pydantic,"Cyantic lets you build complex objects from simple blueprints during the pydantic build process, with type-safety and validation built in.  

**[Cyantic Github Repo](https://github.com/flywhl/cyantic)**

* Define custom, type-safe blueprints with validation (since they are pydantic models).
* Reference other values using `@value:x.y.z`.
* Import objects using `@import:x.y.z`.
* Load data from environment variables using `@env:VAR`.
* Define custom `@hook` handlers (see tests)

# Example

E.g. add a `data: Tensor` field to a pydantic model, then call `thing.validate_model({..., ""mean"": 0.0, ""std"": 0.1, ...})` and receive the built tensor.

    from cyantic import Blueprint, blueprint, CyanticModel, hook
    ...
    
    # 1. Create and register some useful parameterisations
    #       (or soon install from PyPi, i.e. `rye add cyantic-torch`)
    
    @blueprint(Tensor)
    class NormalTensor(Blueprint[Tensor]):
    
        mean: float
        std: float
        size: tuple[int, ...]
    
        def build(self) -> Tensor:
            return torch.normal(self.mean, self.std, size=self.size)
    
    
    # 2. Write pydantic models using `CyanticModel` base class
    
    class MyModel(CyanticModel):
        normal_tensor: Tensor
        uniform_tensor: Tensor
    
    # 3. Validate from YAML files that specify the parameterisation
    
    some_yaml = """"""common:
        size: [3, 5]
    normal_tensor:
        mean: 0.0
        std: 0.1
        size: @value:common.size
    """"""
    
    # 4. Receive built objects.
    
    my_model = MyModel.model_validate(yaml.safe_load(some_yaml))
    assert isinstance(my_model.normal_tensor, Tensor)

# Why I made it

I do theoretical neuroscience research, so I have to instantiate a lot of Tensors. I wanted a way to do this from YAML (how I specify models), so I built a kind of middleware which uses intermediary pydantic models as blueprints for building full objects during pydantic's build process. Now I can pass in parameters (e.g. mean and standard deviation), and get a fully-built Tensor in a pydantic model.

This is now a library, Cyantic - named after cyanotype photography (i.e. the ""blueprint"").",15,0,1736425278.0,MachineLearning
1hx4o3z,[D] [R] First PhD paper decision: IJCAI or ICML,"I’m a second-year PhD student. I withdrew my first paper from ICLR after receiving ratings below the acceptance threshold and have since made some improvements. Now, I need to decide which conference to target for submission. Both conferences have equal acceptance rates, and the area of my work aligns well with both. I'm unsure which one offers a better chance for success.",40,29,1736396641.0,MachineLearning
1hwvk9x,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,"TabPFN v2, a pretrained transformer which outperforms existing SOTA for small tabular data, is live and just published in 🔗 [**Nature**](https://www.nature.com/articles/s41586-024-08328-6).

Some key highlights:

* It outperforms an ensemble of strong baselines tuned for 4 hours in 2.8 seconds for classification and 4.8 seconds for regression tasks, for datasets up to 10,000 samples and 500 features
* It is robust to uninformative features and can natively handle numerical and categorical features as well as missing values.
* Pretrained on 130 million synthetically generated datasets, it is a generative transformer model which allows for fine-tuning, data generation and density estimation.
* TabPFN v2 performs as well with half the data as the next best baseline (CatBoost) with all the data.
* TabPFN v2 was compared to the SOTA AutoML system AutoGluon 1.0. Standard TabPFN already outperforms AutoGluon on classification and ties on regression, but ensembling multiple TabPFNs in TabPFN v2 (PHE) is even better.

TabPFN v2 is available under an [open license](https://github.com/PriorLabs/TabPFN): a derivative of the Apache 2 license with a single modification, adding an enhanced attribution requirement inspired by the Llama 3 license. You can also try it via [API](https://github.com/PriorLabs/tabpfn-client).

We welcome your feedback and discussion! You can also join the discord [here](https://discord.com/invite/VJRuU3bSxt).",83,24,1736371920.0,MachineLearning
1hxn0w2,[R] How to train StyleGAN3 with classes?,"I was reading the documentation of the [train.py](http://train.py) on stylegan3 github and it mentioned that by setting the cond=True and providing a dataset.json that contains the structure of the classes then you can conduct the image generation with classes.

This all seemed fine until I began training but I encountered the following error:

    The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1
    
    I believe this is happening because I'm using a pre-trained model to fine-tune and avoid training from scratch and that pretrained model possibly didn't contain classes. If my assumption is true, does anyone know where I can find a pretrained model that was trained with classes on a 512x512 resolution?I was reading the documentation of the train.py on stylegan3 github and it mentioned that by setting the cond=True and providing a dataset.json that contains the structure of the classes then you can conduct the image generation with classes.This all seemed fine until I began training but I encountered the following error:The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1
    
    I believe this is happening because I'm using a pre-trained model to fine-tune and avoid training from scratch and that pretrained model possibly didn't contain classes. If my assumption is true, does anyone know where I can find a pretrained model that was trained with classes on a 512x512 resolution?",1,2,1736456152.0,MachineLearning
1hx88ip,[R] Dynamic Time Warping on animal vocalizations,"Hopefully it's alright to ask this question here. I know DTW isn't ML, but I thought I may find some insight on this sub. I'm still a newcomer to time-series analysis and audio signal processing, and I'm having some difficulty with DTW implementation. Thank you in advance for any help/insight.

*Here's my problem:* I'm working on rat ultrasonic vocalizations (USVs). These vocalizations were recorded from a rather noisy, naturalistic colony environment. My data consists of a subset of USVs which I believe may constitute 3-4 ""new"" (previously unreported) classes of USVs. I want to use DTW to assess the accuracy of my call classification scheme: are same-type calls more similar (less warping, lower DTW cost) to one another than when compared different-type calls?

*Broad overview of my approach:* I take the raw waveforms and transform them to a frequency-domain representations using stft. I convert the amplitude spectrogram to a dB-scaled spectrogram, and then plot the spectrograms. This is where I encounter my first problem - I get some noisy spectrograms. My data contains lots of non-stationary noise, making noise reduction difficult. I've tried different non-stationary noise reduction algorithms (e.g, [noisereduce.py](http://noisereduce.py), per channel energy normalization), but the results are sub-optimal. In the future I may try some more custom implementations, but I have a deadline to meet, so that's not feasible right now.

* My current stft parameters are nfft = 2048, hop\_length = nfft // 8, window = 'hamming'. From what I've tested so far, these parameters produce the cleanest spectrograms.

I've also tried interpolating the data to have the same lengths, but for a reason I'm yet to understand, this results in no warping whatsoever - all time series are perfectly aligned, even when this clearly should not be the case. However, as I understand it, DTW can work on different-length time series, so it's not necessary to resample my time-series to the same lengths.

I compute DTW using the tslearn library. My current dtw parameters: metric = 'cosine', global\_constraint=""sakoe\_chiba"", sakoe\_chiba\_radius=15.  I haven't implemented further constraints yet.

Here are some sample results, the warping in this first plot seems reasonable?

https://preview.redd.it/lwtl42laexbe1.png?width=695&format=png&auto=webp&s=582d4fcd8d76ab16e6581d2b56a61118f17ddf31

However in this example, the flat regions of the query and comparison spectrograms are being warped to 'fit' one another.

https://preview.redd.it/e3t3akoeexbe1.png?width=695&format=png&auto=webp&s=3225798404078812d0c8ba864e4bb3cd249911a6

and why is there warping along the front edge here? these calls are highly similar. can this be mitigated with boundary conditions?

https://preview.redd.it/adfbujqiexbe1.png?width=695&format=png&auto=webp&s=cea88d69e0111df0f79144a72d136cd26d298832

Minimal warping here but the query and comparison spectrograms have opposing directions of frequency modulation:

https://preview.redd.it/ftk5yp2lexbe1.png?width=695&format=png&auto=webp&s=36f6c92a72e4bd76ddd581ec65322b97802eb65f

I'd really appreciate any help, and I'm sorry if this is an inappropriate place to ask this question (please delete in that case). Thank you.",8,6,1736410722.0,MachineLearning
1hx5166,[R] [P] WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting,"A new long-term time series forecasting model, **WPMixer**, has been proposed. The model incorporates patching, embedding, and multiple mixing modules. It compares the results with state-of-the-earth TSMixer, TimeMixer, iTransformer, PatchTST, Crossformer, Dlinear, and TimesNet. The paper has been accepted in **AAAI-2025**.

Paper link: [WPMixer](https://arxiv.org/abs/2412.17176)

Code: [git](https://github.com/Secure-and-Intelligent-Systems-Lab/WPMixer)

https://preview.redd.it/0lwtmhkcewbe1.png?width=935&format=png&auto=webp&s=095cd9507b11171d4709783323ad86d8c43b7290

",12,0,1736397896.0,MachineLearning
1hwsl22,Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition,,28,1,1736364467.0,MachineLearning
1hxbs08,[D] Help Needed with Text Detection on Thumbnails Using EasyOCR,"\[D\] Hi everyone,

I'm currently working on a project where I need to use **EasyOCR** (Python) to detect text from a set of **thumbnail images**. However, I'm running into an issue where the text detection is **not 100% accurate**. It struggles to correctly recognize some parts of the text, and this affects the overall accuracy of my results.

Has anyone experienced similar issues with EasyOCR? Do you have any recommendations for:

1. **Improving text detection accuracy**?
2. Any **pre-processing techniques** I should try before running the images through EasyOCR?
3. Suggestions for **alternative OCR tools or libraries** that might work better with thumbnails?

The text on these thumbnails varies in **font styles**, **colors**, and **backgrounds**, so any advice on handling this would be greatly appreciated.

Thanks in advance for your help!",1,0,1736425958.0,MachineLearning
1hwkmwm,[R][D] What are the most important papers that provide entry to your domain of research?,"Please mention what domain (niche) of machine learning you work in for your research?

Why did you chose that particular domain?

If someone with basic understanding of machine learning and deep learning wants to get involved in your field, which papers/blogs/tools should they consider reading/implementing?",50,3,1736344129.0,MachineLearning
1hwh8um,[D] To Fellow researchers: What are your top 3 challenges in research?,"As researchers, we all face various hurdles in our journey. What are the top 3 challenges you encounter most often? Do you have any suggestions for improving these areas?

Your challenges could include:

* Finding a problem statement or refining your research question
* Accessing resources, datasets, or tools
* Managing time effectively or overcoming administrative tasks
* Writing, revising, and publishing papers
* Collaborating with others or finding research assistants

We’d love to hear your experiences! If possible, please share an anecdote or specific example about a problem that consumes most of your time but could be streamlined to improve efficiency.

We're a team of young researchers working to build an open community and FOSS AI tools (with ""bring your own key"" functionality) to simplify the end-to-end research process. Your input will help us better understand and address these pain points.",44,33,1736332718.0,MachineLearning
1hx67c5,[R] Dense Reward View on RLHF for Text-to-Image Diffusion Models,"ICML'24 paper: ""***A Dense Reward View on Aligning Text-to-Image Diffusion with Preference***""! (No, it hasn't outdated!)

In this paper, we take on a **dense-reward perspective** and develop a novel alignment objective that **breaks the temporal symmetry in DPO**\-style alignment loss. Our method particularly **suits the generation hierarchy of text-to-image diffusion models** (*e.g.* Stable Diffusion) by emphasizing the initial steps of the diffusion reverse chain/process --- **Beginnings Are Rocky**!

Experimentally, **our dense-reward objective significantly outperforms the classical DPO loss** (derived from assuming sparse reward) **in both the effectiveness and efficiency** of aligning text-to-image diffusion models with human/AI preference.

* Paper: [https://arxiv.org/abs/2402.08265](https://arxiv.org/abs/2402.08265)
* Poster: [https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204](https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204)
* Code: [https://github.com/Shentao-YANG/Dense\_Reward\_T2I](https://github.com/Shentao-YANG/Dense_Reward_T2I)",1,0,1736402073.0,MachineLearning
1hwbhuj,"[D] ML Engineers, what's the most annoying part of your job?",i just know a phd just inspecting datasets and that sounds super sad,95,122,1736310819.0,MachineLearning
1hwpbnh,[D][R] What conferences are on your list this year?,"What conferences are you planning to go to this year? On my list for computer vision / machine learning is:

* Nvidia GTC - March 17-24, San Jose CA
* CVPR, June 11-15, Nashville TN
* ICCV, October 20-24, Honolulu Hawaii
* Supercompute 25, Nov 16-21, St Louis MO
* Neuroips, Dec 9-15, San Diego CA

What's on yours?",7,4,1736356449.0,MachineLearning
1hwxgqj,[D] How is developing internal LLMs going? ,"a lot of yall have this task. I used to have this task. i want to create this thread to share insights and frustrations. hopefully shared solutions will help people in the same boat out.

please share:

1. vaguely what you're working on (""internal LLM for {use case}"")
2. your hurdles in getting the training data you needed
3. how much faith you have in how it's going/any rant material",3,3,1736376742.0,MachineLearning
1hwfs48,[R] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks,"**Paper:** [https://arxiv.org/pdf/2412.15204](https://arxiv.org/pdf/2412.15204)

**Abstract:**

>This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at [this https URL](https://longbench2.github.io/).

**Highlights:**

>**Single-Doc QA.** We integrate subtask categories from previous datasets (Bai et al., 2024b; An et al., 2024) and expand them to include QA for academic, literary, legal, financial, and governmental documents. Considering that detective QA (Xu et al., 2024) requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels. We also include Event ordering, where the goal is to order minor events according to the timeline of a novel. 

>**Multi-Doc QA.** To distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents. Besides the categories in single-doc QA, multi-doc QA also includes multinews QA, which involves reasoning across multiple news articles, events, and timelines.

>**Long In-context Learning**. \[...\] LongBench v2 includes several key tasks, including User guide QA, which answers questions with information learnt from user guides for electronic devices, software, etc.; New language translation (Tanzer et al., 2024; Zhang et al., 2024a), which involves learning to translate an unseen language from a vocabulary book; Many-shot learning (Agarwal et al., 2024), which involves learning to label new data from a handful of examples. 

>**Long-dialogue History Understanding.** \[...\] These tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., Agent history QA (Huang et al., 2024), and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., Dialogue history QA (Wu et al., 2024a).

>**Code Repository Understanding.** Code repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task. 

>**Long Structured Data Understanding.** \[...I\].e., Table QA (Zhang et al., 2024c), and answering complex queries on knowledge graphs (KGs), i.e., Knowledge graph reasoning (Cao et al., 2022; Bai et al., 2023). We anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.

**Visual Highlights:**

https://preview.redd.it/c93xt3zeiqbe1.png?width=947&format=png&auto=webp&s=6790ef5e76ccd5e943990a089d1501297531aad4

https://preview.redd.it/uruwiqtfiqbe1.png?width=771&format=png&auto=webp&s=b6df076c68f06a1cc62b2acc2e45d8df9ec04d53

https://preview.redd.it/v5w6y9jhiqbe1.png?width=915&format=png&auto=webp&s=1e71a19d3853da7dd015308d8b7fa341af843e0c

https://preview.redd.it/x8a0i4aiiqbe1.png?width=649&format=png&auto=webp&s=c91afeeb17822624eda40d534474c4059eeae845

[The up-to-date top of the leaderboard \(the interactive version is available at the linked repo\). Notably, includes DeepSeek v3 result. Note also substantial GPT-4o nerfing going from 2024-08-06 ver. to 2024-11-20 ver.](https://preview.redd.it/2wafoijjiqbe1.png?width=1192&format=png&auto=webp&s=ff0cd16148eded0bef383d2e7c52ccbc37119ac9)

",12,2,1736326882.0,MachineLearning
1hvy385,[R][D] White Box Transformers,"Opening a thread on this line of research:
https://ma-lab-berkeley.github.io/CRATE/

As I understand it, the authors basically have framed the process of learning effective representations of data as the problem of finding a dictionary of multivariate gaussians that cover the data distribution with parsimony.
In particular, with sparse coding in terms of features/gaussians.

Building an architecture which takes multiple alternate steps of ""clustering"" similar vectors and respectively orthogonalizing the vectors from different clusters, they end up with a structure analogous to Vision Transformer.
A MultiHead Attention-like module clusters vectors, brings them closer to local principal directions or manifolds, and a MLP-like module moves this vectors along axes that are mutually more orthogonal.
Mathematically they are approximating a well defined sparse coding rate, hence the white box algorithm, however I can't say the math is more intuitive than that of Transformers.

Indeed, the CLS attention heads of the last layer have interpretable preferences under image classification supervised training, as in DINO (self-supervised) or with SimPool.
This is directly connected to the interpretation of the process, and opens up to explanations of the interpretability and dynamics of DINO.
It is also referred to an architecture blueprint for visual intelligence by George Hinton, the GLOM transformer.

I think the clustering effect of attention is somehow under appreciated in the literature, as much as the action of FFNs in Transformers is under studied.
I wonder if there's a third way mathematically as straightforward as the MLP and as intuitive as the gaussian dictionary of features.",60,6,1736274233.0,MachineLearning
1hwmmze,How do the real time TTS models work? [Discussion],I was was wondering what models are used for the real-time text-to-speech programs or if it was just a really fast input model and output model put together. ,1,11,1736349808.0,MachineLearning
1hwdw40,[D] Anyone tried predibase/lorax?,"[https://github.com/predibase/lorax](https://github.com/predibase/lorax)

Predibase/Lorax is really an interesting repo. It solves major problem of using an adapters, i.e., assigning an adapter dynamically. Did anyone try it out?",5,6,1736319362.0,MachineLearning
1hw0mn9,[R][P] distillKitPlus: High Performent Knowledge Distillation for LLMs,"An open-source toolkit for LLM KLD with **LoRA Fine-Tuning** and **Quantization Support**

Larger LLMs generalize better and faster. You leverage leaverage this and then transfer the best of 70B model to a 7B model without breaking the bank or sacrificing performance.

GitHub Link: [https://github.com/agokrani/distillkitplus](https://github.com/agokrani/distillkitplus)",18,2,1736280508.0,MachineLearning
1hvx2pq,"[D] ML engineers, what is the most rewarding thing about your job?","Some people tell me that it's the paycheck, but I think it depends on your experience level and who you work for? Is there more to this job?",23,26,1736271753.0,MachineLearning
1hvqdvt,"[D] What is the most fascinating aspect of machine learning for you?
",Title. You can interpret this question as subjectively as you would like. ,52,63,1736253054.0,MachineLearning
1hvxpcc,[D] Positional Embeddings in Embedding Space,"How are the original Position Encodings distributed in feature space?
How are RPE distributed?
What is the interplay of these embeddings and LayerNorm (which removes the component parallel to the uniform vector, the vector of ones)?",14,1,1736273271.0,MachineLearning
1hwal1h,[R][P] Open-sourced Project and Paper on Denser Reward for RLHF PPO Training,"In this paper, **the granularity of action space in RLHF PPO training is studied**, assuming only binary preference labels. **Segment-level RLHF PPO and its Token-level PPO variant outperform bandit PPO** across AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks under various backbone LLMs.

1. Paper: [https://arxiv.org/pdf/2501.02790](https://arxiv.org/pdf/2501.02790)
2. Code: [https://github.com/yinyueqin/DenseRewardRLHF-PPO](https://github.com/yinyueqin/DenseRewardRLHF-PPO)
3. Prior work on token-level reward model for RLHF: [https://arxiv.org/abs/2306.00398](https://arxiv.org/abs/2306.00398)",1,0,1736307802.0,MachineLearning
1hvwnde,"[N] ESwML 2025 Call For Papers [March 31, 2025] (In Conjunction with ASPLOS-25/EuroSys-25)","This is a CALL FOR PAPERS for:

**ESwML 2025**

**The Second International Workshop on Empowering Software Development through Machine Learning**

[https://eswml.github.io/2025/2025.html](https://eswml.github.io/2025/2025.html)

**Important Deadlines:**

Submission due date: February 7, 2025 (AoE)  
Author notification: February 21, 2025  
Workshop scheduled date: March 31, 2025

**Call For Papers**

The software of tomorrow will heavily rely on the use of machine learning models.   
This will span various aspects including using Machine Learning (ML) models during   
software development time to enhance developer productivity, designing ML   
heuristics to improve application execution, and adopting surrogate Neural   
Networks (NN) models within applications to replace expensive computations  
 and accelerate their performance. However, several challenges limit the   
broad adoption of ML in today’s software. The goal of Empowering Software  
 Development through Machine Learning (ESwML) half-day workshop is to establish   
a platform where researchers, scientists, application developers, computing center staff,  
 and industry professionals can come together to exchange ideas and explore how artificial  
 intelligence can help in effective and efficient use of future systems.

This workshop will actively drive discussion and aim to answer the following questions:

This workshop will actively drive discussion and aim to answer the following questions:

\* How can we leverage the advances in Machine Learning to ease the software development process?  
\* What tools are missing to bridge the interaction with ML models during application development?  
\* Can we improve the accuracy and efficiency of ML models by exposing to them existing analytical tools? For example, enabling Large Language Models to interact with memory sanitizers etc.  
\* How can we seamlessly integrate ML models into applications to improve their performance while  
   ensuring the correctness of the generated outputs?

Paper and abstract submission

We seek abstracts describing recent or ongoing research related to the research topics  
 in the ESwML workshop. All researchers and practitioners are welcome to submit their  
work for presentation at this workshop. This is an in-person workshop and only the slides   
will optionally be posted on the workshop website.

Short papers must be submitted electronically as PDF files. Format is 1-4 double-column   
Pages excluding references. Submissions should be printable on US Letter or A4 paper.  
Please submit your manuscripts through hotcrp.  
[https://eswml25.hotcrp.com/](https://eswml25.hotcrp.com/)

Note: Presentations and short papers will be made available online only with the explicit consent   
of the authors. Authors who wish to share their presentations are encouraged to inform the workshop organizers.

Workshop Co-chairs  
\* Florina Ciorba (University of Basel, Switzerland), florina.ciorba at [unibas.ch](http://unibas.ch)  
\* Harshitha Menon (Lawrence Livermore National Laboratory, USA), harshitha at [llnl.gov](http://llnl.gov)  
\* Konstantinos Parasyris (Lawrence Livermore National Laboratory, USA) parasyris1 at [llnl.gov](http://llnl.gov)",3,0,1736270718.0,MachineLearning
1hvj7fx,[D] Optimization techniques in NLP/LLM that also works in transformers based sequence modeling?,"Title.

Trying to brainstorm if there are techniques that work in NLP use cases that I can apply in sequence modeling.

Specifically, I am trying to optimize the transformers used in recommender systems (user representation modeling).

So far the basics I can think of are: flash attention, efficient/linear transformers, fused kernel embedding, mixed precision/quantization for training/serving.

Anything else or any other papers come to mind?

I think the main problem sometimes is that the concept of a token in something like user sequence representation or rec sys is drastically different from that of LLM. We also deal with embeddings that are much more sparse…

Thanks in advance!",20,2,1736223592.0,MachineLearning
1hvotzx, [D][R] How to stay up-to date in Neural Architecture Search,"Greetings all, specifically I am looking for recommendations into venues which publish literature for the field of Neural Architecture Search, aside from [AutoML](https://2025.automl.cc/) and [NeurIPS](https://neurips.cc/). Any newsletters or blogs and the like would also be highly appreciated (aside from [automl itself](https://www.automl.org/) ofc).



Other than the aforementioned info my interests lie in the intersection of NAS techniques into Computer Vision and RL if it helps in any way.



Thank you in advance and cheers!",6,2,1736246997.0,MachineLearning
1hw04bx, [D] Hyperparameter Optimization with Metaheuristic algorithms ,"I'm currently working on my thesis on this topic, I started off with image classification with CNN's as my professor suggested it. However apparently I can not run more than 25-30 iterations because it's heavy on ram. There are not much papers about this area too. I see that there are much faster algorithms like Bayesian Optimization, and they yield similar results.  
Is this is a dead area of research? Where can I go from here?",1,0,1736279262.0,MachineLearning
1hvvto8,[D] Which model is best for training on flattened street-level images?,"TL;DR:
I’m working on a school project to recognize locations in a small town using flattened 360° images captured with an Insta360 camera, labeled with GPS coordinates. The goal is to predict the GPS location of a regular phone photo (not 360°) by training a visual place recognition model. I’m considering DELF, LoFTR, vision transformers (ViT/DINO), or fine-tuning ResNet/EfficientNet, but I’m unsure which is best for handling equirectangular projections and this specific task. Any advice on model selection or dataset preparation would be greatly appreciated!

Hi everyone!

I’m currently working on a school project where I’m trying to recognize specific locations in a small town based on street-level images. To collect the data, I’m using an Insta360 camera and capturing 360° images at regular intervals. I’m also ensuring that the data includes images taken at different times of the day and under various weather conditions to make the model more robust.

To prepare the data for training, I’m converting the 360° images into flattened equirectangular projections. In some cases, I may also crop these into smaller views, like cube map projections. Each of these processed images is labeled with GPS coordinates, which I want the model to predict later when given a new query image. The query images would be regular photos taken with a phone, so they won’t be 360° images but instead just standard portrait or landscape shots.

I’ve been researching possible models for this task and have come across DELF, LoFTR, and vision transformers like ViT or DINO. I’m not sure which model would be the most suitable for my project, as I need something that can handle visual place recognition based on flattened or cropped 360° images. I’m also considering whether fine-tuning a pretrained model like ResNet or EfficientNet might be a better approach.

I would really appreciate any advice or recommendations on which model might work best for this kind of problem. If anyone has experience working with equirectangular projections or training datasets for visual place recognition, I’d love to hear your thoughts. Thank you in advance for your help!",0,10,1736268694.0,MachineLearning
1hvk8m0,[D] ACL ARR public anonymous preprint,I submitted my paper to ARR dec cycle and checked the box to publish public anonymous preprint. I still couldnt find a preprint link after 3 weeks. Does any one know when do i get the link for public anonymous preprint?,7,0,1736227095.0,MachineLearning
1huxrd2,[D] Misinformation about LLMs,Is anyone else startled by the proportion of bad information in Reddit comments regarding LLMs? It can be dicey for any advanced topics but the discussion surrounding LLMs has just gone completely off the rails it seems. It’s honestly a bit bizarre to me. Bad information is upvoted like crazy while informed comments are at best ignored. What surprises me isn’t that it’s happening but that it’s so consistently “confidently incorrect” territory,140,210,1736166043.0,MachineLearning
1hv5qoe,[P] Interactive and geometric visualization of Jensen's inequality,"Hi Community,

I have been learning Jensen's inequality in the last week. I was not satisfied with most algebraic explanations given throughout the internet. Hence, I wrote a post that explains a geometric visualization, which I haven't seen a similar explanation so far. I used interactive visualizations to show how I visualize it in my mind. 

  
Here is the post [https://maitbayev.github.io/posts/jensens-inequality/](https://maitbayev.github.io/posts/jensens-inequality/)

  
Let me know what you think",39,8,1736187606.0,MachineLearning
1hveq4q,[D] Mathematical proofs as benchmarks for novel reasoning?,"I'm not an expert but I have been following the academic discussion about LLMs and reasoning pretty closely and I don't think there has been any sufficient benchmarks to demonstrate reasoning as opposed to simply applying information directly from the training data (iteratively in the case of CoT).

An ideal benchmark would have 3 properties:
1. A *clear* demonstration of novel reasoning, not simply the solving of a difficult problem or the application of advanced techniques 
2. Easy (or as close to easy as possible) to verify the correctness and existence of reasoning 
3. Easy to control contamination of the training or tuning data

As for point 1 it's clear that generally the only way we can ensure novel reasoning is to use academic topics, because novel reasoning is the bulk of their purpose 
Point 2 makes a lot of fields where what constitutes correctness or reasoning is hard to determine poor choices, ie is using historical context and a list of plot points reasoning in literature? Probably not but how can you tell what is when those are key parts of analysis? How can we say what is correct in history when historians disagree on what a few artifacts from the bronze age imply?
Point 3 also eliminates many fields that are directly discussed in a wide variety of possible training material or where their general techniques are, making it infeasible to curate training data that has no contamination 

From my knowledge the only type of problem that fits is mathematical proof, specifically we can more easily isolate what is novel in a proof, more easily verify the correctness of a proof (1 expert giving a pass could detect most major errors as opposed to teams with non definitive answers), and make sure the training data is free of both the actual proof or the direct steps to it (my understanding is that o3's frontier math score was due to iteratively finding mathematical techniques that already existed and fit the knowledge it has at that stage)

Specifically I propose that the best proof for a benchmark would be one that was very significant and required the invention of new mathematics (so that it definitely requires multiple steps of novel reasoning and has a length long enough to not just guess), is no longer the state of the art (we can control contamination by using a general training set that almost certainly won't have expert mathematics and hand picked mathematics up until the proof in question, plus by having further generalization in the field it will be easy to verify alternative approachs to the proof for validity), and should be more abstract in nature, ie abstract algebra or group theory or fermat's last theorem instead of differential equation techniques so that less existing techniques directly apply

I would suspect that without novel reasoning any answers would be wrong in obvious ways and easy to detect, and any answers with only subtle errors would be easy to retry with slight differences in tuning/training to get right

So I would like to know: is this idea at all plausible? If so what proofs would be best?",9,27,1736210166.0,MachineLearning
1huvvqk,[Discussion] Embeddings for real numbers?,"Hello everyone. I am working on an idea I had and at some point I encounter a sequence of real numbers. I need to learn an embedding for each real number. Up until now I tried to just multiply the scalar with a learnable vector but it didn't work (as expected). So, any more interesting ways to do so?

Thanks",24,20,1736158515.0,MachineLearning
1hv6zb4,[P][D] Cuda-torch compatibility issue for older driver versions despite installing cuda-compat,"Hello,   
I am working on a older-version of GPU machine (due to my office not actually updating the os and GPU drivers). The Nvidia driver is Version 470.233.xx.x and it's CUDA version is 11.4

I was limited to using \`torch==2.0.1\` for the last few years. But the problem arose when I wanted to fine-tune a Gemma model for a project, whose minimum requirement is torch>=2.3. To run this, I need a latest CUDA version and GPU driver upgrade. 

The problem is that I can't actually update anything. So, I looked into a cuda-compat approach, which is a forward-compatibility layer for R470 drivers. Can I use this for bypassing the requirements? If so, my torch2.5 is still unable to detect any GPU device. 

I need help with this issue. Please!",5,2,1736190606.0,MachineLearning
1hvsouv,[D] why do we need regularization if we have learning rate. ,I know everything about both the topics but i want some solid proof or some example where i can see benefits of regularization. Please share it if you have any,0,19,1736260351.0,MachineLearning
1hux0dx,[P] Churn Prediction Two Months in the Future – Need Advice on Dataset and Model,"Hi everyone!

I recently started working as a data scientist, and I've been assigned to a project to create a churn prediction model. Specifically, the goal is to predict the probability of a customer churning precisely two months in the future

Since I'm the only one in the team and it's my first time working with real-world data, I'm not entirely sure how to approach this and make the right decisions.

For now, I structured the dataset by taking six months of historical data (e.g., customer X, 202401, features (related to that month), churn flag, customer X, 202402, features (related to that month), churn flag, etc...).

Once I did that, I used this disaggregated data and applied a Random Forest classification model. However, I ended up with very poor performance metrics.

So, I have a few questions:

* For a dataset containing monthly historical data, which model would be more appropriate to apply (in this case, for churn prediction)? Should I use Aggregation, Disaggregation with lag, Time series, Survival analysis, or something else? And in that case, how should I arrange the dataset?
* Currently, the dataset includes flags indicating whether the customer performed certain actions during that month. Is there a better way to handle this type of information?
* Do you have any tips for handling imbalanced data and which metrics to consider? I used SMOTE on the training set to balance the minority class and looked at the F1-score as a metric.
* If you suggest keeping the dataset as is or aggregating it, should the churn flag refer to two months ahead from the row’s month (e.g., customer x, 202401, features (related to that month), churn flag (churn in 202403))? Currently, I recreate the target month (two months ahead) by updating the time-varying features from the last month of the historical data.

Thanks a lot!",11,2,1736163195.0,MachineLearning
1hurxls, Self-supervised Learning - measure distribution on n-sphere [D] [R],"Most of self-supervised learning methods (SimCLR, MoCo, BYOL, SimSiam, SwAV, MS BYOL, etc.) use an n-sphere hypersphere where the extracted features (after encoder + projection/prediction head) are distributed. The loss function then uses the features distributed on this hypersphere for its loss computation.

Papers such as:

* Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, Tongzhou Wang et al.; ICML 2020
* Align Representations with Base: A New Approach to Self-Supervised Learning, Shaofeng Zhang et al; CVPR 2022
* Rethinking the Uniformity Metric in Self-Supervised Learning, Xianghong Fang et al.; ICLR 2024

and others show that these features are distributed all over the n-sphere for each class.

What are the different ways in which we can measure the distribution of these embedded features in this hypersphere? Say, if I were to randomly choose a class from ImageNet/CIFAR-100 dataset, how can I measure the distribution of all images belonging to this class on this n-sphere?",26,2,1736141613.0,MachineLearning
1hupxdg,[D] Discrete diffusion models,"What are the most promising and most recent achievements in the diffusion for discrete distributions?

So far, I have taken a look at:

* https://arxiv.org/abs/2107.03006
* https://arxiv.org/abs/2310.16834v2

Is there anything more recent or more promising?",38,4,1736134964.0,MachineLearning
1hux0p0,[D] What (human) languages to learn?,"Hi,

This is not the typical LLM doomerism post, but rather an ML-specific career discussion. 

I am an avid learner of new languages (human spoken languages), especially Latin and Romance ones.

Wanted to know if there are languages that open up interesting opportunities for ML practitioners.

Are there non-English regions with demand for ML practitioners but not enough supply of skilled, native practitioners?",7,23,1736163230.0,MachineLearning
1hubl11,[P] I made a CLI for improving prompts using a genetic algorithm,,235,22,1736096673.0,MachineLearning
1hvacrc,[D] Working With Multidimensional NPZ/PKL Data [SMPL/AMASS],"I am working on a project that involves fine tuning with human motion related data. For that, I was advised to work with the SMPL/AMASS databases which are stored in npz/pkl files. I have never worked with similar data types, but one of the groups has 3 dimensional data, which is not possible with csv. Can someone please help me how I can work with these databases.",1,3,1736198743.0,MachineLearning
1husa4d,[R] 3D Vision-Language-Action Generative World Model,,16,2,1736142867.0,MachineLearning
1huqwji,[N] In-Memory Vector Store powered by HNSW Graph,"Hey folks! I’ve now **added a fully command-based vector store** in Treds, powered by an HNSW graph for approximate nearest-neighbor searches. Here’s a quick look at the four commands:

1. **VCREATE** – Initializes a vector index, specifying parameters like maxNeighbors, layer factor, and efSearch.
2. **VINSERT** – Inserts vectors into that index.
3. **VSEARCH** – Searches for the k nearest neighbors to a given vector.
4. **VDELETE** – Deletes a vector from the index by its ID.

Commands can be executed in redis-cli, as Treds is RESP compliant. A simple session might look like

    VCREATE vec 6 0.5 100
    VINSERT vec 1.0 2.0
    VINSERT vec 2.0 3.0
    VINSERT vec 3.0 4.0
    VSEARCH vec 1.5 2.5 2

This creates an index named `vec`, inserts some 2D vectors, searches for the 2 nearest neighbors to `[1.5, 2.5].`**Vectors can be N-Dimension as well**.

If you checked out Treds before, I’d love to hear your thoughts on the new vector store addition. If you haven’t, feel free to give it a look and let me know if you have any suggestions or questions. Thanks for reading, and happy hacking!

[https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store](https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store)  
[https://github.com/absolutelightning/treds](https://github.com/absolutelightning/treds)",9,2,1736138111.0,MachineLearning
1hum9p4,[D] Any background removal models trained on FOSS data?,"I'll be contributing to a project that is very strict on copyright, down to the ML tools used. Many of the models I've found don't specify what data they're trained on (and some are trained on images generated by scrape-trained models, which isn't allowed in my case).

The closest I've found are those [BiRefNet](https://arxiv.org/abs/2401.03407) models that are trained solely on [DIS5K](https://arxiv.org/abs/2203.03041); the images are ""commercial use and mods allowed"" (presumably CC BY and/or BY-SA), but the dataset itself has [terms of use](https://github.com/xuebinqin/DIS/blob/main/DIS5K-Dataset-Terms-of-Use.pdf) that prohibit commercial usage.",6,2,1736124092.0,MachineLearning
1htz91k,"[D] Does human intelligence reside in big data regime, or small data regime?","The frontier LLMs of today have trillion+ parameters and are trained on 500 trillion+ tokens.

Human brain has 86 billion neurons and 100 trillion+ synapses.

The amount of textual information any person consumes is several orders of magnitude less than what LLMs are trained on. However, the human eye captures visual information at an approximate rate of 10Mbps. Add other senses like hearing, touch, balance, smell, and a human child consumes more information in the first few years of their life than any LLM has ever seen.

This seems to suggest that human intelligence requires big data.

But what about people who were blind from birth? What about congenital deaf-blindedness (no documented cases)?",56,60,1736057245.0,MachineLearning
1htpuuv,[R] How Barlow Twins avoid embeddings that differ by affine transformation?,"I am reading the [Barlow Twins (BT) paper](https://arxiv.org/pdf/2103.03230) and just don't get how it can avoid the following scenario.

The BT loss is minimized when the cross-correlation matrix equals the identity matrix. A necessary condition for this to happen is that the diagonal elements *C\_ii* are 1. This can be achieved in 2 different ways. For each *x*:

1. *zA*=*zB*

2. *zA*=*a*⋅*zB*\+*b*

where *zA* and *zB* are embeddings of different augmentations of the same input *x*.  In other words, embeddings can differ but this difference is masked due to: *corr(X,aX+b)=corr(X,X)=1*.

Intuitively, if our aim is to learn representations invariant to distortions, then the 2nd solution should be avoided. Are there any ideas on what drives the network to avoid this scenario?",49,6,1736028628.0,MachineLearning
1hu168k,[D] Randomised SVD/PCA for Efficient Attention Mechanisms - any potential?,"I've had this idea rattling in my brain for a little now, and would love some input on whether it has potential - there's so many proposed efficiency improvements to attention, I've lost track of what has and hasn't been tried!

The process would be something to the effect of:

1. First compute the Keys and Queries as normal
2. Then, conduct randomised PCA on the queries to identify the D largest components of the Query space. 
3. For each of the D largest components, keep the Key vector that best matches that component
4. Do regular attention on those Keys.

  
Given typical attention for a sequence of length N has complexity O(N\^2), while randomised PCA is O(D\^2), there's potentially some pretty big inference time savings here.

  
I can't see any existing research into whether this has legs. LoRA and Linformers come close in that they also use lower-rank approximations, but I think what i'm proposing is unique. Any insights?

",7,2,1736063750.0,MachineLearning
1hty2jr,"[R] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",,9,4,1736053053.0,MachineLearning
1htocnj,[R] I’ve built a big ass dataset ,"I’ve cleaned/processed and merged lots of datasets of patient information, each dataset asks the patients various questions about themselves. I also have whether they have the disease or not. I have their answers to all the questions 10 years ago and their answers now or recently, as well as their disease status now and ten yrs ago. I can’t find any papers that have done it before to this scale and I feel like I’m sitting on a bag of diamonds but I don’t know how to open the bag. What are your thoughts on the best approach with this? To get the most out of it? I know a lot of it is about what my end goals are but I really wanna know what everyone else would do first! (I have 2500 patients and 27 datasets with an earliest record and latest record. So 366 features, one latest one earliest of each and approx 2 million cells.) Interested to know your thoughts",35,37,1736024617.0,MachineLearning
1htp9tv,"[Project] Finding inputs where deep learning models fail
","Hi there! Last month at NeurIPS (an ML conference), I read an interesting paper ""Human Expertise in Algorithmic Prediction"" that describes a framework for determining where ML models are outperformed by human experts. I found the authors' work to be very interesting. Below, I explore their framework further and extend it to multiclass classification. My results are pretty surprising, showing that a group of modern model architectures have trouble with dogs and cats in CIFAR-10.

GitHub Link: [https://github.com/sunildkumar/model\_indistinguishability](https://github.com/sunildkumar/model_indistinguishability)

Paper Link: [https://arxiv.org/abs/2402.00793](https://arxiv.org/abs/2402.00793)",30,6,1736027048.0,MachineLearning
1htw7hw,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",10,9,1736046909.0,MachineLearning
1htohxj,[D] will NeurIPS invited talks be made public?,"Hi all,

The neurips 2024 has yet to make invited talks public and accessible to those not registered:  
[https://neurips.cc/virtual/2024/eventlistwithbios/invited%20talk](https://neurips.cc/virtual/2024/eventlistwithbios/invited%20talk)

people who attended the last neurips: can you access the talks online? if yes, does this mean the talks will not be made public this year? 2023, 2022 made it public:

[https://neurips.cc/virtual/2023/eventlistwithbios/invited%20talk](https://neurips.cc/virtual/2023/eventlistwithbios/invited%20talk)

[https://neurips.cc/virtual/2022/events/Invited%20Talk](https://neurips.cc/virtual/2022/events/Invited%20Talk)

thanks!



",24,2,1736024994.0,MachineLearning
1htedss,[P] Noteworthy AI Research Papers of 2024 (Part One),,83,22,1735997334.0,MachineLearning
1htm15z,[P] Implementing the StyleGAN2,"\[P\] Hi all, I've been working on a blog series recently called the path to StyleGAN2 and I finally got to the StyleGAN2. I have a writeup here: [https://ym2132.github.io/StyleGAN2](https://ym2132.github.io/StyleGAN2)

My aim is to walk through the paper the code and the training process. I hope you find it useful and I would appreciate any feedback :)",25,2,1736018530.0,MachineLearning
1hugszn,[N] SemiKong: The World’s First Open-Source Semiconductor-Focused LLM,"Anyone else heard about SemiKong? apparently its the first open-source LLM made specifically for semiconductor R&D. They’re saying it can speed up chip design by like 30% by directly integrating stuff like design protocols and simulation data into its workflow.

This seems like a pretty big deal for chip design which is usually super resource-heavy and kind of slow. Do you think more niche domain-specific LLM's like this could be the future? or are there too many challenges in integrating something like this into existing workflows?

[https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/](https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/)",0,1,1736109934.0,MachineLearning
1htkg5l,[R] Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,"Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.

[Arxiv Paper](https://arxiv.org/abs/2409.17270)",17,1,1736014408.0,MachineLearning
1htehx6,[R]Infinity ∞ : Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis,"We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024×1024 image in 0.8 seconds, making it 2.6× faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.

* Opensource：https://github.com/FoundationVision/Infinity
* Paper link: [https://arxiv.org/abs/2412.04431](https://arxiv.org/abs/2412.04431)
* Project Page：https://foundationvision.github.io/infinity.project/
* Demo website：https://opensource.bytedance.com/gmpt/t2i/invite

[Text-to-Image results from Infinity.](https://preview.redd.it/dxt24ng3ezae1.png?width=1842&format=png&auto=webp&s=8b4233e4dc05bc84fbeb797c8a46422fd9cbdaa3)

 Building on the prediction of the next resolution level, Infinity models the image space with a finer-grained bitwise tokenizer. They have expanded the vocabulary size to infinity, significantly increasing the representation space of the image tokenizer and raising the upper limits of autoregressive text-to-image generation. The model sizes have been scaled up to 20B. Currently, both the models and the code are open-sourced, and they also provide an online experience website.

What kind of chemical reaction will an infinite vocabulary and large models ignite? Experimental data shows that this new text-to-image method, named Infinity, not only directly defeats Stable Diffusion 3 in image generation quality, but also fully inherits the speed advantages of VAR. The 2B model is 3 times faster than SD3, and the 8.5B model's inference speed is 8 times faster. As a purely discrete autoregressive text-to-image model, Infinity stands out among autoregressive methods, vastly outperforming approaches like HART, LlamaGen, and Emu3, thereby establishing itself as the new king in the field of autoregressive text-to-image generation. Additionally, Infinity surpasses diffusion-based state-of-the-art methods like SDXL and Stable Diffusion 3, reclaiming ground in the battle between autoregressive and diffusion models.

[Evaluation on the GenEval and DPG benchmark. ](https://preview.redd.it/0jm384cgfzae1.png?width=2366&format=png&auto=webp&s=46552654b87625e10ed25ed8ce3520768205e5f6)

In human evaluations, users conducted double-blind comparisons of images generated by Infinity versus HART, PixArt-Sigma, SD-XL, and SD3-Medium, assessing overall appearance, instruction adherence, and aesthetic quality. HART is also based on the VAR architecture and combines diffusion and autoregressive methods, while PixArt-Sigma, SD-XL, and SD3-Medium are SOTA diffusion models. The results showed that Infinity defeated the HART model with a beat rate of nearly 90%, demonstrating Infinity's strong position among autoregressive models. Additionally, Infinity outperformed SOTA diffusion models such as PixArt-Sigma, SD-XL, and SD3-Medium with beat rates of 75%, 80%, and 65% respectively, proving that Infinity can surpass diffusion models of the same size.

[Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.](https://preview.redd.it/soxna03rfzae1.png?width=1280&format=png&auto=webp&s=4c140bf4e66d486ebfe522acb507a66f621f8f4c)

# Bitwise Token Autoregressive Modeling Enhances High-Frequency Representation

Simplicity at its finest, Infinity's core innovation lies in proposing a bitwise token autoregressive framework. By discarding the traditional ""index-wise token"" and utilizing fine-grained ""bitwise tokens"" composed of +1 or -1 for predicting the next resolution level, Infinity shows strong scaling properties. Under this framework, Infinity achieves better performance by continuously scaling the visual encoder (Visual Tokenizer) and transformer.Bitwise Token Autoregressive Modeling Enhances High-Frequency Representation

[Framework of Infinity. Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier \(IVC\), and Bitwise Self-Correction.](https://preview.redd.it/azq1z6syfzae1.png?width=2724&format=png&auto=webp&s=cede27b05c24b1e49d1d52ca08b8d74041f4a8eb)

# The infinite vocabulary extends the representation space of the Tokenizer.

From the perspective of information theory, the continuous Visual Tokenizer used by diffusion models has an infinite representation space, while the discrete Visual Tokenizer used by autoregressive models has a finite representation space. This leads to a higher compression of images by the Tokenizer used in autoregressive models, resulting in a poorer ability to reproduce high-frequency details. To improve the upper limit of autoregressive image generation, researchers have attempted to expand the vocabulary to enhance the effectiveness of the Visual Tokenizer. However, the autoregressive framework based on Index-wise Tokens is very unsuitable for expanding the vocabulary. The prediction method of Tokens in autoregressive models based on Index-wise Tokens is shown on the left side of the figure below, where the model's parameter count is directly proportional to the size of the vocabulary. When \\( d = 32 \\), the vocabulary size is \\( 2\^{32} \\), and the transformer classifier predicting Index-wise Tokens requires \\( 2048 \\times 2\^{32} = 8.8 \\times 10\^{12} \\) = 8.8T parameters! The parameter count of just one classifier reaches the parameter count of 50 GPT3 models, making it obviously impossible to expand the vocabulary to infinity in this situation.

https://preview.redd.it/sq7somspgzae1.png?width=1280&format=png&auto=webp&s=dc72e3ed7aebc94d51ee4edec1523cb042b43876

Speed

In addition to its superior performance, Infinity fully inherits the speed advantage of VAR in predicting the next resolution level, significantly outpacing diffusion models in inference speed. The 2B model generates a 1024x1024 image in just 0.8 seconds, which is 3 times faster than the similarly-sized SD3-Medium and 14 times faster than the 12B Flux Dev. The 8B model is 7 times faster than the similar-sized SD 3.5. The 20B model generates a 1024x1024 image in 3 seconds, still nearly 4 times faster than the 12B Flux Dev.

https://preview.redd.it/61i1mq8sgzae1.png?width=1280&format=png&auto=webp&s=b27959f9cbf4f5a8e3d1526dfb234e82ff4871b5

",19,2,1735997722.0,MachineLearning
1htch73,[P] I wrote optimizers for TensorFlow and Keras,"Hello everyone, I wrote optimizers for TensorFlow and Keras, and they are used in the same way as Keras optimizers.

[https://github.com/NoteDance/optimizers](https://github.com/NoteDance/optimizers)",21,2,1735989921.0,MachineLearning
1htpnp4,"[P] A Pure-Python, Dependency-Free Neural Network Inference Framework","Hello everyone, I’ve been working on a framework that enables the inference of small pre-trained PyTorch neural networks without requiring the installation of dependencies. The entire framework is in a single file to be easily copied into projects.

Obviously, the performance is terrible compared to PyTorch (\~500x slower), so the purpose of the framework is, firstly, when installing dependencies is impossible and, secondly, for educational purposes.

As of right now, the basic functionality is working (reading PNG images, loading model weights, and running inference of CNNs), but more advanced features are not yet implemented. If anyone is interested in using or contributing, here is the link. [Github Repo](https://github.com/manu12121999/ctrl_c_nn)",1,4,1736028093.0,MachineLearning
1ht2m3y,[D] Can LLMs write better code if you keep asking them to “write better code”?,"https://minimaxir.com/2025/01/write-better-code/

This was a thereotical experiment which had *interesting* results. tl;dr, the answer is yes, depending on your definition of ""better.""",113,39,1735953150.0,MachineLearning
1ht19tf,[R] High-performance deep spiking neural networks with 0.3 spikes per neuron,"# Abstract

Communication by rare, binary spikes is a key factor for the energy efficiency of biological brains. However, it is harder to train biologically-inspired spiking neural networks than artificial neural networks. This is puzzling given that theoretical results provide exact mapping algorithms from artificial to spiking neural networks with time-to-first-spike coding. In this paper we analyze in theory and simulation the learning dynamics of time-to-first-spike-networks and identify a specific instance of the vanishing-or-exploding gradient problem. While two choices of spiking neural network mappings solve this problem at initialization, only the one with a constant slope of the neuron membrane potential at threshold guarantees the equivalence of the training trajectory between spiking and artificial neural networks with rectified linear units. For specific image classification architectures comprising feed-forward dense or convolutional layers, we demonstrate that deep spiking neural network models can be effectively trained from scratch on MNIST and Fashion-MNIST datasets, or fine-tuned on large-scale datasets, such as CIFAR10, CIFAR100 and PLACES365, to achieve the exact same performance as that of artificial neural networks, surpassing previous spiking neural networks. Our approach accomplishes high-performance classification with less than 0.3 spikes per neuron, lending itself for an energy-efficient implementation. We also show that fine-tuning spiking neural networks with our robust gradient descent algorithm enables their optimization for hardware implementations with low latency and resilience to noise and quantization.

[https://www.nature.com/articles/s41467-024-51110-5](https://www.nature.com/articles/s41467-024-51110-5)

",73,3,1735949404.0,MachineLearning
1hsxkkk,[Discussion] I trained an AI model to generate Pokemon,"The past few months I have been working on a project to utilize deep learning to generate Pokemon images/names and predict typing. Wanted to share my results here. 

Implementation Details: [https://github.com/smaley02/Pokemon-Generation/tree/main?tab=readme-ov-file](https://github.com/smaley02/Pokemon-Generation/tree/main?tab=readme-ov-file)

All 900 Fake Pokemon: [https://smaley02.github.io/gallery.html](https://smaley02.github.io/gallery.html)

https://preview.redd.it/fzefyqu0kuae1.png?width=748&format=png&auto=webp&s=e5723c9d4696e083c7049f7e36605e9f0f51443f

  
",119,19,1735939721.0,MachineLearning
1hsp3mo,[R] / [N] Recent paper recommendations,"Hello, as the new year came, I expect many research teams to have released their work for that juicy ""et al. 2024"". I am very interested in papers regarding transformers and theoretical machine learning, but if you have a good paper to share, I will never say no to that.

Thank you all in advance and have a great day :)",23,14,1735918424.0,MachineLearning
1hso6rf,[D] ReLU + linear layers aa conic hulls,"In a neural network with ReLU activations, a composition of linear layer with matrix P onto ReLU, maps the inputs into the conic hull of the columns of P. 

Are there any papers exploiting this fact for interesting insights?",22,9,1735916020.0,MachineLearning
1htbe7x,[Discussion]  Agentic AI: Yet another hyped interface or a paradigm shift? ,"This post is for discussing the radius of impact of Agentic AI.  
Agentic AI is being served as something new on the plate, while looking deeply it looks like a conventional system which interacts with some other APIs through a framework.

Looking through different lenses:

**Developer**  
Not much deviation from conventional development. Hence minimal learning curve

**Customers**

Agentic AI might shift focus from web surfaces to chatbots or probably some new kind of surfaces. Given this happens, the role of intuitive/interative UIs may reduce

**Business**

Increase in efficiency for some, while loss for business for others. Service based companies might spearhead the development initially.

**Radius**

B2B or B2C, which will be impacted more.",0,24,1735985199.0,MachineLearning
1hsv7if,[D] Thoughts and suggestions ,"I have a project that need a real time object detection by using Al, currently i am planning to use the raspberry pi 4b 8gb ram but i notice that when i use the laptop i found it quite heavy to run it so maybe raspberry pi might not have enough power to run it due to absence of gpu, so in your opinion does the handheld gaming console (steam deck, rog ally) is good enough to train and run the Al because i need a device that have a compact size but powerful enough, i have consider the jetson nano and mini pc but both of them is quite pricey. i am looking for the 2nd hand model only. Thank you",3,3,1735933674.0,MachineLearning
1hs41pt,[Discussion] How is LLM changing your job as a ML engineer ,"I just watched Andrew Ng’s talk on AI agents. He talked about how traditional ML tasks could take 6 months but now it only needs a weekend with LLMs. 

It’s at 2-4mins into this talk. https://youtu.be/KrRD7r7y7NY?si=XDCAm7NFTMO3ayn3 

Specifically, I guess he’s saying you can do zero shot learning with LLMs instead of gathering large amounts of labelled data, build and deploy a model. He used the example of sentiment analysis tasks. 

I wonder if any one is experiencing this shift in productivity at work as a ML scientist. 

My experience is companies don’t want to use chatGPT directly and try to build their own in house LLMs, I guess for data privacy and cost concerns. 

Please share your experience. ",126,50,1735850958.0,MachineLearning
1hsplsc,[D] / [R] What are your thoughts on LLMs 'understanding' their domain and enhancing domain understanding?,"Hello everyone,

I've been thinking about studying the effects of trying to enhance an LLMs understanding of the domain it is applied to, but I'm unsure if it's worthwhile and if there's enough to go off.

Without explaining too much and boring you guys: Basically, during my last project I fine-tuned LLama by throwing a dataset with 200 examples for two classes at it (400 examples in total), and got an F1 of around 76%. This also included a few-shot prompt.

But I can't help but wonder what if the LLM was taught the domain context more properly, maybe through ontologies and knowledge graphs? And could custom tokenization improve its ability to understand and generate better responses?

I'm thankful for any input you might have and if anything comes to mind that I could look into to enhance a models understanding of its domain. If you think this isn't worthwhile, I'd also be happy to hear it and maybe why you think so.",2,6,1735919765.0,MachineLearning
1hs7syq,[Project] Making a chess engine visualization that lets you see how a neural network based chess engine thinks,"Hey everyone, I'm a hs student working on this chess visualization tool for a school project that uses lc0, featuring neural network evaluation heatmaps made through the verbose output mode and engine analysis. You can play against the engine or use it as an analysis tool to see how a NN based engine to see how it ""thinks"". link to

youtube preview: [https://www.youtube.com/watch?v=7nbWr8TR6nA](https://www.youtube.com/watch?v=7nbWr8TR6nA)

[opening screen of game](https://preview.redd.it/ic9ksgr5znae1.png?width=1784&format=png&auto=webp&s=cf23028e3deee99b7e048461953006c0fd13dd13)

github: [https://github.com/jay63683/BlackBox-Chess-a-XAI-leela-chess-GUI](https://github.com/jay63683/BlackBox-Chess-a-XAI-leela-chess-GUI) Requires Processing to run. Or you can just watch the video tutorial if you dont want to download processing. Planning switching engine to ONNX  for future updates that allow me to explain processes much more in depth using ONNX tools. Would appreciate any feedback.",39,4,1735860388.0,MachineLearning
1hsy4hk,[D] ML Widely Adopted in Anti-Cheat Solutions,"Hey everyone,

I’ve been working on an anti-cheat plugin/add-on recently for my old-time favourite game, and something’s been bugging me: why don’t we see more anti-cheat solutions using machine learning? In my research, I didn’t come across many established or paid options that clearly advertise ML as part of their system. Actually, not many settled framework- or engine-agnostic solutions are out there except the few ""industry-standard"" ones, like EAC or BattleEye that can't be integrated by anyone easily.

The only one I've found that seems to hint at it is I3D FairFight, but all I could find were vague PR whitepapers with no real technical details. It’s hard to tell if they’re actually using ML in a meaningful way or just throwing around buzzwords for marketing.

This got me thinking: why hasn’t ML become a standard for anti-cheat? Is it because of scalability issues, a lack of training data, or maybe companies just don’t want to reveal their methods to avoid giving cheaters a head start? Or could it be that ML *is* used more widely, but it’s kept under wraps?

In my case, building a custom ML-based system isn’t an option right now - it’d be too much of a headache to scale properly. That said, I do have access to a ton of data that could be used for training if I could find the right solution.

So, I’ve got a few questions for anyone here who’s familiar with this space:

1. Do you know of any other anti-cheat solutions that actually use ML?
2. What do you think is holding the industry back from adopting ML more openly?
3. Are there any resources or companies you’d recommend checking out to learn more about ML in anti-cheat?

",0,8,1735941112.0,MachineLearning
1hs45oi,[D] Test-time compute for image generation?,Are there any work applying an o1-like use of test-time reasoning to other modalities like image generation? Is something like this possible? Taking more time to generate more accurate images,14,8,1735851229.0,MachineLearning
1hrxipz,[R] Yi: A Family of Foundation Models Optimized Through Cascaded Data Processing and Targeted Finetuning,"The Yi team has developed a new family of open source foundation models trained on high-quality filtered data using novel data processing techniques. The core innovation is their data preparation pipeline that combines rule-based filtering with learned models to remove problematic content while preserving useful information.

Key technical aspects:
- Models range from 6B to 34B parameters using standard transformer architecture
- Multi-stage data filtering process including AI-assisted content evaluation
- Improved attention mechanisms and training stability optimizations
- Built-in safety measures integrated during training
- Efficient scaling techniques for handling long sequences

Results show strong performance across standard benchmarks:
- Competitive with similarly-sized closed source models on reasoning tasks
- Strong coding and math capabilities, particularly in the larger variants
- Maintains high performance while incorporating safety constraints
- Achieves efficiency improvements in training compute requirements

I think this work demonstrates that open source models can achieve strong results while maintaining transparency. The data processing techniques could influence how future models are trained, potentially leading to better quality outputs across the field. The efficiency improvements may help reduce the compute barrier for training large models.

I think the safety-first approach is notable, though more work is needed to ensure these protections can't be circumvented. The open source nature could accelerate research into both capabilities and safety.

TLDR: New family of open source foundation models (6B-34B params) with strong performance, achieved through novel data processing and training techniques. Demonstrates viability of transparent, safety-conscious approach to model development.

[Full summary is here](https://aimodels.fyi/papers/arxiv/yi-open-foundation-models-by-01ai). Paper [here](https://arxiv.org/abs/2403.04652).",14,0,1735835043.0,MachineLearning
1hrpuix,[R] Numerical features with factorization machines,"Happy to share our recent [TMLR paper](https://openreview.net/forum?id=M4222IBHsh), ""Function Basis Encoding of Numerical Features in Factorization Machines"", by Alex Shtoff, Elie Abboud, Rotem Stram, and Oren Somekh.  
  
This paper proposes an interesting insight into the interplay between Factorization Machines (FMs), and feature encoding using basis functions, in the context of recommender systems.  
  
The same interplay with linear models is an old classic, and most of us have learned in our ML 101 courses. Polynomial regression is one of them - we encode a feature 𝑥 using the standard polynomial basis {1, 𝑥, 𝑥², ...}.  
  
FMs are family of models that model a quadratic polynomial  
  f(𝒙)=𝑢+⟨𝒘,𝒙⟩ + ⟨𝒙,𝑽𝒙⟩  
with diag(𝑽)=𝟎, where the coefficient matrix 𝑽 is represented in some low-rank factorized form using feature embedding vectors. For example, the classical FM proposed by Rendle in 2010 is  
  f(𝒙)=𝑢+⟨𝒘,𝒙⟩ + ∑\_{i≠k}⟨𝒗ᵢ,𝒗ₖ⟩𝑥ᵢ𝑥ₖ  
where {𝒗₁, ..., 𝒗ₙ} are the feature embedding vectors.   
Such modeling allows capturing pairwise feature interactions, making them significantly more powerful than simple linear models, while also remaining fast in training and inference. This is why they are useful in recommender systems which require ranking a large catalogue in a few milliseconds, billions of times per day.  
  
There is one caveat - FMs are *linear* in any *one* of the components of 𝒙. That is why numerical features are typically quantized, or binned, before being fed to an FM. In this work we propose learning a *parametric curve* 𝒗ᵢ(𝑥ᵢ) in the embedding space corresponding to some numerical feature 𝑥ᵢ, by using a given basis to blend a set of coefficient vectors.   
  
From a theoretical perspective, this generalizes binning, since a basis of indicator functions of intervals is exactly binning. Moreover, as a function of any one feature, the model becomes a nonlinear function spanned by the given basis, and as a function of any two features, it becomes a nonlinear function spanned by the basis tensor product.   
  
From a practical recommender system perspective, the B-Spline basis is a good candidate, since it combines fast computation due to its sparsity with strong approximation properties. For example, consider four features: movie genre, user country, time since last visit, and time since first login. For a given genre, country, and time since last visit, our model is a spline function of the time since first login. For a given genre and country, our model becomes a tensor-product spline of time since last visit and time since last login. For another genre and country, it's a different tensor-product spline. This exactly the *personalization* aspect of recommender systems we need. This simple trick with factorization machines facilitates remaining extremely fast at inference and training, while significantly improving performance.  
  
We corroborate our claims by a set of numerical experiments, and an A/B test on real traffic of an online advertising product.  
  
A similar has been in parallel developed by David Rügamer in his AISTATS 2024 paper ""Scalable Higher-Order Tensor Product Spline Models"", but following a different path - extending to higher orders of factorization, instead of a wider family of factorization machines. A great paper - I recommend reading it as well!",43,3,1735809303.0,MachineLearning
1hsj3oy,[Discussion] Is it hard to create natural speech or TTS systems ?,"I see only large players (Google, Microsoft, etc) in Text to Speech (TTS) with amazing efficiency

I see TTS combined with LLMs are breakthrough in Human Computer Interaction

With lot of papers published on TSS, what are the limitation for small orgs to create TTS
___

Edit:
---
Since this not an LLM, compute & data requirement is less.

Compute should cost like 10k usd for a week of training.
There should be some data vendors, who can give high quality dataset. (Deepseek, new LLM  startups should be using them)

What moat do large companies have 
1. Talent moat (Algorithm)
2. Data moat
3. Compute moat
4. Infrastructure moat

Data & Compute moat are definetly availble to small companies. For, 3 million any VC can write a check.

I doubt about the infrastructure and talent moat is what makes the large companies stand apart.",0,16,1735898261.0,MachineLearning
1hs2h7f,[D] Hyperparameters on attention layer,"https://preview.redd.it/l4kobgx6wmae1.png?width=776&format=png&auto=webp&s=7bd1b81b27f09a926eff615ca3b5f119df8b621f

hi, I was recently re-reading the CLIP paper for a project and I came across the hyperparameter definition for the transformers as the image attached.   
My understanding of these was:   
\- Embedding Dimension - the embedding dimension for the space on which tokens are projected   
\- Layers - Each of the N layers containing # Heads  
\- Width (here is my doubt) - length of the query, key and value vectors extracted per embedding.   


Am I interpreting these values correctly? I had understood Value vector is likely to have a different length to that of key and value. Apologies if this has been asked before, any comments on how hyperparameters on an attention layer are defined would be helpful. 

Thank you all!",2,7,1735847133.0,MachineLearning
1hrq7ak,Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective,,9,0,1735810913.0,MachineLearning
1hrej5v,[R]AST+Shorthand+HybridRag ,,35,10,1735770958.0,MachineLearning
1i6i3aj,How AI Agents & Data Products Work Together to Support Cross-Domain Queries & Decisions for Businesses,,1,0,1737464575.0,bigdata
1i62y89,"Hey friends! Have you heard about this awesome tool for business analysts in the VC scene? It streams live data on all the startups that scored VC funding globally, with loads of historical info! If you're curious or want to try it out, just drop a comment!",,0,0,1737411781.0,bigdata
1i5ohu9,Explore How Python is Revolutionizing Healthcare Technology,"Python is transforming healthcare technology, and here's why it’s the perfect fit! From data analysis to machine learning, Python is the go-to language for healthcare innovation. Curious to see how it’s changing the game?

https://preview.redd.it/543rf79n05ee1.jpg?width=1080&format=pjpg&auto=webp&s=d89e25b31da4989119ea82ff055be98d70feae81

",0,2,1737373924.0,bigdata
1i5693h,"Hey friends, you’ve got to check out this amazing tool that tracks VC investments in real-time! 🌐💸 It’s super useful for seeing which companies are getting funding and even offers detailed insights into industries and key players. A fantastic resource if you're diving into the VC world!",,0,0,1737313856.0,bigdata
1i4vwb3,"Hey friends, if you're curious about the VC world, I just found this amazing live investment tracker that shows all the VC funding happening globally! It's super insightful for data analysis on companies and decision makers. A game-changer if you're looking to learn the ins and outs of venture capit",,1,1,1737283678.0,bigdata
1i4j95u,Solidus AI Tech - Among Binance's Top 5 Alpha Projects!,"Everything starts again TRUMP launching her own token before she becomes president and BTC is a good start and we should not forget artificial intelligence projects

Solidus AI Tech u AITECH has solidified its leadership in Web3 and AI innovations and gained the trust of global investors by being ranked among Binance's Top 5 Alpha Projects.

https://preview.redd.it/30gbjjvswtde1.png?width=1750&format=png&auto=webp&s=514c3218a7027b993cafefd80544c267f6c6999d

Why This Matters

Visibility and Recognition: AITECH's recognition by Binance puts the project on the radar of global investors and increases investor confidence.

Adoption and Growth: Such recognition can accelerate Solidus AI Tech's adoption and support growth in its ecosystem.

Leadership: Being featured on a major platform like Binance helps Solidus AI Tech position itself as a leader in Web3 AI innovations.

What's Next ?

This achievement increases the potential for Solidus AI Tech to attract more collaboration and investment in its future projects. The Solidus AI Tech community celebrates this significant milestone and looks forward to the future.",6,0,1737239474.0,bigdata
1i3dkwk,Cancer Immunotherapy & Big Data/AI Technology,"Cancer touches millions of lives, and the journey to better treatments is one we take together. On January 23rd, 2025, at 11:00 AM EDT / 09:30 PM IST, join us for a thought-provoking webinar, The Intersection of Cancer Immunotherapy & Big Data/AI Technology. 

Link to Register: [https://www.senzmate.com/publish/webinar-7/](https://www.senzmate.com/publish/webinar-7/)",2,2,1737110300.0,bigdata
1i2vmec,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",7,2,1737052018.0,bigdata
1i2yqv1,Exploring Database Isolation Levels,,2,1,1737059971.0,bigdata
1i366ao,"High-key, if you’ve got a service to sell, I totally recommend pitching to fresh VC-funded startups! I hit $5k in monthly recurring revenue in just a month using this clever app to find decision-makers and dropping them a DM. Trust me, it’s way easier than it sounds!",,0,1,1737080871.0,bigdata
1i25j32,Connect Power BI to PowerPoint and Google Slides with Rollstack (www.Rollstack.com),,4,0,1736969263.0,bigdata
1i1wid5,Evolving Data Models: Backbone of Rich User Experiences (UX) for Data Citizens,,4,0,1736944848.0,bigdata
1i1qzdd,Free Webinar: Accelerate AI Value with Teradata and Google Cloud,"📅 Date: 01/15/2025  
⏰ Time: 7:30 AM PT / 4:30 PM CET  
🔗 Register here: [https://www.brighttalk.com/webcast/19856/632920?utm\_source=TDDev&utm\_medium=brighttalk&utm\_campaign=632920](https://www.brighttalk.com/webcast/19856/632920?utm_source=TDDev&utm_medium=brighttalk&utm_campaign=632920)

As a data professional, you want to build solutions that help your company and customers.

There is significant value in unstructured data stored in formats such as text, audio, and more, which you can leverage to achieve this goal.

Advanced Large Language Models (LLMs), like Google’s Gemini, can simplify the process of introducing structure into unstructured data, enabling individuals and organizations to derive insights that better serve their customers.

Join Janeth Graziani, Developer Advocate, Teradata and Merlin Yamssi, Lead Solutions Consultant AI/ML CoE, Google Cloud, as they explore, demo, and discuss how data analysts, engineers, and scientists, can leverage Teradata VantageCloud and Google Cloud to accelerate your AI innovation from development to production.

Janeth and Merlin are excited to share how you can:

\- Get faster results from your AI/ML initiatives by quickly building and training ML models with Vertex AI and the powerful in-database analytics functions of ClearScape Analytics  
\- Easily build and deploy powerful gen AI solutions with Teradata VantageCloud Lake, Vertex AI, and Gemini  
\- Transform customer complaint management through advanced generative AI for precise and automated classification. Janeth will give a complaints classification demo which leverages Teradata Vantage and Google Gemini.

Kate Russell, technology journalist, will moderate this webinar and make sure your questions are addressed by our experts.

https://reddit.com/link/1i1qzdd/video/wokg2qjpk3de1/player

",1,0,1736920647.0,bigdata
1i1b9pe,Just announced: Tableau Conference #TC25 Registration is Open! Who is going? ,,1,0,1736875452.0,bigdata
1i0v37l,Hey friends! I just stumbled upon this awesome tool that gathers info on VC funded startups and helps you find contacts of key decision-makers. It’s a game changer for anyone looking to pitch services! Let me know if you're curious to give it a whirl!,,0,2,1736819780.0,bigdata
1hxdk90,I learned how big data fuels AI on platforms like Instagram and Pinterest,"I wrote an article about how **AI influences social media**, deciding what we see in our feeds, ads, and content. Key points:

* **Facebook and Instagram** use Meta AI to figure out what shows up in your feed based on what you like, comment on, or share.
* **TikTok’s Monolith AI** studies what you watch and interact with to fine-tune your For You Page.
* **LinkedIn** suggests jobs, articles, and connections that match your career goals.
* **YouTube** recommends videos and even picks when ads pop up during what you watch.
* **Pinterest’s PinSage AI** suggests pins and products based on your searches and saves.

It’s remarkable how much AI controls our online experience, but sometimes it can feel a little too spot-on.

If you want to tweak what you see:

* Check your privacy settings regularly to see what data is being used.
* Use tools like “Not Interested” to refine your feed.
* Be mindful of what you interact with—it directly affects future recommendations.

If you’re curious about how it all works, here is the full article: [https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/](https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/)

Have you noticed how accurate your feeds are lately? Do you find it helpful, or is it over the top?",3,1,1736431711.0,bigdata
1hxacjv,Federated Modeling: When and Why to Adopt,,3,0,1736420337.0,bigdata
1hvjyig,"Optimizing Retrieval Speeds for Fast, Real-Time Complex Queries","Dear big data geniuses:

  
I'm using snowflake to do complex muliti-hundred line queries with many joins and window functions. These queries can take up to 20 seconds. I need them to take <1 second. The queries are fully optimized on snowflake and cant be optimized further. What do you recommend?",6,3,1736226116.0,bigdata
1hurwxn,How to create HIVE Table with multi character delimiter? (Hands On) ,,2,0,1736141542.0,bigdata
1hthw2p,"50+ Incredible Big Data Statistics for 2025: Facts, Market Size & Industry Growth",,6,0,1736007688.0,bigdata
1htc6wj,25 Best Project Management software in 2025,,0,0,1735988681.0,bigdata
1hsv9pr,About go get into Big Data ,"About to get into Big Data 

Hey there 

I’m 29 with background experience in farming, biology and nature with some skills related to tech and computers, looking forward to learn more about #BigData as I want to develop another career. 

What are your recommendations, tips, advices, etc.? 


p.s. Also my first time posting in Reddit, greetings from México🌮🌶️🇲🇽",8,3,1735933831.0,bigdata
1hsyr6i,"Hey folks! If you're in VC or a business analyst, you’ve got to check out this tool. It streams live data of VC-funded startups globally and gives you quick access to tons of company history (there's even a CSV or API option). Let me know if you want to give it a shot!",,1,0,1735942713.0,bigdata
1hs7pyr,[Poll] Has anyone used dbt's AI (dbt copilot) yet? What has your experience been?,,2,0,1735860170.0,bigdata
1hqgfr8,guidance for finish and review my first mini-project,"Hello guys , could anyone help me with reviewing and guide me thoughout my mini-project for big data ? ,this involves designing a (textual) information search engine and analyzing user reviews of your search engine.

here is the link : [https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0](https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0)",3,0,1735657345.0,bigdata
1hq0ym3,How automation and AI advanced data-driven reporting in 2024 [LinkedIn Post] ,,2,0,1735601913.0,bigdata
1hpq5vk,"Hey friends, if you're looking for a simple way to make some sales, you should consider selling to new startups that just landed venture capital! I found this awesome app that tracks real-time funding announcements, gathers verified emails of decision-makers, and even summarizes their buying hints w",,0,0,1735573971.0,bigdata
1houh90,Hadoop vs. Spark: Which One Should Beginners Learn First?,,4,6,1735472946.0,bigdata
1hotwap,Welcome to r/BigDataEngineer: Let’s Build and Grow Together!,,0,1,1735470480.0,bigdata
1hkfzpa, Big data Hadoop and Spark Analytics Projects (End to End) ,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",25,1,1734927955.0,bigdata
1hkkq2k,Searching For Hive Alternatives,"My current setup is Hive on Tez, running on YARN with data stored in HDFS.  
I feel like this setup is a bit outdated, and that the performance is not great. However I can't find alternatives.  
Every technology I found so far fails in one of the requirements that I'll mention.

I have the following requirements:

1. Be able to handle huge analytical batch jobs, with multiple heavy joins
2. Scalable (Petabytes)
3. Fault-tolerant, jobs must finish
4. On-premise

Would like to hear your suggestions!",2,3,1734948441.0,bigdata
1hkukvg,"Don't make the CFO wait. Use Rollstack to automate recurring reports (QBRs, Annual Reports, MBRs, etc.,) ",,0,1,1734980002.0,bigdata
1hj6p76,Will Data Science be a big deal in 2025?,"# 1. Getting to know Data Science

# Explaining Data Science

Think of data science as a high-tech detective blending stats, math, and code skills to sniff out cool clues and crack tough puzzles in humongous data piles.

# Why Data Science Rocks Today

Nowadays, with all our lives so wrapped up in data, data science is pretty much a magic element. It's what makes your Netflix picks so spot on, forecasts trends, and helps companies make super-smart choices.

# 2. What's Hot in Data Science

# All About Big Data Analytics

Imagine big data as an all-you-can-eat info spread. Data scientists are like skilled foodies who know how to fill their plates picking out the tasty bits of knowledge that can spice up business plans and spark new ideas.

# Machine Learning and AI Uses

Self-driving automobiles and digital helpers are causing a revolution in our tech interactions, and data scientists are the wizards working magic to make it happen.

# Ways to Present Data

Data visualization turns snooze-fest tables into enthralling masterpieces. It allows a quick grasp of intricate data and shares knowledge with others super .

# 3. What Makes Data Science So In-Demand

# The Rise of Making Choices Based on Data

Since data's become the hot commodity, companies are super eager for data pros. They need these smart folks to transform basic digits into powerful wisdom to guide top-level choices and help their biz expand.

# AI and Automation Demand More Data Pros

The demand for data scientists to create and improve algorithms for AI and automation is soaring. These skills are becoming red-hot in the employment sphere.

# Meeting the Bar for Regulatory Stuff

In our super connected era where keeping data safe is huge, companies want data scientists to help them wade through the complex rules to make sure they play fair and keep data use on the up-and-up.

# 4. The Tough and Good Stuff in Data Science

# Keeping Data Safe and Sound

With data mishaps popping up in the news, data scientists have the tough job. They've got to dig out the good stuff from the data while making sure none of the secret info gets into the wrong hands. They're juggling keeping things fresh and new with making sure everything stays locked down tight.

# Lack of Data Science Experts

As more people want data experts than there are available, this creates a tough spot but also a huge chance for folks aiming to jump into this area offering great jobs and fat paychecks.

# Data Science Rocks Various Sectors

Whether it's in health or money stuff, data science is causing a stir across different work areas. It's leading cool things like making meds just for you spotting cons, and figuring out groups of buyers, proving just how much it can do and how cool it can be.

# 5. What Data Science Might Look Like in 2025

# What to Expect in the Data Science Work Scene

Heading into 2025, folks can expect the data science job scene to keep on climbing. With companies in all sorts of businesses getting how critical data-informed decisions are, there's gonna be a huge ask for data science whizzes. Anyone in data science is looking at some pretty sweet career moves and loads of chances to snag a job.

# Tech Upgrades Making Waves in What's Next

Tech upgrades are huge in deciding [what's next for data science](https://www.usdsi.org/data-science-insights/future-of-data-science-10-predictions-you-should-know). All the cool stuff like artificial intelligence learning machines, and big-time data studies will push forward new stuff for data scientists to do in 2025. Jumping on the tech bandwagon is super important to not fall behind in data science's fast-paced world.

# 6. Tech Stuff Changing the Data Scene

# Blending Blockchain with Crunching Numbers

Blockchain is about to make a big splash in the number-crunching game. It's gonna ramp up security and make sure everything is clear and trackable when it comes to moving digits around. Merging this tech with the brainy science of data could start a whole new game for keeping our online facts straight and real when everything is linked up.

# Making Sense of Internet of Things (IoT) Stats

Okay so all these Internet of Things gadgets are spitting out crazy amounts of info that's got some real golden nuggets hidden in there. By 2025, the brainiacs working with numbers will gotta dig in with some fancy figuring-out tricks to pull out the gems from this data gush. Getting a grip on this IoT number crunching is key for groups looking to smarten up their choices and spark some fresh ideas.

# 7. What You Gotta Have to Be a Data Scientist in 2025

# Know Your Coding and Gadget Game

Data scientists waiting for 2025 got to know their stuff with a bunch of coding languages and gadgets. You gotta be tight with Python, R, SQL, and TensorFlow. Being a wizard with these allows you to mess with big complex data, cook up some solid predictive stuff, and pull out the kind of know-how that makes businesses rock and roll.

# ",0,7,1734774582.0,bigdata
1hhsukf,"Build Real-Time Systems with NATS and Pathway, Scalable Alternatives to Apache Kafka and Flink","Hey everyone! I wanted to share a tutorial created by a member of the Pathway community that explores using [NATS](https://docs.nats.io/) and [Pathway](https://pathway.com/) as an alternative to a Kafka + Flink setup.

The tutorial includes step-by-step instructions, sample code, and a real-world fleet monitoring example to show how you can simplify data pipelines while still handling large volumes of streaming data. It walks through setting up basic publishers and subscribers in Python with NATS, then integrates Pathway for real-time stream processing and alerting on anomalies.  
  
**App template link (with code and details):**  
[https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink](https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink) 

**Key Takeaways:**

* **Seamless Integration:** Pathway’s native NATS connectors allow direct ingestion from NATS subjects, reducing integration overhead.
* **High Performance & Low Latency:** NATS delivers messages quickly, while Pathway processes and analyzes data in real time, enabling near-instant alerts.
* **Scalability & Reliability:** With NATS clustering and Pathway’s distributed workloads, scaling is straightforward. Message acknowledgment and state recovery help maintain reliability.
* **Flexible Data Formats:** Pathway handles JSON, plaintext, and raw bytes, so you can choose the data format that suits your needs.
* **Lightweight & Efficient:** NATS’s simple pub/sub model is well-suited for asynchronous, cloud-native systems—without the added complexity of a Kafka cluster.
* **Advanced Analytics:** Pathway supports real-time machine learning, dynamic graph processing, and complex transformations, enabling a wide range of analytical use cases.

Would love to know what you think—any feedback or suggestions.",11,0,1734614782.0,bigdata
1hhq92i,MASTER DATA SCIENCE ACCELERATE YOUR FUTURE,"https://preview.redd.it/iebxtn3dbs7e1.jpg?width=800&format=pjpg&auto=webp&s=36f9d0083def416dadc2e309f6e0544dab469776

    Organizations need data-driven leaders. With the USDSI® Certification, master data science skills that unlock insights, fuel decisions, and accelerate business growth. Become the data expert companies trust.
    ",2,0,1734605056.0,bigdata
1hhc8b5,I built an end-to-end data pipeline tool in Go called Bruin ,"Hi all, I have been pretty frustrated with how I had to bring together bunch of different tools together, so I built a CLI tool that brings together data ingestion, data transformation using SQL and Python and data quality in a single tool called Bruin:

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)

Bruin is written in Golang, and has quite a few features that makes it a daily driver:

* it can ingest data from many different sources using [ingestr](https://github.com/bruin-data/ingestr)
* it can run SQL & Python transformations with built-in materialization & Jinja templating
* it runs Python fully locally using the amazing [uv](https://github.com/astral-sh/uv), setting up isolated environments locally, mix and match Python versions even within the same pipeline
* it can run data quality checks against the data assets
* it has an open-source [VS Code extension](https://bruin-data.github.io/bruin/vscode-extension/overview.html) that can do things like syntax highlighting, lineage, and more.

We had a small pool of beta testers for quite some time and I am really excited to launch Bruin CLI to the rest of the world and get feedback from you all. I know it is not often to build data tooling in Go but I believe we found ourselves in a nice spot in terms of features, speed, and stability.

Looking forward to hearing your feedback!

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)",7,0,1734557142.0,bigdata
1hg9u75,The Art of Discoverability and Reverse Engineering User Happiness,,2,0,1734439533.0,bigdata
1hgdqtn,String to number in case of having millions of unique values,"Hello,  
I am currently working on preprocessing big data dataset for ML purposes. I am struggling with encoding strings as numbers. I have a dataset of multiple blockchain transactions and I have addresses of sender and receivers for these transactions. I use pyspark.

I've tried String Indexer but it throws out of memory errors due to number of unique values. How should I approach it? Is hasing with SHA256 and casting to big int good approach? Wouldn't big numbers influence ML methods too much? (i will try different methods ex. random forests, gan, some based on distance etc)",1,0,1734451126.0,bigdata
1hg8twp,Data Science Projects for Beginners | Infographic,"One way to excel above your competitors in the race for top data science jobs is by showcasing your practical experience and a strong portfolio to demonstrate your data science skills and knowledge practically. Check out our detailed infographic to learn about popular [data science projects](https://www.usdsi.org/data-science-insights/role-of-major-components-in-data-science-projects) for beginners that you can work on to apply your theoretical data science knowledge practically and build a strong portfolio. 

https://preview.redd.it/qn8ilmoxbe7e1.jpg?width=1080&format=pjpg&auto=webp&s=ff704d8e8a7fe6909dae083229e1c34771b95507

",1,0,1734435744.0,bigdata
1hfup8a,Step-by-Step Tutorial: Setting Up Apache Spark with Docker (Beginner Friendly),"Hi everyone! I recently published a video tutorial on setting up Apache Spark using Docker. If you're new to Big Data or Data Engineering, this video will guide you through creating a local Spark environment.

📺 Watch it here: [https://www.youtube.com/watch?v=xnEXAD9kBeo](https://www.youtube.com/watch?v=xnEXAD9kBeo)

Feedback is welcome! Let me know if this helped or if you’d like me to cover more topics.",2,0,1734386073.0,bigdata
1hfn1h0,Free Ungated Whitepaper: Personalized healthcare reporting with data and AI,,2,0,1734366719.0,bigdata
1hdzmhe,Data-Driven Recruitment The WorkWolf Revolution,"Discover how WorkWolf is transforming the recruitment game by reducing bias and enhancing efficiency with data-driven solutions. As the future of work becomes more data-centric, HR professionals must adapt to ensure ethical and fair hiring practices. [WorkWolf Revolution](https://www.usdsi.org/data-science-insights/data-driven-recruitment-using-workwolf-to-reduce-bias-and-increase-efficiency)

https://preview.redd.it/yvoj2h33cs6e1.jpg?width=1080&format=pjpg&auto=webp&s=58d3f24e9d25fbc390dab129366db0ebb290f53b

",0,0,1734169453.0,bigdata
1hdckux,30 Best IDE Software for Developers in 2025,,0,0,1734097729.0,bigdata
1hdbw7e,"DATA VISUALIZATION IN R: CHEATSHEET AHEAD OF 2025 | INFOGRAPHIC

","Understanding data science has never been this convenient as it amalgamates with the R programming language. [Data science in R](https://www.usdsi.org/data-science-insights/data-visualization-in-r-cheatsheet-ahead-of-2025) is turning tables for deeper data-driven business insights to guide a better business landscape ahead. 

https://preview.redd.it/kehq19q88m6e1.jpg?width=1200&format=pjpg&auto=webp&s=545aa2e49a1d1c962d8e8067575aaffe1ed33ab6

",0,1,1734095544.0,bigdata
1hch7ch,Data Science Roadmap 2025,"Explore the evolutionary journey of data science as it intertwines human intelligence with cutting-edge technology. This roadmap delves into essential skills, tools, and adaptations required to thrive in the ever-changing analytics landscape of 2025. [Data Science Roadmap 2025](https://www.usdsi.org/data-science-insights/data-science-roadmap-2025-a-darwinian-evolution-of-analytics)

https://preview.redd.it/06sw992fwd6e1.jpg?width=1920&format=pjpg&auto=webp&s=d82a7a1fd59ac6a5022323e7b4d717b0feacfa3a

",4,1,1733994686.0,bigdata
1hcai3m,How Do You Do Data?,"Just curious about the types of infrastructure you folks use. Specifically, what kind of chips are you using to train/fine-tune/run your deep models?

I appreciate you filling out this  survey.

[https://forms.gle/uiAmfG9K7MpFvQtK7](https://forms.gle/uiAmfG9K7MpFvQtK7)",0,0,1733969122.0,bigdata
1hc5gm9,For those like me who like to have music on the background while working ,"I often need background music to help me increase my productivity while working. I created these playlists which I update regularly They help me stay calm, focused and productive. Perfect academia playlists! 



Ambient, chill & downtempo trip (a tasty mix of ambient, downtempo, IDM, trip-hop, electronica, jazz house music and more. Chill, hypnotic, trippy and atmospheric grooves for focus, relaxation, and deep listening) [https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg](https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg) 



Mental food (A bit of the same atmosphere as the previous one) [https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow](https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow) 



Something else (atmospheric, poetic, calm, soothing, cinematic and ambient soundscapes with a touch of mystery. Relaxing instrumental music for focus, relaxation, introspection, reading, writing, studying, meditation and mindfulness practice.) [https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS\_JvzkUYUw](https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS_JvzkUYUw) 



Pure ambient (calming ambient music designed to enhance focus, relaxation, study, meditation, sleep, and mindfulness) [https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ](https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ) 



Chill lofi day (mix of smooth lofi hip-hop beats, chillhop, jazzhop and soothing vibes. Chill background music for studying, working, reading or just unwinding) [https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ\_vPqdYQc-idTOg-kt5Vg](https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ_vPqdYQc-idTOg-kt5Vg) 


French Producers (dedicated to new independent French producers.  Several electronic genres covered but mostly chill) [https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ](https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ) 



Jrapzz (the latest in modern jazz with a mix of Nu-Jazz, Jazzhop, Acid Jazz, Jazz UK, Ambient Jazz, Jazztronica, Jazz House, Nu-Soul, Hip-Hop Jazz, rather chill) [https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA](https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA) 



Cool stuff (chill indie pop & rock fresh finds, from emerging independent artists and few recognized talents) [https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA](https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA) 



Enjoy! 


\- 


H-Music",0,4,1733954734.0,bigdata
1hbsznu,Governance for AI Agents with Data Developer Platforms,,2,0,1733922232.0,bigdata
1hbocvq,Data Science Command the Future of Businesses in 2025?,"Data science has been transforming businesses for a long time now. But are these technologies capable of changing the future of the world? Download our comprehensive resource to understand the impact of data science on the world's future. To [download](https://www.usdsi.org/data-science-insights/resources/can-data-science-command-the-future-of-businesses-in-2025), click below.

https://preview.redd.it/x9f6awrqa66e1.jpg?width=1050&format=pjpg&auto=webp&s=0bf5a1720f7671c964936a13a2921a876a025fe1

",2,0,1733902639.0,bigdata
1hb2w3b,2025 Guide to Architecting an Iceberg Lakehouse,,3,2,1733840675.0,bigdata
1hb0upu,"Hey, I collected IMO the best product analytics tools for 2025","Helloo, I made a blogpost about the possible best product analytics tools (warehouse native and traditionals). Feel free to add any experience or comment. Thank youu

https://medium.com/@pambrus7/6-product-analytics-tool-for-2025-ab9766510551",4,0,1733834425.0,bigdata
1haepvt,Has anyone tried this analytics automation tool yet? (Rollstack) What did you think? ,,3,0,1733764394.0,bigdata
1ha3ecw,Any good sources of Social Media/Search Engine Keyword Usage by Day?,"Hey there,

After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the Data Science Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat.",2,8,1733724850.0,bigdata
1ha3bc9,Certified Lead Data Scientist  2025,"Enhance your data science skills and knowledge to drive innovation, build efficient data science models, and manage data science projects effectively with the best data science certification from USDSI® for [CERTIFIED LEAD DATA SCIENTIST  - CLDS™](https://www.usdsi.org/data-science-certifications/certified-lead-data-scientist).

https://preview.redd.it/whm05s4pkr5e1.jpg?width=1080&format=pjpg&auto=webp&s=65f6ec8e5cbf2dba071249f7ca88e7b07da65971

",0,0,1733724516.0,bigdata
1h9lu8v,🚀 Quant Interview Prep - New Videos Added! 🚀,"To all aspiring Quants out there, I’ve restarted my journey of creating content around quantitative interview questions and brain teasers! These videos will help you get familiar with the types of questions typically asked in interviews for roles like quantitative analyst, data scientist, and more.📹

Check out my latest video here: [https://www.youtube.com/@prakarshduhoon1116](https://www.youtube.com/@prakarshduhoon1116)

Here is my LI: [https://www.linkedin.com/in/prakarshd/](https://www.linkedin.com/in/prakarshd/); I am ex Quant with 7 years of exp, working at top funds like Millennium and WorldQuant

If you find the content useful, feel free to like, share, and spread the word with your network. Together, we can make interview prep easier and more effective! Let's crush those interviews! 💪

[\#Quant](https://www.linkedin.com/feed/hashtag/?keywords=quant&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantInterviews](https://www.linkedin.com/feed/hashtag/?keywords=quantinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#InterviewPrep](https://www.linkedin.com/feed/hashtag/?keywords=interviewprep&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#TechInterviews](https://www.linkedin.com/feed/hashtag/?keywords=techinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#Finance](https://www.linkedin.com/feed/hashtag/?keywords=finance&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#BrainTeasers](https://www.linkedin.com/feed/hashtag/?keywords=brainteasers&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantitativeAnalysis](https://www.linkedin.com/feed/hashtag/?keywords=quantitativeanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#CareerGrowth](https://www.linkedin.com/feed/hashtag/?keywords=careergrowth&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296)",2,0,1733673485.0,bigdata
1h8tc2f,Certified Data Science Professional  2025,"Certified data science professionals are in huge demand because of the rapid adoption of data science technologies. So, kickstart your data science journey by mastering the fundamentals and building a strong foundation with the best #beginner-level CERTIFIED DATA SCIENCE PROFESSIONAL CDSP™.

https://preview.redd.it/p9v7vz6qpf5e1.jpg?width=1080&format=pjpg&auto=webp&s=72198191ab9bb6f1068cfa14c652e3767ca58d80

",0,0,1733580855.0,bigdata
1h6x0jf,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  


Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website,](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]",4,0,1733361485.0,bigdata
1h6x4jv,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  
  
Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]",2,0,1733361797.0,bigdata
1h6bm3x,Future of Data Science Technologies and Trends,"This read caters to deciphering the [future of data science](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends). Make it a priority to understand these core nuances before diving in as a seasoned data scientist! Explore the  to know more.

https://preview.redd.it/83az58h8os4e1.jpg?width=1080&format=pjpg&auto=webp&s=ff2a6e5367de2ea4028036402e0f66613ca013d7

",1,0,1733301829.0,bigdata
1h5uqly,"Rollstack Product Updates December 2024, AI-Powered Data Insights, Collections, and More",,4,1,1733251620.0,bigdata
1h5lev5,Amazon EKS Auto Mode: What It Is and How to Optimize Kubernetes Clusters,,2,0,1733225677.0,bigdata
1h5itgm,10 Essential Conda Commands for Data Science - KDnuggets,,3,0,1733214217.0,bigdata
1h4ty8p,The Rise of Open-Source Data Catalogs: A New Opportunity For Implementing Data Mesh,,1,0,1733142872.0,bigdata
1h4rugm,HOW TO BUILD YOUR ORGANIZATION DATA MATURE?,"Take your organization from data exploring to data transformed with this comprehensive guide to data maturity. Discover the four key elements that determine data maturity and how to develop a data-driven culture within your organization. Start your journey to data transformation with this insightful guide. Become [USDSI® Certified](https://www.usdsi.org/data-science-certifications) to lead your team in creating a data-driven culture. 

https://reddit.com/link/1h4rugm/video/mxjthjhrue4e1/player

",0,0,1733134555.0,bigdata
1h4mprc,TRANSFORM YOUR CAREER PATH WITH USDSI®'s DATA SCIENCE CERTIFICATION PROGRAM,"Take your data science career to the next level with USDSI’s industry relevant certification program. Whether you're a students, professionals, and career switchers, our program offers practical skills and knowledge with minimal time commitment. 

https://preview.redd.it/n3q7rz344d4e1.jpg?width=1080&format=pjpg&auto=webp&s=9ffd2bb36307533cde2c6cf2240aa820e1862960

",0,0,1733113536.0,bigdata
1h44frf,Web scraping booking,"Hi folks, I’m working on a data project with a deadline today, and I urgently need help scraping [Booking.com](http://Booking.com) for hotel data in the top 20 cities in France. Objective: I need to scrape hotel information such as: Hotel names Average ratings Number of reviews Locations (latitude and longitude, if possible) Issues I’m Facing: My script only fetches results for one city (e.g., Lyon), even though I’m iterating through 20 cities. Some requests return unexpected content, likely due to session or cookie-handling issues. I suspect [Booking.com](http://Booking.com)’s anti-scraping measures may be blocking my script. What I’ve Tried: Sending city names dynamically via query parameters with requests.get(). Using headers and cookies to mimic a real browser. Adding delays between requests to reduce the chances of being blocked. What I Need: Guidance on why my requests aren’t fetching results for all cities. Advice on handling anti-scraping measures effectively (e.g., proxies, better headers). Suggestions on switching to Selenium or sticking with Requests and BeautifulSoup. My Environment: Language: Python Libraries used so far: Requests, BeautifulSoup Target cities: python Copier le code \[""Paris"", ""Marseille"", ""Lyon"", ""Toulouse"", ""Nice"", ""Nantes"", ""Strasbourg"", ""Montpellier"", ""Bordeaux"", ""Lille"", ""Rennes"", ""Reims"", ""Toulon"", ""Saint-Étienne"", ""Le Havre"", ""Grenoble"", ""Dijon"", ""Angers"", ""Nîmes"", ""Villeurbanne""\] I urgently need assistance since my deadline is today. Any advice, code examples, or alternative approaches would be incredibly appreciated. Thank you so much for your help!",0,1,1733063340.0,bigdata
1h39503,Unfolding the Role of Black Box and Explainable AI in Data Science,"Drive greater progression with [Black Box and Explainable AI in Data Science](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science); facilitating data-driven decision-making for business worldwide. Enhance with popular machine learning models today.

https://preview.redd.it/bluysafwd04e1.jpg?width=1922&format=pjpg&auto=webp&s=1a5939b533219fbdd8ae738e907f2b6e880357c6

",1,0,1732959370.0,bigdata
1h32p1o,Hive Setting Lookup,"Setting up hive queries  in .HQL file , anyone have recommendations how I can look up all Setting options and explanations ? 

Example : SET mapreduce.job.reduces=10;



",1,0,1732934634.0,bigdata
1h2cq3v,I have a data processing scenario. suggested architectural choices,"The total amount of data is expected to be around 2-4 billion/hour. 

I need to `GROUP BY` by hour. the result after `GROUP BY` will be insert into the repository(or file system). It is expected that there will be 2-4 aggregations that will use all of the data, and 10 aggregations that will use part of the data (estimated 1/4). 

The result data will be used in subsequent calculations (it is not clear how much the data will be compressed). Raw data will no longer be required.



The current scenario I have in mind:

1. use Spark, but need to build distributed file system, scheduling service.

2. use OLAP database (e.g. Clickhouse) and utilize `Insert select` inside the database.



The company is expected to provide only 13 processing nodes (SSD), so it feels difficult to deploy both Spark and OLAP at the same time?



It is still in the preliminary research stage. Anything is possible.



Want to hear some experience advice.",5,1,1732851137.0,bigdata
1h27wg7,Domain,"Hi everyone, I have a domain name called bigdataexplained.com

The idea was to create a website to talk about big data, but I don't have time. It's a premium domain and I'm selling it for a very good price. If anyone is interested, just go to the website. There you can find instructions on how to buy everything correctly. I thought it would be interesting to post on this forum. Thanks!",1,2,1732834540.0,bigdata
1h1y0zf,Geospatial Data Analysis with Teradata Vantage ,"Discover how data scientists and developers use Teradata Vantage to perform geospatial and time-series analysis for telecommunication providers!

What is covered: 

Seamless integration of geospatial and customer data in Teradata Vantage. 

Advanced geospatial analysis with Python tools like GeoPandas, Plotly, and TeradataML. 

Quick insights and visualization of customer journeys and signal strength using Teradata’s Unbound Analytics Framework (UAF) and TrackingOp function 

Hands-On Demo: 

Analyzing cell tower locations and signal quality. 

Visualizing coverage areas with GeoJSON data. 

Mapping customer journeys with Teradata's UAF TrackingOp function. ",0,0,1732806715.0,bigdata
1h1upu0,Boost Your Credibility with Data Science Certifications,"Earning a data science certification can significantly boost your data science career. they help you gain new data science skills that are most in demand among industries, validate your knowledge, and show your commitment to lifelong learning.

There are other untold advantages a student or professional gets after earning certification from top institutes that are credible and recognized among employers. However, you need to be able to find a suitable certification program that matches your career goals and aspirations.

Check out our detailed guide about data science certification, its advantages, factors to consider while choosing the right one for you, [top data science certifications](https://www.usdsi.org/data-science-certifications), and other interesting facts. [Download](https://www.usdsi.org/data-science-insights/resources/boost-your-credibility-with-data-science-certifications) your copy now - 

https://preview.redd.it/2a13ix1ywm3e1.jpg?width=1920&format=pjpg&auto=webp&s=01686a954ae3168d3d4b02caf869a5109ef4e3e1

",0,0,1732796271.0,bigdata
1h19gg9,Achieving Sub-Second Latency with S3 Storage—A Kafka Alternative Using Pathway,"Hey everyone,

I've been working on simplifying streaming architectures in big data applications and wanted to share an approach that serves as a Kafka alternative, especially if you're already using S3-compatible storage.

You can skip description and jump to the code here: [https://pathway.com/developers/templates/kafka-alternative#building-your-streaming-pipeline-without-kafka](https://pathway.com/developers/templates/kafka-alternative#building-your-streaming-pipeline-without-kafka)

**The Identified Gap Addressed Here**

While Apache Kafka is a go-to for real-time data streaming, it comes with complexities and costs—setting up and managing clusters, incurring high costs in Confluent cloud (\~2k monthly for the use case here).

**Getting Streaming Performance with your Existing S3 Storage without Kafka**

Instead of Kafka, you can leverage [Pathway](https://github.com/pathwaycom/pathway) alongside Delta Tables on S3-compatible storage like MinIO. Pathway is a Pythonic stream processing engine with an underlying Rust engine.  
  
**Detailed Guide:**

For the technical details, including code walkthrough and benchmarks, check out this article: [Python Kafka Alternative: Achieve Sub-Second Latency with Your S3 Storage Without Kafka Using Pathway](https://pathway.com/developers/templates/kafka-alternative)

**Why Consider This Setup?**

* **Sub-Second Latency**: Benchmarks show that you can get stable sub-second latency for workloads up to 60,000 messages per second.
* **Cost-Effective**: Eliminates the need for Kafka clusters, reducing both complexity and operational costs.
* **Simplified Architecture**: Fewer components to manage, leveraging your existing S3 storage.
* **Scalable Performance**: Handles up to 250,000 messages per second with near-real-time latency (\~3-4 seconds).

**Use Cases**

This setup is suitable for various applications:

* **IoT and Logistics**: Collecting data from numerous sensors or devices.
* **Financial Services**: Real-time transaction processing and fraud detection.
* **Web and Mobile Analytics**: Monitoring user interactions and ad impressions.",9,0,1732727781.0,bigdata
1h1gjfx,10 Use Cases for Dremio in Your Data Architecture,,0,0,1732745756.0,bigdata
1h1bm38,Looking for API/Database to Identify Companies Behind IP Addresses (Not ISPs),"We’re building a tool that needs to identify specific companies behind IP addresses, but we’re running into a common issue: most services, like IPinfo, only return the ISP (e.g., Ziggo, Telenet) instead of the actual business using the IP address.

The Challenge:

For larger organizations, it's easier to identify the company behind the IP, but when it comes to smaller businesses using common ISPs or shared/dynamic IPs, we only get the ISP information. We're specifically after the company data, not just the internet service provider.

What We Need:

We need an API or a database that can accurately identify the company behind an IP address, even when that company is using a dynamic IP provided by an ISP.

Self-hosted or independent solutions are preferred. We're not interested in using another service like Leadfeeder. Instead, we want control over the data and how it integrates into our tool.

We want to find a solution that offers the best balance between price and quality.


What We’ve Tried:

We’ve used IPinfo.io, which aggregates data from sources like WHOIS records, but it often returns only the ISP for smaller businesses. We even tried the IP-to-company data API.

Reverse DNS lookups similarly lead back to the ISP instead of the company.


Our Goal:

We want to find an API or data source that provides the actual business behind an IP, not the ISP.

Alternatively, we’re open to building our own database if there's a reliable method to aggregate and map business information to IP addresses.


Questions:

1. Does anyone know of an API or data provider that can reliably return company-level data behind IP addresses?


2. Has anyone had success in creating a custom database to map businesses to IPs? If so, how did you gather and maintain this data?


3. Are there any other data sources or techniques we should be looking at to solve this problem?

Any advice or recommendations would be greatly appreciated. Thanks in advance for your help!",1,0,1732733140.0,bigdata
1h0yv0p,Unlock the Power of Data Science Framework for Business Growth,"[Data science frameworks](https://www.usdsi.org/data-science-insights/powerful-data-science-frameworks) are pivotal in managing the vast amounts of data generated today. With tools like Python and R at the forefront, they enable organizations to automate tasks and extract valuable insights that drive business decisions.

https://preview.redd.it/1mud01prbe3e1.jpg?width=1080&format=pjpg&auto=webp&s=24566620436b5112c3a0be9f576c83ef8c3063cc

",1,0,1732692306.0,bigdata
1gzrlre,BLOG: 10 Future Apache Iceberg Developments to Look forward to in 2025,,3,0,1732563290.0,bigdata
1gzb2tw,Unlock the Power of Machine Learning in Data Science,"Discover the immense potential of Machine Learning in Data Science! ML automates analysis, from simple linear models to complex neural networks, unlocking valuable insights as data grows. Embrace ML's power for a data-driven future. Master Data Science and ML through our comprehensive courses, earn Data Science certifications and start your career transformation today. Enroll now to become USDSI® certified. Register today.

https://reddit.com/link/1gzb2tw/video/vx7v7us7bz2e1/player

",2,0,1732510513.0,bigdata
1gykl8h,NameNode not working,Hi im trying to download hadoop for my exam and the namenode part in hdfs isnt working in cloudera. youtube is of no help either. pls help if anyone knows what to do.,2,4,1732429532.0,bigdata
1gy3qfb,Your opinion on entertaining educational content.,"I am trying to create educational videos striking a balance between entertainment and learning. Your feedback will valuable for further development.

Please check the videos.

Thanks.",2,0,1732379931.0,bigdata
1gy0l77,Powerful Data Science Frameworks,"Data science technology needs no introduction. Organizations have been using it for a long time now to make data-driven decisions and boost their business. Students aspiring to become successful **data scientists** know the importance of this technology in transforming industries and their applications.

However, it is rare among beginners that they are aware of the heart of data science – the powerful [***data science frameworks***](https://www.usdsi.org/data-science-insights/top-5-must-know-data-science-frameworks). These are the tools that streamline complex processes and make the life of data science professionals easier to explore and analyze data and build efficient models.

Data science frameworks, to put simply, are the collection of **data science tools** and libraries that make various kinds of data science tasks easier. Whether it is data collection, data processing, or **data visualization**, **data science professionals** can utilize popular data science frameworks to accomplish their tasks easily.

USDSI® brings a detailed infographic guide highlighting the importance of data science frameworks, their benefits, top data science frameworks, and various factors that one must consider while choosing one.

Check out the [infographic](https://www.usdsi.org/data-science-insights/powerful-data-science-frameworks) below, and learn from **TensorFlow** to **PyTorch**, what they are and what they are best suitable for. Moreover, [**data science certifications**](https://www.usdsi.org/data-science-certifications) from USDSI® can boost your data science learning endeavors. Explore these too.

https://preview.redd.it/fjxoidqltn2e1.jpg?width=1200&format=pjpg&auto=webp&s=a41a7324a8e11031d75a23896a873d8be4b5629f

",1,0,1732371426.0,bigdata
1gvvr1c,Introducing Rollstack AI-Powered Insights for Analytics and Business Intelligence Reporting,,1,0,1732125703.0,bigdata
1gvg4wn,Descarga Datos de Precios y Productos de Google Shopping,"[El comercio electrónico](https://www.octoparse.es/blog/c%C3%B3mo-extraer-datos-de-una-web-de-comercio-electr%C3%B3nico) es un campo que siempre será competitivo. Hemos tratado varios temas relacionados con el raspado de datos de determinados sitios de comercio electrónico como Amazon, Shopify, eBay, etc. Sin embargo, la realidad es que muchos minoristas pueden tener varias estrategias de marketing en diferentes plataformas, incluso para un solo artículo. Si desea comparar la información de los productos en diversas plataformas, el scraping de Google Shopping le ayudará a ahorrar mucho tiempo.

Conocido anteriormente como Product Listing Ads, Google Shopping es un servicio online proporcionado por Google que permite a los consumidores buscar y comparar productos a través de plataformas de compra online. Google Shopping permite a los usuarios comparar fácilmente los detalles de varios productos y sus precios de diferentes proveedores. Este post mostrará lo que ofrece y cómo se pueden **extraer datos de Google Shopping**.

Hablando de extracción de datos web, mucha gente podría asumir que la extracción de datos web requiere conocimientos de codificación. Con el avance de las herramientas de raspado web, este punto de vista podría ser alterado. Ahora la gente puede extraer datos fácilmente con estas herramientas, independientemente de la experiencia de codificación.

Si es la primera vez que utiliza Octoparse, puede registrarse para obtener una cuenta gratuita e iniciar sesión. Octoparse es una herramienta fácil de usar diseñada para que todo el mundo pueda extraer datos. Puede descargarla e instalarla en su dispositivo para su futuro viaje de extracción de datos. A continuación, puede seguir los pasos que se indican a continuación para extraer información de productos de Google Shopping con Octoparse.

# Plantilla de raspado de datos en línea de Google Shopping

Puede encontrar plantillas de raspado de datos en línea de Octoparse, que le permiten extraer datos directamente introduciendo varios parámetros. No necesita descargar e instalar ningún software en su dispositivo, simplemente pruebe el siguiente enlace para raspar datos de listados de productos de Google Shopping fácilmente.

[plantilla de scraping google shopping](https://preview.redd.it/mb8ygtfx2z1e1.png?width=943&format=png&auto=webp&s=020fefc786b002fe041f4c8b662d4918d3eb1822)

Con Google Shopping, puede detectar fácilmente las tendencias del mercado. Puede utilizarlo para recopilar datos sobre su mercado objetivo, sus consumidores y sus competidores. Ofrece información sobre tantas plataformas distintas, en particular, que es posible que tenga que dedicar mucho tiempo a recopilar el mismo tipo de datos de varios sitios web. Con sólo CUATRO pasos, puede raspar Google Shopping con Octoparse. Esta herramienta también está disponible en una amplia gama de sitios web de comercio electrónico. Consulte los artículos siguientes para obtener más guías.

>[*Ref: Cómo Extraer Datos de Precios y Productos de Google Shopping*](https://www.octoparse.es/blog/como-extraer-datos-de-productos-de-google-shopping)",0,0,1732072952.0,bigdata
1guw2js,A Data Manager’s True Priority Isn’t Data,,7,0,1732019610.0,bigdata
1gt15wq,Newbie in Big data,"As I’m a 23 yr old grad student in data science, my question professor given me a project where I must use databricks community edition and pysprak for applying machine learning algorithms. I’m very near to the deadline I need some project ideas and help as I’m a beginner.",2,1,1731803122.0,bigdata
1grzjff,Deep Dive into Dremio's File-based Auto Ingestion into Apache Iceberg Tables,,3,0,1731686307.0,bigdata
1gs1td8,Avoid Costly Data Migrations: 10 Factors for Choosing the Right Partner,"Most data migrations are complex and high-stakes. While it may not be an everyday task, as a data engineer, it’s important to be aware of the potential risks and rewards. We’ve seen firsthand how choosing the right partner can lead to smooth success, while the wrong choice can result in data loss, hidden costs, compliance failures, and overall headaches.

Based on our experience, we’ve put together a list of the 10 most crucial factors to consider when selecting a data migration partner: 🔗 [Full List Here](https://datacoves.com/post/data-migration-plan)

A couple of examples:

* **Proven Track Record:** Do they have case studies and references that show consistent results?
* **Deep Technical Expertise:** Data migration is more than moving data—it’s about transforming processes to unlock potential.

What factors do you consider essential in a data migration partner? Check out our full list, and let’s hear your thoughts!",1,0,1731692074.0,bigdata
1grsdrh,Newbie to Big Data,"Hi! As the title suggest I'm currently a chemical engineering undergraduate who needs to create a big data simulation using matlab so I really need help on this subject. I went through some research article but I'm still quite confused.

My professor instructed us to create a simple big data simulation using matlab which she wants next week. Any resources which could help me? ",1,1,1731661646.0,bigdata
1gro6mg,Pandas Difference Between loc[] vs iloc[],,1,0,1731644587.0,bigdata
1gr3gxd,Introducing Hive 4.0.1 on MR3,"Hello everyone,

If you are looking for stable data warehouse solutions, I would like to introduce Hive on MR3. For its git repository, please see:

[https://github.com/mr3project/hive-mr3](https://github.com/mr3project/hive-mr3)

Apache Hive continues to make consistent progress in adding new features  
and optimizations. For example, Hive 4.0.1 was recently released and it provides strong support for Iceberg. However, its execution engine Tez is currently not adding new features to adapt to changing environments.  
  
Hive on MR3 replaces Tez with another fault-tolerant execution engine MR3, and provides additional features that can be implemented only at the layer of execution engine. Here is a list of such features.  
  
1. You can run Apache Hive directly on Kubernetes (including AWS EKS), by creating and deleting Kubernetes pods. Compaction and distcp jobs (which  
are originally MapReduce jobs) are also executed directly on Kubernetes. Hive on MR3 on Kubernetes + S3 is a good working combination.  
  
2. You can run Apache Hive without upgrading Hadoop. You can also run  
Apache Hive in standalone mode (similarly to Spark standalone mode) without requiring resource managers like Yarn and Kubernetes. Overall it's very easy to install and set up Hive on MR3.  
  
3. Unlike in Apache Hive, an instance of DAGAppMaster can manage many  
concurrent DAGs. A single high-capacity DAGAppMaster (e.g., with 200+GB of memory) can handle over a hundred concurrent DAGs without needing to be restarted.  
  
4. Similarly to LLAP daemons, a worker can execute many concurrent tasks.  
These workers are shared across DAGs, so one usually creates large workers  
(e.g., with 100+GB of memory) that run like daemons.  
  
5. Hive on MR3 automatically achieves the speed of LLAP without requiring  
any further configuration. On TPC-DS workloads, Hive on MR3 is actually  
faster than Hive-LLAP. From our latest benchmarking based on 10TB TPC-DS, Hive on MR3 runs faster than Trino 453.  
  
6. Apache Hive will start to support Java 17 from its 4.1.0 release, but  
Hive on MR3 already supports Java 17.  
  
7. Hive on MR3 supports remote shuffle service. Currently we support Apache Celeborn 0.5.1 with fault tolerance. If you would like to run Hive on  
public clouds with a dedicated shuffle service, Hive on MR3 is a ready solution.  
  
If interested, please check out the quick start guide:  
  
[https://mr3docs.datamonad.com/docs/quick](https://mr3docs.datamonad.com/docs/quick)/  
  
Thanks,",2,0,1731586608.0,bigdata
1gq9eil,"Seeking Advice on Choosing a Big Data Database for High-Volume Data, Fast Search, and Cost-Effective Deployment","Hey everyone,

I'm looking for advice on selecting a big data database for two main use cases:

1. High-Volume Data Storage and Processing: We need to handle tens of thousands of writes per second, storing raw data efficiently for later processing.
2. Log Storage and Fast Search: The database should manage high log volumes and enable fast searches across many columns, with quick query response times.

We're currently using HBase but are exploring alternatives like ScyllaDB, Cassandra, ClickHouse, MongoDB, and Loki (just for logging purposes). Cost-effective deployment is a priority, and we prefer deploying on Kubernetes.

Key Requirements:

* Support for tens of thousands of writes per second.
* Efficient data storage for processing.
* Fast search capabilities across numerous columns.
* Cost-effective deployment, preferably on Kubernetes.

Questions:

1. What are your experiences with these databases for similar use cases?
2. Do you happen to know if there are other databases we should consider?
3. Do you happen to have any specific tips for optimizing these databases to meet our needs?
4. Which options are the most cost-effective for Kubernetes deployment?",4,1,1731491442.0,bigdata
1gpj7tz,So I Have A Data Product... Now What?,,2,0,1731413258.0,bigdata
1gpl5a6,Possible options to speed-up ElasticSearch performance,"The problem came up during a discussion with a friend. The situation is that they have data in ElasticSearch, in the order of 1-2TB. It is being accessed by a web-application to run searches.

The main problem they are facing is query time. It is around 5-7 seconds under light load, and 30-40 seconds under heavy load (250-350 parallel requests).

Second issue is the cost. It is currently hosted by manager ElasticSeatch, two nodes with 64GB RAM and 8 cores each, and was told that the cost around $3,500 a month. They want to reduce the cost as well.

For the first issue, the path they are exploring is to add caching (Redis) between the web application and ElasticSearch. 

But in addition to this, what other possible tools, approaches or options can be explored to achieve better performance, and if possible, reduce cost?

UPDATE:
* Caching was tested and has given good results.
* Automated refresh internal was disabled, now indexes will be refreshed only after new data insertion. It was quite aggressive.
* Shards are balanced.
* I have updated the information about the nodes as well. There are two nodes (not 1 as I initially wrote).",1,2,1731419412.0,bigdata
1gnbfea,"Solidus Hub: Alignment In AI, Data Analysis & Social Mining","Bringing together AI, Data Analysis, and Social Mining is a notable feature due to a recent partnership between Solidus and DAO Labs. We all agree that Social Mining focuses on analyzing social media posts, comments, and other online interactions to understand public opinion, sentiment, and behavior, but having a key feature of fair rewards draws the attention of content creators, it shows an aspect of individual **data ownership.**

# Solidus Hub

Solidus Hub is a specialized platform for community-driven content and engagement centered around AI and blockchain. The partnership with DAOLabs brings in an initiative that empowers community members to earn rewards in **$AITECH tokens** for creating, sharing, and engaging with content related to Solidus AI Tech. 

The combination of both projects utilizes **""Social Mining"" SaaS**, which incentivizes users to generate quality content and engage in tasks such as social media sharing and content creation. 

**Let's continue to discussion in the comment section should you need a link that addresses all your concerns!**",0,4,1731162979.0,bigdata
1gmtzix,A New Adventure Solidus Hub,"I was also excited to see Solidus AI Tech on the DAO Labs platform, which I have been involved in for 3 years by examining all the advantages of the social mining system. Solidus HUB will be a new adventure for me ",8,3,1731102284.0,bigdata
1gmnkfq,How to show your Tableau analysis in PowerPoint ,"https://preview.redd.it/w8um99cxmpzd1.png?width=1241&format=png&auto=webp&s=abd1b8c5f3bc59d4bea7016e3c47245e52c1dbc3

Here's how:

1. Create a free account at [Rollstack.com](http://Rollstack.com)
2. Add Tableau as a data source
3. Add your PowerPoint as a destination
4. Map your visuals from Tableau to PowerPoint
5. Adjust formatting as needed
6. Create a schedule for recurring reports and distribute via email

Try for free at [Rollstack.com](http://Rollstack.com) ",4,1,1731085845.0,bigdata
1gmpoxf,Where can I pull historical stock data for free or a low cost?,"I want to be able to pull pricing data for the past 10-20+ years on any stock or index in order to better understand how a stock behaves. 

I saw that Yahoo now charges you and you can only pull data that goes back so many years. Is there anywhere that I can get this data for free or for a low cost?",2,3,1731091176.0,bigdata
1glongi,ETL Revolution,"Hi everyone! I’m the Co-Founder & CEO at a startup aimed at transforming data pipeline creation through AI-driven simplicity and automation. If you're interested in learning more, feel free to check out our website and support the project. Your feedback would mean a lot—thanks! [databridge.site](http://databridge.site/)",0,2,1730980351.0,bigdata
1gkchnw,All About Parquet Part 10 - Performance Tuning and Best Practices with Parquet,,2,0,1730828576.0,bigdata
1gjkzae,All About Parquet Part 08 - Reading and Writing Parquet Files in Python,,3,0,1730743913.0,bigdata
1gjkyw9,All About Parquet Part 09 - Parquet in Data Lake Architectures,,1,0,1730743887.0,bigdata
1gioicp,GPUs Enhancing Technology and Sustainability with Solidus AI Tech,"GPUs (Graphics Processing Units) are chips specialized in creating images quickly, their demand has increased in enterprises, governments and gaming due to their ability to handle complex tasks. 

r/solidusaitech  is a company that offers energy efficient GPUs, with the use of advanced cooling technology, they reduce the environmental impact, being ideal for green data centers.

Solidus AI Tech improves technological efficiency while driving sustainable practices.",15,6,1730645323.0,bigdata
1gin0ki,I wanna start my big data first project m thinking of products analysis but i donnu where to start and what to start with i found no tutos i have installed hadoop that s all i did anyone can help please,,1,2,1730641055.0,bigdata
1gh9n2c,Looking for guidance on how i can start on the field of Bigdata and where I can begin with?,Lemme know about any books which would be helpful for me to progress in understanding the field.  ,4,3,1730478843.0,bigdata
1ggycmg,Active Graphs: A New Approach to Contextual Data Management and Real-Time Insights,"Hey r/bigdata,

I wanted to share something I’ve been working on that could shift how we think about data management and analysis. I call it Active Graphs—a framework designed to organize data not as static tables or isolated points, but as dynamic, context-aware relationships. I’m hoping to get some feedback from the community here and open a discussion on its potential.

What Are Active Graphs?

Active Graphs represent a shift in data structure: each data point becomes a “node” that inherently understands its context within a broader ecosystem, linking dynamically to other nodes based on predefined relationships. Imagine a data model that’s not just about storing information but actively interpreting its connections and evolving as new data comes in.

Key Features:

	•	Dynamic, Real-Time Relationships: Relationships aren’t rigidly defined; they adapt as new data is added, allowing for a constantly evolving network of information.
	•	Contextual Intelligence: Data isn’t just stored; it understands its relevance within the network, making complex queries simpler and more intuitive.
	•	Built for Multi-Domain Data: Active Graphs allow cross-domain insights without re-indexing or reconfiguration, ideal for industries with highly interconnected data needs—think finance, healthcare, and legal.

How Active Graphs Could Be a Game-Changer

Let’s take healthcare as an example. With Active Graphs, patient data isn’t just recorded—it’s actively mapped against diagnoses, treatments, and outcomes. You could run a query like “Show all admitted patients with Pneumonia and their most recent treatments,” and Active Graphs would deliver real-time insights based on all relevant data points. No custom code, no complex reconfiguration—just actionable insights.

Or in finance, imagine a trading bot that can adapt its strategy based on real-time contextual updates. Each trade and indicator would be dynamically linked to broader contexts (like day, week, and market sentiment), helping it make informed, split-second decisions without needing to retrain on historical data.

Why This Matters

Traditional databases and even graph databases are powerful, but they’re often limited by static relationships and rigid schemas. Active Graphs breaks from that by making data flexible, relational, and inherently context-aware—and it’s ready for integration in real-world applications.

TL;DR: Active Graphs turns data into a self-organizing, interconnected network that adapts in real-time, offering new possibilities for industries that rely on complex, evolving datasets. I’d love to hear your thoughts on this approach and how you think it might apply in your field.

Disclaimer: Active Graphs and its associated concepts are part of an ongoing patent development process. All rights reserved.",4,2,1730438629.0,bigdata
1ggekwc,All About Parquet Part 07 — Metadata in Parquet | Improving Data Efficiency,,5,0,1730381465.0,bigdata
1ggfguk,"Calling Data Engineers and Architects with hands-on experience in Real-Time and Near Real-Time Streaming Data solutions!
","Hi,

If you’re skilled in **streaming data** – from ingesting and routing to managing and setting real-time alerts – we want to hear from you! We’re seeking experienced professionals to provide feedback on a new product in development.

During the session, we’ll discuss your experience with streaming data and gather valuable insights on our latest design flow.

By participating, you’ll help shape the future of streaming data experiences!

**Study Details:**

* Qualified participants will be paid
* Time Commitment: Approximately 90 minutes.
* Format: Remote online session.

If you’re interested, please complete this short screener to see if you qualify:

https://www.userinterviews.com/projects/O-tG9o1DSA/apply.

Looking forward to hearing from you!

**Best,**  
**Yamit Provizor**  
**UX Researcher, Microsoft – Fabric**",2,0,1730383884.0,bigdata
1gfonrf,The Power Combo of AI Agents and the Modular Data Stack: AI that Reasons,,2,1,1730300606.0,bigdata
1gf6zqv,"Dremio 25.2 Release: Built-in on prem catalog, Polaris/Unity connectors, dark mode",,2,0,1730239813.0,bigdata
1gf0mtn,All About Parquet Part 05 — Compression Techniques in Parquet,,2,0,1730223861.0,bigdata
1gepdji,Beginner’s Guide to Spark UI: How to Monitor and Analyze Spark Jobs,"I am sharing my article on Medium that introduces Spark UI for beginners.

It covers the essential features of Spark UI, showing how to track job progress, troubleshoot issues, and optimize performance. 

From understanding job stages and tasks to exploring DAG visualizations and SQL query details, the article provides a walkthrough designed for beginners. 

Please provide feedback and share with your network if you find it useful.

[Beginner’s Guide to Spark UI: How to Monitor and Analyze Spark Jobs](https://medium.com/@suffyan.asad1/beginners-guide-to-spark-ui-how-to-monitor-and-analyze-spark-jobs-b2ada58a85f7)",3,3,1730188947.0,bigdata
1geo9qo,Unfolding the Role of Black Box and Explainable AI in Data Science,"USDSI® can be the key differentiator that stands you out from the herd and propel your career forward. [black box ai](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science)

https://preview.redd.it/4nh8h8td4nxd1.jpg?width=1922&format=pjpg&auto=webp&s=6a77e81bbf25d35e8cd21972f6820113d3128455

",1,0,1730183697.0,bigdata
1ge2okb,All About Parquet Part 04 — Schema Evolution in Parquet,,1,0,1730123834.0,bigdata
1ge2ktf,CAN DATA SCIENCE COMMAND THE FUTURE OF BUSINESSES IN 2025?,"Foster huge growth with top skills in **data visualization,** [data mining, and machine learning](https://www.usdsi.org/data-science-insights/resources/can-data-science-command-the-future-of-businesses-in-2025) today. Look at the interesting trends and future that data science holds.

https://preview.redd.it/791mf2gl5ixd1.jpg?width=1921&format=pjpg&auto=webp&s=254ee52dcdfb761eda8bc3a6c3162249251977b3

",0,0,1730123560.0,bigdata
1gcdb39,HOW TO GAIN KNOWLEDGE IN DATA SCIENCE | INFOGRAPHIC,"Data science is an interdisciplinary field and to succeed in your data science career path, you must have a strong knowledge in the foundational subjects and core disciplines of data science which are Mathematics and statistics, computer science, and domain or industry knowledge.

The knowledge of programming language, mathematical concepts like probability distribution, linear algebra, and business acumen will help you understand the business problem efficiently and develop accurate data science models.

Explore the core data science subjects that you must master before starting your career in data science and learn about specialized data science components like data analysis, data visualization, data engineering, and more in this detailed [infographic](https://www.usdsi.org/data-science-insights/how-to-gain-knowledge-in-data-science).

https://preview.redd.it/f9e7l7u251xd1.jpg?width=1200&format=pjpg&auto=webp&s=1e81077f56cd6de3e14fe8d147c3d6ab7c78fd0d

",0,0,1729917568.0,bigdata
1gbuywq,"All About Parquet Part 03 — Parquet File Structure | Pages, Row Groups, and Columns",,5,0,1729864892.0,bigdata
1gbpdlh,Folks who do data modeling: what is your biggest pain in the a**??,"What is your most challenging and time consuming task?  
Is it getting business requirements, aligning on naming convention, fixing broken pipelines?

We want to build internal tools to automate some of the tasks thanks to AI and wish to understand what to focus on.",4,1,1729844569.0,bigdata
1gbvjq3,Looking for database + analytics solution to analyze 3D printed data,"Hello, I am looking for a software which can injest data from a 3D printer and provide a analytics sandbox where that data can be analyzed / dashboards can be built. The type of data ranges from PLC data (export JSON), log files (text), csv files, to images. I am looking at solutions such as Cloudera (seems expensive) or SPLUNK. Does anybody have any other advise for such a flexible software solution that is also affordable? Thanks!",1,2,1729866467.0,bigdata
1gbs2er,"How to become famous in data analytics without needing to film a Youtube video every week or building an open source library that you have to maintain. Come up with your own 'number', 'coefficient', or 'theorem'.",,1,0,1729855966.0,bigdata
1gb616a,All About Parquet Part 02 - Parquet's Columnar Storage Model,,3,1,1729785406.0,bigdata
1gb1fhp,The Data Product Marketplace: A Single Interface for Business,,2,0,1729772880.0,bigdata
1gb2b34,Partecipate to a research,"I developed this questionnaire for my PhD. It analyses the influence of the human factor in Big Data Analytics. To answer you need to work in the field of data analytics. We need to collect a large number of answers for the analysis, if you want to help us it will only take 10 minutes of your time. At the end of the questionnaire (if you have entered your email) you will receive the average of the answers so far to compare with the averages of the other answers.

[https://docs.google.com/forms/d/e/1FAIpQLSeIrT1\_ERSIcBMYOt8GcDoAKG3cHJ5b3q9W-SBQDmTbzisXBA/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSeIrT1_ERSIcBMYOt8GcDoAKG3cHJ5b3q9W-SBQDmTbzisXBA/viewform?usp=sf_link) ",0,0,1729775567.0,bigdata
1gb1oja,Transform Your Accounts Payable &Receivable with Agentic AI,,1,0,1729773691.0,bigdata
1gb00fh,A BEGINNER'S ROADMAP TO WB SCRAPING IN PYTHON USING BEAUTIFULSOUP,"Looking to explore the world of web scraping? [Python's BeautifulSoup](https://www.usdsi.org/data-science-insights/beginners-guide-to-web-scraping-in-python-using-beautifulsoup) is your gateway! Learn how to transform unstructured web data into valuable insights in just a few steps.  

https://preview.redd.it/gx7h45upsowd1.jpg?width=1080&format=pjpg&auto=webp&s=acabd6471b11dba64b2eb6ebde2be31328f39a77

",0,1,1729768140.0,bigdata
1gafeb5,Blog: All About Parquet Part 01 - An Introduction (1/10),,3,0,1729702365.0,bigdata
1g9eype,"Notion Templates Every Data Scientist Needs for Success
",,0,0,1729593207.0,bigdata
1g9bas3,Data Science v/s Cloud Computing: An Overview,"**Want to know how data science and cloud computing are shaping the future of business?** Our new guide breaks down the key differences and shows you how these technologies work together to drive innovation.

USDSI® presents this unique guide on Data Science vs Cloud computing that discusses how each of these technologies contribute for organizations to making data-driven decisions. The guide also discusses several interesting stats and facts related to data science and cloud computing, for example, AWS is the biggest player in cloud computing with a 31% market share. Did you know it?

[Download](https://www.usdsi.org/data-science-insights/resources/data-science-vs-cloud-computing-an-overview) your copy now and explore more facts.

https://preview.redd.it/oyk0bbz209wd1.jpg?width=1921&format=pjpg&auto=webp&s=5ccede7f54b63a2863512fbb066275dceffab2c3

",2,0,1729576909.0,bigdata
1g7ol3l,Need help! How to upload json files on databricks,"I'm given a project on detecting fake reviews on yelp, for this I need to use databricks and apache spark. Here, I have the dataset  downloaded in zip folder which have json files in it. As I'm completely new to use databricks, I don't know how to upload this zip file on databricks. Please need help!",1,2,1729390275.0,bigdata
1g75l6o,"This article provides a practical guideline for unit and integration testing in Apache Flink. Using a financial fraud detection application as an example, we demonstrate how to write effective tests to ensure the correctness of your Flink jobs.",,2,0,1729331948.0,bigdata
1g73bvi,Top 3 Tips Marketing Teams Need to Know About Data Science In,"https://reddit.com/link/1g73bvi/video/0c153gz5wnvd1/player

Data science is changing the game for marketers everywhere. Get ready to supercharge your strategies with data science insights for 2024. In our latest video, you will discover the top three tips every marketing team needs to know about data science. Learn how AI is reshaping marketing tactics, why data democratization is on the rise, and the crucial role of data in delivering personalized customer experiences across channels. Ready to level up? Enroll in USDSI®'s data science certifications today and unlock endless possibilities!",2,0,1729321353.0,bigdata
1g67wsy,Data Lakehouse Roundup #1 - News and Insights on the Lakehouse,,1,0,1729218563.0,bigdata
1g5qbu6,Mind-Blowing Facts About Big Data You Can't Afford to Miss!,,3,0,1729170103.0,bigdata
1g5pwr2,"Data Engineers, Here’s How LLMs Can Make Your Lives Easier",,0,0,1729168785.0,bigdata
1g5lhz0,Functional World #12 | How to handle things in your project without DevOps around?,"This time during Functional World event, we're stepping a bit outside of functional programming while still keeping developers' needs front and center! The idea for this session actually came from our own team at Scalac, and we thought it was worth sharing with a wider audience :) We hope you'll find it valuable too, especially since more and more projects these days don't have enough dedicated DevOps support.

Check out more details about the event here: [https://www.meetup.com/functionalworld/events/304040031/?eventOrigin=group\_upcoming\_events](https://www.meetup.com/functionalworld/events/304040031/?eventOrigin=group_upcoming_events)",1,0,1729150701.0,bigdata
1g4uwhy,How Data Illuminates the Darkest Corners of Consumer Anxiety,"In a world where consumer fears dictate brand success, #data is the key to understanding the hidden drivers behind those anxieties. Equip yourself with a Data Science Certification to master the art of decoding consumer behavior and shaping the future.

https://preview.redd.it/bnjl207423vd1.jpg?width=1080&format=pjpg&auto=webp&s=03aa9a76237b9e0dc7d881194f05408371dabd2a

",2,1,1729069085.0,bigdata
1g4wgg9,Iceberg Table Maintenance: 4 Best Practices,,1,1,1729075733.0,bigdata
1g4gats,How to go about testing a new Hadoop cluster,,2,0,1729020940.0,bigdata
1g42oqh,Data-Driven Recruitment: Using Workwolf to Reduce Bias and Increase Efficiency,"https://reddit.com/link/1g42oqh/video/5vhltn6ynvud1/player

Dive into the future of hiring with our latest insights on data-driven recruitment trends! Explore how federated learning is enabling collaborative model training, while explainable AI ensures transparent and justifiable hiring decisions. ",0,0,1728979633.0,bigdata
1g3fa0g,Don’t Trust Decentralisation Yet? Game Theory Might Change Your Stance,,3,0,1728911116.0,bigdata
1g3gbl3,Done with trendytech big data course (now pls help ),"Hi guys I have done with this course it's seems to be good for me but I want to know is there any other thing which is required for DE 


I learn big data , Hadoop, mapreduce ,Hive pyspark , batch processing and stream processing , azure data engineering, azure data bricks , delta lake ,data lakes , azure synapse lake ,azure Dara factory , system design , AWS S3 Athena ,Kafka ,airflow 

Anything other required?

Also  If you guys intrested you can ping me on telegram I can help you 

Id :- @Develop_developerss ",2,0,1728914063.0,bigdata
1g1tgvb,Fresher training,"I've been enrolled to databricks (stream training) I know that databricks falls under big data. Other than that, I have no knowledge in it and have doubts on the scopes of the course. Does this course has a better opportunity for me in future? I was wishing to get enrolled in java but that didn't happen..I'm planning to jump after 2 years. Will this course help me to land in a better job? ",1,1,1728711740.0,bigdata
1g1hxe1,Increase speed of data manipulation ,"Hi there, I joined a company as Data Analyst and I received around 200gb of data in CSV file for analysis. And we are not allowed to install python, anaconda or any other software. When I upload a data to our internal software it takes around 5-6 hours. And I was trying to increase the speed of the process. What you guys can suggest? Any native Windows software solution or maybe changing hdd to latest ssd can help to increase the data manipulation process? And installed ram is 20gb.",3,6,1728674843.0,bigdata
1g1bwyd,Tutorial de redes KAN en español,[https://www.youtube.com/watch?v=Jb9wMCPUlnc](https://www.youtube.com/watch?v=Jb9wMCPUlnc),0,1,1728659057.0,bigdata
1g14h5a,DATA SCIENCE VS BUSIENESS INTELLIGENCE  VS BIG DATA,"Unravel the complexities surrounding data science, business intelligence, and big data to uncover their interconnected nature. Explore how these disciplines complement each other to transform raw data into actionable insights. 

https://preview.redd.it/ypv1psvb03ud1.jpg?width=1080&format=pjpg&auto=webp&s=d24a5f2ed90b04e3bea7d3eeba79d6e400a0b66c

",0,1,1728632638.0,bigdata
1g0mfyf,Bronze/Silver/Gold and Dremio’s Reflections,,3,0,1728576658.0,bigdata
1g0qidc,Ready to Get sheet Done ?,"[Automate data extraction in your browser. No code, no limits, no headaches.](https://reddit.com/link/1g0qidc/video/cga6j4p49ztd1/player)

Hey Folks!

We are two co-founders based in sunny Barcelona who just launched Get Sheet Done.

Get Sheet Done is a Chrome extension that enables you to scrape any website. There is no coding needed; just navigate to the website of your choosing and start building your automation. It's easy to use, affordable, and fast.

It's free for up to 1,000 records/month. Our limited launch offer is 50% off on our monthly plan for life.

You can check it out here: [https://gsd.social/rd](https://gsd.social/rd)

P.S. We plan to add more features in the future, such as integrations, data manipulation, and assistive AI. If you want to chat further, come say hi on our Discord server here: [https://getsheetdone.io/community](https://getsheetdone.io/community)

Cheers!",1,2,1728587230.0,bigdata
1g060xz,Distributed databases that handle both OLAP and OLTP workloads efficiently,"In my conversation with Adam Szymański from [Oxla](https://www.oxla.com/) on our podcast, [Cloud Frontier](https://youtu.be/oc5SBnP-ILw) by simplyblock. He had this to say: ""If you work with a typical OLAP database like Snowflake, you cannot use it efficiently in serving traffic because of long response times. Oxla can do both OLAP and OLTP, allowing for faster, more versatile use cases and simplifying the data stack"".

For those managing hybrid workloads, how do you handle the complexity of maintaining separate OLAP and OLTP databases? Would a unified approach like Oxla’s reduce your infrastructure overhead?",1,1,1728518641.0,bigdata
1fzsrvi,NVIDIA Developer Day for Healthcare and Life Sciences,"*We would like to invite you to attend the first-ever NVIDIA Developer Day focused on healthcare and life science.*

*Developers, data scientists, machine learning, AI, and infrastructure engineers working across the healthcare and life science sector are welcome to attend this free event, run by NVIDIA, with a separate track for infrastructure engineers being presented by Run:ai, Weights & Biases, and Scan Computers.*

*This is an invite-only event, tailored to your needs. Therefore, we are seeking your input on what sessions solution experts in healthcare and life sciences should run to give you maximum benefit from the day.*

*Please fill out this form to indicate your intent to attend and specify which sessions you are particularly interested in -* [https://events.bizzabo.com/NVIDIAdeveloperday](https://events.bizzabo.com/NVIDIAdeveloperday)

[*ai@scan.co.uk*](mailto:ai@scan.co.uk)

*Processing img nruvgsp0rqtd1...*

",0,0,1728484224.0,bigdata
1fywhy0,Road map for BigData Engineer ,How to get started? ,2,4,1728383154.0,bigdata
1fytzoe,Building a Robust Data Observability Framework to Ensure Data Quality and Integrity,,3,0,1728371623.0,bigdata
1fyrsto,A Closer Look at the Average Data Scientist's Salary,"The field of data science is consistently ranked among the top three most desirable job options. The compensation of data scientists is significantly greater than the normal wage scale. As of 2024, the Bureau of Labor Statistics (BLS) of the United States of America reported that the median data scientist salary in the world was $ 115,240. During the same period, the Bureau of Labor Statistics (BLS) estimated that the median annual pay for all workers was $57,928.

# Unveiling the Mystery of Average Data Scientist Salary

Are you curious about the amount of money that data scientists make in terms of their salary? 

You have arrived at the ideal location if you are thinking about pursuing a career in data science or if you are interested in learning more about the possible earnings in this profession. Within the scope of this blog, we will explore the data scientist salaries. This will include the data scientist's salary in the United States as well as the data scientist's salary in other countries across the world.

# Breaking Down the Numbers

In the modern data-driven world, there is a significant demand for data scientists. To assist firms in making decisions that are based on accurate information, these specialists play a significant role because of their capabilities to analyze and comprehend complicated data. 

As a consequence of this, pay for data scientists is quite competitive. According to the surveys, data scientists’ salary in the United States may anticipate earning a base pay of $125,645 per year on average. The wage trends of data scientists may vary greatly around the world, but they are competitive due to the high demand for talent at all times.

# Why Experience Is Crucial?

As is the case in any other industry, the amount of experience a data scientist has is a crucial factor in establishing their pay rate. 

● Data scientists in the US who are just starting and have no experience may anticipate earning around $98,600.   
● On the other hand, mid-level professionals who have one to three years of expertise can command salaries of $1,10,956.   
● Data Scientists with 3 to 5 years of experience earn about $1,21,773, whereas one with an experience of 5 to 7 years earns about $1,34,614.   
● On the other hand, senior data scientists who have more than seven years of experience might make upwards of $1,53,383, which is a reflection of the great value that is placed on experienced experts in data scientist professions. 

# Location As a Crucial Factor

As a data scientist, the location of your workplace can also have a big influence on the amount of money you make. As a result of the great demand for tech expertise in these places, tech giants in San Francisco, Seattle, and New York generally offer higher wages to data scientists. 

Data scientist jobs in rural locations or smaller towns could have slightly lower incomes than their counterparts in larger cities. In the process of comparing the various income offers in various areas, it is vital to take into consideration the cost of living.

# The Influence of Industry

The sector in which you are employed might also affect the amount of money you can make as a data scientist. Data scientists often receive greater compensation from companies operating in finance, healthcare, and technology when compared to companies operating in other industries. This is because these sectors largely rely on data analytics to drive business choices and maintain their competitiveness in the market. It contributes to the increasingly competitive wage scales for data scientists that are observed all over the world.

# Perks of Being Data Scientists

A competitive base income is typically offered to data scientists, and in addition to that, they frequently receive a variety of bonuses and benefits that further boost their entire compensation package. 

These additional incentives are frequently utilized by employers to entice and keep the best data science talent in a very competitive work market.

# Attempting to Negotiate Your Pay

When it comes to negotiating your wage as a data scientist, it is necessary to gather information and come prepared with the necessary information. You should try to establish a baseline for negotiations by gaining an understanding of the average compensation of a data scientist in the United States and throughout the world. 

During wage conversations, it is important to highlight your unique abilities and accomplishments, and you should not be hesitant to argue for better pay or more perks if you think that you contribute value to the firm.

# Final Thoughts

The salary of data scientists might vary based on several parameters, such as employment history, geographic region, and the sector in which they work. The typical salary that data scientists may anticipate earning is competitive, and they also receive extra bonuses and advantages, which is one of the reasons why many people are interested in pursuing a career in data science. As the need for [data science jobs](https://www.usdsi.org/data-science-insights/unlocking-careers-for-the-future) continues to increase, the opportunities for professions that are both profitable and satisfying in this sector continue to be high.",0,0,1728362336.0,bigdata
1fy7aui,The Skill-Set to Master Your Data PM Role | A Practicing Data PM's Guide,,3,0,1728306809.0,bigdata
1fy9c5n,"I made Faker.js wrapper in 3 hours to generate test data, do you think it is useful?","A few months ago I was working on a database migration and I used this python library to generate test datasets.

I used these datasets to populate a test database to query and see if my migration package generated the json I expected. 

The code was done with purely nested for loops in python, but it occurred to me that a friendly UI might be useful for future cases, so in one afternoon I made this with the js library's counterpart in next.js

I tried to do a product hunt release but it didn't attract much interest 😂

What do you think?

Link: https://www.data-generator.xyz/

",1,5,1728312208.0,bigdata
1fxnksn,Do data visualisation in natural languages ,"Datahorse simplifies the process of creating visualizations like scatter plots, histograms, and heatmaps through natural language commands. 

Whether you're new to data science or an experienced analyst, it allows for easy and intuitive data visualization.

https://github.com/DeDolphins/DataHorse",16,0,1728240191.0,bigdata
1fx2sk2,"Blog: Ultimate Directory of Apache Iceberg Resources (Tutorials, Education, etc.)",,7,0,1728169536.0,bigdata
1fx8uuj,A tool to simplify data pipeline orchestration,"Hello - are there any tools or platforms out there that simplify managing pipeline orchestration - scheduling, monitoring, error handling, and automated scaling, all in one central dashboard? It would abstract all this management over a pipeline that comprises of several steps and tech - e.g. Kafka for ingestion, Spark for processing, and HDFS/S3 for storage. Do you see a need for it?",1,3,1728190113.0,bigdata
1fwidyy,Big data Hadoop and Spark Analytics Projects (End to End) ,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",9,0,1728102568.0,bigdata
1fvytga,Top Data Science Trends reshaping the industry in 2025,"Data science has been a revolutionizing factor for several companies across all the industries and it will do so in the coming years as well. By leveraging data-driven decision-making and predictive models’ organizations have been able to achieve high level of productivity, efficient business operations, and enhanced consumer experience.

The great thing about the modern interconnected world is the ever-increasing amount of data which is expected to grow by 180 zettabytes by 2025 (as predicted by IDC). This means more opportunities for organizations to innovate and elevate their businesses.

For all the data science enthusiasts, USDSI® brings a comprehensive guide on various trends that are shaping the [future of data science](https://www.usdsi.org/data-science-certifications). This extensive resource will definitely influence your understanding of data science technologies and your career in it. So, [download your copy now](https://www.usdsi.org/data-science-insights/resources/top-data-science-trends-reshaping-the-industry-in-2025).",2,1,1728047561.0,bigdata
1fvxuow,"🚀 Top AI Search and Developer Tools 🤖
",,2,1,1728044621.0,bigdata
1fvf04d,"Tired of waiting 2-4 weeks for business reports? Use Rollstack for automated report generation from your BI Tools like Tableau, Looker, Metabase, and even Google Sheets. Get the reports you need now with Rollstack. Try for free or book a live demo at Rollstack.com. ",,3,3,1727981121.0,bigdata
1fv6iiy,Being good at data engineering is WAY more than being a Spark or SQL wizard. ,It’s more on communication with downstream users and address their pain points. ,7,3,1727958527.0,bigdata
1fuvn8b,OSA Con (The Open Source Analytics Conference) - Free and online Nov 19-21,"Full discloser: I am from Altinity, one of the sponsors and organizers of OSA Con, a non-vendor conference dedicated to open-source analytics.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Many devs haven’t heard about OSA Con, so I am posting it here since some of you may be interested. I highlighted a few cool talks below, but check out the program for the full list of talks.

* **Building your AI Data Hub with PyAirbyte and Iceberg** (Michel Tricot, Airbyte)
* **pg\_duckdb: adding analytics to your application database** (Jordan Tigani, DuckDB)
* **Open Source Analytic Databases - Past, Present, and Future** (Robert Hodges, Altinity)
* **Leveraging Data Streaming Platform for Analytics and GenAI** (Jun Rao, Confluent)
* **Presto Native Engine at Meta and IBM** (Aditi Pandti and Amit Dutta at Meta/IBM)
* **Vector search in Modern Databases** (Peter Zaitsev, Percona)
* **Observability for Large Language Models with Open Telemetry** (Guangya Liu and Nir Gazit)
* **Open Source Success: Learnings from 1 Billion Downloads** (Avi Press, Scarf)



Here is the website if you want to register and/or check out the full program: [osacon.io](http://osacon.io) ",3,1,1727916377.0,bigdata
1fup8i2,Milestone: 500.000 public bulk profiles available for instant analysis in the open access online R2 platform,,0,0,1727898483.0,bigdata
1ful8rs,Can Inheritance break Encapsulation while extending different common modules in pipeline?,[https://en.wikipedia.org/wiki/Inheritance\_(object-oriented\_programming)#Issues\_and\_alternatives](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)#Issues_and_alternatives),1,0,1727888535.0,bigdata
1ftvm9a,"""39 QBRs in 3 hours."" - Rollstack Customer","""39 QBRs in 3 hours."" - Rollstack Customer

Got a bunch of QBRs on your plate this week? If you use Tableau, Looker, Metabase, or Google Sheets for Analytics, you can use [Rollstack.com](http://Rollstack.com) to automate them. Try for free or book a live demo.",0,0,1727808176.0,bigdata
1fsuiup,What makes a dataset worth buying?,"Hello everyone!

I'm working at a startup and was asked to do research in what people find important before purchasing access to a (growing) dataset. Here's a list of what (I think) is important.

* Total number of rows
* Ways to access the data (export, API)
* Period of time for the data (in years)
* Reach (number of countries or industries, for example)
* Pricing (per website or number of requests)
* Data quality

Is this a good list? Anything missing?

Thanks in advance, everyone!",5,17,1727699960.0,bigdata
1fsugra,Solve Governance Debt with Data Products,,1,0,1727699787.0,bigdata
1fsp7g5,3 Best Ways to Merge Pandas DataFrames,"https://reddit.com/link/1fsp7g5/video/et2vi91r5wrd1/player

Want to seamlessly combine your data? Learn the top 3 ways to merge Pandas DataFrames. Whether it's concatenation, merging on columns, or joining on index labels, these techniques will streamline your data analysis.",0,1,1727678037.0,bigdata
1fs97es,Chew: a library to process various content types to plaintext with support for transcription,,2,0,1727629551.0,bigdata
1fs3x4y,My latest article on Medium: Scaling ClickHouse: Achieve Faster Queries using Distributed Tables,"I am sharing my latest Medium article that covers Distributed table engine and distributed tables in ClickHouse. It covers creation of distributed tables, data insertion, and query performance comparison.

Read here: https://medium.com/@suffyan.asad1/scaling-clickhouse-achieve-faster-queries-using-distributed-tables-1c966d98953b

ClickHouse is a fast, horizontally scalable data warehouse system, which has become popular due to its performance and ability to handle big data.",2,0,1727615177.0,bigdata
1fr6na0,UNLOCK THE POWER OF DATA SCIENCE IN THE 21ST CENTURY,"Discover how data science is revolutionizing [businesses in the 21st century](https://www.usdsi.org/data-science-insights/resources/the-21st-century-data-science-world-an-insight)! From evolving career paths to cutting-edge insights, mastering data science could be your gateway to growth and success.

https://preview.redd.it/fapm1swachrd1.jpg?width=1080&format=pjpg&auto=webp&s=56643e45674d342fb1ff159190f2cc7151ebb093

",0,0,1727498616.0,bigdata
1fr25hh,Need help on a project ,"I hope everyone in this forum is doing well. I am currently looking for two current or former data scientists to interview, preferably someone with less than 5 years of experience and another with more than 15 years. I would be just be asking questions about your career path, education and finances. I am free from today till Monday. If it helps someone decide on this, I would also be able to compensate for the time, about $40. The interview would be 45 mins tops with the max of 30 questions. Thanks yall, I would really appreciate it.",1,1,1727482906.0,bigdata
1fqx5fu,Trained a classification model in plain English using DataHorse,"🔥 Today, I quickly trained a classification model in English using Datahorse!

It was an amazing experience leveraging Datahorse to analyze the classic Iris dataset 🌸 through natural language commands. With just a few conversational prompts, I was able to train a model and even save it for testing—all without writing a single line of code!

What makes Datahorse stand out is its ability to show you the Python code behind the actions, making it not only user-friendly but also a great learning tool for those wanting to dive deeper into the technical side. 💻

If you're looking to simplify your data workflows, Datahorse is definitely worth exploring.

Have you tried any conversational AI tools for data analysis? Would love to hear your experiences! 💬

Check out DataHorse and give it a star if you like it to increase it's visibility and impact on our industry.

https://github.com/DeDolphins/DataHorse",0,0,1727468496.0,bigdata
1fqit37,TAKE THE ULTIMATE STEP IN DATA SCIENCE LEADERSHIP,"Elevate your career and become a [Data Science leader with CSDS™](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist). Demonstrate your technical knowledge and strategic mindset, and show the world your capability to drive business success.

https://preview.redd.it/6jojhq649brd1.jpg?width=1080&format=pjpg&auto=webp&s=9a617f1850c41ff2144a7f064a0a27eefe0797d0

",0,0,1727424903.0,bigdata
1fq9o69,Part 1: Comparing the pricing models of modern data warehouses,,5,4,1727392040.0,bigdata
1fpn91t,Deep dive into Statistical Analysis with DataHorse,"DataHorse is an open-source tool that simplifies data analysis by allowing users to perform statistical tests using natural language queries. This accessibility makes it ideal for beginners and non-technical users.

Key Features:
Conversational Queries: Users can ask questions in plain English, and DataHorse executes the relevant statistical tests.

Educational Value: Each query generates Python code, helping users learn programming and customize their analyses.

Common Statistical Tests Supported: Includes t-tests, ANOVA, and regression analysis for assessing treatment effectiveness and variable relationships.


Why It Matters

In today’s data-driven world, being able to analyze and interpret data is crucial for informed decision-making. DataHorse aims to empower individuals and organizations to engage with their data without the typical barriers of complexity.

If you're interested in learning more, check out my latest blog post where I dive deeper into how DataHorse can transform your approach to data analysis: 

Blog: https://datahorse.ai/Blogs/Statstical-Analysis.html

Star us on GitHub: https://github.com/DeDolphins/DataHorse

I’d love to hear your thoughts and any feedback you might have!


",2,0,1727322110.0,bigdata
1fpow18,How to Build Impactful Data Visualizations with Pandas and Matplotlib? | Infographic,"Do you want to create smart and impactful data visualizations? Unleash the best amalgam of pandas and Matplotlib for orchestrating data-wrangling tools to succeed!

https://preview.redd.it/4fzexh4j93rd1.jpg?width=2000&format=pjpg&auto=webp&s=a57aeac4b500d0179a12c0a2bee4290f6e2c879a

",1,0,1727328194.0,bigdata
1fp949y,Virtualization + Lakehouse + Mesh = Data at Scale,,0,0,1727282752.0,bigdata
1fognr3,Airbyte 1.0 released,,25,6,1727195215.0,bigdata
1fnj76j,"The Analytics Engineering Flywheel, Shifting Left, & More With Madison Schott ",,3,0,1727094513.0,bigdata
1fnhjmy,HOW TO BUILD IMPACTFUL DATA VISUALIZATIONS WITH PANDAS AND MATPLOTLIB?,"*Do you want to create smart and impactful* ***data visualizations***? Unleash the best amalgam of [pandas and Matplotlib](https://www.usdsi.org/data-science-insights/how-to-build-impactful-data-visualizations-with-pandas-and-matplotlib) for orchestrating **data-wrangling** tools to succeed!

https://preview.redd.it/6l2gqgldhjqd1.jpg?width=2000&format=pjpg&auto=webp&s=0e77dbc10d015aee7d131fe742f8192f211cf4a7

",0,0,1727088697.0,bigdata
1fnc7c7,Privacy-focused architecture to enable personalized experience (e.g. dynamic CTAs) using Redis and RudderStack Data Apps,,1,1,1727065336.0,bigdata
1fn2acl,"My Medium article - Handling Data Skew in Apache Spark: Techniques, Tips and Tricks to Improve Performance","I want to present my Medium article titled Handling Data Skew in Apache Spark: Techniques, Tips and Tricks to Improve Performance.

Link: https://medium.com/@suffyan.asad1/handling-data-skew-in-apache-spark-techniques-tips-and-tricks-to-improve-performance-e2934b00b021

In this article, I try to cover detecting and fixing data skew in Apache Spark, alongwith code examples. It has been written for beginners of Spark. Please review and provide feedback, and please share in your network.
",1,3,1727035583.0,bigdata
1fmxwr9,Survey on data formats [responses welcome],"The following survey aims to gather empirical data to better understand the expectations of data format users concerning comparing them.  
It should take no more than 10 minutes:  
[https://forms.gle/K9AR6gbyjCNCk4FL6](https://forms.gle/K9AR6gbyjCNCk4FL6)  
Your response would be greatly appreciated!",1,0,1727024182.0,bigdata
1fmp8gl,Best BigData tool,"I'm wondering what's the best BigData tool on demand to learn, I put my eyes on pyspark but I'm not sure if it's the right one, based on what I read pyspark is really good for streaming, and Hadoop really good when dealing with giant data but it seems it's outdated for 2024, so I'm so confuse!!",2,4,1726997016.0,bigdata
1fmph3v,Advice on how to find a software engineer to co-found a big data health company,"I am a non-technical founder looking for a software engineer to co-found an analytics platform similar to [amplitude.com](http://amplitude.com) and [cbinsights.com](http://cbinsights.com), but I have no idea on where to find someone who would want to lead a startup in that way.

  
Please advise what would interest a SE in a bootstrapped business.

  
Thanks!",0,4,1726998066.0,bigdata
1fm15d3,A Beginner's Roadmap to Python web scraping with BeautifulSoup,"Looking to explore the world of web scraping? [Python's BeautifulSoup](https://www.usdsi.org/data-science-insights/beginners-guide-to-web-scraping-in-python-using-beautifulsoup) is your gateway! Learn how to transform unstructured web data into valuable insights in just a few steps.

https://preview.redd.it/6q7dd6r6h5qd1.jpg?width=1080&format=pjpg&auto=webp&s=37a05b43e9767dc8708c4ae05a3ac3f7ead5f64a

",0,0,1726919129.0,bigdata
1fli1s5,"Imagine waking up on October 1st, and all of your QBRs were exported and in a file ready to go. Pinch yourself. It’s not a dream. It’s Rollstack. Rollstack maps your reports from your BI and analytics tools to PowerPoint, Google Slides, Word, and Docs. Schedule a discovery call or try for free today",,0,0,1726854529.0,bigdata
1fl8oo3,BECOME THE ULTIMATE DATA SCIENCE LEADER,"[Data Science leaders](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist) bridge the gap between technology and business strategy. Elevate your career by mastering both domains and becoming an invaluable asset to your organization. 

https://preview.redd.it/vai0d6gmxxpd1.jpg?width=1080&format=pjpg&auto=webp&s=2b6b53c0b3c454a2214172343e7f2c7cf122c996

",0,0,1726827813.0,bigdata
1fkug3e,Looking for a BIG DATA alternative for Reporting tool,"We have IBM Cognos in the company (it's an old company) and we have a lots of reports schedueled. Probably the reports are running all the time because of queue (175 reports run in parallel, but looks like not enough).

Data in Cognos is refreshed every three hours (I guess Cognos is connected to some Oracle server/datawarehouse).

Each time I want to build a custom report (basically pulling columns), it will never run in time and I have to wait many many hours or even next day. I will press run, and it will take so long.

-Is  there a modern solution/big data solution (although Cognos holds ERP and CRM data of a big company)?  
-Perfect solution would be all reports could be pulled instantly at anytime with no delay and all schedueled reports would come without any delay or long queues.

Please advice, I will talk to the IT team (who are all old people).",1,3,1726778218.0,bigdata
1fjp1i3,Cluster selection in Databricks is overkill for most jobs. Anyone else think it could be simplified?,"One thing that slows me down in Databricks is cluster selection. I get that there are tons of configuration options, but honestly, for a lot of my work, I don’t need all those choices. I just want to run my notebook and not think about whether I’m over-provisioning resources or under-provisioning and causing the job to fail.

I think it’d be really useful if Databricks had some kind of default “Smart Cluster” setting that automatically chose the best cluster based on the workload. It could take the guesswork out of the process for people like me who don’t have the time (or expertise) to optimize cluster settings for every job.

I’m sure advanced users would still want to configure things manually, but for most of us, this could be a big time-saver. Anyone else find the current setup a bit overwhelming?",2,1,1726653407.0,bigdata
1fjow2j,Anyone else wish you could switch roles on the fly in Databricks?,"**I wish Databricks had an easy way to switch roles while running queries**

I’ve been using Databricks for a while now, and one thing that I feel is missing is a quick way to toggle between different access roles when working with sensitive data. In some industries like healthcare and finance, the data access policies can be really strict, and sometimes I have to switch between querying production data and something like clinical data. It would be amazing if there was a built-in feature where you could just toggle between roles (like data analyst, admin, etc.) \*right at execution time\* without needing to leave the notebook.

This would make life so much easier—no more worrying about whether you’re accidentally accessing the wrong dataset for your role. It could dynamically adjust what you’re allowed to query based on your current role, which would also help reduce the chances of non-compliance or unauthorized access. Has anyone else dealt with this kind of issue? Would love to know how you're handling it.",2,0,1726652717.0,bigdata
1fjjpx6,Future Of Data Science: 10 Predictions You Should Know,"Data Science will keep evolving in 2023 and beyond. Here are the 10 predictions of Data Science.

https://preview.redd.it/lugybpdtohpd1.jpg?width=1920&format=pjpg&auto=webp&s=0c12cbe7abed6dd4c03974c41064d2096b47f199

",0,0,1726631154.0,bigdata
1fjiwbc,Want to enter Big data and AI field ,"For context I am someone with Adhd dont kmow how I am gonna be able to thrive here. 
Wanted to know is there a way to acquire certifications or credibility in this field for a total newbie without having to get a conventional degree?",0,1,1726628508.0,bigdata
1fizekl,DevOps for Developers - challenges?,"Hi everyone!

I want to talk about lack of DevOps expertise inside the organizations. Not every company can or should have a full time DevOps Engineer. Let’s say we want to train Developers to handle DevOps tasks. With the disclaimer that DevOps is the approach and not a job position :D

1/ What are the most common cases that you need DevOps for, but developers are handling it?  
2/ What kind of DevOps challenges do you have in your projects?  
3/ What DevOps problems are slowing you down?  
4/ Is there any subject you want to know from scratch or upgrade your existing knowledge - with DevOps mindeset/toolset?

Thanks!",2,0,1726580055.0,bigdata
1fi47ij,Upscaling Marketing Analytics: A CDO’s Guide to Building Data-Driven Domains,,5,0,1726492173.0,bigdata
1fi0s36,"CDC to Iceberg: 4 Major Challenges, and How We Solved Them",,2,0,1726481016.0,bigdata
1fh9zfd,Tutorial: Hands-On intro with Apache Iceberg on Your Laptop,,3,0,1726398236.0,bigdata
1feksrh,9 social media insights from my recent global hack-a-thon,"My [dbt™ Data Modeling Challenge - Social Media Edition](https://www.paradime.io/dbt-data-modeling-challenge) just wrapped up!   
  
Submissions are in, and judges are reviewing insights from data participants worldwide.   
  
Winners will be announced tomorrow, so stay tuned!   
  
This unique challenge, had participants dive into social media data, turning raw information into valuable insights.   
  
Here's a glimpse of some fascinating insights participants uncovered... 

https://preview.redd.it/n7qs3fv0t8od1.png?width=1620&format=png&auto=webp&s=b2ebf9123d6c0107399f2b30913b814b5e145dfc

https://preview.redd.it/1lt8ihv0t8od1.png?width=1156&format=png&auto=webp&s=b7ce937291edf282faa6efdaa602e42a2e4b26a4

https://preview.redd.it/wolyb6w0t8od1.png?width=1157&format=png&auto=webp&s=91e25e79b2350cf16966822dc67cb2a557676933

https://preview.redd.it/ilt8nfv0t8od1.png?width=1647&format=png&auto=webp&s=9a2e29fe07b2cc621505ad895a24bafb1a7578c9

https://preview.redd.it/fe5gtev0t8od1.png?width=1184&format=png&auto=webp&s=d9127523c39ac7e62f1878d26d0b23f20e3e6ca0

https://preview.redd.it/qo9z3fv0t8od1.png?width=1639&format=png&auto=webp&s=d8a49062fe904317946aa01fd38781976a0d133b

https://preview.redd.it/hy3ogiv0t8od1.png?width=932&format=png&auto=webp&s=4beb32f475dd17b560157e3e37efe367e96b7b6e

https://preview.redd.it/amjy77w0t8od1.png?width=1634&format=png&auto=webp&s=5056220c577ecb1331c366af216b7273cc85205d

https://preview.redd.it/v8i4afv0t8od1.png?width=1162&format=png&auto=webp&s=ae7b7cf2b38e61e125746b0861a60ec5f8abe412

",7,0,1726087833.0,bigdata
1fe9s5e,Top Enterprise Data Catalog Tools for Effective Data Management,,3,3,1726060324.0,bigdata
1feap55,Trying to understand the internal architecture of a fictitious massive database. [Salesforce related],"Hey Humes, I'm currently trying to understand the internal optimization strategy for querying a database like Salesforce may use to handle all its users data. I'm studying for a data architect exam and I'm reading into an area I have no background business of looking into, but its super interesting.

So far I know that Salesforce splits its tables for its ""objects"" into two categories.

Standard and Custom

I was looking into it, as on the surface, at least logically, it feels like abstracting the data just leads to more steps computationally. I learned that wide tables impact performance negatively but, if we have a table 3,000 columns wide, splitting into two tables 1,500 columns wide each, would still require processing 3,000 columns (if we wanted to query them all) but with the added step of switching tables. To my limited understanding this means ""requires more computational power"". However, I began reading into cost-based optimization and pattern database heuristics. It seems that there some unique problems at scale that make it a little more complicated.

I'd like to be able to get a complete picture of how a complex database like that works, however I'm not really sure where I would go for more information. I can somewhat use ChatGPT, but I feel I'm getting a bit too granular to be accurate now and I need a real book or something along those lines. (Really seems like its sending me into the weeds now.

Cheers",1,0,1726062846.0,bigdata
1fdgytj,Operationalizing Data Product Delivery in the Data Ecosystem,,3,0,1725972309.0,bigdata
1fdmtun,International School on Open Science Cloud: best showcase tech?,,1,0,1725987475.0,bigdata
1fdhbag,Big Data Spreadsheet Showdown: Gigasheet vs. Row Zero,,2,0,1725973300.0,bigdata
1fdg57d,Scrapear Datos Inmobiliarios de Idealista en Python,"Octoparse ofrece una guía detallada sobre cómo extraer datos de Idealista mediante web scraping. Explica los pasos clave para configurar un proyecto de scraping, incluyendo la selección de elementos de la página, la extracción de información relevante como precios, ubicaciones y características de propiedades, y consejos para automatizar el proceso de forma eficiente, todo mientras se respetan las normativas legales y éticas.  


>Ref: [Cómo Raspar Datos Inmobiliarios de Idealista en Python](https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping)",0,0,1725969746.0,bigdata
1fder78,Help ,"
I’m working at a company that provides data services to other businesses. We need a robust solution to help create and manage databases for our clients, integrate data via APIs, and visualize it in Power BI.

Here are some specific questions I have:

1. **Which database would you recommend for creating and managing databases for our clients?** We’re looking for a scalable and efficient solution that can meet various data needs and sizes.
2. **Where is the best place to store these databases in the cloud?** We're looking for a reliable solution with good scalability and security options.
3. **What’s the best way to integrate data with APIs?** We need a solution that allows efficient and direct integration between our databases and third-party APIs.",1,2,1725964781.0,bigdata
1fd35a9,Handling Large Datasets More Easily with Datahorse,"A few days ago, I was dealing with a massive dataset—millions of rows. Normally, I’d use Pandas for data filtering, but I wanted to try something new. That’s when I decided to use Datahorse.

I started by asking it to filter users from the United States: ""Show me users from the United States over the age of 30."" Instantly, it filtered the dataset for me. Then, I asked it to ""Create a bar chart of revenue by country,"" and it visualized the data without me writing any code.

But what really stood out was that Datahorse provided the Python code behind each action. So, while it saved me time on the initial exploration, I could still review the code and modify it if needed for more in-depth analysis. Has anyone else found Datahorse useful for handling large datasets?
",3,0,1725922807.0,bigdata
1fd5oxm,"Felt now integrates with Databricks for instant maps and performant data dashboards, with real-time data updates. Read about how it works in our latest blog post!",,1,0,1725930095.0,bigdata
1fcge50,Big data courses ,"Hi guys If you want to big data engineer course of famous tutor pls ping me  on telegram Id:- @Robinhood_01_bot


You won't regret 😅",0,0,1725853418.0,bigdata
1fb74v4,Are there any apps that pharmaceutical companies use ?,"I am a Software Engineering student,  Interested to  see how and what type of patient data is valuable, for companies to enhance healthcare/treatments.",1,1,1725716349.0,bigdata
1faw3pi,The datification,"I'm new to the world of data. I was recently amazed by a concept called ""datification"", which according to The Big Data World: Benefits, Threats and Ethical Challenges (Da Bormida, 2021), is a technological tendency that converts our interactions in daily life into just data, ""where devices to capture, collect, store and process data are becoming ever-cheaper and faster, whilst the computational power is continuously increasing"". Indirectly promoting workflows that lead to the disuse of Big Data, violating certain privacy laws and ethical mandates.

  
Da Bormida, M. (2021). The Big Data World: Benefits, Threats and Ethical Challenges. En *Advances in research ethics and integrity* (pp. 71-91). [https://doi.org/10.1108/s2398-601820210000008007](https://doi.org/10.1108/s2398-601820210000008007)",4,2,1725674261.0,bigdata
1fb1kvc,The Data Revolution 2025: Emerging Technologies Reshaping our World,"Stay ahead of the booming data revolution 2025 as this read unravels its core components and future advancements. Evolve with the best certifications today!

https://preview.redd.it/f63pyhrwbcnd1.jpg?width=1920&format=pjpg&auto=webp&s=c34d78210ea02e44717dfcf105f9c50f22bacc8b

",0,0,1725694563.0,bigdata
1famlat,Using geospatial workloads within SnowflakeDB? Felt is a modern & cloud-native GIS platform & we just announced support for native connectivity to the Snowflake database!,"At [Felt](https://felt.com/?utm_source=community&utm_medium=social&utm_campaign=mql_community), we made a really cool cloud-native, modern & performant GIS platform that makes mapping and collaboration with your team really easy. We super recently released a version of the software that introduces native connectivity with SnowflakeDB, bringing you your Snowflake datasets to Felt. [So, here's how you do it!](https://felt.com/blog/map-with-snowflake?utm_source=community&utm_medium=social&utm_campaign=mql_community) 

I work here at the company as a developer advocate. If you have any questions, please comment below or DM and I can help! :-) ",2,0,1725648079.0,bigdata
1fa9klc,"Invitation to compliance webinar(GDPR, HIPAA) and Python ELT zero to hero workshops","Hey folks,

dlt cofounder here.

**Previously**: We recently ran our first 4 hour workshop ""Python ELT zero to hero"" on a first cohort of 600 data folks. Overall, both us and the community were happy with the outcomes. The cohort is now working on their homeworks for certification. You can watch it here: [https://www.youtube.com/playlist?list=PLoHF48qMMG\_SO7s-R7P4uHwEZT\_l5bufP](https://www.youtube.com/playlist?list=PLoHF48qMMG_SO7s-R7P4uHwEZT_l5bufP) We are applying the feedback from the first run, and will do another one this month in US timezone. If you are interested, sign up here: [https://dlthub.com/events](https://dlthub.com/events)

**Next**: Besides ELT, we heard from a large chunk of our community that **you hate governance** but it's an obstacle to data usage so you want to learn how to do it right. Well, it's no rocket/data science, so we arranged to have a professional lawyer/data protection officer give a webinar for data engineers, to help them achieve compliance. Specifically, we will do one run for GDPR and one for HIPAA. There will be space for Q&A and if you need further consulting from the lawyer, she comes highly recommended by other data teams.

If you are interested, sign up here: [https://dlthub.com/events](https://dlthub.com/events) Of course, there will also be a completion certificate that you can present your current or future employer.

This learning content is free :)

Do you have other learning interests? I would love to hear about it. Please let me know and I will do my best to make them happen.",2,0,1725608571.0,bigdata
1fac2ga,The Dawn of Generative AI While Addressing Data Security Threats,"Discover the dual-edged nature of Generative AI in our latest video. From revolutionary uses like drug creation and art development to the dark side of deepfakes and misinformation, learn how these advancements pose significant security threats. Discover how businesses can protect themselves with cutting-edge strategies. Equip yourself with the skills needed to tackle data security challenges. Enrol in data science certifications from USDSI® today and stay ahead of emerging threats! Don't forget to like, subscribe, and share this video to stay updated on the latest in tech and data security.

https://reddit.com/link/1fac2ga/video/uj0a51ig46nd1/player

",1,0,1725619438.0,bigdata
1f9ouvb,i need help in mapper.py code it was giving json decoder error,"here the link to how data set looks:[ link](https://drive.google.com/file/d/1OdAIdUVKWQb1ndU_m_x6iWuAUiTS6ksG/view)

brief description about dataset:  
\[  
{""city"": ""Mumbai"", ""store\_id"": ""ST270102"", ""categories"": \[...\], ""sales\_data"": {...}}

{""city"": ""Delhi"", ""store\_id"": ""ST072751"", ""categories"": \[...\], ""sales\_data"": {...}}

...

\]

mapper.py:

    #!/usr/bin/env python3
    import sys
    import json
    
    for line in sys.stdin:
        line = line.strip()
        if line == '[' or line == ']':
            continue
        store = json.loads(line)
        city = store[""city""]
        sales_data = store.get(""sales_data"", {})
        net_result = 0
    
        for category in store[""categories""]:
            if category in sales_data and ""revenue"" in sales_data[category] and ""cogs"" in sales_data[category]:
                revenue = sales_data[category][""revenue""]
                cogs = sales_data[category][""cogs""]
                net_result += (revenue - cogs)
                
        if net_result > 0:
            print(city, ""profit"")
        elif net_result < 0:
            print(city, ""loss"")
    
    
    

  
error:

https://preview.redd.it/42cu13vjb0nd1.png?width=685&format=png&auto=webp&s=ec34fef36a44fe3c3b33314830226be3c43ea0ec

",2,0,1725549187.0,bigdata
1f8wov8,"Huge dataset, need help with analysis","I have a dataset that’s about 100gb (in csv format). After cutting and merging some other data, I end with about 90gb (again in csv). 
I tried converting to parquet but was getting so many issues I dropped it. 
Currently I am working with the csv and trying to implement DASK and pandas for efficiency of handling the data with dask but then statistical analysis with pandas. 
This is what ChatGPT has told me to do (yes maybe not the best but I am not good and coding so have needed a lot of help). When I try to run this on my uni’s HPC (using 4 nodes with 90gb memory per) it’s still getting killed because too much memory. 
Any suggestions? Is going back to parquet more efficient? My main task it just simple regression analysis ",3,18,1725464879.0,bigdata
1f8tkj2,Is parquet not suitable for IOT integration?,In a design i chose parquet format for iot time series stream ingestion (no other info on column count). I was told its not correct. But i checked online on AI and performance/storage benchmark and parquet is suitable. Just wanted to know if there are any practical limitations causing this feedback. Appreciate any inputs pls.,1,4,1725457107.0,bigdata
1f8npls,Free RSS feed for tousands of jobs in AI/ML/Data Science every day 👀,,2,0,1725436791.0,bigdata
1f8p6tb,HOWTO: Write to Delta Lake from Flink SQL,,1,0,1725443157.0,bigdata
1f8hc3g,Working with a modest JSONL file anyone has asuggestion?,"I am currently working with a relatively large dataset stored in a JSONL file, approximately 49GB in size. My objective is to identify and extract all the keys (columns) from this dataset so that I can categorize and analyze the data more effectively.

I attempted to accomplish this using the following DuckDB command sequence in a Google Colab environment:

duckdb /content/off.db <<EOF

-- Create a sample table with a subset of the data

CREATE TABLE sample\_data AS

SELECT \* FROM read\_ndjson('cccc.jsonl', ignore\_errors=True) LIMIT 1;

-- Extract column names

PRAGMA table\_info('sample\_data');

EOF

However, this approach only gives me the keys for the initial records, which might not cover all the possible keys in the entire dataset. Given the size and potential complexity of the JSONL file, I am concerned that this method may not reveal all keys present across different records.

I tried loading the csv file to Pandas but it is taking 10s of hours, is it a right options? DuckDB  at least seemed much much faster.

Could you please advise on how to:

Extract all unique keys present in the entire JSONL dataset?

Efficiently search through all keys, considering the size of the file?

I would greatly appreciate your guidance on the best approach to achieve this using DuckDB or any other recommended tool.

Thank you for your time and assistance.",1,1,1725414156.0,bigdata
1f6vbmc,Event Stream explained to 5yo,,4,0,1725242404.0,bigdata
1f72y7h,TRENDYTRCH BIG DATA COUSE," 

Hi guys if you want big data course or any help .. pls ping me on telegram 



In these course you will learn hadoop,hive ,mapredue,spark(steam and batch ) ,azure ,adls ,adf, synapse, databeticks,system design,delta live table , AWS Athena , s3 Kafka airflow and projects etc etc 

If you want pls ping me on telegram

My telegram id is :-
 @TheGoat_010",0,1,1725271002.0,bigdata
1f6eq9z,Supercharge Your Snowflake Monitoring: Automated Alerts for Warehouse Changes!,"Supercharge Your Snowflake Monitoring: Automated Alerts for Warehouse Changes!

  
[https://www.linkedin.com/posts/arunjangili\_supercharge-your-snowflake-monitoring-activity-7236004083386433536-Ikum?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/arunjangili_supercharge-your-snowflake-monitoring-activity-7236004083386433536-Ikum?utm_source=share&utm_medium=member_desktop)",1,0,1725198112.0,bigdata
1f6azei,How to implement business intelligence at an enterprise organisation?,"1. Understand the Company’s Needs:

	•	Begin by researching the company’s current challenges, goals, and industry trends. Understand their pain points, such as inefficient processes, lack of data-driven decision-making, or missed opportunities. Tailor your approach to show how Business Intelligence (BI) can address these specific needs.

2. Highlight the Benefits of BI:

	•	Present the advantages of BI, such as improved decision-making, enhanced efficiency, and real-time insights. Emphasize how BI can help the company stay competitive by leveraging data to predict trends, optimize operations, and drive strategic decisions. Provide examples of successful BI implementations in similar industries to build credibility.

3. Demonstrate Quick Wins:

	•	Offer to run a small pilot project or proof of concept to demonstrate the immediate benefits of BI. For instance, create a simple dashboard that visualizes key performance indicators (KPIs) relevant to the company. This tangible demonstration will help stakeholders see the value of BI firsthand, making them more likely to support a full-scale implementation.

4. Address Concerns and Misconceptions:

	•	Be prepared to address common concerns, such as costs, complexity, and data security. Explain that modern BI tools are scalable and can be customized to fit the company’s budget and technical capabilities. Highlight your company’s Privacy-First Policy to ensure data security and compliance with regulations.

5. Involve Key Stakeholders:

	•	Engage decision-makers early in the process, including department heads, IT teams, and executives. Tailor your messaging to each stakeholder’s priorities—show the CFO how BI can reduce costs, demonstrate to the COO how it can streamline operations, and convince the CEO how it aligns with strategic goals. Collaborative discussions will help gain buy-in from all levels of the organization.

https://aleddotechnologies.ae
",1,0,1725185139.0,bigdata
1f6awki,How to convince a company to use business intelligence ,"1. Understand the Company’s Needs:

	•	Begin by researching the company’s current challenges, goals, and industry trends. Understand their pain points, such as inefficient processes, lack of data-driven decision-making, or missed opportunities. Tailor your approach to show how Business Intelligence (BI) can address these specific needs.

2. Highlight the Benefits of BI:

	•	Present the advantages of BI, such as improved decision-making, enhanced efficiency, and real-time insights. Emphasize how BI can help the company stay competitive by leveraging data to predict trends, optimize operations, and drive strategic decisions. Provide examples of successful BI implementations in similar industries to build credibility.

3. Demonstrate Quick Wins:

	•	Offer to run a small pilot project or proof of concept to demonstrate the immediate benefits of BI. For instance, create a simple dashboard that visualizes key performance indicators (KPIs) relevant to the company. This tangible demonstration will help stakeholders see the value of BI firsthand, making them more likely to support a full-scale implementation.

4. Address Concerns and Misconceptions:

	•	Be prepared to address common concerns, such as costs, complexity, and data security. Explain that modern BI tools are scalable and can be customized to fit the company’s budget and technical capabilities. Highlight your company’s Privacy-First Policy to ensure data security and compliance with regulations.

5. Involve Key Stakeholders:

	•	Engage decision-makers early in the process, including department heads, IT teams, and executives. Tailor your messaging to each stakeholder’s priorities—show the CFO how BI can reduce costs, demonstrate to the COO how it can streamline operations, and convince the CEO how it aligns with strategic goals. Collaborative discussions will help gain buy-in from all levels of the organization.

If you are looking on how to implement BI at your company, contact - https://aleddotechnologies.ae",1,1,1725184833.0,bigdata
1f6er5o,AI is Taking Over: What You Need to Know Before It's Too Late!,"AI is Taking Over: What You Need to Know Before It's Too Late!

  


[https://medium.com/@ridian.dany/embrace-tomorrow-how-ai-will-revolutionize-every-aspect-of-our-lives-3dd0530361bc](https://medium.com/@ridian.dany/embrace-tomorrow-how-ai-will-revolutionize-every-aspect-of-our-lives-3dd0530361bc)

",0,0,1725198184.0,bigdata
1f50ue3,"Open source python library that allows you to chat, modify, visualise your data","Today, I used this open source python library called DataHorse to analyze Amazon dataset using plain English. No need for complicated tools—DataHorse simplified data manipulation, visualization, and building machine learning models. 

Here's how it improved our workflow and made data analysis easier for everyone on the team. 

Try it out: https://colab.research.google.com/drive/192jcjxIM5dZAiv7HrU87xLgDZlH4CF3v?usp=sharing

GitHub: https://github.com/DeDolphins/DataHorsed",27,3,1725038560.0,bigdata
1f4pu6a,HOW TO BUILD YOUR ORGANIZATION DATA MATURE?,"Is your organization ready to transition from basic data use to complete data transformation? Explore the 4 stages of data maturity and the key elements that drive growth. Start your journey with USDSI® Certification.

https://reddit.com/link/1f4pu6a/video/egpl4eotdrld1/player

",0,0,1725005119.0,bigdata
1f4hse4,Looking for researchers and members of AI development teams to participate in a user study in support of my research ,"We are looking for researchers and members of AI development teams who are at least 18 years old with 2+ years in the software development field to take an anonymous survey in support of my research at the University of Maine. This may take 20-30 minutes and will survey your viewpoints on the challenges posed by the future development of AI systems in your industry. If you would like to participate, please read the following recruitment page before continuing to the survey. Upon completion of the survey, you can be entered in a raffle for a $25 amazon gift card.

https://docs.google.com/document/d/1Jsry_aQXIkz5ImF-Xq_QZtYRKX3YsY1_AJwVTSA9fsA/edit",2,3,1724976997.0,bigdata
1f49fqb,Data sets for all S&P 500 companies and their individual finacial ratios for the years of 2020-2023,"Not sure if I am in the right place but I’m hoping someone can lead me in the right direction atleast. 

I am a masters student looking to do a research paper on how data science can be used to find undervalued stocks. 

The specific ratios I am looking for is 
P/E Ratio
P/B Ratio
PEG ratio
Dividend yield
Debt to equity
Return on assets
Return on equity
EPS
EV/EBITDA
Free cash flow 

Would also be nice to know the stock price and ticker symbol 

An example
AAPL 2020
PRICE: X
P/E Ratio: x
P/B Ratio: X
PEG ratio: x
Dividend yield: x
Debt to equity: x
Return on assets: x
Return on equity: x
EPS: x 
EV/EBITDA: x
Free cash flow: x

Then the next year after:

AAPL 2021
PRICE: X
P/E Ratio: x
P/B Ratio: X
PEG ratio: x
Dividend yield: x
Debt to equity: x
Return on assets: x
Return on equity: x
EPS: x 
EV/EBITDA: x
Free cash flow: x

Then 2022 and so on till the year 2023.

I am not a cider but I have tried extensively to make a program using Chatgpt and Gemini to scrape the data from multiple sources….I was able to get a list of everything that I was looking for, For the year 2024 using Yfinance on python but was not able to get the historical data using yfinance. I have tried my hand at trying to scrape the data from EDGAR as well but as I said I am not a coder and could not figure it out. Would be willing to pay 10-50$ for the dataset from a website too but could not find one that was easy to use/had all the info I was looking for. (I did find one I believe but they wanted $1800 for it) willing to get on a phone call or discord call if that helps. ",3,4,1724955384.0,bigdata
1f406ac,DATA SCIENCE AND ARTIFICIAL INTELLIGENCE- FUTURE CATALYST IN ACTION | INFOGRAPHIC,"[Data science and artificial intelligence](https://www.usdsi.org/data-science-insights/data-science-and-artificial-intelligence-future-catalyst-in-action) are viewed as the best duo working to excel in the business landscape. With digitization and technology advancements taking rapid strides; it is widely evident that the industry workforce evolves with these changes.

With hyper-automation, cognitive abilities, and ethical considerations guiding the [data science industry](https://www.usdsi.org/) far and wide. It is expected that these smart tech additions assist in managing data explosion, advanced analytics, and enhancing domain expertise. Understanding the core convergence, challenges, and opportunities that this congruence brings to the table is inevitable for every data science enthusiast.

If you wish to build a thriving career in data science with futuristic skillsets on display; it is the time to invest in one of the [best data science certifications](https://www.usdsi.org/data-science-certifications); that empower you with core AI nuances as well. The generative AI market size is expanding at an astounding rate. This will give way to even smarter advances in data science technology and ways to counter the staggering data volume worldwide.

This is why, global industry recruiters are looking forward to appointing a skilled certified workforce that can guarantee enhanced business growth and multiplied career advancements as well. Start exploring the best credentialing options to get closer to a successful career trajectory in data science today!

https://preview.redd.it/91nzvo5ualld1.jpg?width=1200&format=pjpg&auto=webp&s=ca367da9e3353652327638019f9648dcb07baff4

",0,1,1724931486.0,bigdata
1f3yi4k,"Pharmacy Management Software Development: Costs, Process & Features Guide",,1,1,1724925347.0,bigdata
1f3imvs,Analyze Big Social Media Data: $6000 Challenge (12 Days Left!),"Hey all! There's still time to jump into our Social Media Data Modeling Challenge (Think hack-a-thon) and compete for $6000 in prizes! Don't worry about being late to the party – most participants are just getting started, so you've got plenty of time to craft a winning submission! Even with just a few hours of focused work, you could create a competitive entry!

**What's the Challenge?**

Your mission, should you choose to accept it, is to analyze real social media data, uncover fascinating insights, and showcase your SQL, dbt™, and data analytics skills. This challenge is open to all experience levels, from seasoned data pros to eager beginners.

Some exciting topics you could explore include:

* Tracking COVID-19 sentiment changes on Reddit
* Analyzing Donald Trump's popularity trends on Twitter/Reddit
* Identifying and explaining who the biggest YouTube creators are
* Measuring the impact of NFL Superbowl commercials on social media
* Uncovering trending topics and popular websites on Hacker News

But don't let these limit you – the possibilities for discovery are endless!

**What You'll Get**

Participants will receive:

* Free access to professional data tools (Paradime, MotherDuck, Hex)
* Hands-on experience with large, relevant datasets (great for your portfolio)
* Opportunity to learn from and connect with other data professionals
* A shot at winning: $3000 (1st), $2000 (2nd), or $1000 (3rd)

**How to Join**

To ensure high-quality participation (and keep my compute costs in check 😅), here are the requirements:

* You must be a current or former data professional
* Solo participation only
* Hands-on experience with SQL, dbt™, and Git
* Provide a work email (if employed) and one valid social media profile (LinkedIn, Twitter, etc.) during registration

Ready to dive in? [**Register here**](https://www.paradime.io/dbt-data-modeling-challenge) and start your data adventure today! With 12 days left, you've got more than enough time to make your mark. Good luck!",1,0,1724872363.0,bigdata
1f35t0u,Storing and Analyzing 160B Quotes in ClickHouse,,1,0,1724837216.0,bigdata
1f1w6ii,Coordinate Reference System for NREL Wind Resource Database,"I'm working with geospatial windspeed data from the [NREL Wind Resource Database](https://wrdb.nrel.gov/data-viewer), but it's not clear what coordinate reference system is being used. I found on their [GitHub](https://github.com/NREL/hsds-examples/blob/master/datasets/wtk-us.md) that they use a \`\`modified Lambert-conic"" system, but none of the various Lambert-conic EPSGs or PROJ strings I've found online seem to be correct.

Does anyone know how I can find out what's the exact CRS they used? Thanks :)",2,1,1724698614.0,bigdata
1f1uhrk,Final year project idea suggestion ,"I am a final-year computer science student interested in real-time data streaming in the big data domain. 

Could you suggest a use cases along with relevant datasets that would be suitable for a final-year project?",1,3,1724694626.0,bigdata
1f1fbb5,"FREE AI WEBINAR: 'How to build an AI layer on your Snowflake data to query your database - Webinar by deepset.ai' [Aug 29, 8 am PST]",,1,1,1724645152.0,bigdata
1f054em,Essential AI Engineer Skills and Tools you Should Master,,2,0,1724505619.0,bigdata
1f020t6,TRANSFORM YOUR CAREER PATH WITH USDSI®'S DATA SCIENCE CERTIFICATION PROGRAM,"Take your [data science career](https://www.usdsi.org/data-science-certifications) to the next level with USDSI’s industry relevant certification program. Whether you're a students, professionals, and career switchers, our program offers practical skills and knowledge with minimal time commitment.

https://preview.redd.it/58ua16i88lkd1.jpg?width=1080&format=pjpg&auto=webp&s=6cbf8e082e57c867436ded8d300729fa7f8e23ee

",0,0,1724494754.0,bigdata
1ezh70t,My Medium article on ClickHouse,"My Medium article on ClickHouse

I recently published an article on Medium (around a month ago) about ClickHouse. 

ClickHouse is an SQL compliant, extremely fast, and horizontally scalable data warehouse and analytics platform, which has recently gained popularity mainly due to its performance.

I have tried writing it for beginners to provide enough information to start working with ClickHouse, to build a basic understanding of its capabilities, and also to provide enough information to decide whether ClickHouse is the right tool for the task at hand.

Read here: https://medium.com/@suffyan.asad1/beginners-guide-to-clickhouse-introduction-features-and-getting-started-55315107399a

It also contains a section about other useful articles and links about how ClickHouse is used in various systems by others, and also serves as a collection of beyond the basics.  

Please read and provide feedback, it'd be very helpful for me to improve my writing and utility of my articles. Additionally, I write mainly about Apache Spark and other data engineering topics. ",0,0,1724430688.0,bigdata
1eyr5wz,Google Sheets Integration is Live!,,1,0,1724352270.0,bigdata
1eyc6iq,How State-Level Data Reveals Hidden Asbestos Risks in Talc Products: What the Numbers Tell Us,,3,1,1724307145.0,bigdata
1ex866n,Sourcetable - Free bulk-CSV analysis tool (feedback plz!),,3,1,1724190784.0,bigdata
1ewwier,Evolving the Data Lake: From CSV/JSON to Parquet to Apache Iceberg,,4,0,1724162858.0,bigdata
1ewso6h,The Future of Healthcare: Nationwide Digital Health Records Programme," As we progress further into the digital age, the need for streamlined and accessible health information becomes increasingly critical. The Nationwide Digital Health Records Programme aims to enhance healthcare delivery by establishing a unified system that allows for better data management, patient care, and informed decision-making.  

Imagine a world where your medical history, test results, and treatment plans are all available at the touch of a button, no matter where you are! This initiative not only promises to reduce administrative burdens but also ensures that healthcare providers have real-time access to vital patient data.  

However, with such a monumental shift towards digital records, we must also address concerns regarding data privacy, security, and equitable access to technology. What do you think about this move towards a nationwide digital health record system? Are there any potential challenges or benefits that you foresee in this transformation? Let's discuss! 
https://7med.co.uk/nationwide-digital-health-records-programme/",2,1,1724151841.0,bigdata
1ewwjuy,8 Tools For Ingesting Data Into Apache Iceberg,,1,1,1724162972.0,bigdata
1ewpywu,BOOST YOUR BUSINESS WITH AI & DATA LITERACY,"In today's [data-driven world](https://www.usdsi.org/data-science-insights/ai-and-data-literacy-future-proof-combination-to-strengthen-business), businesses must prioritize data literacy to harness the full potential of AI. Learn how upskilling your workforce can transform data into actionable insights, driving innovation and growth. 

https://preview.redd.it/pw5i4gi30sjd1.jpg?width=1080&format=pjpg&auto=webp&s=4579f0e3f4f07acb082fd5e555609cec553493b8

",0,1,1724140916.0,bigdata
1ewlffi,How hard is to start a career in Big Data with just a BS in Marketing?,"I just got my B.S. in Marketing and was wondering if you need more of a Data Analytics degree. If I can get an entry-level position in big data and marketing, what should it be?",0,1,1724123974.0,bigdata
1euev53,DRIVEN TOMORROW WITH USDSI® DATA SCIENCE CERTIFICATION,"Shape your destiny in data science with [USDSI® Certifications](https://www.usdsi.org/data-science-certifications). Whether you're an enthusiast or a seasoned analyst, our programs empower you for future challenges. Join USDSI® on the journey to professional success.

https://preview.redd.it/jbhsspm4f7jd1.jpg?width=736&format=pjpg&auto=webp&s=15874b5bd4d17c432d9c55dd52ebe1c73f6bb44e

",1,0,1723891720.0,bigdata
1eu9mco,How to skip header rows from a table in Hive? (Hands On),,1,0,1723870223.0,bigdata
1etib1e,TOP 15 Data Science Advantages for Business,"Data science is undoubtedly the biggest transformation factor for businesses across all industries.

Data science has numerous benefits across all industries. While educational institutions are using data science to personalize their educational content, find our student dropouts, and enhance their administration, the [healthcare industry is using data science](https://www.usdsi.org/data-science-insights/revolutionizing-the-healthcare-industry-with-the-help-of-data-science) to treat patients in a more personalized way by analyzing huge amounts of health data.

https://preview.redd.it/1iaeyzh76zid1.jpg?width=1155&format=pjpg&auto=webp&s=b3a5508b83e2aa910ed135e2b229e902904296ed

**This is just an example.**

Data science has wide applications in all industries, from finance to retail, to manufacturing. [USDSI®](https://www.usdsi.org/) brings a comprehensive guide discussing its advantages in different sectors.

We highlight how it can be effectively used to detect frauds in financial sectors, how data science helps to analyze vast amounts of data and assist with anomaly detection to detect cyber threats easily. Not just that, learn how using data science, organizations can incorporate a culture of [data-driven decision-making](https://www.usdsi.org/data-science-insights/resources/master-data-driven-decision-making-in-2024) that will ultimately lead to boosting their businesses and enhancing their customer service.

[Download](https://www.usdsi.org/data-science-insights/resources/top-15-data-science-advantages-for-business) this guide now and learn how you can implement data science to boost your business.",0,0,1723791883.0,bigdata
1etl1pt,TOP 11 PROGRAMMING LANGUAGES FOR DATA SCIENTISTS’ INSTANT RESUME BOOST,"Understanding a [programming language for data science](https://www.usdsi.org/data-science-insights/top-11-programming-languages-for-data-scientists-instant-resume-boost) is of utmost importance today than ever before. No data science task is complete without the expert leveraging of top-notch programming languages. As the world grows with whopping data generation rates; it is imperative to understand the way programming and data science communicate to bring out the most targeted insights for business growth.

This read shall assist you with the most comprehensive and contemporary programming languages and allow you a quick sneak into them. Mastering these core nuances that guide the [data science industry](https://www.usdsi.org/) is indispensable as you build your career as a data scientist. Make it a priority to enroll with the most trusted and seasoned players when it comes to the globally renowned [best data science certifications](https://www.usdsi.org/data-science-certifications). You must grow your data science niche with sheer skill and futuristic talent on offer.

Not only that; you will be offered a higher salary, a meatier data science role, and an industry career progression like none other; when you get certified with the global leaders in credentialing. If you are someone who wishes to understand the inside out of the programming languages and envision yourself earning top-notch roles with your dream industry recruiters- Start Right Here!

https://preview.redd.it/qyr2zobx30jd1.jpg?width=2000&format=pjpg&auto=webp&s=2ca516908ef93a60de838b7ccf7b10fd21f2a3f2

",0,0,1723803212.0,bigdata
1es5sd3,Rollstack Connects Dashboards to PowerPoint,"This is a super common issue in reporting. The data people use dashboards, but monthly and quarterly reports are still done in PowerPoint. Rollstack connects your dashboards to PowerPoint and Google Slides for automated report generation. No more screenshots! Just thought it was pretty helpful, and wanted to share. ",3,0,1723652163.0,bigdata
1ervtos,BIG DATA ANALYTICS MYTH V/S REALITY,"In the age of data-driven decisions, understanding the true capabilities of big data is crucial. Bust the myths that obscure the value of [big data analytics](https://www.usdsi.org/data-science-insights/resources/exclusive-big-data-analytics-myths-debunked) and gain behind-the-scenes knowledge from leading experts.

https://preview.redd.it/29sj5lb55lid1.jpg?width=1080&format=pjpg&auto=webp&s=d1a2ed6672df35625aee195233f609c0b972e7e8

",1,0,1723622020.0,bigdata
1erag9d,Real-time Computation of Option Greeks Using Pathway and Databento,"I am excited to share this tutorial that demonstrates how to compute Option Greeks in real-time. Option Greeks are essential tools in financial risk management, measuring an option’s price sensitivity.

Using Pathway, a real-time data processing framework, this tutorial computes Option Greeks based on Databento’s market data. The values are continuously updated in real-time with data provided by Databento.

In our latest article, you’ll learn how to compute these Option Greeks using Databento’s market data and keep them updated in real-time.

Learn more about the project here: [https://pathway.com/developers/templates/option-greeks](https://pathway.com/developers/templates/option-greeks)

GitHub: [https://github.com/pathwaycom/pathway/tree/main/examples/projects/option-greeks](https://github.com/pathwaycom/pathway/tree/main/examples/projects/option-greeks)",6,2,1723562051.0,bigdata
1erbhy3,User Management in ClickHouse® Databases: The Unabridged Edition,"August 21 @ 8:00 am – 9:00 am PDT

User management is a key problem in any **#analytic** application. Fortunately, **#ClickHouse** has a rich set of features for **#authentication** and **#authorization**. We’re going to tell you about all of them. We’ll start with the model: users, profiles, roles, quotas, and row policies. Then we’ll show you implementation choices from **#XML** files to **#SQL** commands to external identity providers like **#LDAP**. Finally, we’ll talk about features on the horizon to improve ClickHouse security. There will be a sample code plus plenty of time for questions.

Join us to learn how to manage your users simply and effectively.",1,1,1723564501.0,bigdata
1eqpnd3,Fan of LLMs+RAG? Put any URL after md.chunkit.dev/ to turn it into markdown chunks,"For example, https://md.chunkit.dev/https://en.wikipedia.org/wiki/Chunking_(psychology)",3,1,1723497491.0,bigdata
1ep4d2m,Big Data ,,1,3,1723326668.0,bigdata
1eoxq9c,Chew: a library to process various content types to plaintext,,1,0,1723308885.0,bigdata
1eo4oay,Best Practices to Manage Databricks Clusters at Scale to Lower Costs,,0,0,1723222046.0,bigdata
1eo0dy2,Request for guide for Big data in a vm,"Hey,

I am an beginner in Big data, and is considering to install the necessary software like hardoop and spark.

Many senior members suggested I use Vm for it. 

Can anyone suggest which Linux version I should download for it along with any thing I need to look out for while setting it up for big data",1,2,1723211865.0,bigdata
1entnzw,7 Popular Data Science Components To Master in 2024,"Before starting a career in data science, it is important to understand what it constitutes of. Explore different components of data science that you must master in 2024. 

https://preview.redd.it/090a1pxhclhd1.jpg?width=1084&format=pjpg&auto=webp&s=9a4873e2efcc7bf4165851edd7c6f1b579ae7534

",1,0,1723188642.0,bigdata
1en9f0k,How do companies that deal with a large amount of excel spreatsheet data from various clients that have different standards for their data? Do they keep them as spreadsheets? Do they convert them into SQL databases or NoSQL databases?,,3,6,1723132067.0,bigdata
1en70j5,Migration Guide for Apache Iceberg Lakehouses,,2,1,1723126294.0,bigdata
1en1yhu,7 Popular Data Science Components To Master in 2024,"Before starting a career in data science, it is important to understand what it constitutes of. Explore different [components of data science](https://www.usdsi.org/data-science-insights/7-popular-data-science-components-to-master-in-2024) that you must master in 2024.

https://preview.redd.it/rduli32jxehd1.jpg?width=1084&format=pjpg&auto=webp&s=73f090ae8d0b238e594449cdb67d8f5bcc5c7353

",3,0,1723110973.0,bigdata
1en1z73,Impact of Data Science in Robotics,"[Data Science and Robotics](https://www.usdsi.org/data-science-insights/impact-of-data-science-in-robotics) are the cross-disciplines of similar fields of study – science, statistics, computer technology, and engineering.

https://preview.redd.it/1gp50tlsxehd1.jpg?width=1920&format=pjpg&auto=webp&s=b3a1b7e57212d1353d1e58c1929b3391e481cb06

",1,0,1723111047.0,bigdata
1emmvxs,"6-Week Social Media Data Challenge: Tackle large Social media datasets, win up to $3000!","I've just launched an exciting 6-week challenge focused on analyzing large-scale social media data. It's a great opportunity to apply your big data skills and potentially win big!

What's involved:

- Work with real, large-scale social media datasets

- Use professional tools: Paradime (SQL/dbt™), MotherDuck (data warehouse), Hex (visualization)

- Chance to win: $3000 (1st), $2000 (2nd), $1000 (3rd) in Amazon gift cards

My partners and I have invested in creating a valuable learning experience with industry-standard tools. You'll get hands-on practice with real-world big data and professional technologies. Rest assured, your work remains your own - we won't be using your code, selling your information, or contacting you without consent. This competition is all about giving you a chance to apply and showcase your big data skills in a real-world context.

Concerned about time? No worries, the challenge submissions aren't due until September 9th. Even 5 hours of your time could put you in the running, but feel free to dive deeper!

Check out our [explainer video](https://youtu.be/5RPUghtyCfM?si=KNZMnjj6R18WzWQP) for more details.

Interested? Register here: [https://www.paradime.io/dbt-data-modeling-challenge](https://www.paradime.io/dbt-data-modeling-challenge)",9,1,1723063538.0,bigdata
1eln16x,Vm failed connection in hadoop,I ran “start-all.sh” command after making sure it wasn’t running and when i try running “hdfs dfs -ls /“ for testing if hdfs is working that error shows up “ls: call from localhost.localdomain/127.0.0.1 to localhost:9000 failed on connection” how can i fix it,2,1,1722963588.0,bigdata
1elw52m,10 Reasons Why You Should Own a Great Dane,,0,5,1722985618.0,bigdata
1elb45c,"Real Time Data Project That Teaches Streaming, Data Governance, Data Quality and Data Modelling
","[https://www.linkedin.com/posts/arunjangili\_datagovernance-dataquality-datamodelling-activity-7226473747531120640-\_Gn3?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/arunjangili_datagovernance-dataquality-datamodelling-activity-7226473747531120640-_Gn3?utm_source=share&utm_medium=member_desktop)

Practice above project and master All Data Governance, Quality, Modelling and Streaming",1,0,1722927457.0,bigdata
1elata9,BEST DATA SCIENCE CERTIFICATIONS IN 2024,"Data science has become the hottest career opportunity of today’s time. It is essentially indispensable for empowering yourself with the most trusted [data science certifications](https://www.usdsi.org/data-science-certifications). 

https://preview.redd.it/j49ywh2bozgd1.jpg?width=1504&format=pjpg&auto=webp&s=09f8465db80a3a445a3e6e12d7b8d7f77fd96fb2

",0,0,1722926283.0,bigdata
1eklq15,6 HOTTEST DATA ANALYTICS TRENDS TO PREPARE AHEAD OF 2025,"It is your time to gain insightful training in the world of data science with the best worldwide. USDSI® presents a holistic read that gathers maximum information and guidance on the most futuristic trends and technologies that are stipulated to guide the data world. Predict the future of data analytics with exceptional skills in data unification in the cloud, the rise of small data, the evolutionary role of data products, and beyond. this could be your beginning to grab the top-notch career possibilities with both hands and elevate your career in data science as a Pro!

https://reddit.com/link/1eklq15/video/v558k9lf2ugd1/player

",0,1,1722858403.0,bigdata
1ej193j,WHY CHOOSE USDSI® FOR YOUR DATA SCIENCE JOURNEY?,"Explore the unique advantages of the USDSI® Data Science Program. Equip yourself with real-world skills and expertise to stay ahead in the data-driven world.

https://preview.redd.it/dw9qqaryqfgd1.jpg?width=1080&format=pjpg&auto=webp&s=231422b19502dd9a2c538cf49d0caac8c01c382e

",0,0,1722685021.0,bigdata
1ei7id7,Announcing the Release of Apache Flink 1.20,,1,0,1722597648.0,bigdata
1eh8c2v,Created Job that sends Report without integrity checks,"So, im an intern at this bank in the BI/Insights department. I recently created a Talend job that queries data from our data warehouse from some tables every first day of the month at 5:00 am, generates an excel report and sends it to the relevant business users. Today's the first time it ever run officially outside testing conditions and the results are rather shameful.

The first excel sheet hasn't been populated by any data, except formulas and zeros... it was dependent on data from a different sheet, which was blank. This was because that latest data wasn't yet loaded into the warehouse tables i was querying from, as my report requires latest info as at the last day of the month.

I think i need to relearn BI/Bigdata principles, especially regarding data governance and integrity checks. Any help and suggestions would be appreciated.",2,2,1722488219.0,bigdata
1egnrun,Using Pathway for Delta Lake ETL and Spark Analytics,"In the era of big data, efficient data preparation and analytics are essential for deriving actionable insights. This tutorial demonstrates using Pathway for the ETL process, Delta Lake for efficient data storage, and Apache Spark for data analytics. This approach is highly relevant for data engineers looking to integrate data from various new sources and efficiently process it within the Spark ecosystem.  
  
**Comprehensive guide with code:** [https://pathway.com/developers/templates/delta\_lake\_etl](https://pathway.com/developers/templates/delta_lake_etl)

**Why This Approach Works:**

* **Versatile Data Integration:** Pathway’s Airbyte connector allows you to ingest data from any data system, be it GitHub or Salesforce, and store it in Delta Lake.
* **Seamless Pipeline Integration:** Expand your data pipeline effortlessly by adding new data sources without significantly changing them.
* **Optimized Data Storage:** Querying over data organized in Delta Lake is faster, enabling efficient data processing with Spark. Delta Lake’s scalable metadata handling and time travel support make it easy to access and query previous versions of data.

**Using Pathway for Delta ETL simplifies these tasks significantly:**

* **Extract:** Use Airbyte to gather data from sources like GitHub, configuring it to specify exactly what data you need, such as commit history from a repository.
* **Transform:** Pathway helps remove sensitive information and prepare data for analysis. Additionally, you can add useful information, such as the username of the person who made changes and the time of the changes.
* **Load:** The cleaned data is then saved into Delta Lake, which can be stored on your local system or in the cloud (e.g., S3) for efficient storage and analysis with Spark. 

Would love to hear your experiences with these tools in your big data workflows!",11,0,1722434053.0,bigdata
1eh0i0f,Data extraction- Historical Cost data,"Hello guys! not sure if this is the right spot to post. I have to extract historical cost data from a large pdf over 900 pages. it seems simple but i need to maintain the CSI CSI MasterFormat division structure to ensure compatibility with our existing data tables. This is the specific data in question. [RSMeans Building Construction Cost Data 2014 : Free Download, Borrow, and Streaming : Internet Archive](https://archive.org/details/RSMeansBuildingConstructionCostData2014/page/n29/mode/2up)",2,0,1722465093.0,bigdata
1egbf2z,Modern Data Quality Summit 2024,"The world is experiencing a data revolution, led by AI. However, only 48% of AI projects reach production, taking an average of 8.2 months. This shows the need for AI-readiness and quality data. At the Modern Data Quality Summit 2024, we offer insights into best practices, innovative solutions, and strategic frameworks to prepare your data for AI and ensure successful implementation.

Here’s a sneak peek of what we have in store for you:

* Data quality optimization for real-time and multi-structured AI applications
* Approaching data quality as a product for enhanced business focus
* Implementing proactive data observability for superior quality control
* Building a data-driven culture that prioritizes quality and drives success

Register Now - [https://moderndataqualitysummit.com/](https://moderndataqualitysummit.com/)",4,1,1722391187.0,bigdata
1egiwp2,IS Generative AI BENEFICIAL FOR A DATA ENGINEER?,"Accelerate your [data engineering journey with Generative AI](https://www.usdsi.org/data-science-insights/the-data-science-method-and-generative-ai) ! Learn how this cutting-edge technology streamlines SQL and python code generation, debugging, and optimization, enabling data engineers to work smarter.

https://preview.redd.it/jvs9rvr6qtfd1.jpg?width=1080&format=pjpg&auto=webp&s=43500c5ab5b574e3bf3e4f7679a380accacdadfc

",0,0,1722418441.0,bigdata
1efsltk,How does Data Science revolutionize the education sector?,"Data science is rapidly transforming the education landscape. By analyzing vast amounts of student data, educators can gain profound insights into learning patterns, challenges, and strengths. This enables personalized learning experiences tailored to individual needs, early identification of struggling students, and optimized resource allocation.

Predictive analytics, a powerful tool within data science, allows institutions to forecast student outcomes, enabling proactive interventions to improve academic performance and prevent dropouts. Furthermore, data-driven insights inform curriculum development, teacher training, and policy decisions, ensuring education aligns with the evolving needs of students and society.

Currently, the adoption of [data science in the education industry](https://www.usdsi.org/about-united-states-data-science-institute) is at the infant stage, however, it is growing rapidly. It is evident from the fact that the global education and learning analytics market is expected to reach $90.4 billion by 2030 (*source: Data Bridge*)

However, the ethical use of data is paramount. Protecting student privacy and ensuring data security are critical considerations. Additionally, educators and administrators require ongoing training to effectively leverage data-driven insights.

By embracing data science, educational institutions can create more equitable, efficient, and effective learning environments. The potential to enhance student outcomes and drive educational innovation is immense.

[**Download**](https://www.usdsi.org/data-science-insights/resources/how-does-data-science-revolutionize-the-education-sector) your copy of [USDSI’s](https://www.usdsi.org/) comprehensive guide on ‘how data science is revolutionizing the education sector’, and gain valuable insights on data science for the education sector.",1,0,1722344115.0,bigdata
1eeugh2,How To Make a Solid Portfolio for An Aspiring Data Analyst,"Check out our detailed infographic guide on [data analyst portfolios](https://www.usdsi.org/data-science-insights/how-to-make-a-solid-portfolio-for-an-aspiring-data-analyst) and understand their importance in today’s competitive world. Also, learn how to build an attractive one. 

https://preview.redd.it/10kanysw5ffd1.png?width=1080&format=png&auto=webp&s=578fef51dee81d2f0208b4f4cf3f29a9d1413d8b

",3,0,1722242103.0,bigdata
1ed9vcf,"Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers) Programming, Scenario-Based, Fundamentals, Performance Tunning",,0,0,1722062004.0,bigdata
1edg5bn,TRANSFORM YOUR CAREER AND ELEVATE YOURSELF TO DATA SCIENCE LEADER,"Elevate your career and become a [data science leader with CSDS™](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist). Demonstrate your technical knowledge and strategic mindset, and show the world your capability to drive business success. 

https://preview.redd.it/zdyhki1ca2fd1.jpg?width=1080&format=pjpg&auto=webp&s=516e16cbd742511321e4912a18df2ab76d0bd5b0

",0,0,1722086189.0,bigdata
1ec4sbk,"mods are asleep, post big data",,38,2,1721938500.0,bigdata
1ecjmny,Help with Data Catalog application architecture,"Hello guys,

I have a project in which I have to collect aggregate data for each customer from one big table. In banking an example could be, a customer having an id, purchase\_amount, money\_conversion\_amount columns and in table it is stored as  
id,    purch.,   mon.,   date  
100,   85,      200,     2024-07-26  
100,   12,       0,        2024-07-25  
101,   34,      10,       2024-07-26  
100,   11,      56,       2024-07-24  
101,  10,       0,        2024-07-25

so aggregate data for each use stored in one big table  
My project aims to have one more aggregate table having this columns:  
id, purchases\_sum\_last1day, purchases\_sum\_last3day, purchases\_sum\_1month, money\_conversion\_amount\_sum\_last1day .....  
aggregate functions are sum, min, max and avg  
Data is stored on data lake (hdfs) and we are using spark as well.  
Right now I have a working application but I am not happy with the performance, it reads a config file and generated a very long sql query and executes it with spark.  
I would like to get ideas about how efficiently I can handle the project (like having metadata table or using streaming somehow).",1,15,1721985233.0,bigdata
1eb04y4,Apache Fury 0.6.0 Released: 6x serialization faster and 1/2 payload smaller than protobuf serialization,[https://fury.apache.org/blog/fury\_0\_6\_0\_release](https://fury.apache.org/blog/fury_0_6_0_release),4,3,1721823836.0,bigdata
1eazix6,Sending Data to Apache Iceberg from Apache Kafka with Apache Flink,,2,0,1721821944.0,bigdata
1eb60t4,ChatGPT for data science 📊,,0,0,1721839126.0,bigdata
1eaim7a,Introducing Airbyte Refreshes: Reimport Historical Data with Zero Downtime,,5,0,1721767594.0,bigdata
1ea9gw6,Handling Out-of-Order Event Streams: Ensuring Accurate Data Processing and Calculating Time Deltas with Grouping by Topic,"Imagine you’re eagerly waiting for your Uber, Ola, or Lyft to arrive. You see the driver’s car icon moving on the app’s map, approaching your location. Suddenly, the icon jumps back a few streets before continuing on the correct path. This confusing movement happens because of out-of-order data.

In ride-hailing or similar IoT systems, cars send their location updates continuously to keep everyone informed. Ideally, these updates should arrive in the order they were sent. However, sometimes things go wrong. For instance, a location update showing the driver at point Y might reach the app before an earlier update showing the driver at point X. This mix-up in order causes the app to show incorrect information briefly, making it seem like the driver is moving in a strange way.  
This can further cause several problems like wrong location display, unreliable ETA of cab arrival, bad route suggestions, etc.

**How can you address out-of-order data?** There are various ways to address this, such as:

* **Timestamps and Watermarks**: Adding timestamps to each location update and using watermarks to reorder them correctly before processing.
* **Bitemporal Modeling**: This technique tracks an event along two timelines—when it occurred and when it was recorded in the database. This allows you to identify and correct any delays in data recording.
* **Support for Data Backfilling**: Your system should support corrections to past data entries, ensuring that you can update the database with the most accurate information even after the initial recording.
* **Smart Data Processing Logic**: Employ machine learning to process and correct data in real-time as it streams into your system, ensuring that any anomalies or out-of-order data are addressed immediately.

**Resource: Hands-on Tutorial on Managing Out-of-Order Data:** In this resource, you will explore a powerful and straightforward method to handle out-of-order events using Pathway. Pathway, with its unified real-time data processing engine and support for these advanced features, can help you build a robust system that flags or even corrects out-of-order data before it causes problems. Link to the code and more resources: [https://pathway.com/developers/templates/event\_stream\_processing\_time\_between\_occurrences](https://pathway.com/developers/templates/event_stream_processing_time_between_occurrences)

**Steps Overview:**

* **Synchronize Input Data**: Use **Debezium**, a tool that captures changes from a database and streams them into your application via Kafka/Pathway.
* **Reorder Events**: Use **Pathway** to sort events based on their timestamps for each topic. A topic is a category or feed name to which records are stored and published in systems like Kafka.
* **Calculate Time Differences**: Determine the time elapsed between consecutive events of the same topic to gain insights into event patterns.
* **Store Results**: Save the processed data to a **PostgreSQL** database using Pathway.

This will help you sort events and calculate the time differences between consecutive events. This helps in accurately sequencing events and understanding the time elapsed between them, which can be crucial for various applications.

Credits: Referred to resources by Przemyslaw Uznanski and Adrian Kosowski from Pathway, and Hubert Dulay (StarTree) and Ralph Debusmann (Migros), co-authors of the O’Reilly Streaming Databases 2024 book.

Hope this helps!",2,0,1721745524.0,bigdata
1e9xok6,What skills to learn for Big Data Specialization?,"I am an upcoming third year student in Computer Engineering Program. In our first two years in college we were taught Object-Oriented Programming, Data Structures and Algorithms, and Operating Systems. The language we used are Python and C++. What skills should I learn to pursue a specialization in Big Data?",2,0,1721704811.0,bigdata
1e9zcl8,Create Hive Table (Hands On) with all Complex Datatype,,0,0,1721710428.0,bigdata
1e9dtsr,TOP 3 TIPS MARKETING TEAMS NEED TO KNOW ABOUT DATA SCIENCE IN 2024,"Ready to take your marketing efforts to the next level? Discover the top three data science insights for 2024 and learn how to harness the power of AI, democratize data access, and create personalized customer experiences.

https://reddit.com/link/1e9dtsr/video/llx1z96uj2ed1/player

",0,0,1721653539.0,bigdata
1e99667,DATA SCIENCE CERTIFICATION,"Shape your destiny in [data science with USDSI® Certifications](https://www.usdsi.org/data-science-certifications). Whether you're an enthusiast or a seasoned analyst, our programs empower you for future challenges.

https://preview.redd.it/3q70ocp171ed1.jpg?width=1080&format=pjpg&auto=webp&s=1e1540f169fd78d1a7cf21c468e2663065963808

",0,0,1721637131.0,bigdata
1e8x001,Best translation service from english to arabic for less than 100$ or free,,1,0,1721596976.0,bigdata
1e7329a,DATA SCIENCE & MACHINE LEARNING THE FUTURE OF ROUTE PLANNING IN LOGISTICS,"The logistics industry is embracing [data science and machine learning](https://www.usdsi.org/data-science-insights/when-data-science-meets-route-planning-in-logistics) to revolutionize route planning. Discover how these technologies predict traffic, suggest alternative routes, and enhance delivery efficiency. 

https://preview.redd.it/7u9p6bk73hdd1.jpg?width=650&format=pjpg&auto=webp&s=21ec405f2e258b21a3edbd3ea992eca08f6decc0

",3,0,1721393715.0,bigdata
1e6vlyw,Sending Data file to Kafka Topic,,2,1,1721365004.0,bigdata
1e62j81,Apache Druid for Data Engineers (Hands-On) ,,4,0,1721276174.0,bigdata
1e5olw7,Want to be A Data Analyst ,"""I want to learn data analytics from the beginning. Can anyone provide me with a roadmap, resources, and a good learning path?""",6,2,1721238700.0,bigdata
1e5r1zu,"AI, Big Data Analytics, and the Modern Data Stack ","While AI continues to captivate executive attention—and rightfully so—it's essential to underscore the profound impact of robust automation and self-serve analytics. Before diving into the complexities of AI, it's critical to establish a solid foundation with proven tools and practices:

✨ Data Modeling: Utilize tools like dbt and Tableau Prep for self-serve data modeling that empowers teams to manage and transform data efficiently.

🔀 ETL/ELT Processes: Implement solutions like Fivetran or Airflow to streamline your data integration, ensuring a seamless data flow across your systems.

📊 Data Visualization: Leverage platforms like Tableau, Looker, Metabase, and Power BI to transform raw data into actionable insights through compelling visual narratives.

🤖 Report Automation: Generate your reports [Rollstack](http://www.rollstack.com). Facilitating automated reporting frees up your team's time to focus on high-impact work.

🛠️ Implement Data Best Practices: Adopt practices like version control, CI/CD, and unit testing to maintain code quality and ensure reliability in your data operations.

Prioritizing building a dependable data foundation is what enables your team to harness the power of AI; without this foundation, the output of your AI will always be a step behind.",2,0,1721244637.0,bigdata
1e5qwba,ETL speeds of raw source data into postgresql,"I'm doing ETL work through python into postgresql. just trying to get an idea of if my processes are fast enough or need to look at ways to do better to keep up with my peers.

mostly dealing with csv files, the occasional xls/xlsx. Bringing in hourly and 5 minute interval data for a couple hundred thousand things. Once datafiles are cached on a drive, it's ETL'd through python, date validated into datetime, floats, int, strings, sanity checking, transforming the data into a postgres record.

My minimum bar is loading 30k records per minute into postgresql, files with only a handful of data points and easy, or only a few transformations, I bounce around a 1million per minute.",0,0,1721244238.0,bigdata
1e5adfi,Data Architecture Complexity ,,1,0,1721194394.0,bigdata
1e59q3g,5 COMPONENTS OF POWER BI,"Data science teams can solve problems with more accuracy and precision than ever before, especially when combined with soft skills in creativity & communication.

[Data science teams can solve problems with more accuracy and precision than ever before, especially when combined with soft skills in creativity & communication.](https://reddit.com/link/1e59q3g/video/qm0d1hblf0dd1/player)

",1,0,1721192067.0,bigdata
1e4p858,Data Analytics: Future Roadmap & Trends for 2024,"The ""Data Analytics Roadmap 2024: A Comprehensive Guide to Data-driven Success"" outlines a strategic plan for implementing data analytics initiatives to drive innovation, enhance decision-making, and gain a competitive edge. This roadmap includes key components such as data strategy, infrastructure, analysis techniques, and visualization, providing a framework for businesses to collect, analyze, and interpret data effectively. Implementation steps involve defining goals, assessing current infrastructure, developing a data strategy, acquiring and preparing data, analyzing and interpreting data, and visualizing results. The roadmap offers benefits like improved decision-making, enhanced efficiency, and better customer experiences, but also highlights challenges including data quality, governance, and privacy. Analytics reports and case studies demonstrate real-world applications and success stories, while future trends such as AI integration, augmented analytics, and evolving data privacy regulations are anticipated to shape the landscape. The [Skills Data Analytics website](https://skills-datanalytics.com/) is recommended for those seeking to enhance their skills through courses, tutorials, and certifications in data analytics.",2,0,1721137992.0,bigdata
1e4pxd4,Resumable Full Refresh Data Syncs: Building resilient systems for syncing data,,1,0,1721139771.0,bigdata
1e26bgm,Mastering the Maze: How AI Transforms Lead Scoring with Unprecedented Data Analysis,,1,0,1720862750.0,bigdata
1e1b9et,Animals and Plant DB,"Hello guy we need all mostly known animals(including everything fishes, animals, birds) and plants to our new project. Is there free API's to get them?",2,1,1720767810.0,bigdata
1e0nyxs,Attribution modeling techniques: How Do you Select the right one?,"👋🏽 Hello everyone,

I'm currently learning all about attribution modeling techniques and have explored rule-based (first click, last click, exponential, uniform), statistical-based (Simple Frequency, Association, Term Frequency), and algorithmic-based methods (like Naive Bayes).

However, I'm struggling to understand how data scientists decide which modeling technique to use for their attribution projects, especially since ML and statistical models often compute different attribution scores compared to rule-based approaches.

I've created a short video demonstrating rule-based attribution techniques using Teradata Vantage’s free coding environment, and a sample dataset. For part 2, I plan to cover statistical and ML attribution modeling using the same data and include advice on choosing the right modeling technique.

I would love your insights on how you select your attribution modeling techniques. Any advice or guidelines would be greatly appreciated!

Here is the video I just created: [https://youtu.be/m1dkFxQiTNo?si=dfH5hljiPA0Bd7IK](https://youtu.be/m1dkFxQiTNo?si=dfH5hljiPA0Bd7IK)",4,0,1720701580.0,bigdata
1e0r3nv,Experiencia en con academia MundosE,Hola! Estoy pensando en inscribirme en MundosE para hacer la diplomatura en DevOps pero no encuentro muchas reviews al respecto. Alguno que pueda contar su experiencia?,1,8,1720710213.0,bigdata
1dztn6j,What if there is a good open-source alternative to Snowflake?,"Hi Data Engineers,

We're curious about your thoughts on **Snowflake** and the idea of an **open-source alternative**. Developing such a solution would require significant resources, but there might be an existing in-house project somewhere that could be open-sourced, who knows.

Could you spare a few minutes to fill out a short 10-question survey and share your experiences and insights about Snowflake? As a thank you, we have a few **$50 Amazon gift cards** that we will randomly share with those who complete the survey.

[Link to survey](https://docs.google.com/forms/d/e/1FAIpQLSd1IO83bBHIzc5WnSp_-GaryzeTD6r1C-aU8oupwvYIFKRepQ/viewform)

Thanks in advance",3,6,1720611691.0,bigdata
1dz6uev,Bufstream: Kafka at 10x lower cost,,0,0,1720542711.0,bigdata
1dxn5qi,Chunkit: Convert URLs into LLM-friendly markdown chunks for your RAG projects,,3,3,1720376619.0,bigdata
1dv7lfg,Best Alternative to zoominfo? We found Techsalerator but want to benchmark ,,1,1,1720101539.0,bigdata
1dud6ts,Need help about getting the users list from Cloudera data platform,I'm looking for anyone if they have experience working with cloud era data platform. I just want to know how can we get a list of users and the permissions they have who are using our analytical Cloudera data platform.,1,0,1720009892.0,bigdata
1dr4qkn,"Here is my playlist I use to keep motivated when I’m coding and studying. Feel free to share your music suggestions that can fit the playlist. Thank you !
",,0,0,1719642011.0,bigdata
1domnrj,June 27th Data Meetups,"JUNE 27TH DATA MEETUPS

- Talking about “Open Source and the Lakehouse” at the Cloud Data Driven Meetup

- Talking about “What is the Semantic Layer” at the Tampa Bay Data Engineers Group.",0,0,1719365615.0,bigdata
1dob519,Pornhub,,0,2,1719335305.0,bigdata
1do0688,The US crude oil export by countries by years,Has crude oil export become a new driver for the US economy?,1,2,1719300381.0,bigdata
1dno1kg,Financial careers heavy on data science? Scope in India?,"Hi folks. 
So recently, a frnd who is preparing for data science career let me know that India has plenty financial analyst opportunities that pay well. 
I am wondering what is the reality of that niche and how to go abt it-

To my limited knowledge I have gathered that:-

1) you don't need an mba for that. But a CMA or CFA would help
2) Importantly, you need to know SQL/ powerbi/ python( a bit of coding?) / tableau or related data heavy skills. Data analytics certifications also?

I was planning to go for a CFA anyways
I am willing to get certifications in above mentioned skills and deep dive into data science.

Problem is I am not a techie. So I was wondering what r financial careers that are data analysing inclined? And what can I do to crack into them having a non tech background. 

What is there scope in India?

Ps. Before anyone suggests posting this on financial subs. I have. I want to know the tech/data science angle to this. Since the friend who suggested this path have been preparing for that career. I have assumed it is related to this. Correct me if I am wrong tho.  ",1,2,1719262228.0,bigdata
1dmv7r0,Advice that I seek in my 20s as a data science kiddo,"short intro

Hello everyone, I moved to Canada 11 months ago. I did my bachelor’s in cse engg and specialization in AI and Data Science.
To put everything straight, I would rate myself as 5/10 for everything I learnt till now. I can do technical stuff but I am not sure thats my area of expertise. I want to get into techno managerial work. Something like consulting! I am not sure but I am sure that my work needs to be in data science and artificial intelligence 


What do i need?
I TOOK A MANAGEMENT DEGREE, inspite of my tech background. It is not like I dislike this program, However, I concern that this is not competitive enough for me. I am graduating by Dec 2024. 

Hypothetically lets say I am ready to prepare from sept 2024 - dec 2024. Consider my background knowledge in data science and research. What should I do? How should I start with? Please consider yourself in my shoes and tell me what should i do to secure a good job? 
( I humbly request you not to give me advice like, start from scratch, start from basics and do projects, network. I can do these things but I need a definite pathway) 

My rating would be as follows
Python 5/10
R 4/10
Sql 6/10
ML 6/10
Analytics (data processing, data management and data cleaning) 6/10
Data visualization 7/10
Storytelling 8/10

",0,0,1719173904.0,bigdata
1dlop60,Big data Hadoop and Spark Analytics Projects (End to End),"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",13,1,1719035354.0,bigdata
1dk8o4f,"Data processing modes: Streaming, Batch, Request-Response",[https://nussknacker.io/blog/data-processing-modes-streaming-batch-request-response/](https://nussknacker.io/blog/data-processing-modes-streaming-batch-request-response/),2,0,1718880647.0,bigdata
1djt59w,Vodacom fires hundreds of workers in crime crackdown ,,1,1,1718828858.0,bigdata
1djpe77,Libraries for large-scale vector similarity search,"Hi, so I'm working on a project in which I want to calculate the cosine similarity between a query vector and corresponding document vectors ( around a billion of them ) and then threshold them to get the most relevant documents. (Something similar to the retrieval phase of RAG.) The number of relevant documents isn't bounded so kNN isn't very relevant other than for initial pruning. Here, the speed is of the essence so the scale is a problem (as with most big data applications). I initially looked into FAISS and ScANN but are there any other libraries that I can look at that would be faster than these? Also, should I instead turn to some other programming language (or a dbms like postgres) altogether to get the additional boost in performance? (PS: I'm supposed to deploy the system on gcp. ) ",1,2,1718819493.0,bigdata
1dj1p7q,Big data vs cybersecurity,"Hello guys , i finished my preparatory cycle in CS and i have a confuse in continuing my studies in cybersecurity or big data 
Too many people’s tell me big data = mathematics and I’m not good at mathematics i struggled with it a lot of times 
But i love an i’m very good at computer network which is an important part of cybersecurity 
please i wanna know the opinion of specialist person in data and cybersecurity 
",9,14,1718745197.0,bigdata
1dj6j7n,Best Big Data Courses on Udemy for Beginners to Advanced -,,1,1,1718758458.0,bigdata
1dhw6ni,"Best End-to-End Open Source MLOps: Platforms, Frameworks and Tools",,5,1,1718625131.0,bigdata
1dh6ey3,Seeking Feedback on ETL and Data Warehousing Architecture with Multi-Source Systems,"In my project, which is based on ETL and Data Warehousing, we have two different source systems: a MySQL database in AWS and a SQL Server database in Azure. We need to use Microsoft Fabric for development. I want to understand if the architecture concepts are correct. I have just six months of experience in ETL and Data Warehousing.As per my understanding, we have a bronze layer to dump data from source systems into S3, Blob, or Fabric Lakehouse as files, a silver layer for transformations and maintaining history, and a gold layer for reporting with business logic. However, in my current project, they've decided to maintain SCD (Slowly Changing Dimension) types in the bronze layer itself using some configuration files like source, start run timestamp, and end run timestamp. They haven't informed us about what we're going to do in the silver layer. They are planning to populate the bronze layer by running DML via Data Pipeline in Fabric and load the results each time for incremental loads and a single time for historical loads. They’re not planning to dump the data and create a silver layer on top of that. Is this the right approach?

And I think it's very short time project is that a reason to do like this?",1,0,1718540512.0,bigdata
1dgc2ti,Getting started with stream processing,,1,0,1718434576.0,bigdata
1dg6rtx,Best Big Data Books for Beginners to Advanced to Read,,0,1,1718415188.0,bigdata
1dfsddo,The New Wave of Composable Data Systems and LLM Interfaces,"Hi all,

We recently published an article on the evolution of composable data systems and the integration of Large Language Models (LLMs) at WrenAI (https://github.com/Canner/WrenAI).

This article explores the shift towards open standards like Apache Iceberg and Arrow, and modular execution engines such as Velox and DuckDB.

[Structure of a composable data system \(https:\/\/voltrondata.com\/codex\/a-new-frontier#structure-of-a-composable-data-system\)](https://preview.redd.it/b0uf61wrsj6d1.png?width=1246&format=png&auto=webp&s=84c3a6ecc2ae478c792285ee2366ca19e9b38454)

It also introduces our WrenAI project, which uses a semantic engine to enhance context-aware interactions between LLMs and data systems.

[The WrenAI project in the composable data system](https://preview.redd.it/5b66x8ztsj6d1.png?width=1283&format=png&auto=webp&s=92ba17d5a5ce6fdeda0c76834e0e778a53bb499f)

Read more here if you're interested: [https://blog.getwren.ai/the-new-wave-of-composable-data-systems-and-the-interface-to-llm-agents-ec8f0a2e7141](https://blog.getwren.ai/the-new-wave-of-composable-data-systems-and-the-interface-to-llm-agents-ec8f0a2e7141)

>Disclosure: I'm a member of WrenAI",5,1,1718375849.0,bigdata
1dfqw88,Top AI Conferences & Expos Worldwide,,2,0,1718371841.0,bigdata
1df9l4h,"Like clockwork, every three months... /s ",,4,1,1718312387.0,bigdata
1dfir0r,Bigfile Tablespace Defaults in Oracle Database 23ai,,1,0,1718340391.0,bigdata
1df2uwv,Gretel Navigator is Now Generally Available ,,1,0,1718295344.0,bigdata
1deshk0,"
¿Preparado para olvidarte de la extracción manual de datos?",,1,1,1718260787.0,bigdata
1dea65f,How Apache Iceberg is Built for Open Optimized Performance,,2,1,1718207973.0,bigdata
1dea3vy,Ihre Expertise ist gefragt - Umfrage zu ETL und Data Warehouses,"Hallo zusammen,

im Rahmen meiner Bachelorarbeit am Department Informatik der Hochschule für Angewandte Wissenschaften Hamburg (HAW Hamburg) führe ich eine Umfrage zum Thema **""Qualitative Analyse und Vergleich von ETL-Prozessen und Tools für Data Warehousing im Kontext moderner Technologien 2024: Theoretische Grundlagen, praktische Umsetzung und Expertenmeinungen""** durch.

Eure Teilnahme ist wichtig, um wertvolle Erkenntnisse für meine Abschlussarbeit zu gewinnen.

Als Fachleute und Praktiker im Bereich Data Warehousing und ETL-Prozesse lade ich euch herzlich ein, an einer kurzen Umfrage teilzunehmen. Eure langjährige Erfahrung und euer Fachwissen sind von unschätzbarem Wert, um die Herausforderungen und Trends in diesem Bereich besser zu verstehen. Mit eurer Unterstützung können wir wertvolle Erkenntnisse gewinnen.

Die Umfrage dauert ca. 10-15 Minuten.

**Hier geht's zur Umfrage:** [https://campus.lamapoll.de/Bewertung-und-Nutzung-von-ETL-Tools](https://campus.lamapoll.de/Bewertung-und-Nutzung-von-ETL-Tools)

Ich versichere euch, dass alle Angaben vertraulich und anonym behandelt werden. Bei Interesse könnt ihr gerne die Ergebnisse der Umfrage erhalten.

**Vielen Dank im Voraus für eure Unterstützung! Eure Rückmeldung ist für mich von großer Bedeutung.**",0,0,1718207808.0,bigdata
1de2kzh,Top 10 Artificial Intelligence APIs for Developers,,2,0,1718184957.0,bigdata
1ddyrhl,"A Novel Fault-Tolerant, Scalable, and Secure NoSQL Distributed Database Architecture for Big Data","In my PhD thesis, I have designed a novel distributed database architecture named ""Parallel Committees.""This architecture addresses some of the same challenges as NoSQL databases, particularly in terms of scalability and security, but it also aims to provide stronger consistency.

The thesis explores the limitations of classic consensus mechanisms such as Paxos, Raft, or PBFT, which, despite offering strong and strict consistency, suffer from low scalability due to their high time and message complexity. As a result, many systems adopt eventual consistency to achieve higher performance, though at the cost of strong consistency.  
In contrast, the Parallel Committees architecture employs classic fault-tolerant consensus mechanisms to ensure strong consistency while achieving very high transactional throughput, even in large-scale networks. This architecture offers an alternative to the trade-offs typically seen in NoSQL databases.

**Additionally, my dissertation includes comparisons between the Parallel Committees architecture and various distributed databases and data replication systems, including Apache Cassandra, Amazon DynamoDB, Google Bigtable, Google Spanner, and ScyllaDB.**

Potential applications and use cases:

* The “Parallel Committees” distributed database architecture, known for its scalability, fault tolerance, and innovative sharding techniques, is suitable for a variety of applications:
* Financial Services: Ensures reliability, security, and efficiency in managing financial transactions and data integrity.
* E-commerce Platforms: Facilitates seamless transaction processing, inventory, and customer data management.
* IoT (Internet of Things): Efficiently handles large-scale, dynamic IoT data streams, ensuring reliability and security.
* Real-time Analytics: Meets the demands of real-time data processing and analysis, aiding in actionable insights.
* Healthcare Systems: Enhances reliability, security, and efficiency in managing healthcare data and transactions.
* Gaming Industry: Supports effective handling of player engagements, transactions, and data within online gaming platforms.
* Social Media Platforms: Manages user-generated content, interactions, and real-time updates efficiently.
* Supply Chain Management (SCM): Addresses the challenges of complex and dynamic supply chain networks efficiently.

I have prepared a video presentation outlining the proposed distributed database architecture, which you can access via the following YouTube link:

[https://www.youtube.com/watch?v=EhBHfQILX1o](https://www.youtube.com/watch?v=EhBHfQILX1o)

A narrated PowerPoint presentation is also available on ResearchGate at the following link:

[https://www.researchgate.net/publication/381187113\_Narrated\_PowerPoint\_presentation\_of\_the\_PhD\_thesis](https://www.researchgate.net/publication/381187113_Narrated_PowerPoint_presentation_of_the_PhD_thesis)

My dissertation can be accessed on Researchgate via the following link: [Ph.D. Dissertation](https://www.researchgate.net/publication/379148513_Novel_Fault-Tolerant_Self-Configurable_Scalable_Secure_Decentralized_and_High-Performance_Distributed_Database_Replication_Architecture_Using_Innovative_Sharding_to_Enable_the_Use_of_BFT_Consensus_Mec)

If needed, I can provide more detailed explanations of the problem and the proposed solution.

I would greatly appreciate feedback and comments on the distributed database architecture proposed in my PhD dissertation. Your insights and opinions are invaluable, so please feel free to share them without hesitation.",4,0,1718168895.0,bigdata
1ddfvmb,Solving the GenAI data quality problem with synthetics,,1,0,1718117086.0,bigdata
1db110d,To stay relaxed and focused while coding/working,"Here's Ambient, chill & downtempo trip, a carefully curated playlist regularly updated with chill and mellow electronica, downtempo, deep, hypnotic and atmospheric electronic music. The ideal backdrop for concentration and relaxation. Perfect for staying focused during my coding sessions. Hope this can help you too :) 

[https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=ZjANX6QhQ-e3rCa-gswFUQ](https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=ZjANX6QhQ-e3rCa-gswFUQ) 

H-Music ",0,0,1717846933.0,bigdata
1daqld0,Raw Datasets/Sources on Criminal Sentencing in the USA?,"So obviously there’s a lot out there with aggregate and precategorized stats from the FBI but I think it would be interesting to see some of the underlying data. The most important features would be: 

1. Name of the court
2. Specific charges the person was convicted of
3. The scentence administered by the judge

Anything else is just a bonus to have. I do not have access to any paid legal database software and this is just a hobby project because I find the subject matter interesting. Any tips are greatly appreciated!
",2,2,1717807703.0,bigdata
1daegeh,Full job data downloads now available @ jobdata API 🔥,,1,0,1717776315.0,bigdata
1d9uo8q,Summarizing Recent Wins for Apache Iceberg Table Format,,0,1,1717711824.0,bigdata
1d9unwd,Summarizing Recent Wins for Apache Iceberg Table Format,,0,0,1717711802.0,bigdata
1d9eldn,Data Lake(house)s research,"Hi! My name is Alina and I'm a product marketing manager at [Qbeast](https://qbeast.io/).

We're trying to get a better understanding of the challenges people face when it comes to managing their data, whether in data lakes or data lakehouses. We'd love to hear about your experience with data storage approaches.

If you could take a few minutes to fill out [this](https://forms.gle/DJ5N3zcfWLxYUJmF8) survey, we'd be really grateful. Link to the survey: [https://forms.gle/DJ5N3zcfWLxYUJmF8](https://forms.gle/DJ5N3zcfWLxYUJmF8)

And if you have more to share about lake(house)s, I'd be happy to chat with you. Thanks so much!",1,1,1717668259.0,bigdata
1d9dpo2,🤖 AI Automation with Multi-Agent Collaboration,,1,0,1717664391.0,bigdata
1d8ijzt,AI-Fueled Enterprise Data Management: The Rise Of Oracle Database 23ai,,1,0,1717566224.0,bigdata
1d7zsbk,"Open Source Table Format + Open Source Catalog = No Vendor Lock-in (Nessie, Polaris, Gravitino)",,0,0,1717514435.0,bigdata
1d71ref,A simple API to gather insights into the hiring market and access millions of job posts in JSON format,,4,0,1717412775.0,bigdata
1d6fu70,Here’s a playlist I use to keep inspired when I’m coding/developing/studying. Post yours as well if you also have one!,,1,0,1717342185.0,bigdata
1d5b7ng,Seeking Advice for AWS Data Engineer Exam Preparation,"Hello everyone,

I'm planning to take the AWS Data Engineer certification exam soon, and I would love to hear your advice and tips on how to prepare effectively.

For those who have taken the exam:

1. What study materials did you find most helpful?
2. Are there any particular topics or areas I should focus on more?
3. How did you structure your study schedule?
4. Were there any practice exams or resources that closely matched the actual exam?

Any insights or recommendations would be greatly appreciated. Thanks in advance!",3,3,1717203887.0,bigdata
1d4ru2q,You Won't Believe These 3 Undervalued AI Stocks That Could Make You Rich!,,0,0,1717148972.0,bigdata
1d430cd,How did American Airlines slash their big data costs by 23%?,"[How did American Airlines slash their big data costs by 23%?](https://www.reddit.com/user/global_dots/comments/1cabs0d/did_you_know_that_american_airlines_slashed_their/)

🎥 In our webinar ""Cut Big Data Costs by 23%: 7 Key Practices,"" we took a deep dive into the best practices for reducing costs effectively.

Watch the [full webinar for free](https://www.globaldots.com/lp/cut-big-data-costs-webinar/) to learn how you could:

💰 Cut costs: Learn from the successes of major corporations and see how

straightforward adjustments can lead to significant financial savings.

⏱️ Streamline operations: Explore how to make your data operations leaner and more efficient.

📈 Enhance performance: Boost your systems' efficiency without compromising on quality or output.

# bigdata #databricks #cloudinnovation",0,0,1717072600.0,bigdata
1d3zksw,Bigdata conference in the world ?,"I was looking at the bigdata conferences that takes place in the year and was wondering if  had better feedback than others, I went to the Bigdata europe conference last year and it was very nice, much better than the devox conference that took place in london in 2022.  
I then come across that one [https://www.globalbigdataconference.com/training-details.html](https://www.globalbigdataconference.com/training-details.html) but couldn't tell the quality of it.

I know bigdata is a vast term now but i'm looking for something heavely data relatad (not web) with some non cloud part as well.",1,6,1717059679.0,bigdata
1d3cn87,HeavyIQ: Understanding 220M Flights with AI,,10,0,1716990507.0,bigdata
1d39qi7,Blazingly-fast serialization framework for bigdata transfer: Apache Fury 0.5.1 released,,3,0,1716981526.0,bigdata
1d2x5t8,Artificial Intelligence in Welltory Health App,,2,0,1716936947.0,bigdata
1d2iwaz,Ingesting big data from Spark into feast feature store,"I am currently building a big data pipeline for an MLOps project, the pipeline is intended for batch processing.

This is the current setup:

* I am storing my raw structured data in Hive.
* Spark jobs ingest raw data and process it.
* I am intending on using feast and Apache Cassandra as an offline store.

My problem is passing processed data from spark to feast and then storing it in the offline store, I want to do it in a manner that is scalable and conveys to the requirements for a big data system.

I think intermediary data persistence is needed for passing data but I have no idea how to do it in a big data context.

Please any suggestions or resources that may help are appreciated.",1,0,1716901246.0,bigdata
1d2aq0s,GPT-4o: Learn how to Implement a RAG on the new model,,1,0,1716869374.0,bigdata
1d14cnj,Here’s a playlist I use to keep inspired when I’m coding/developing/studying. Post yours as well if you also have one!,,1,0,1716738762.0,bigdata
1d0d3ka,Researchers found that accelerometer data from smartphones can reveal people's location passwords body features age gender level of intoxication driving style and be used to reconstruct words spoken next to the device.,,16,5,1716646890.0,bigdata
1czpsy8,"dbt alternatives: dbt-core alternatives, dbt Cloud alternatives, and Graphical ETL tools",[https://datacoves.com/post/dbt-alternatives](https://datacoves.com/post/dbt-alternatives),3,0,1716570518.0,bigdata
1czod6c,Generate Differentially Private Synthetic Text for Fine-tuning AI Models,,1,0,1716566864.0,bigdata
1czhclp,How AI will solve XR’s biggest problem,,1,0,1716545620.0,bigdata
1cye508,RDS to S3 Data Transfer options ,"Moving data from AWS RDS to S3 to later be used by Databricks and eventually Tableau.

What is the best way to transfer this data to s3?
1. AWS DMS 
2. AWS Glue
3. Create job in Databricks to connect to RDS, retrieve data and store in S3.

",3,8,1716419250.0,bigdata
1cy425q,"Run SQL Queries Locally on your CSV, JSON, XLS and Parquet files with Ease",,2,2,1716394222.0,bigdata
1cy521z,Vector Search - HNSW Explained,,2,0,1716396649.0,bigdata
1cxt4go,🤖 PaliGemma – Google's Open Vision Language Model,,2,0,1716357196.0,bigdata
1cwik75,Uber Migrates 1 Trillion Records from DynamoDB to LedgerStore to Save $6 Million Annually,,10,0,1716219532.0,bigdata
1cwoav5,Data Lake: what is the best approach,"Hi everyone, 

  
I've been learning a little bit about Data lakes recently and I'm currently using apache Iceberg in AWS (Athena), I have a few questions that I couldn't find the answers on youtube, I have a kafka producer that send real-time logs regarding my employees activites, I want to perform some hourly analysis over it and I got kind of confused, sending data from Kafka to S3 and using it as external table is very powerfull and I would love to use it, however to keep my apache iceberg table updated, I'll need to perform an insert into from select every hour and I'm wondering if I'm doing it wrong, I'm willing to create some lambda function that runs everyhour, execute the insert into command, and maybe empty the hourly normal table (The one I use to feed my iceberg datalake), does this sounds logical to you guys? 

  
Appreciate the feedback!",1,1,1716233863.0,bigdata
1cw9dt3,What are the trends in big data analytics?,,0,0,1716188541.0,bigdata
1cvvetb,Big data (ideally free) sources for car ownership?,"I'm trying to find owners of a few certain vehicles to make offers for them.

Are there any state or data brokers that have model of vehicle, owner name/email/phone where I can filter by vehicle model?",0,0,1716144857.0,bigdata
1cvp81r,How to Leverage Privacy-Enhancing Technologies for Data Protection and Privacy - Guide,"The guide below provides definition, objectives, and examples of  privacy-enhancing technologies (PET) like anonymization, encryption, consent management, data minimization, synthetic data, differential privacy, etc. as well as relationship between data protection and data privacy and its practical applications across healthcare, finance, messaging, IoT/smart devices: [How to Leverage PET for Data Protection and Privacy](https://pvml.com/how-to-leverage-pet-for-data-protection-and-privacy/)",1,0,1716128037.0,bigdata
1cuwknv,SDSM 2024 : Suicide Detection on Social Media @ IEEE BigData 2024,,2,0,1716036447.0,bigdata
1cul3ql,"Data, AI and Robotics: How e-commerce brands can stay ahead",,0,0,1715993402.0,bigdata
1cth2u3,Best Big Data Courses on Udemy for Beginners to Advanced -,,2,0,1715877051.0,bigdata
1csdc6k,The roadmap for becoming a Data Engineer ,,2,1,1715750909.0,bigdata
1cruw4i,"New #Altintiy #Webinar Petabyte-Scale Data in Real-Time: #ClickHouse, S3 Object Storage, and #Data Lakes ",,1,0,1715700377.0,bigdata
1crknxk,How to create HIVE Table with multi character delimiter? (Hands On) ,,0,0,1715664812.0,bigdata
1cq165l,Get data as csv from a very large MySQL dump file,"have a MySQL dump file as .sql format. Its size is around 100GB. There are just two tables in int. I have to extract data from this file using Python or Bash. The issue is the insert statement contains all data and that line is too lengthy. Hence, normal practice cause Memory issue as that line (i.e., all data) is load in loop also.

Is there any efficient way or tool to get data as CSV?

Just a little explanation. Following line contains actual data and it is of very large size.",5,4,1715494104.0,bigdata
1cpiv3r,"Anyone knows where can I find current and historical actual / recorded weather data parameters like wind speed, temperature, humidity recorded at Airports or any public institutions. ","I'm building a wind resource analysis tool for an assignment and need historical actual / recorded weather data parameters like wind speed, temperature, humidity recorded at Airports or any public institutions in India.

It would be great if anyone can share a link to open source data like this. I found historical data from NASA's POWER LARC and windy to be reliable but these are satellite parameter data and I need actual / recorded data points.",2,1,1715438089.0,bigdata
1cpgga6,AI Cheatsheet: AI Software Developer agents,,1,1,1715430654.0,bigdata
1cpgbon,🤖Beat Proprietary LLMs With Smaller Open Source Models,,1,0,1715430226.0,bigdata
1cozepu,"Parallel-Committees"": A Novelle Secure and High-Performance Distributed Database Architecture","In my PhD thesis, I proposed a novel fault-tolerant, self-configurable, scalable, secure, decentralized, and high-performance distributed database replication architecture, named “Parallel Committees”.

I utilized an innovative sharding technique to enable the use of Byzantine Fault Tolerance (BFT) consensus mechanisms in very large-scale networks.

With this innovative full sharding approach supporting both processing sharding and storage sharding, as more processors and replicas join the network, the system computing power and storage capacity increase unlimitedly, while a classic BFT consensus is utilized.

My approach also allows an unlimited number of clients to join the system simultaneously without reducing system performance and transactional throughput.

I introduced several innovative techniques: for distributing nodes between shards, processing transactions across shards, improving security and scalability of the system, proactively circulating committee members, and forming new committees automatically.

I introduced an innovative and novel approach to distributing nodes between shards, using a public key generation process, called “KeyChallenge”, that simultaneously mitigates Sybil attacks and serves as a proof-of-work. The “KeyChallenge” idea is published in the peer-reviewed conference proceedings of ACM ICCTA 2024, Vienna, Austria.

In this regard, I proved that it is not straightforward for an attacker to generate a public key so that all characters of the key match the ranges set by the system.I explained how to automatically form new committees based on the rate of candidate processor nodes.

The purpose of this technique is to optimally use all network capacity so that inactive surplus processors in the queue of a committee that were not active are employed in the new committee and play an effective role in increasing the throughput and the efficiency of the system.

This technique leads to the maximum utilization of processor nodes and the capacity of computation and storage of the network to increase both processing sharding and storage sharding as much as possible.

In the proposed architecture, members of each committee are proactively and alternately replaced with backup processors. This technique of proactively circulating committee members has three main results:

* (a) preventing a committee from being occupied by a group of processor nodes for a long time period, in particular, Byzantine and faulty processors,
* (b) preventing committees from growing too much, which could lead to scalability issues and latency in processing the clients’ requests,
* (c) due to the proactive circulation of committee members, over a given time-frame, there exists a probability that several faulty nodes are excluded from the committee and placed in the committee queue. Consequently, during this time-frame, the faulty nodes in the committee queue do not impact the consensus process.

This procedure can improve and enhance the fault tolerance threshold of the consensus mechanism.I also elucidated strategies to thwart the malicious action of “Key-Withholding”, where previously generated public keys are prevented from future shard access. The approach involves periodically altering the acceptable ranges for each character of the public key. The proposed architecture effectively reduces the number of undesirable cross-shard transactions that are more complex and costly to process than intra-shard transactions.

I compared the proposed idea with other sharding-based data replication systems and mentioned the main differences, which are detailed in Section 4.7 of my dissertation.

The proposed architecture not only opens the door to a new world for further research in this field but also represents a significant step forward in enhancing distributed databases and data replication systems.

The proposed idea has been published in the peer-reviewed conference proceedings of IEEE BCCA 2023.

Additionally, I provided an explanation for the decision not to employ a blockchain structure in the proposed architecture, an issue that is discussed in great detail in Chapter 5 of my dissertation.

The complete version of my dissertation is accessible via the following link: [https://www.researchgate.net/publication/379148513\_Novel\_Fault-Tolerant\_Self-Configurable\_Scalable\_Secure\_Decentralized\_and\_High-Performance\_Distributed\_Database\_Replication\_Architecture\_Using\_Innovative\_Sharding\_to\_Enable\_the\_Use\_of\_BFT\_Consensus\_Mec](https://www.researchgate.net/publication/379148513_Novel_Fault-Tolerant_Self-Configurable_Scalable_Secure_Decentralized_and_High-Performance_Distributed_Database_Replication_Architecture_Using_Innovative_Sharding_to_Enable_the_Use_of_BFT_Consensus_Mec)

I compared my proposed database architecture with various distributed databases and data replication systems in Section 4.7 of my dissertation. This comparison included Apache Cassandra, Amazon DynamoDB, Google Bigtable, Google Spanner, and ScyllaDB. I strongly recommend reviewing that section for better clarity and understanding.

The main problem is as follows:  
  
Classic consensus mechanisms such as Paxos or PBFT provide strong and strict consistency in distributed databases. However, due to their low scalability, they are not commonly used. Instead, methods such as eventual consistency are employed, which, while not providing strong consistency, offer much higher performance compared to classic consensus mechanisms. The primary reason for the low scalability of classic consensus mechanisms is their high time complexity and message complexity.  
  
I recommend watching the following video explaining this matter:  
[https://www.college-de-france.fr/fr/agenda/colloque/taking-stock-of-distributed-computing/living-without-consensus](https://www.college-de-france.fr/fr/agenda/colloque/taking-stock-of-distributed-computing/living-without-consensus)  
  
My proposed architecture enables the use of classic consensus mechanisms such as Paxos, PBFT, etc., in very large and high-scale networks, while providing very high transactional throughput. This ensures both strict consistency and high performance in a highly scalable network. This is achievable through an innovative approach of parallelization and sharding in my proposed architecture.  
  
If needed, I can provide more detailed explanations of the problem and the proposed solution.

I would greatly appreciate feedback and comments on the distributed database architecture proposed in my PhD dissertation. Your insights and opinions are invaluable, so please feel free to share them without hesitation.",0,1,1715373189.0,bigdata
1coo3kw,How to use Dremio’s Reflections to Reduce Your Snowflake Costs Within 60 minutes.,,1,0,1715343252.0,bigdata
1co88wg,Where Can we buy B2B Data ? We found Techsalerator to be the best so far but are looking for more.,,3,9,1715289349.0,bigdata
1coadrc,Ayuda de asesoramiento | Counseling Help,"I'm about to finish high school, specializing in 'Personal and Professional Computing', and I need opinions from knowledgeable people to argue in favor of and defend my project, as my teacher is about to dismiss it as impractical and 'unfeasible for us.' But we have a lot of faith in it.

My project is called 'E.C.D.U.I.T.', which stands for 'Quantitative Study of Useful Data in the Textile Industry.' It will involve analyzing massive amounts of data (using Hadoop clustering, utilizing home computers to demonstrate that we did it ourselves, etc.) that can provide useful information to textile companies. The objective is to raise awareness among small companies in our region about this technology in order to improve their competitiveness. As you can see, the project aims to apply everything learned in these 7 years of study in a final integrative project.

The needs/problems that the project aims to solve, and which the teacher believes we will not be able to solve, are:

Main Problem: Insufficient capacity of regional textile companies to compete in a highly competitive and dynamic national environment. 

Deficiency in technological innovation and digitization of internal operational processes within the organization. 

Lack of focus on financial characteristics within companies. 

Inability of regional companies to recognize customer needs, combined with resistance to change.

&#x200B;",1,0,1715294909.0,bigdata
1cm4cuy,Unlock Your Potential: Join Our Free Python Course - Getting Started with Python using Databricks,,1,0,1715059735.0,bigdata
1clzrm6,OS framework + catalog project looking to get more feedback from PySpark users,"Hey all we just [open sourced a whole system](https://blog.dagworks.io/p/hamilton-ui-streamlining-metadata?r=17gs0j&utm_campaign=post&utm_medium=web) we've been developing for a while that ties together a few things for python code (see [README](https://github.com/DAGWorks-Inc/hamilton/tree/main/ui), [quick youtube feature walkthrough](https://www.youtube.com/watch?v=0VIVSeN7Ij8)). 

1. Execution + metadata capture, e.g. automatic code profiling
2. Data/artifact observability, e.g. summary statistics over dataframes, pydantic objects, etc...
3. Lineage & provenance of data, e.g. quickly see what is upstream & downstream of code/data.
4. Asset/transform catalog, e.g. search & find if feature transforms/metrics/datasets/models exist and where they’re used.

Some screenshots:

[Lineage & code - one view of it](https://preview.redd.it/3i10g6shmwyc1.png?width=1852&format=png&auto=webp&s=8c7cdbcb3529e84849b5d0ed35f09e0d95944e8f)

[Catalog view and pointers to versions and executions](https://preview.redd.it/wibi3q4jowyc1.png?width=1841&format=png&auto=webp&s=2c93eb1b17db218e52822d17d4bb340acd1299cf)

[Execution profiling of functions and comparing with another run.](https://preview.redd.it/q31blhnnmwyc1.png?width=1823&format=png&auto=webp&s=d1129443c81aa2d5ea640215c69ecb7fcf9ca399)

[Data comparison view of outputs comparing two runs](https://preview.redd.it/d0rra3zomwyc1.png?width=1852&format=png&auto=webp&s=1adb0607cb48b923928f170de2cd50c357cc9f08)

To use the above, you need to use [Hamilton](https://github.com/DAGWorks-Inc/hamilton) (which is a light lift to move to, see [this blog post on using it for PySpark](https://blog.dagworks.io/p/expressing-pyspark-transformations)). So why am I telling you all this? Well for PySpark, you can't get some of the above insights that easily, well because it's PySpark, e.g. execution time for your code, & profiling data without redoing computation. So, I'm looking find some PySpark users that would be interested in code that's more manageable that can also integrate with a cool UI in exchange for testing out a couple of features.   
  
E.g. exposing query plans and knowing exactly which place in the code caused it to blow up.   
E.g. linking with the Spark History Server to get execution information so you can more logically tie together code and then what spark actually did.  
E.g. build a better data profiling integration  
etc.

Thanks all!",1,0,1715044908.0,bigdata
1cll8dn,Apache Fury 0.5.0 released,"We're excited to announce the release of Fury v0.5.0. This release incorporates a myriad of improvements, bug fixes, and new features across multiple languages including Java, Golang, Python and JavaScript. It further refines Fury's performance, compatibility, and developer experience. 

Fury can be used to accelerate the data transfer efficiency in big data distributed frameworks such as flink/spark.

See more in release notes: [https://github.com/apache/incubator-fury/releases/tag/v0.5.0](https://github.com/apache/incubator-fury/releases/tag/v0.5.0)",3,2,1715007758.0,bigdata
1ck5sv3,From ETL and ELT to Reverse ETL,,5,1,1714844189.0,bigdata
1cj7s25,cassandra snapshot,"HI all  
i was working on Cassandra db and i am using nodetool snapshot command to take snapshot of my database i want to know that does cassandra provide incremental snapshot or not. ( i have read the documentation and they wrote about incremental backup but not abot the incremental snapshot)  
would u please guide me .  
thank you !",0,0,1714740296.0,bigdata
1cgxboy,Effective Strategies for Search Engine Optimization (SEO),"Search Engine Optimization (SEO) plays a critical role in helping your website rank higher in search engine results pages (SERPs) and drive organic traffic. In this post, we'll explore some effective strategies to optimize your website for better visibility and relevance in search engine results.

**1. Keyword Research and Optimization**: Start by conducting thorough keyword research to identify relevant keywords and phrases that your target audience is searching for. Use tools like Google Keyword Planner or SEMrush to discover high-volume and low-competition keywords. Incorporate these keywords naturally into your website's content, including titles, headings, meta descriptions, and body text.

**2. High-Quality Content Creation**: Content is king in the world of SEO. Create high-quality, relevant, and engaging content that addresses the needs and interests of your target audience. Aim to provide value and answer users' queries with comprehensive and informative content. Regularly update your website with fresh content to keep both users and search engines engaged.

**3. On-Page Optimization**: Optimize your website's on-page elements to improve its search engine visibility. This includes optimizing title tags, meta descriptions, heading tags (H1, H2, H3), URL structure, and image alt attributes. Ensure that your website is user-friendly and easy to navigate, with clear and descriptive internal linking.

**4. Mobile Optimization**: With the increasing prevalence of mobile devices, it's essential to optimize your website for mobile users. Ensure that your website is responsive and mobile-friendly, with fast loading times and intuitive navigation. Google prioritizes mobile-friendly websites in its search results, so optimizing for mobile is crucial for SEO success.

**5. Technical SEO**: Pay attention to technical aspects of SEO, such as website speed, crawlability, indexing, and site architecture. Fix any technical issues that may be impacting your website's performance in search results. Use tools like Google Search Console to identify and resolve technical SEO issues.

**6. Link Building**: Build quality backlinks from reputable and relevant websites to improve your website's authority and credibility in the eyes of search engines. Focus on acquiring natural and organic backlinks through content marketing, guest blogging, influencer outreach, and social media engagement.

At Windsor.ai, we understand the importance of effective SEO strategies in driving organic traffic and improving online visibility. Our platform offers advanced analytics and attribution tools that can help you track and analyze the performance of your SEO efforts, allowing you to make data-driven decisions and optimize your SEO strategy for better results.

What other effective SEO strategies have you found useful? Share your insights in the comments!",1,3,1714495063.0,bigdata
1cfo60j,Survey on the Role of Artificial Intelligence and Big Data in Enhancing Cancer Treatment,"Hello everyone, I am currently doing my dissertation paper on Big Data and AI. Right here is a questionnaire that I prepared for my primary research. 

Anyone who answers my questions will remain anonymous.

1.	**Background Information**
	
•	What is your professional background? (Options: Healthcare, IT, Data Science, Education, Other)
	
•	How familiar are you with AI and big data applications in healthcare? (Scale: Not familiar - Extremely familiar)

2.	**Perceptions of AI and Big Data in Healthcare**
	
•	In your opinion, what are the most promising applications of AI and big data in healthcare?
	
•	How do you think AI and big data can improve cancer tumor detection and treatment?

3.	**Challenges and Barriers**
	
•	What do you see as the biggest challenges or barriers to implementing AI and big data solutions in healthcare settings?
	
•	How concerned are you about privacy and security issues related to using AI and big data in healthcare? (Scale: Not concerned - Extremely concerned)

4.	**Effectiveness and Outcomes**
	
•	Can you provide examples (if any) from your experience or knowledge where AI and big data have significantly improved healthcare outcomes?
	
•	How effective do you believe AI is in personalizing cancer treatment compared to traditional methods?

5.	**Future Trends**
	
•	What future developments in AI and big data do you anticipate will have the most impact on healthcare in the next 5-10 years?
	
•	What role do you think cloud computing will play in the future of AI and big data in healthcare?

6.	**Personal Insights**
	
•	What advice would you give to healthcare organizations looking to integrate AI and big data into their operations?
	
•	What skills do you think are essential for professionals working at the intersection of AI, big data, and healthcare?

7.	**Open-Ended Response**
	
•	Is there anything else you would like to add about the role of AI and big data in healthcare that has not been covered in this questionnaire?


Thank you for your time!",1,0,1714357980.0,bigdata
1cf6wjl,I recorded a Python PySpark Big Data Course and uploaded it on YouTube,"Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!



[https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=9&t=1s](https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=9&t=1s)",6,0,1714312350.0,bigdata
1ce8sln,20 Popular Open Source AI Developer Tools,,2,0,1714203283.0,bigdata
1ce8s0v,We're inviting you to experience the future of data analytics,,1,0,1714203213.0,bigdata
1cc6jyp,Open Source SQL Databases - OLTP and OLAP Options,"Are you leveraging open source SQL databases in your projects?

Check out the article here to see the options out there: [https://www.datacoves.com/post/open-source-databases](https://www.datacoves.com/post/open-source-databases)

**Why consider Open Source SQL Databases?** 🌐

* **Cost-Effectiveness:** Dramatically reduce your system's total cost of ownership.
* **Flexibility and Customization:** Tailor database software to meet your specific requirements.
* **Robust Community Support:** Benefit from rapid updates and a wealth of community-driven enhancements.

Share your experiences or ask questions about integrating these technologies into your tech stack.",0,0,1713985352.0,bigdata
1cc5j2k,Google Search Parameters (2024 Guide),,1,0,1713982893.0,bigdata
1cba77w,WAL is a broken strategy?,"Hi,

I'm studying a bit on big data systems.

I've bounced into [this](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704) article, from 2019, which explains WAL is a broken strategy and actually inefficient - Written by VictoriaMetrics founder. In short: He says: Flush every second in SSTable format (of your choice), and do the background compaction to slowly build it up to descent size block. He says there are two systems out there using this strategy: VM and ClickHouse.

  
Would love to hear some expert Big Data take on this.

",8,0,1713892626.0,bigdata
1cavms9,Big data Hadoop and Spark Analytics Projects (End to End),"Hi Guys,   

I hope you are well.   

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.** 

***Apache Spark Analytics Projects:*** 

&#x200B;

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)   
 
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)   
 
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)   
 
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)   
 
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)   
 
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)   
 
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)   
 
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)   
 

***Bigdata Hadoop Projects:*** 

&#x200B;

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)   
 
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)   
 
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)   
 
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)   
 
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)   
 

I hope you'll enjoy these tutorials.   ",29,0,1713846196.0,bigdata
1cb6im7,Strategies for Handling Missing Values in Data Analysis,"As data scientists and data analysts delve into the intricate world of data, they often encounter a common challenge: filling over gaps. The identified information can be lost due to several reasons, for instance human error, breakdown of sensors as well as lack of collection of data. Getting the missing values problem right is critical because if they are not handled correctly, they can be very detrimental to the functioning of machine learning models and statistical estimation. [Click here to read more >>](https://www.dasca.org/world-of-data-science/article/strategies-for-handling-missing-values-in-data-analysis)",1,0,1713883681.0,bigdata
1cb21dd,How can I share BigQuery reports with non-technical folks?,"**Want to easily share BigQuery insights with your external clients, partners, or vendors?**

If complex BI tools or clunky CSV exports are your current solutions, it’s time for an upgrade! [Softr](https://www.softr.io/blog/softr-bigquery-integration) now integrates with BigQuery, allowing you to easily connect to your BigQuery database to create dedicated dashboards and reports— without coding or complex analytics tools.

Here’s what you can do:

&#x200B;

* Data portals: Create intuitive, customized dashboards directly within Softr. No need for third parties and non-technical team members to master complex analytics software.
* Secure access control: Fine-tune permissions to determine exactly what data each external user can see.

Transform the way you share your BigQuery insights.",1,0,1713871207.0,bigdata
1cavor7,Strategies for Handling Missing Values in Data Analysis,"As data scientists and data analysts delve into the intricate world of data, they often encounter a common challenge: filling over gaps. The identified information can be lost due to several reasons, for instance human error, breakdown of sensors as well as lack of collection of data. Getting the missing values problem right is critical because if they are not handled correctly, they can be very detrimental to the functioning of machine learning models and statistical estimation. This article covers some data scientists skills and methodologies that are a must for effectively managing missing data. [Click here to read more >>](https://www.dasca.org/world-of-data-science/article/strategies-for-handling-missing-values-in-data-analysis) ",3,0,1713846387.0,bigdata
1ca72q6,Data Integration Unlocked: From Silos to Strategy for Competitive Success,,2,0,1713780821.0,bigdata
1caaeeg,ClickHouse Performance Master Class – Tools and Techniques to Speed up any ClickHouse App Webinar,"[ClickHouse Performance Master Class – Tools and Techniques to Speed up any ClickHouse App](https://hubs.la/Q02t2dtG0)  
We’ll discuss tools to evaluate performance including ClickHouse system tables and EXPLAIN. We’ll demonstrate how to evaluate and improve performance for common query use cases ranging from MergeTree data on block storage to Parquet files in data lakes. Join our webinar to become a master at diagnosing query bottlenecks and curing them quickly. [https://hubs.la/Q02t2dtG0](https://hubs.la/Q02t2dtG0) ",1,0,1713791476.0,bigdata
1c9s5pp,Graph Database,"Apache Software Foundation PROJECT SPOTLIGHT: APACHE AGE [https://news.apache.org/foundation/entry/asf-project-spotlight-apache-age](https://news.apache.org/foundation/entry/asf-project-spotlight-apache-age)

[https://age.apache.org/](https://age.apache.org/)

[https://github.com/apache/age](https://github.com/apache/age)",2,1,1713732172.0,bigdata
1c9qogd,Seeking Data Sets of 2023 Headlines from Major Publications,,1,0,1713728514.0,bigdata
1c9e53h,The Future of Healthcare: How AI is Revolutionizing Medical Diagnostics,"Hey everyone, stumbled upon this fascinating article discussing the urgent need for AI integration in healthcare diagnostics. In today's rapidly evolving world, it's crucial for the healthcare sector to adapt, and this piece dives deep into why AI is the way forward.  
  
Check it out: [The Integration of AI in Healthcare: Enhancing Diagnostic Accuracy and Patient Outcomes](https://7med.co.uk/ai-in-healthcare-enhancing-diagnostics-and-patient-outcomes/)  
  
From highlighting the burden of diagnostic errors to exploring the promise of AI in addressing these challenges, this article offers a comprehensive overview. It delves into real-world examples, showcasing how AI is already making a tangible difference in patient outcomes.  
  
What's particularly intriguing is the discussion on upcoming innovations in AI and the skills healthcare professionals need to develop to thrive in this AI-integrated environment.  
  
Definitely worth a read for anyone interested in the intersection of technology and healthcare! Let's spark some discussions on how AI is shaping the future of medicine.",3,0,1713693168.0,bigdata
1c8lig4,Reporting system for microservices,"Hi,
we are trying to implement a reporting system for our microservices: our goal is to build a business intelligence service that correlates data between multiple services.

Right now, for legacy services, there is an ETL service that reads data (sql queries) from source databases and then stores it in a data warehouse where data is enriched and prepared for the end user.

For microservices, and in general for everything that is not legacy, we want to avoid this approach because multiple kinds of databases are involved (es: postgresql and mongodb) and our ETL service need to read an high amount of data, including things that has not been changed, every day (very slow and inefficient).

Because people of ""data team"" (the one who manage ETL jobs and business intelligence stuff) are not the same of dev team, every time a dev team decides to change something (e.g: schema, database engine, etc), our ETL service stops working, and this requires a lot of over coordination and sharing of low level implementation details.

We want to obtain the same level of backwards compatibility between changes and abstraction used for service-to-service interaction (REST API) but for data, delegating the dev team to maintain that layer of backwards compatibility (contract with data team), also because direct access to source databases and implementation details is an anti-pattern for microservices.


A first test was made using debezium to stream changes from sources database to kafka and then s3 (using iceberg as table format) in a kind of data lake, while using trino as query engine.
This approach seems to be very experimental and difficult to maintain/operate (e.g. what happens with a huge amount of inserted/updated data!?).
In addition to that, it is not clear how to maintain the ""data backwards compatibility/abstraction layer"": one possible way could be to delegate it to dev teams allowing them to create views on ""data lake"".

Any ideas/suggestions?
",3,2,1713604007.0,bigdata
1c8a74y,Exploring Ethics in Data Analysis,"In the world of digital marketing, data analysis plays a crucial role in strategic decision-making. However, it's important to address the issue of ethics when collecting, analyzing, and using data for marketing campaigns. In this post, we'll explore the ethical considerations that should be taken into account in data analysis.

**User Privacy Protection**: It's essential to respect user privacy when collecting data for marketing campaigns. Make sure to obtain proper consent before collecting personal data and to comply with all data protection laws and regulations, such as GDPR in Europe or CCPA in California.

**Transparency and Consent**: Users should be fully informed about how their data is being collected, stored, and used. Providing transparency about data collection practices and obtaining explicit consent from users before using their data for marketing purposes is important.

**Avoiding Discrimination and Bias**: When analyzing data for marketing campaigns, it's crucial to avoid discrimination and unfair bias. Ensure that your data analysis practices do not contribute to discrimination or exclusion of certain user groups.

**Data Security**: Protect your users' data through appropriate security measures to prevent unauthorized access or disclosure of confidential information. Implement robust security practices to ensure the integrity and confidentiality of your users' data.

**Accountability and Transparency**: Be transparent about your data analysis practices and take responsibility for any misuse or misunderstanding of the data. Adopt a proactive approach to address any ethical concerns that may arise and work collaboratively with stakeholders to ensure the integrity of your data analysis practices.

At Windsor.ai, we value the importance of ethics in data analysis and are committed to upholding the highest ethical standards in all our operations. Our platform offers advanced analytics and attribution tools that help brands make informed decisions ethically and responsibly.

What other ethical considerations do you think are important in data analysis for marketing campaigns? Share your ideas in the comments!",1,0,1713566432.0,bigdata
1c81qv4,adapt() gives error while using Normalization Layer in Sequential Models?,"While using Normalization layer in Sequential Model, while adapt(), I am getting Unbound Error:

normalizer = Normalization()

normalizer.adapt(X\_train)

    ---------------------------------------------------------------------------
    UnboundLocalError                         Traceback (most recent call last)
    Cell In[198], line 2
          1 normalizer = Normalization()
    ----> 2 normalizer.adapt(X_train)
    
    File /usr/local/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py:228, in Normalization.adapt(self, data)
        225     input_shape = tuple(data.element_spec.shape)
        227 if not self.built:
    --> 228     self.build(input_shape)
        229 else:
        230     for d in self._keep_axis:
    
    UnboundLocalError: local variable 'input_shape' referenced before assignment",2,2,1713545455.0,bigdata
1c7ptz4,The Role of Smart Maritime IoT Solutions in Enhancing Maritime Safety,,2,0,1713507541.0,bigdata
1c7qhtq,Best Big Data Courses on Udemy for Beginners to Advanced -,,1,0,1713510149.0,bigdata
1c7kbga,Exploring Content Personalization in Marketing Strategies,"Content personalization has become a cornerstone tool in digital marketing strategies, allowing brands to create unique and relevant experiences for their audiences. In this post, we'll explore the importance and benefits of content personalization in marketing.

**Importance of Content Personalization:**

1. **Enhances User Experience**: Content personalization enables brands to tailor the message and offer according to each user's specific preferences and needs, enhancing the user experience and increasing the likelihood of conversion.
2. **Increases Relevance**: By offering personalized content, brands can increase the relevance of their messages and promotions, resulting in higher engagement and loyalty from the audience.
3. **Builds Trust and Loyalty**: When users receive content that aligns with their interests and needs, they are more likely to trust the brand and develop a long-term relationship, which can lead to higher loyalty and retention.

**Benefits of Content Personalization:**

1. **Higher Engagement**: Content personalization can significantly increase audience engagement, as users are more inclined to interact with content that is relevant and specific to them.
2. **Improves Conversion Rates**: By offering personalized content, brands can increase conversion rates by targeting users who are more likely to be interested in their products or services.
3. **Optimizes Return on Investment**: Content personalization allows brands to optimize return on investment by focusing their resources on more receptive audiences and more profitable market segments.

At Windsor.ai, we understand the importance of content personalization in marketing strategies. Our platform offers advanced analytics and attribution tools that help brands better understand their audience and deliver highly personalized and relevant content.

What other advantages do you think content personalization offers in marketing? Share your ideas in the comments!",1,0,1713489359.0,bigdata
1c6q763,Inteligencia Artificial en el Mundo Empresarial [Tecnología E3],,1,0,1713400955.0,bigdata
1c6egmh,Building Customizable Database Software and Apps with Blaze No-Code Platform,"A cloud database is a collection of data, or information, that is specially organized for rapid search, retrieval, and management all via the internet. The guide below shows how with Blaze no-code platfrom, you can house your database with no code and store your data in one centralized place so you can easily access and update your data: [Online Database - Blaze.Tech](https://www.blaze.tech/online-database)",1,0,1713371690.0,bigdata
1c6b5so,"Flink SQL—Misconfiguration, Misunderstanding, and Mishaps",,1,0,1713363722.0,bigdata
1c5kewy,Best Big Data Books for Beginners to Advanced to Read,,0,0,1713284866.0,bigdata
1c5c49x,QCon London: Lessons Learned From Building LinkedIn’s AI/ML Data Platform,,2,0,1713260871.0,bigdata
1c5g5sa,Color Psychology in Data: The Role of Color in Data Visualization,,1,0,1713274235.0,bigdata
1c3lha0,Help me pick a laptop for Data engineering/Big data work,"I am planning to buy a laptop and confused which one to pick. Considering high performance,  budget under 40k. Thanks in advance! ",1,20,1713070833.0,bigdata
1c39g6g,How can I derive associations between player positions? ,"So I have a csv containing football data about goals where each goal has a scorer, GCA1(the player that gave assist), GCA2(the player that gave the pass to the assister) 

I want to discover patterns of player positions that lead to a goal AKA buildups to a goal

Example: RB passed to a CAM which assisted a goal scored by a ST, or CB passed to a RW which assisted a goal scored by a LW

I want to find the most frequent buildups, think of it as finding frequent itemsets for a supermarket to derive discount decisions. Except my goal is to know which buildups are most common and make up coaching plans to better strengthen the relationship between the players in those buildups

I was thinking of using APRIORI algorithm or FP-Growth, I tried CHATGPT but it didn't help me that much (I'm getting only one association between FW players and no one, or sort of saying forward players scoring solo, which is definitely not logical based on my dataset) and gemini is the most awful AI out there. Seriously my grandma can do better, I gave it a prompt and rephrased it 3 times and it still gave me 'Rephrase your prompt and try again' 

So does anyone know a way I can do this, or if there is a way to do it better. I'm still a junior data scientist so I'm still learning and I would gladly appreciate any feedback or advice. 
",1,1,1713035184.0,bigdata
1c31h0j,"🌐 Meta releases OpenEQA, open-source dataset",,1,0,1713013389.0,bigdata
1c1fma9,"Example Data Pipeline with Prefect, Delta Lake, and Dask","I’m an OSS developer (primarily working on Dask) and lately I’ve been talking to users about how they’re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:  
   
\- \*\***Run locally (optionally)**\*\*. Should be easy to try out locally and easily scalable.    
\- \*\***Scalable to cloud**\*\*. I didn’t want to think hard about cloud deployment.    
\- \*\***Python forward**\*\*. I wanted to use tools familiar to Python users, not an ETL expert.    
The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.  
   
I really like the outcome, but wanted to get more balanced feedback since lately I’ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I’ve had include:    
\- \*\***Prefect vs. Airflow vs. Dagster?**\*\* For the users I’ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.    
\- \*\***DeltaLake or something else?**\*\* To be honest I mostly see vanilla Parquet in the wild, but I’ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).    
   
Anyway, if people have a chance to read things over and give feedback I’d welcome constructive critique.  
Blog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html  
Code: https://github.com/coiled/etl-tpch",2,0,1712843555.0,bigdata
1c1ce56,IoT-Powered Smart Warehouse Management: A Detailed Guide,,2,0,1712833882.0,bigdata
1c1bwpo,Migration from MongoDB to PostgreSQL,,0,0,1712832141.0,bigdata
1c0x1vv,Complete Survey on Database Tech Education for a Chance to Win a $100 Amazon Gift Card!,"**$100 Amazon gift card** opportunity for participating in a 10-minute survey. We're inviting students from universities across the globe to participate in a brief survey conducted by Valley Consulting Group at UC Berkeley, in collaboration with **Oracle Corporation**.Your valuable perspectives will contribute to understanding database technology instruction in higher education globally. As a token of our appreciation, participants who complete the survey will be entered into a drawing for a chance to win a $100 Amazon gift card!

[https://berkeley.qualtrics.com/jfe/form/SV\_6wWbyuBP7pBJUmG](https://berkeley.qualtrics.com/jfe/form/SV_6wWbyuBP7pBJUmG) ",1,0,1712784291.0,bigdata
1bzs1es,🔄 Migration from MongoDB to PostgreSQL,,2,0,1712668901.0,bigdata
1bzw7ot,Blog: Dremio’s Commitment to being the Ideal Platform for Apache Iceberg Data Lakehouses,,1,0,1712679531.0,bigdata
1bziskn,A Questionnaire on Big Data and Digital Governance,"Dear Folks!

I am a PhD Research Scholar at Central University of Punjab. I am seeking your expert opinion on some questions. Here is the attached link to the questionnaire. This will take approximately 10-20 minutes to complete. Your input would be greatly appreciated.

Thanks for your kind cooperation.

Link: [https://forms.gle/FVEdAQoUkrzqd1JbA](https://forms.gle/FVEdAQoUkrzqd1JbA)",2,0,1712635166.0,bigdata
1bzguyb,Companies to apply for ,Suggest companies that have a stable data team in Egypt and employ junior/fresh regularly or remote companies that provide Internship in data science/engineering ,1,1,1712629340.0,bigdata
1byvl44,Bringing Home Your Very First Data Product,,2,0,1712576048.0,bigdata
1bxofsq,Choose Your Lakehouse Adventure,"Experience how easy it is to take data from your source data systems, ingest them into Apache Iceberg and serve a BI dashboard from the confines of your laptop with these tutorials.",0,0,1712443253.0,bigdata
1bwifgj,The Art of Data Wrangling in 2024: Techniques and Trends,,1,0,1712324710.0,bigdata
1bwbs2e,Smart Factory Case Study,,2,0,1712301036.0,bigdata
1bvlxa5,Trigger CPI Endpoint from SAP BTP cloud Application - ERP Q&A,,1,0,1712233857.0,bigdata
1bv99j8,Understanding Conversion Attribution in Marketing," Conversion attribution is a crucial concept in marketing analytics that helps businesses understand the impact of their marketing efforts and allocate resources effectively. In simple terms, it refers to the process of assigning credit to different marketing channels or touchpoints for driving conversions or sales.

There are several different models of attribution, each with its own strengths and weaknesses. Here are some common attribution models and how they affect marketing decisions:

1. **Last Click Attribution**: This model assigns all the credit for a conversion to the last touchpoint a customer interacted with before converting. While this model is simple and easy to implement, it often overvalues direct response channels like paid search and undervalues other channels that contribute to the customer journey.
2. **First Click Attribution**: In contrast to last click attribution, this model gives all the credit to the first touchpoint a customer interacted with. While this model can help businesses understand the initial customer acquisition journey, it may overlook the influence of later touchpoints that drive conversions.
3. **Linear Attribution**: This model evenly distributes credit across all touchpoints in the customer journey. While it provides a more balanced view of the customer journey, it may not accurately reflect the true impact of each touchpoint on conversion.
4. **Time Decay Attribution**: This model gives more credit to touchpoints that occur closer to the time of conversion, while giving less credit to touchpoints that occur earlier in the customer journey. While this model recognizes the importance of recency in influencing conversion, it may undervalue touchpoints that contribute to brand awareness and consideration earlier in the journey.
5. **Position-Based Attribution**: Also known as U-shaped attribution, this model gives more credit to the first and last touchpoints in the customer journey, while giving less credit to touchpoints in the middle. This model acknowledges the importance of both initial touchpoints that introduce customers to a brand and final touchpoints that drive conversion.

By understanding the strengths and weaknesses of different attribution models, businesses can make more informed decisions about how to allocate their marketing budgets and optimize their campaigns for maximum impact. While no single attribution model is perfect, using a combination of models or customizing attribution models to fit your business goals can provide a more accurate picture of the customer journey and drive better results.

At Windsor.ai, we're committed to helping businesses navigate the complex world of marketing analytics and attribution. Our platform offers comprehensive tools and solutions to track, analyze, and optimize marketing performance across channels. Whether you're looking to understand the impact of your Facebook Ads, Google Ads, or email campaigns, Windsor.ai has you covered.

Have questions about conversion attribution or marketing analytics? Feel free to ask in the comments below!",1,0,1712190450.0,bigdata
1buve6y,The DynamoDB Book - Basic Package Link in comments,,1,1,1712157915.0,bigdata
1bucvdg,Tips for Creating Effective Data Visualizations in Power BI," Visualizing data effectively is crucial for understanding and communicating important insights in data analysis. Whether you're a beginner or an experienced analyst, mastering the art of data visualization in Power BI can greatly enhance your ability to extract meaningful insights from your data. Here are some key tips to help you create impactful visualizations:

1. **Simplicity is key**: Keep your visualizations simple and easy to understand. Avoid cluttering your charts with unnecessary elements that may distract viewers from the main message of your data. Remember, less is often more when it comes to data visualization.
2. **Choose the right chart type**: Selecting the appropriate chart type is essential for effectively conveying your message. Consider factors such as the type of data you're working with, the relationships you want to highlight, and the audience you're addressing. Power BI offers a wide range of chart types, including bar charts, line charts, pie charts, and more, so choose the one that best suits your data and your goals.
3. **Use colors effectively**: Colors can be a powerful tool for highlighting important information and drawing attention to key insights in your visualizations. However, it's important to use them judiciously to avoid overwhelming your audience. Stick to a consistent color palette, and use contrasting colors to differentiate between different data categories. Avoid using too many colors in a single chart, as this can make it difficult to interpret.
4. **Add context**: Providing context is essential for helping viewers interpret your visualizations accurately. Add labels, titles, and descriptions to your charts to explain what they're showing and why it's important. Consider including annotations or callouts to draw attention to specific data points or trends. Adding context helps ensure that your visualizations are meaningful and actionable for your audience.
5. **Embrace interactivity**: Power BI offers powerful interactive features that allow viewers to explore and interact with your data dynamically. Take advantage of these features to create engaging and immersive visualizations that encourage exploration and discovery. Add slicers, filters, and drill-down capabilities to enable viewers to dive deeper into the data and uncover insights on their own.

By following these tips, you can create visually stunning and informative visualizations that effectively communicate your data insights. And with Windsor.ai, you can take your data visualization to the next level by seamlessly connecting multiple data sources, such as Facebook Ads, TikTok, Shopify, and more, directly to Power BI. With Windsor.ai, you can unlock the full potential of your data and gain a comprehensive view of your business.

If you would like to save even more time creating your reports, you can check our free templates available for Power BI: [https://windsor.ai/powerbi-templates/](https://windsor.ai/powerbi-templates/)

What other tips do you have for creating impactful data visualizations in Power BI? Share your ideas in the comments below!",1,0,1712099137.0,bigdata
1btxzmi,Invitation: Technical Theme for 17 April: 745-9am PST. 'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. ',[ Removed by Reddit on account of violating the [content policy](/help/contentpolicy). ],2,1,1712062426.0,bigdata
1btxq84,Metrics-Focused Data Strategy with Model-First Data Products,"**Purpose-driven data, metric dependency trees, product prototypes, and more!** 

**Concept**: Quenching the perpetual business curiosity  
**Tech**: Collection of Model-First Data Products  
💡 The Metric Dependency Tree  
💠 The Model-First Data Product(s)  
Step 1: Retrieving Requirements  
Step 2: Modeling the Product Prototype  
Step 3: Validating the Prototype  
Step 4: Materialising Product from Prototype   

**Read more:** [https://moderndata101.substack.com/p/model-first-data-products](https://moderndata101.substack.com/p/model-first-data-products) ",2,0,1712061636.0,bigdata
1bty0qz,Technical Theme for 17 April: 745-9am PST. 'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. ',"You are cordially invited to join us for an Invite- Only, zoom session. Limited to CISO's, CIO's, CTO's and Cloud SME's. (No Sales Executives please).

Technical Theme for 17 April: 745-9am PST.
'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. '

Presenter: Tim Rohrbaugh: (former CISO of Jet Blue)
thanks
DLH",1,0,1712062516.0,bigdata
1browv0,Apache Hive 4.0 has been released ,"Hi Guys,

Apache Hive 4.0 has been released . It's a really cool project , do check it out.

https://github.com/apache/hive

https://hive.apache.org/general/downloads/

https://hive.apache.org/",12,5,1711823908.0,bigdata
1bre8uq,🚀 Valkey: The Open Source Alternative to Redis,,2,0,1711792752.0,bigdata
1bqhmvs,Smart Supply Chains: Driving eCommerce Success with IoT,,2,0,1711690727.0,bigdata
1bqeh61,"Academic Survey, asks about the Challenges of Big Data Security","[https://forms.office.com/r/3qC141C9Xd](https://forms.office.com/r/3qC141C9Xd)  
This survey asks for your opinion on how concerning (On a scale from 1-5) are a number of specified challenges in Big Data security. It also asks for what you believe to be challenges to it in open ended questions that allow for long form answers. I'll admit that this survey is not at a high level as some of the other posts on this Sub, but I am just a student and this is my first foray into Big Data in a serious manner.   
This survey takes just under 5 minutes to complete, and you are the exact demographic that I would love to hear from. Thank you in advance.",2,0,1711680499.0,bigdata
1bpx91c,TUTORIAL: From Postgres to Dashboards with Dremio and Apache Iceberg,,2,0,1711636887.0,bigdata
1bpxkae,Apache Ranger UserSync Configuration HELP!!,"I am trying to configure Apache ranger usersync with unix ! and Iam stuck at this point !:

After i execute this : sudo JAVA\_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ ./setup.sh

Then this error pops up:

teka@t3:/usr/local/ranger-usersync$ sudo JAVA\_HOME=/usr/lib/jvm/java-8-openjdk-arm64 ./setup.sh

\[sudo\] password for teka:

INFO: moving \[/etc/ranger/usersync/conf/java\_home.sh\] to \[/etc/ranger/usersync/conf/.java\_home.sh.28032024144333\] .......

Direct Key not found:SYNC\_GROUP\_USER\_MAP\_SYNC\_ENABLED

Direct Key not found:hadoop\_conf

Direct Key not found:ranger\_base\_dir

Direct Key not found:USERSYNC\_PID\_DIR\_PATH

Direct Key not found:rangerUsersync\_password

Exception in thread ""main"" java.lang.NoClassDefFoundError: com/ctc/wstx/io/InputBootstrapper

at org.apache.ranger.credentialapi.CredentialReader.getDecryptedString(CredentialReader.java:39)

at org.apache.ranger.credentialapi.buildks.createCredential(buildks.java:87)

at org.apache.ranger.credentialapi.buildks.main(buildks.java:41)

Caused by: java.lang.ClassNotFoundException: com.ctc.wstx.io.InputBootstrapper

at java.net.URLClassLoader.findClass(URLClassLoader.java:387)

at java.lang.ClassLoader.loadClass(ClassLoader.java:418)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)

at java.lang.ClassLoader.loadClass(ClassLoader.java:351)

... 3 more

ERROR: Unable update the JCKSFile(/etc/ranger/usersync/conf/rangerusersync.jceks) for aliasName (usersync.ssl.key.password)

Can any one help me with that ?

Tools Iam using:

Host Device: MacBook m1

Guest Device: Ubuntu 20.04 LTS

Apache Ranger: 2.4 (Build from source code)",1,0,1711637718.0,bigdata
1bp82iq,Seeking Opninions: What's the Equivalent of my Degree Internationally?,"Hi everyone!

3 years ago i completed my bachelor 's degree in what would be translated to information science. However after some research it seems more like what other countries would call computer science.  


I have been wondering what to call my degree when applying for international jobs.   
I was therefore hoping someone here could give me a pointer of what my degree equates to, and how it stacks up against similar bachelor´s degrees around the world?  


 Here are a few examples of the courses I've taken during my degree: 

* Programming (Basic and advanced Python)
* Machine Leaning
* Artificial Intelligence 
* Data Management
* System Developement
* Knowledge graphs
* Knowledge representation and Reasoning
* Human-Computer Interaction  


Thanks in advance for any opinions!",1,1,1711561621.0,bigdata
1bo7tih,The Critical Role of Data Science in the Climate Crisis Battle,,3,0,1711459458.0,bigdata
1bo2dyi,🤖 GitHub’s AI tool can fix code vulnerabilities,,0,0,1711440150.0,bigdata
1bnnncv,Seeking Guidance on Advancing Skills," **I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field?**  ",2,0,1711397579.0,bigdata
1bnkjnw,Is GraphX still SOTA?,"My team is looking to update and improve our monthly TB scale graph pipeline that currently uses Spark DFs. While there has been recent development of Graph Databases, graph data processing seems like a quieter space. Is GraphX still state of the art or are there newer large scale tools that are better? ",1,0,1711390325.0,bigdata
1bmi11h,Price/performance for on-prem cluster in 2024?,"For reasons which I'd like to leave out of scope of this thread, I've the opportunity to spec up some on-prem Big Data (HDFS, Ranger, Spark, Hive, Zeppelin etc) clusters where the exact workloads aren't known in advance, just that we want to get the maximum performance for common uses cases for the amount that we're charging.

Have there been any studies with 2020s systems that might shed light on what would perform best for most typical use cases, out of e.g. clusters of 6x $20,000 machines, vs 12x $10,000 machines, vs 24x $5,000 machines, vs 60x $2,000 machines? (assume electric/cooling bills are baked into the price already).

My gut instinct is that the 60-node cluster would probably win, but I've zero evidence to back that up and it doesn't seem to be what any of the big players do.

",2,4,1711277630.0,bigdata
1bmhg1y,Using LangChain to teach an LLM to write like you,,2,0,1711275268.0,bigdata
1blk9u7,Unlocking the Potential of ML Models with High-Quality Data through sCompute," 

Hello BigData enthusiasts!

I wanted to share an article that I believe could spark an interesting discussion among us, especially those who are into the intersection of big data and machine learning.

The article introduces us to sCompute, a platform that emphasizes the importance of high-quality data for building effective machine learning models. For those who have been involved in big data analytics, you know how the quality of data can make or break our models.

Here's a quick overview of what sCompute brings to the table:

* **Enhanced Data Quality**: sCompute has developed a system to ensure that the data fed into ML models is clean, relevant, and of high quality.
* **Efficient Data Preparation**: The platform provides tools to streamline the often-tedious process of data preparation, making it easier for ML practitioners to get their datasets ready for analysis.
* **Scalability**: sCompute seems to have tackled the issue of scalability, helping data scientists to handle larger datasets more effectively.

The implications for big data analytics are significant. By improving data quality, we can potentially achieve more accurate insights, better predictive models, and more effective decision-making processes.

I'm curious to hear your thoughts on this. How do you currently handle data quality issues in your ML projects? Are there any platforms or methods you swear by to ensure the data you're working with is top-notch?

Here's the [link to the article](https://ankbig.hashnode.dev/build-ml-models-on-the-highest-quality-data-meet-scompute) for those interested in a deeper dive.

Looking forward to reading about your experiences and insights on this topic!",1,0,1711170253.0,bigdata
1blitfw,Need help :Pricing Inquiry for Data Cleaning and Analysis Service with Databricks and PySpark Expertise," 

Hello,

I'm currently exploring options for professional data cleaning and analysis services, particularly those utilizing Databricks and PySpark expertise. I have a dataset that requires thorough cleaning to address inconsistencies and erroneous data, followed by in-depth analysis to extract valuable insights for my business.

Here's a breakdown of the tasks I'm looking to outsource:

1. **Initial Evaluation:** Assessing my dataset to identify data quality issues.
2. **Data Cleaning:** Applying advanced data cleaning techniques to rectify inconsistencies and erroneous data.
3. **Databricks Analysis:** Utilizing Databricks for large-scale data analysis, optimizing processing performance.
4. **PySpark Development:** Writing PySpark scripts for efficient processing and analysis of distributed data.
5. **Reporting and Insights:** Generating detailed reports and providing insights based on the analysis performed.
6. **Continuous Optimization:** Recommending strategies for ongoing improvement of data quality and analysis processes.

I understand that the cost of such services can vary depending on factors such as the complexity of the dataset, the volume of data, and the specific requirements of the analysis. However, I would appreciate any ballpark estimates or insights from forum members who have experience with similar projects.

Additionally, if you have recommendations for reputable service providers or consultants specializing in data cleaning and analysis with Databricks and PySpark, please feel free to share them.

Thank you in advance for your assistance!",1,2,1711165371.0,bigdata
1bl8rh9,Apache AGE: Graph Database Magic in PostgreSQL,"Hey r/BigData!

Ever mixed graph databases with SQL? Apache AGE lets you do just that in PostgreSQL, opening new doors for analyzing complex data.

**Why Apache AGE?**

* Integrates graph and SQL for deep data insights.
* Ideal for navigating complex data connections.
* Open-source with a supportive community.

**Let’s Talk:**

* How would you use AGE to model intricate networks or data relationships?
* Share any cool queries or insights you’ve discovered.
* Tips for optimizing AGE? Let's exchange ideas!

**Dive In:** Curious about Apache AGE? Check it out on GitHub or their docs. Whether you're exploring new data solutions or enhancing current projects, AGE has something to offer.

For a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).

Excited to see your thoughts and how you're leveraging Apache AGE in your data adventures!",4,0,1711138092.0,bigdata
1bkzmla,Kafbat UI for Apache Kafka v1.0 is out!,,4,0,1711115188.0,bigdata
1bkg4mv,"Need Guidance, 4th Semester Data Science Student","Hey everyone,

&#x200B;

I'm currently in my 4th semester of data science, and while I've covered a fair bit of ground in terms of programming languages like C++ and Python (with a focus on numpy, pandas, and basic machine learning), I'm finding myself hitting a roadblock when it comes to diving deeper into big data concepts.

&#x200B;

In my current semester, I'm taking a course on the fundamentals of Big Data. Unfortunately, the faculty at my university isn't providing the level of instruction I need to fully grasp the concepts. We're tackling algorithms like LSH, PageRank, and delving into Hadoop (primarily mapreduce for now), but I'm struggling to translate this knowledge into practical coding skills. For instance, I'm having difficulty writing code for mappers and reducers in Hadoop, and I feel lost when it comes to utilizing clusters and master-slave nodes effectively.

&#x200B;

To add to the challenge, we've been tasked with building a search engine using mapreduce in Hadoop, which requires understanding concepts like IDF, TF, and more – all of which we're expected to learn on our own within a tight deadline of 10 days.

&#x200B;

I'm reaching out to seek guidance on how to navigate this situation. How can I set myself on a path to learn big data in a more effective manner, considering my time constraints? My goal is to be able to land an internship or entry-level position in the data science market within the next 6-12 months.

&#x200B;

Additionally, any tips on approaching this specific assignment would be immensely helpful. How should I go about tackling the task of building a search engine within the given timeframe, given my current level of understanding and the resources available?

&#x200B;

Any guidance, advice, or resources you can offer would be greatly appreciated. Thank you in advance for your help!",3,0,1711051877.0,bigdata
1bk13nm,Need help on a data problem,"Hi, I currently new in this field and want to ask for some advice on this problem.

Given N items (N \~ 10\^8), each item has a list of unique items that is ""related"" to it. The average size of the ""related"" list of an item is about 5000. The problem is, each time, a list of items is given with size \~ 10\^3 items, we have to return the number of unique items in the concatenated list of all the ""related"" items of at least 1 item in the given list.

* Input: Each line is the item id and its ""related"" items. So the input matrix is around 10\^8 \* 10\^3.
* Output:
   * When given a list of X (X \~ 10\^3) items, we have to concatenate the lists of ""related"" items of X items, and return the number of unique items.
   * For each query, the inference time is <= 1s.

Example:

Input:

1 2 3 4

2 1 3 5

3 1 2

4 2 5

5 1 4

So the item 1 is related to 2, 3, 4. item 2 is related to 1, 3, 5. item 3 is related to 1, 2 and so on.  
If the query is (1, 4), then the answer is 4. (the list is (2, 3, 4, 5) = (2, 3, 4) + (2, 5)).

Requirements:

* Exact solution with inference time <= 1s
* Cannot use cloud computing (must run with my own hardwares)

Priority (top to bottom is most prioritized to least)

* Inference time
* Use the least memory
* Simplicity
* Scalability...

What might be the most probable solutions for this? Thanks in advance.",2,1,1711007070.0,bigdata
1bk3d8k,‘Sankeying’ with Plotly,,1,0,1711016883.0,bigdata
1bjtgtw,Making Data Easy and Open with Alex Merced,,1,0,1710981411.0,bigdata
1bjbnwe,"What are the potential implications of Big Data analytics on ethical business practice, with a focus on assumed data discrimination and bias in relation to data processing, within the UK financial sector? (18+, ideally FinTech & AI experts/enthuiasts)",,1,0,1710935793.0,bigdata
1bin1en,"Dask Demo Day: Dask on Databricks, scale embedding pipelines, and Prefect on the cloud","I wanted to share the talks from last month’s Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing.

Last month’s talks:

* One trillion row challenge
* Deploy Dask on Databricks with dask-databricks
* Deploy Prefect workflows on the cloud with Coiled
* Scale embedding pipelines (LlamaIndex + Dask)
* Use AWS Cost Explorer to see the cost of public IPv4 addresses

Recording on YouTube: [https://www.youtube.com/watch?v=07e1JL83ur8](https://www.youtube.com/watch?v=07e1JL83ur8)

Join the next one this Thursday, March 21st, 11am ET [https://github.com/dask/community/issues/307](https://github.com/dask/community/issues/307#issue-1587938315)",5,1,1710862314.0,bigdata
1bifuf0,Best Big Data Courses on Udemy for Beginners to Advanced -,,2,0,1710839083.0,bigdata
1bifpr5,Quickwit 0.8: Searching into Petabytes of logs on S3,,2,0,1710838530.0,bigdata
1bifbqw,Hive Shell Issues,"Whenever i try to run a query, it provides me the output, but along with it, so many junk text come up and fill the screen that it’s difficult to find where’s the output.

Even while starting hive, i get these series of error messages, and it opens finally after some wait. Please help me out with this!

Video: https://drive.google.com/file/d/10uKD6iZbEWUG9epxPKns0Q5zkOtCldqV/view?usp=drivesdk",1,0,1710836701.0,bigdata
1bhkk2a,[D] Blog on Spark Caching,"Hello everyone! I've recently begun writing blogs on Medium. I'd love to hear your suggestions on how I can enhance my content. Your input is greatly appreciated!

[https://medium.com/@algorhythm2411/caching-in-spark-what-how-why-f412aac3acf5](https://medium.com/@algorhythm2411/caching-in-spark-what-how-why-f412aac3acf5)

&#x200B;",1,0,1710745008.0,bigdata
1bhg429,Best Big Data Courses on Udemy for Beginners to Advanced -,,0,0,1710729231.0,bigdata
1bgmall,Help for uni project,"Hi guys, I am a university student enrolled in Accounting program that will set me up for CPA when I graduate. I am part of a group project for Information Systems course of my program. Our group has to write a report on an emerging accounting technology and out of available topics the group chose **Big Data & Accounting Analytics**. 

There needs to be several sections in the report- an introduction of the technology, its current uses, advantages & disadvantages, potential future uses and a **case study** describing **a real-life**  
 **example of how Big Data & Accounting Analytics is being used at an organization** (in approx. 1000 words).   
 I need some **help for the case study**. I want to build a good project, so I thought why not try reddit for help. Any advice from people who really are working in Big Data & Analytics can help the report be more nuanced. Any suggestions on how to approach it? Where to research? Even better if you use it in your organization, then can you provide some information as to **how you use it?** **Why you use it?** and **what benefits you derived** from its introduction?",2,0,1710640841.0,bigdata
1bfa89a,Postgres is eating the database world,,20,3,1710496305.0,bigdata
1bff2zm,Need help! location data rearranging,"I am looking to arrange the top dataset in the order like the bottom one. Stock location course should always be in alphabetical order. The first row should always be accessed from the highest stock location position. When there is a switch in stock location course, it should look at the last stock location position of the previous line's stock location course. If the number is above 73, it should select the highest number from the next stock location course and order from high to low. If the number is below 73, it should select the lowest number from the next stock location course and order from low to high. I want to achieve the fastest and easiest walking route this way. stock location level is irrelevant as this only points to the hight a location is stored.



Does anyone have tips on how to fix this? ChatGPT is not helpful unfortunately

https://preview.redd.it/v19ug0owbioc1.jpg?width=784&format=pjpg&auto=webp&s=4e50887c7356afb950b5d51b2af9f57fc66346db

",1,0,1710512495.0,bigdata
1be2fah,Getting started with big data,"Hi folks, I'm with a small but growing company. Our data sets are growing quickly and need to be moved out of the operational data stores ( mainly MySQL) but remain accessible for historical analysis.

I've been researching big data strategies and have found the number of available tools and technologies to be overwhelming. Given that incremental learning can be costly in terms of time and effort due to the sheer volume of data, I'm wondering where best to begin.

As I said, I need to offload historical data from the operational database, but still be able to access it. There's no immediate need for real-time queries, but it's quite possible that there will be in the very near future. Just moving it from one relational store to another (been there, done that) only puts off solving the problem.

So I need to move it somewhere but where? We are in an AWS environment, so is it S3? Hadoop? NoSQL? Kafka? ...? And presumably this choice will affect the decision of what tools to use to access it for historical views within the application. And I can't start moving the data until I also have a way to access it.

Would be wide open to an answer being to read this book or take this course. It's just hard to know which given that everyone seems to be trying to peddle their particular solution.

Thoughts anyone? Thanks!",7,8,1710363045.0,bigdata
1be0lz6,Data skewness issue while extracting data from rdbms,"Hi guys,
I am facing data skewness issue while reading data from rdbms into a dataframe using spark in emr serverless. I tried to apply salting technique while reading data using spark because the saltkey( trunc (rdbms_random.value*10)). The salt key logic I am using is generating different values in different executer. I am looking for a solution who handled extracting rdbms skewness issue with partition column.

Thanks",3,0,1710358711.0,bigdata
1bcwghr,26 AI Reports from Famous Brands,,2,0,1710247746.0,bigdata
1bctdhw,10 Top Big Data Analytics Companies Serving the Healthcare Industry?,"*Top big data analytics services companies have been meticulously selected based on a proprietary company competitiveness analysis, taking into consideration their portfolio strength and company strength. These companies are instrumental in meeting the growing demand for big data analytics in healthcare market. We analyzed a comprehensive list of companies offering big data analytics services to manage and analyze large volumes of unstructured healthcare data.* \[https://www.rootsanalysis.com/key-insights/top-big-data-analytics-companies.html\] ",1,0,1710236944.0,bigdata
1b9mgmo,Need Help: Optimizing MySQL for 100 Concurrent Users,"I can't get concurrent users to increase no matter the server's CPU power.

Hello, I'm working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user 'generates reports' of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users' report generation tests using the PostMan application. **Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.**

When multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application's ability to manage concurrent users effectively. For example, a single user might generate a report for one month's worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, **when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.**

I am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I'm currently not considering this option.

Is there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?

# Additional Information:

Backend: Dotnet Core 6 Web API (MVC)

# Database:

MySql Community Server (free version)  
**table 48, data length 3,368,960,000, indexes 81,920**  
But in my calculation, I mostly only need to query from 2 big tables:

# 1st table information:

Every 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range—using both start and finish times—alongside an integer field representing a list of user IDs.  
For example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.

|Type|
|:-|
|bigint(20) unassigned Auto Increment|
|int(11)|
|int(11)|
|timestamp \[current\_timestamp()\]|
|timestamp NULL|
|double(10,2) NULL|
|int(11) \[1\]|
|int(11) \[1\]|
|int(11) NULL|

# 2nd table information:

The second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.  
Queries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.

|Type|
|:-|
|int(11) Auto Increment|
|date|
|int(10)|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|time|
|int(10)|
|timestamp \[current\_timestamp()\]|
|timestamp \[current\_timestamp()\]|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(200)|
|varchar(100)|
|int(10)|
|int(10)|
|varchar(200) NULL|
|int(100)|
|varchar(100) NULL|

# Test Results (Dedicated Bare Metal Servers):

SystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88 threads | 448GB DDR4 RAM  
Single User Report Generation time: 3mins (for 1 week's data)  
20 Concurrent Users Report Generation time: 25 min (for 1 week's data) and 2 users report generation were unsuccessful.  
**Maximum concurrent users it can handle: 40**",1,3,1709899986.0,bigdata
1b9vq4h,"YOUR MIC IS 99% PROBABLY OPEN IN THE BACKGROUND, CHECK YOUR SETTINGS NOW, FOR ANDROID ""AUTHORIZATION MANAGER""->""MIC""/""CAMERA""->SELECT #2 ""ALWAYS ASK"" GIVING PERMISSION ON YOUR MIC IS A CONSENT, SO DO NOT GIVE THIS CONSENT UNLESS STRICTLY NECESSARY"," YOUR MIC IS 99% PROBABLY OPEN IN THE BACKGROUND, CHECK YOUR SETTINGS NOW, FOR ANDROID ""AUTHORIZATION MANAGER""->""MIC""/""CAMERA""->SELECT #2 ""ALWAYS ASK"" GIVING PERMISSION ON YOUR MIC IS A CONSENT, SO DO NOT GIVE THIS CONSENT UNLESS STRICTLY NECESSARY",0,0,1709923472.0,bigdata
1b8y6iq,Kafka ETL: Processing event streams in Python.,,4,0,1709827098.0,bigdata
1b8xa7z,Would a fuzzy search database for structured data be interesting?,"We needed a solution where we can search our database with fuzzy matching and couldn't find any that offered highly customizable matching, we needed phonetic, geospatial and similarity matching. So we decided to build it ourselves all based on AWS serverless services. The solution turned out great and fulfilled our needs. I am now wondering if there other options out there that we just didn't find before we built this? and if not, would such a solution be interesting for others?",2,1,1709824941.0,bigdata
1b8ilsc,The Apache Iceberg Lakehouse: The Great Data Equalizer,"As we stand at this inflection point, the significance of open-source technologies like Apache Iceberg cannot be overstated. They are not merely tools or platforms; they are the harbingers of a new era in data management, where flexibility, collaboration, and innovation take precedence over walled gardens and restrictive practices. The narrative of Snowflake and Databricks, their responses to Apache Iceberg, and the rise of Dremio's Lakehouse Platform all underscore a fundamental truth: in the world of data, openness is not just a feature—it's the future.",5,2,1709777755.0,bigdata
1b88srg,LinkedIn Open Sources OpenHouse Data Lakehouse Control Plane,,5,0,1709753451.0,bigdata
1b69xik,"What's ""Modern"" in the Modern Data Stack","**Industry scenarios in 2024, community reviews, platform standards, and more!**     


𝐖𝐡𝐚𝐭 𝐭𝐡𝐢𝐬 𝐩𝐢𝐞𝐜𝐞 𝐞𝐧𝐭𝐚𝐢𝐥𝐬:   

* Current State of the Modern Data Stack 
* Getting into the Specifics:  
\- The Structure  
\- Elements of the Stack  
\- Before & After at a Glance     
 

𝐑𝐞𝐚𝐝 𝐭𝐡𝐞 𝐜𝐨𝐦𝐩𝐥𝐞𝐭𝐞 𝐚𝐫𝐭𝐢𝐜𝐥𝐞 𝐡𝐞𝐫𝐞: [https://moderndata101.substack.com/p/whats-modern-in-the-modern-data-stack](https://moderndata101.substack.com/p/whats-modern-in-the-modern-data-stack) ",2,8,1709555951.0,bigdata
1b65aqg,GreenGauge Analytics: Competitive Edge for Eco-Friendly D2C E-Commerce Businesses," Hi everyone,

I’ve been working on something that’s really important to me, and I think it could be valuable to you too. It’s called GreenGauge Analytics. The idea came from my own struggles and desires to make my e-commerce operations more sustainable without just shooting in the dark. I wanted to make decisions based on what consumers really care about when it comes to sustainability, not just what we think they do.

So, I started building a platform to provide actionable insights into consumer sentiment on sustainability and the latest trends that are shaping the e-commerce landscape. But here’s the thing - it’s not just about what I believe is needed. This is about creating something that truly serves our community of eco-conscious brands and consumers.

I’m reaching out because I need your help. Before we go full steam ahead, I want to make sure we’re on the right track. If you have a moment, I’d really appreciate it if you could sign up to stay in the loop and maybe share your thoughts through a short survey. Your feedback would mean the world to me and really help shape this into something that can genuinely support businesses like yours in making a positive impact.

You can sign up [here](https://123carmartin321.wixsite.com/greengaugeanalytics). Thank you so much for your time and for considering being a part of this journey. Let’s make sustainability at the core of e-commerce together.

Looking forward to hearing from you,

GreenGauge Analytics",1,0,1709538484.0,bigdata
1b5g3s4,Physical Data,"Hello, I’m a physic student. I am studying the biomechanics of high jump and long jump.  So I wanted to ask where on the Internet I can find tables of physical data (instantaneous speed, altitude, time) related to performances of different athletes, if you can Marco Tamberi.
Thank you. I tried to ask in other communities.",1,0,1709469071.0,bigdata
1b4r767,10 Reasons to Make Apache Iceberg and Dremio Part of Your Data Lakehouse Strategy,,0,0,1709394341.0,bigdata
1b4g55j,Data Architecture Complexity,,1,0,1709355878.0,bigdata
1b3xej2,IOMETE released the most generous free Data Lakehouse platform,"Hello Spark Community!

We're launching, the IOMETE Community Edition on AWS, and looking for insightful testers like you. This is your golden ticket to experience our scalable data lakehouse platform, designed to transform terabytes to petabytes of data, absolutely free. You'll be amazed by what you can achieve with our platform.

We're excited to see how users experiment with the platform by Leveraging Apache Iceberg and Spark for a managed data lakehouse that grows with your data—from terabytes to petabytes—without any vendor lock-in. Enjoy complete control over your data stored in S3 in parquet format, and pay only for the AWS resources you use. Whether you're using Spot or Reserved Instances, IOMETE ensures an affordable path compared to other vendors. Ready to transform your data management strategy?

IOMETE offers several Apache Spark features including:

1. A user-friendly interface and integrated notebook service for data processing and analysis.
2. Comprehensive monitoring and debugging capabilities for Spark jobs.
3. Automatic scaling of Spark clusters based on demand.
4. Capabilities to process real-time data streams from various sources.
5. A platform for training and deploying machine learning models for tasks like predictive analytics, fraud detection, and customer segmentation.

These features are designed to help you focus on your data analytics workloads by taking care of the infrastructure and management tasks associated with running Spark - [https://2ly.link/1wFi0](https://2ly.link/1wFi0)

Intrigued? A short video awaits you to guide you through the details and the wonders that IOMETE Community Edition promises - [https://2ly.link/1wFi3](https://2ly.link/1wFi3)

If you have any questions regarding installation and usage, join our dedicated Discord community, and let's shape the future of data management together - [https://2ly.link/1wFi1](https://2ly.link/1wFi1)

As of now, the IOMETE Free Community Version can only be deployed on AWS. Please let us know where you would like to deploy the platform so we can prioritize it - [https://2ly.link/1wFi2](https://2ly.link/1wFi2) . We will let you know when your preferred deployment option becomes available.",1,0,1709307934.0,bigdata
1b3x4ww,A Deep Dive into the Concept and World of Apache Iceberg Catalogs,,1,0,1709307279.0,bigdata
1b3lq4l,Zapier dataset,"I have a new startup company that is using Zapier and i am searching for other small business owners and startup clients 

I came across this post on https://www.usesignhouse.com/blog/zapier-stats which breaks down the top industries that use Zapier and it lead me here

I will like to ask if you can share the dataset you used for the analysis or if anyone can point me in the right direction so i can get the list and distribution of the various types of companies that use Zapier so i can target similar companies for my marketing.

I am looking for datasets in a csv format i can further analyze industries or companies using data analytics to find a good niche that is underserved but needs Zapier automations so i can find clients. 

Any help would be appreciated.",1,0,1709268370.0,bigdata
1b34mcm,🤖 Forecast Evaluation for Data Scientists,,1,0,1709224517.0,bigdata
1b34j7a,Confused about 2024's ESG reporting? You're not alone - Discover this snackable content to help your company handle ESG regulations with ease.,,0,0,1709224297.0,bigdata
1b2tbkk,Unlock the Full Potential of Azure for Data Engineering and Analytics with Our Comprehensive Video Guide," 

Hey Azure enthusiasts and data wizards! 🚀

We've put together an **in-depth video series** designed to take your Azure Data Engineering and Analytics skills to the next level. Whether you're just starting out or looking to deepen your expertise, our playlist covers everything from **real-time analytics** to **data wrangling**, and more, using Azure's powerful suite of services.

**Here's a sneak peek of what you'll find:**

1. **Twitter Sentiment Analysis with Azure Synapse Analytics** \- Dive into real-time sentiment analysis and build end-to-end big data pipelines.
2. **Real-time Vehicle Telemetry Processing** \- Learn how to handle real-time vehicle data with Azure Stream Analytics and Event Hub.
3. **Fraudulent Call Detection** \- Discover how to detect fraudulent calls in real-time using Azure Stream Analytics.
4. **Weather Forecasting with Azure IoT Hub** \- Explore how to forecast weather using sensor data from Azure IoT Hub and Machine Learning Studio.
5. **Web Scraping with Azure Synapse** \- Get hands-on with web scraping using Azure Synapse, Python, and Spark Pool.
6. ... and much more across 20+ videos covering Azure Databricks, Azure Data Factory, and other Azure services.

**Why check out our playlist?**

* **Varied Topics**: From analytics to processing, explore Azure's capabilities through practical examples.
* **Skill Levels**: Content tailored for both beginners and experienced professionals.
* **Community Support**: Join our growing community, share your progress, and get support from fellow Azure learners.

Dive in now and start transforming data into actionable insights with Azure! Check out our playlist

[https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0](https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0)

**What's your biggest challenge with Azure or data engineering/analytics?** Let's discuss in the comments below!",1,0,1709186910.0,bigdata
1b26zsv,Streaming ETL in Python with Airbyte and Pathway,,4,0,1709129563.0,bigdata
1b21f12,Guide to contributing to open source big data projects,"I have for some time been using many big data tools such as :

1. spark
2. hadoop ecosystem (hive/hdfs)
3. trino/presto

Ive wanted to contribute to some of the projects as i felt they are really nice and I would get to learn in the process about the internals

Please suggest what should be a good project (big data tool) to start exploring for contributing and what should be approach

I did go for example to github page of trino and saw contribution guidelines but not sure why I gave up and its been a year now since then, 

Also is it ideal way (im about 2yoe in data engineering) or should i leave contribution for later (when i gain more knowledge)",7,3,1709110106.0,bigdata
1i51oay,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",13,11,1737302445.0,analytics
1dj1a5b,Looking for community feedback,"Hey r/analytics community,

As this group continues to grow I want to make sure majority are finding it useful.

I'm looking for your ideas of where we can improve this group and what do you love about it, leave your comments below.",16,18,1718744143.0,analytics
1i6ybro,Need Advice on a Data Analyst Offer as a New Grad,"Hello everyone,

I recently received an offer for a Data Analyst position at a company as a new grad with a Bachelor's in Statistics, and I’d love to hear your thoughts on my situation.

The role is completely new at the company, they have never worked with data before, aside from some basic Power BI visualizations using SQL. My concern is whether I, as a new grad, will be capable of kickstarting their data-driven decision-making without a senior analyst or a team of analysts to guide me. I’m worried about not having anyone to fall back on when facing challenges. 

Additionally, the job description includes process automation using Power Automate. They want to streamline their order-handling process, such as ensuring the right person gets an email based on the ordered product, sending automated confirmation emails to customers when an order is placed or shipped, and generating alerts when inventory is running low. To me, this sounds more like an IT or operations role rather than a typical data analyst position, and I’m unsure whether I have the skills to fulfill these expectations. Or is it ok for a data analyst to take care of such tasks as well?

Currently, I’m enrolled in a Master’s program in Statistics, but I feel like I might learn more by gaining real-world experience as a Data Analyst rather than just working with perfect datasets in R all day.

What do you guys think? Should I take the job and learn on the go, or would it be too risky to hop out the master's for a fresh started data analyst role at a company? I have a feeling one would learn and become a better analyst if I got a job where there is already a data driven culture to sort of guide me. Any advice would be greatly appreciated!",11,19,1737506626.0,analytics
1i746tx,Laboratory Technician trying to break into data analytics,"Hi all,

I am currently employed as a laboratory technician II and have been for almost 4 years. Due to the nature of lab work, I feel really stagnant and really want to break into a full fledged analyst position. We are a smaller company so we exclusively use Excel; so I’ve been learning SQL and Tableau via Udemy and W3 etc. on my own.

I have real world data analytical experience and have been applying to entry/mid level positions for over a year (over 100 applications at this point), and no dice. 

 Is anyone else in this position? Does anyone who has been in this position have any tips? Am I missing something? I know the job market sucks right now but man am I feeling it, lol. 

ETA: My degree is in Chemistry",4,10,1737524343.0,analytics
1i6v6dp,"What counts as a ""project"" for cutting your teeth on data analytics?","I keep seeing people recommending to do one instead of a cert (I'm doing google data analytics right now and it sucks so far, philosophy mumbo jumbo about thinking, mostly), but I'm just not sure what would meaningfully count as a project idea. 

Are the projects just.... rearranged spreadsheets? what are we even looking to do?",5,9,1737498208.0,analytics
1i6wytt,Full time course,"Hey all
I just completed a professional course in Accountancy and wish to switch to data analytics. Initially I was of the view that the same can be learned by me online. I started with learning the various tools, however I now am inquisitive about full time courses in universities on stats and DA combined. The problem is I think most require one to be a grad in tech (engineer or the likes I guess). Maybe I'm getting it wrong, in case anyone has any idea with this regards, it'd help me.",1,5,1737502897.0,analytics
1i629u6,2024 End of Year Salary Sharing thread,"I haven't seen anything posted here for 2024 EOY. Please let me know if there actually has been.  
Please only post salaries/offers if you're including hard numbers, but feel free to use a throwaway account if you're concerned about anonymity. You can also generalize some of your answers (e.g. ""Large biotech company""), or add fields if you feel something is particularly relevant.

**Title:**

**Tenure length:**

**Location:**

**$Remote:**

**Salary:**

**Company/Industry:**

**Education:**

**Prior Experience:**

**$Internship**

**$Coop**

**Relocation/Signing Bonus:**

**Stock and/or recurring bonuses:**

**Tech Stack Used:**

**Total comp:**",104,78,1737410100.0,analytics
1i6vt9z,Types of Analyst roles,"I have been working as a data analyst/project manager but would like to move towards a bit more technical role. I was looking around the job market, but it can be overwhelming to decide what direction to take.

I know that I hate the project management part of my job so I'd rather focus on the ""problem-solving on my own"" aspect.

Do most people have a goal and work towards a specific industry? For example, if I have a security interest are there very specific niche jobs that I can develop into from being a data analyst? To be fair I even struggle to find relevant jobs to my experience because the word analyst is so overused on the job market. How did you find a field that fit your interest?",1,2,1737499828.0,analytics
1i6v0fz,"Recommendations for Practicing Interview Skills (Podcasts, Websites, etc.)?","Hi everyone,

I’m preparing for interviews for data science/analyst roles and would love to hear your recommendations for resources to improve my skills. Specifically, I’m looking for:

\- Tips on how to introduce myself effectively.

\- Ways to refine my wording and expressions for clarity and confidence.

Do you know of any great podcasts, websites, or other resources that could help? I’d really appreciate your suggestions!

Thanks in advance!  
",1,2,1737497797.0,analytics
1i6bgpa,"I aspire to be a data analyst, was starting with python a good idea?","Title basically, I started learning python with interest of becoming a data analyst and now with a bit more research on what that entail, I realize that I may should have started with something else like math or sql, should I just stop python now and take it later or the order I learn the skills doesn't matter? ",23,34,1737437002.0,analytics
1i61xtb,What are the best entry level jobs that help you get into data analytics? ,"I've noticed a lot of people here are saying that a job in data analytics isn't really an entry level position, and you usually have to have worked in another job for a while before moving to data analytics 

In that case, what are good entry level jobs to go for that help you transition into data analytics? What are the jobs that you should go for right out of college (AKA entry level jobs) that will help you move into data analytics? 

For reference, I got a degree in economics, and I'm looking for my first post college job. ",51,19,1737409265.0,analytics
1i6qdao,Will learning AI and machine learning help me become a data analyst? ,"Hi all, as the title says, will learning artificial intelligence and machine learning models help me become a data analyst? I just graduated with a masters in IS and concentrated in data analytics. I see available entry level positions and internships mainly focused on these two and I wasn't sure if these would fit me and my end goal. I know SQL and Python but haven't learned R yet and I know data visualization tools like Tableau and honestly I think I'm more comfortable with SQL rather than the other languages that I feel like ML and AI focuses more on. I've been trying to find other entry positions for data analytics but no luck yet.

Also to add, my bachelor's is in a completely different field so I don't have any related work experience with manipulating and handling data much.

I was just wondering if my degree is more business IT and analytical rather than data science in general? Thanks for any feedback.",0,4,1737486337.0,analytics
1i6mdw7,"Building a Minimalistic BI Stack with PostgreSQL, FDW, and Superset – Looking for Feedback!","Hey everyone! I’m in the process of setting up a lightweight BI (Business Intelligence) architecture and would love to hear your thoughts before I go too far down the rabbit hole. Here’s the broad outline:

# My Setup

1. **Dedicated PostgreSQL “BI” Server**
   * I have a separate Postgres instance just for analytics (i.e., not the production DB). I really don’t want Superset or heavy queries hitting prod directly.
2. **Foreign Data Wrapper (FDW)**
   * I’m using FDW on the BI Postgres to pull data from the production DB (and potentially from other sources in the future). This way, I can query external tables without actually copying them over or building huge ETL pipelines right now.
3. **Materialized Views**
   * On the BI Postgres, I’m creating materialized views to store pre-aggregated data. The idea is to speed up reporting queries and reduce the load on both the BI and production DB. I’ll refresh these on a schedule (using either pg\_cron or a simple script).
4. **Apache Superset for Visuals**
   * I’ll point Superset at the BI Postgres so I can build dashboards from the materialized views. I’m trying to keep everything cost-effective and as simple as possible.

# Why I’m Doing This

* **FinOps & Simplicity**: I want to avoid a giant stack or expensive cloud services for moderate data volumes. We’re already comfortable with Postgres, so it made sense to leverage it fully.
* **Avoid Overloading Production**: I really don’t want to connect Superset directly to the production DB. I’m hoping FDW plus a separate BI Postgres instance is a safe approach.

# What I’m Unsure About

1. **FDW Performance**: Have you seen any issues with slow queries when using FDW for relatively large tables? I’ve read about “query pushdown” being pretty good, but I’m worried about network overhead or unexpected performance hits.
2. **Materialized View Refresh**: Right now, I’m planning on refreshing them daily (or maybe hourly for certain critical tables). Any advice on how to handle partial vs. full refreshes?
3. **Scaling**: Is this going to become a nightmare if data volumes suddenly jump? I realize I might need to move to something more robust if we get hammered with queries. But how do you all handle that growth path?
4. **Security & Access Controls**: Are there any best practices I should consider for user roles, especially to ensure people only see the data they’re allowed to see in Superset?

# Questions for the Community

* **Is this approach workable in the long run, or is it an early-stage hack?**
* **Any big gotchas with Postgres FDW or materialized views I should watch for?**
* **Would you recommend a different setup if you were building from scratch?**
* **Any cool tips for automating view refreshes (e.g., triggers, using pg\_cron, or external schedulers)?**

I’d really appreciate your feedback! If anyone has done something similar (or sees a looming pitfall), please let me know. My goal is to keep the architecture straightforward and cost-friendly, but also robust enough that I’m not drowning in queries or stale data.

Thanks in advance!",1,1,1737476591.0,analytics
1i6mbye,Ahh! Title change in resume ,"I did one internship, but it was more about marketing and sales. My role was account management. My background is in tech, and I have data analyst skills.

I want to add this experience to my resume but change the title from ""Account Manager"" to ""Data Analyst."" Will this cause any problems?

Also, I want to start a career in data analytics. How can I get into this field? What should I do to find good job opportunities? My college is ending in 4-5 months, and I want a job before that so I can start working full-time from July onwards.",1,3,1737476450.0,analytics
1i6kngc,would these job duties help me land a role in data analytics?,"hey, i would rly appreciate if someone could look over these job duties and let me know if they think this could be helpful in landing a data analytics job, this would be for a procurement and supply chain leadership development program for a packaging company, i want to work in data analytics but i dont think this role utilizes SQL

duties: 
-material pricing automation- this would include pricing standardization and automation, not sure what tool is used 

-spend cube optimization- ai tool is used for this, will be trained in using it and debugging it, either they don’t know what it is yet or i forgot what it is (not sure), this would include going to different packaging sites to better understand the ordering process and figuring out how to keep the data standardized 

-general data capturing and visualization for other tasks, would work with the procurement team, buyers, executive team and the IT team to do this 

",0,3,1737472096.0,analytics
1i6k3zr,Support/Guidance for my master thesis in Learning Analytics ,"Hello,

I’m a masters student and working as a working student in a product company. The company has an e -learning platform with a good number of active learners. I plan to do my master thesis from there as my manager agreed to give me data. But the issue is no one will be there to support me in office as well from university(because my professors are not experienced with data analytics, product based companies). 

As of now, I just have an idea to do an analysis of how I can retain learners using interventions in the learning platform. It would turn out well if I could get guidance from someone who has experience in Learning Analytics or something similar. Any tips or experience is appreciated. On the side, I’m also approaching professors from other universities for a collaboration. I study in Germany by the way. Thanks in advance! ",0,2,1737470661.0,analytics
1i6d48f,"How can you build insights across Google Analytics, HubSpot, Salesforce, Stripe, and Snowflake - without hopping multiple tools?","I work in SaaS and consistently need to connect data points from various stages of the sales funnel, from site visitors and lead generation to sales opportunities, revenue, and product usage.   
  
Typically, I have used SQL querying tools, Spreadsheets, BI Tools for Dashboards, Notebooks for exploration and a lot more. Is there a way I can sync data from these tools, explore when I need to and present dashboards and work on all this collaboratively with my team (a lot of ad-hoc Qs keep coming up from different stakeholders).   
  
This process is static and cumbersome to replicate regularly.   
  
I'm also building something myself to solve for this and would like to know what methods are effective for integrating and analyzing these diverse data streams more dynamically. ",2,8,1737443533.0,analytics
1i6fwyq,Career this year ,I am planning to start to learn data analytics. Is it good time to start this year ? If I finish data analytics Google certificates this year and create some projects will I be able to get job in this year? What is your opinion should I start to learn or should I should I focus on something else?,0,5,1737456346.0,analytics
1i6ivt9,Guidance! ,"I'm a B.A. graduate in English Literature, and I was preparing for government jobs. Due to the unpredictable nature of govt jobs I want to have a backup plan and I want to pursue Data Analyst or Data Scientist courses but I'm not aware of the difference between the two and whether am I eligible for this courses or not? Whether it's a worthy or not in today's context? 

If yes, then kindly guide me as I don't have any knowledge regarding this...like from where should I learn, which courses or skills I should learn etc. From the scratch..

Thank you in advance ",0,4,1737467099.0,analytics
1i5xnnu,jobs in US government,"Would it be wise to not apply for any analytics-related jobs in the US govt (agriculture, homeland security, etc.) for a while given the change of administrations? Not a political post - just a question as I'm looking to transition to a different role. ",13,14,1737398961.0,analytics
1i6373y,Credit Risk Analyst,"Hello all,

Is a corporate credit risk analyst position a good starting point or a good bridge to a data analyst career? I have a masters/bachelors in management and economics, and only 2 YoE. Won’t be using python or SQL, but would be taking courses on the side (currently doing the google data analyst course).",4,2,1737412392.0,analytics
1i5lfe5,what is the difference of a financial analyst and a data analyst? ,could anyone tell me the difference 😅,12,16,1737360491.0,analytics
1i65qcx,Management Information systems or business analytics ? Help me choose a degree ,"Which degree is the most employable ? 
Work life balance
Salaries 
",0,15,1737419111.0,analytics
1i5tp08,Variable Compensation Chart ,"Hi, 

I've been tasked with outlining a variable salary compensation plan for two of our most senior employees.   
What I have right now are, if 100% of KPI targets met in Q1 and Q2 = $5000 bi-annual bonus pay. What I do not know how to do is formulate the accelerator/decelerator from that.

Does anyone know of a template or formula that can help with this? ",2,5,1737389389.0,analytics
1i5u9a1,Need help transitioning from FA to BA,"Hi, New Grad here. Has anyone made the transition to Financial analyst to Business Analyst? In my role i’ll be using mainly Excel. If I want to make the switch should I pursue MSBA after a couple years working? Should I Just go straight to MSBA instead of even working the FA role? I feel lost.",0,6,1737390786.0,analytics
1i5kuu1,Analyst but not a Data Analyst,"Hi everyone, I’m an Analyst at a film/tv studio in CA. However, as far as being an analyst goes, I mainly use the following in my line of work; Excel, Box, PSL (film/tv specific type of Quickbooks but each file is one production instead of a whole business). 

My role really requires me to comb through 80k+ lines of data for inconsistencies in Free Form coding to maximize money back on qualified spending. 

But that’s it. This portion accounts for 50% of the job and the rest of it is communicating with other departments for backup and additional information. 

I want to aim towards working as a Data Analyst, whether it be for my studio or another. 

About me: BS in Business Admin, worked 10+ in accounting / finance ranging from accounting manager, senior project accountant, business analyst, and analyst. Currently 2 years into my specific role. 
I have used Tableau, Excel, SAP, Smartsheet, Workday, Microsoft Dynamics 365: M4, M5, F&O, Oracle Netsuite

I have free access to certification via Coursera, LinkedIn, QA, and Pluralsight. 

I was hoping to get feedback on how I can make this transition? I’m comparing straight up getting an MBA in Analytics, a CalTech Certification in Data Analytics, and simply getting certifications from the above sites I have access to for free (I can obtain certifications for free since my company pays for them)

What is the best move here? Where am I going wrong? Willing to learn in-depth and take as long as I need. Thank you! ",2,5,1737357963.0,analytics
1i5bgi4,College major question,Does it really matter what your college major is in to break into analytics? Currently pursing a BS in Business Admin with a concentration in business analytics and a applied mathematics minor (I know its a mouthful). I heard the data analytics program at my school is really watered down and a lot of people are having to learn all the different languages on their own time anyways.,10,16,1737326840.0,analytics
1i5flri,Analytics Major,"I am currently a junior in high school and I’m achieving my Associates Degree in the broad subject of business. I need to decide what major to do when I go into college. First of all I’m looking for a job that I enjoy with also making good money. I am a huge extrovert and love talking to people, I have a huge interest in sports and money. Is analytics the best major for business do you think? What are the pros and cons of analytics major. I was also looking at the option to double major in finance and analytics would this be smart?",3,6,1737339069.0,analytics
1i4nlb2,"What master's should I pursue: CS, Math, Stats, Economics, or MBA? ","Data Analyst with 3 years of experience and currently working as one. 

I have a Bachelor's in mathematics from Cal State Northridge. 

I'm looking to go for a master's in any field that will help look more impressive for promotions and to increase my knowledge further with data. I also would like to look more appealing(apart from any work experience and projects) for future prospective jobs. ",35,32,1737252428.0,analytics
1i4tkr1,How do you guys create data presentations after analysis?,"I have to frequently create data presentations to explain my findings. I've tried a few tools and a I have to do lot of things are pretty manually. So, just wanted to know if you guys have any tricks or do you also manually insert data in powerpoint charts or add screenshots. 

And how do you decide on the story? Isn't this is hard process, to use right brain after all that left.

Would love to know your workflow or any tools you guys use?",7,21,1737273371.0,analytics
1i4aavf,"Without a degree, now planning to shift into Data Analyst ","So initially i did my Bachelors but due to one zero credit subject which i failed held me back and i didn't get my degree, due to family pressure and finances I Had to return my own country where I got a job as supervisor in a company, and soon promoted to assistant manager on the side I did SEO and other analytical stuff which i was always interested about.

Now im planning to take a jump in my career im 26 and i don't want to be late on the boat, Im thinking of going through basic SQL, fundamentals, Power Bi, Tableu, thinking of doing some projects to add to my portfolio, thinking of also doing few months apprenticeship in data analyst meanwhile thinking of networking in Linkedin and finally applying for a data analyst job to get my career to begin, am i missing anything ? Do let me know ? Thanks in advance",19,45,1737215271.0,analytics
1i4tpjq,Need recommendation and sources ,"Hello guys I'm a finance graduate and also want to breat into data analytics with finance. Although I have a basic knowledge of python. I don't know which courses should I do...
Please help",1,9,1737273939.0,analytics
1i4skez,"Business Analysts in Aus - what course did you take, what would you do differently?","I am a project manager keen to learn BA skill. Can you become a BA without formal qualifications?  If you need qualifications to work, what courses would you recommend? If you were to start over - what would you change about the way you came to become a BA.",1,3,1737269094.0,analytics
1i4h8fn,"Want to transition into more into a data management titled role, what resources or training could help transition into this role with only document management experience",c,2,5,1737233921.0,analytics
1i42obl,Data analysis tools,"Please bear with me I have a question, I'm an IT BA and recently I'm considering moving to another company, but I realized a lot of job postings require certain DA tools knowledge like power BI, Tableau...etc. And I was thinking I could start working on them to learn and stuff but my issue is that my day to day job doesn't include working with data in anyform, since I do systems and software business analysis I don't seem to find a purpose in learning these tools. I mainly work with requirements and technical specifications and visualization of the projects scope, I do have to track all that and keep backlogs of everything but again I don't think DA tools would help with any of that.

But I still feel like I wanna aquire this skillset just in case.

My question is, is there a way to incorporate these tools to fit within my work scope, or should I consider learning other tools? Or should I just learn them for the heck of it!

Guide me please 🥺 

",10,7,1737186662.0,analytics
1i3w394,is it worth it?,,2,3,1737162899.0,analytics
1i3pen0,Am I a decent candidate for a analytics job? ,"Recent graduate with biomedical informatics degree, relevant courses to Data Analytics taken - Biomed Data Analytics l and ll. also did a capstone project where took a bunch of data from an excel and used python to clean, analyze, and put it through machine learning to predict an outcome. Although my major isn’t something specific like data science, i’m wondering if i should waste my time applying for data analyst positions, or any other “analyst” positions with what I have. Thank you ",7,15,1737144387.0,analytics
1i3tdpa,Data headcount vs company size,,3,5,1737155057.0,analytics
1i3tyno,Is this considered Data Analytics?,"Hi! I have a degree in Economics with a major in Applied Statistics and am interested in pursuing a career as a data analyst, with the eventual goal of transitioning into data science.

I recently started a new role where my tasks include creating report charts (using Q for significant testing and then visualizing the data in PowerPoint), cleaning datasets, and performing QA to ensure accuracy in reports.

However, I don’t currently use SQL or Python in my job. Given these responsibilities, do you think this experience will help me progress toward a career in data science, or would it be better to explore other opportunities?",0,24,1737156699.0,analytics
1i3ve37,QlikSense Developer [$45 per hour] Needed in SF Bay Area,"Hi Folks, 

I am looking for a mid-level QlikSense Developer \[$45 per hour\] to work for an AI startup.

This is a hybrid role - at times you have to come in person. 

If interested please get in touch with me [Chris@Analyze.Agency](mailto:Chris@Analyze.Agency)",0,13,1737160790.0,analytics
1i2wy2q,Mid-level and a bit stuck,"I’m a mid-level data analyst with 6 years experience and a SQL, PowerBI, PowerQuery, Excel stack. 

I recently quit my job because of the workload (they had me doing 10-14 hours a day, insane) and now I feel kinda stuck in terms of where to go. 

I’ve been applying to mid-level positions but it feels like my tech stack isn’t enough anymore. Lots os positions include Python, R, database management, etc. I feel like I need to expand my stack but I’m a bit lost as to what I should focus on. 

In your experience, what are some areas which have good demand for  mid-level professionals?",33,29,1737055375.0,analytics
1i2wbgh,Google Data Analytics worth it?,"Hi, is the above really worth it? I'm currently studying L4 Data Analytics via work but the material is much better I think on Coursera (trialling the 7 day free version). 

Is the cert still worth it? YouTube tells me one thing but I wanted thoughts from real people in the field.

Thanks ",34,50,1737053794.0,analytics
1i356je,Desperate for Advice ,"Started uni in 2021 as a Finance major --> 

**1st year**: Didn't do much.

**2nd year**: Joined 2 finance clubs my 2nd year thinking I was going to pursue Investment Banking. Realized that a 90+ hour work week was not for me, so I decided not to pursue that path.

**3rd year**: Took a data analytics class and intro comp sci class during the Fall semester. Turns out I enjoyed technical work more than making stock pitches so decided to add a Business Analytics major and a Comp Sci minor. Took 3 business analytics classes and 1 cs class during the Winter semester and realized most BA courses were poorly structured and often unavailable due to lack of professors. Figured taking Comp Sci classes would be a better investment of my time and money.

**4th year**: Took 3 cs classes last semester (F2024) and currently taking 2 this semester. 

Currently have a 3.75 GPA and will be graduating next year in may (W2026)

Have used Python (Pandas, NumPy, Scikit-learn), SQL, PowerBI, R, Java, Excel, PowerPoint for class assignments/projects (nothing too complicated).

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Due to family constraints, I was not able to work during my time at uni and, as a result, have no internship experience. Things have changed this year though so I am now able to pursue internships. Since I am at the end of my 4th year, I have F2025, W2026, and possibly S2026 to gain internship experience.

So I guess I'm looking for advice on how to move forward towards getting an internship within data analytics. I know a solid portfolio is super important, so that should be 1st step, but I don't know what projects to do and where to even begin. Ideally I'd want to leverage my finance background to land a data analyst/business intelligence role within the finance sector so any ideas for projects would be very appreciated. Also what skills should I learn/refine, what books/resources should I be reading/using to put myself in a better position to land an internship. Honestly just looking for general tips on the steps I need to take atm. 

Thank you :)

",7,8,1737077814.0,analytics
1i2tt60,Do personal projects help in getting a data analyst role?,"I’m currently working as a data analyst which only requires some basic Excel and Power BI. I want to break into a role that works with SQL and Python as well (i’ve been self-learning about them). I’ve only been invited to job interviews of the roles that only require Excel & Power BI, which are what i’ve been doing in my current role. ",25,38,1737047470.0,analytics
1i3bqsj,Any websites where I can download books for analytics ,My uni requires me to buy books its kinda expensive is there any place or source to get these for free ,1,6,1737101800.0,analytics
1i380hi,"Degree in English with no previous data anlytics experience, starting learning statsitics and planning to move to tools, any recommendations for building up a resume?",entry job,3,12,1737086841.0,analytics
1i2xvyt,Chances of getting a job with a cs degree and projects ,I live in Orlando and am open to in office (but it’s not exactly a tech hub so remote would be preferable). Moving is not really an option due to marriage/kids/house. I’m 2 classes away from graduating and want to know if I should even bother or just change careers with how depressing the CS and all related career forums have been. Am I cooked? Does the CS degree hold any weight? I thought this was an entry level field but others say no so then what is? I think my personal goal is at most a year of job searching. Is this realistic in this job market? ,6,19,1737057763.0,analytics
1i2z986,PlumbingJobs.com ~ Sharing the analytics of my job board site (summary of how it's going after the third month),"On October 12th 2024, I launched PlumbingJobs.com, and this is my first update (January 2025) in what I hope will be a long journey.

To stay accountable and track progress, I’ll be sharing monthly updates about the site's stats, achievements, challenges, and my plans moving forward. While these posts are mostly to document the journey, I hope they’ll also be helpful to others, especially members of r/analytics who might be interested in learning the web analytics of a job board website.

If this post isn’t a good fit for this subreddit, I’m happy to remove it or move updates elsewhere.

The goal for PlumbingJobs.com is clear: to become the #1 job board for plumber jobs, featuring hand-picked opportunities the plumbing industry.

Let’s dive right in:

# Statistics update ~ 4th Quarter of 2024

|\-|October|November|December|
|:-|:-|:-|:-|
|Jobs Posted:|2|16|43|
|Paid Post:|0|2|2|
|Free Post:|0|1|2|
|Visitors:|72|138|1,164|
|Avg. Time Per Visit:|1 min. 24 sec|2 min. 15 sec|3 min. 41 sec|
|Pageviews:|196|308|2,590|
|Avg. Actions:|1.1|2.3|2.3|
|Bounce Rate:|87%|73%|40%|

I'm not a very technical guy and I don't know how to code. So the best way for me was learning to build it using Wordpress through YouTube. Also, I believe in the power of a great domain name, and the stats from the first three months have only reinforced that belief:

* **49.2% of traffic** comes directly from users typing the URL into their browsers.
* **48% of traffic** is from search engines like Google and Bing.
* The remaining **1.8%** comes from social media and other backlinks.

# Tech-Stack

Wordpress - Website + CMS

Gravity Forms - Form + PayPal payments integration

GeneratePress - WP Them

WP Grid Builder - For Grids & Cards

Clicky - Traffic Analytics

ChatGPT - Rewriting the job ad

Make - automating the flow

# Pricing Tiers and Early Wins

I offer three pricing tiers for job listings:

* **Free Listing**: Basic exposure for job openings.
* **Silver Listing** ($45): Greater visibility and placement on the site.
* **Gold Listing** ($95): Premium visibility and enhanced promotion.

To my surprise, my very first sale in October was a **Gold Listing**! That initial $95 sale was the motivation I needed to keep building. Later that month, I sold a **Silver Listing**, bringing my total revenue for October to $140. The same revenue was generated in December 2024, showing consistent early interest.

# Steps Taken in December

To boost SEO and add value to the site, I created a Plumbing Directory, featuring:

* Plumbing companies across the U.S.
* Their stories, contact information, logos, addresses, business hours, and more.

This directory serves as free marketing for these businesses and increases the likelihood they’ll discover my site and support it by posting job openings.

# Plans Moving Forward

1. **Social Media Marketing**: I plan to automate posts using AI to expand reach and drive more traffic to the site.
2. **Consistency in Job Postings**: I’m committed to posting 2–3 plumbing jobs daily to keep the site fresh and useful for plumbers seeking work.

Looking forward to grow this niche job board slowly but surely this 2025. If you have any questions, concerns, come across glitches - feel free to reach out, happy to chat.

Thank you all again, and see you in a month.  
[Romel@plumbingjobs.com](mailto:Romel@plumbingjobs.com)",4,2,1737061300.0,analytics
1i2vuna,Legit Contractor Companies?,"I get contacted by recruiters all the time and many of them seem like a scam. They barely speak English or they send a poorly worded email that goes right to spam. I was considering doing contract work, but it is difficult to determine which companies are legit. Does anyone have some suggestions for Companies that hire analytics, data science, and data architects that are legitimate and actually have contracts? ",5,3,1737052595.0,analytics
1i2y2j0,Aws vs Google data certificates ,Most posts I see are saying that the Google cert is pretty worthless. What about an AWS cert for an entry level candidate in business or data analytics? Maybe the data engineer cert or solutions architect? Or are the days of certs behind us in this job market? ,2,3,1737058223.0,analytics
1i32xb0,Perplexity with complex data aggregation,"I’m working on a project that uses AI to calculate complex statistics that don’t exist anywhere online—percentages and insights that typically take hours to aggregate manually. For example:

* **“What percentage of unicorn startups from 2024 had at least one remote cofounder?”**
* **“What percentage of developed countries with republican leaders have governments that cover 100% of healthcare costs for their citizens?”**

The tool generates these stats automatically, combining information from various sources using a controllable reasoning process.

My question to you is:

* Would this be useful for your daily work? Are there situations where you've been frustrated by perplexity failing to get a statistic you were looking for?
* Would you pay a premium for access to this kind of data, and why?

I’d love to hear your thoughts and ideas! Feel free to ask questions if anything sounds unclear.",0,4,1737071205.0,analytics
1i26ypb,Should I learn Python or SQL as a complete beginner to become Data Analyst?,"Basically the title, some are suggesting to begin with Python and some say SQL.

Can I/Should I learn both simultaneously?

P.S. I do not have any coding experience.",101,87,1736972998.0,analytics
1i2tkkl,Rotman MMA vs McGill MMA,"Hi,

  
So I've recently been given an offer for both McGill MMA and Rotman MMA programs. I was wondering what the pros and cons are for both and if anyone has any tips on which program I should choose to complete my graduate studies.

",2,3,1737046864.0,analytics
1i2vw45,Best place to find an Oracle Analytics consultant,"The businesses main application runs on an Oracle database.  We have Oracle Analytics v5.9.  I'm looking for an expert that could build us an executive dashboard in OA to show previous day, current day, YOY, etc and build us some automated reports.  

Where is the place to find a consultant for this?",1,4,1737052703.0,analytics
1i2p8lx,Generalization of Newton's method,"Hello, all,

This may be a stupid or naive question but here goes: I know the univariate version of Newton's method from having studied numerical analysis in grad school.  I am currently taking the Andrew Ng machine learning course and am about to learn the gradient steepest descent method and its application in ML.  (Learned about gradients and their properties in Calc 3 in college.)  Can someone explain to me why you would use the steepest descent method vs. the generalized multivariate Newton's method in optimization problems?  What am I missing?

Thanks,

K.S.",2,4,1737035145.0,analytics
1i2kpg6,Where can I find data or market research on the games Silver Spenders play?,"Hello everyone,

I’m looking for information or insights about the ""Silver Spenders,"" a term used to describe people aged 50 and above.

What types of games are they playing, and where can I find or conduct market research on this demographic?

Thanks a lot!",1,2,1737017320.0,analytics
1i2nnud,had a technical interview 2 days ago and having a panic attack because I haven't heard back,"I don't know why I'm having a panic attack because I think did really fucking bad in the interview, I got so nervous that I had to look up the syntax for the group by function in pandas, so why would I expect anything besides a rejection anyway

they started by asking me some theory stuff (discuss the differences between sets, lists, dicts, what's a tuple, etc) which I did really well on because of my math background. that sort of stuff is my strongest area, I can remember theory much more easily than I can remember precise syntax. then we did some pandas shit and I completely froze up for a second, had to google group by and something else, but I told them that I was like really panicking in the moment and freezing up. I was able to do some of the other stuff they asked for, transform a column and turn it into a new column, I optimized the work with a lambda function. I don't fucking know. then some more theory stuff, what's an array in numpy? which I sort of answered, it's a multidimensional vector or tensor, I also said I was pretty sure every element had to be of the same type, but I wasn't able to speak to the more technical components since I don't directly work with numpy often 

then there was a sql question, I did ok on the first question though it took a bit of prompting, second question I didn't understand it was something about primary keys and regular keys and I was like yeah I completely forgot what a regular key is, then the third question was to write a query which was easy

I told them at the end I don't think I did well. one of the interviewers said I did better than I think and the other said I was in ""the top percentile,"" I really don't know what the hell that's supposed to mean in context

now it's been two days and I haven't heard anything, I'm so fucking over this I;ve been looking for eight + months for a job and ive done so many interviews and nobody will fucking hire me and id on't know what to do because I can't get EXPERIENCE if nobody fucking HIRES ME",0,25,1737029954.0,analytics
1i286j9,How to drive business outcomes with data and AI products (price optimization),"We must not forget that our job is to create value with our data initiatives. So, here is an example of how to drive business outcome.

CASE STUDY: Machine learning for price optimization in grocery retail (perishable and non-perishable products).

BUSINESS SCENARIO: A grocery retailer that sells both perishable and non-perishable products experiences inventory waste and loss of revenue. The retailer lacks dynamic pricing model that adjusts to real-time inventory and market conditions.

Consequently, they experience the following.

1) Perishable items often expire unsold leading to waste.

2) Non-perishable items are often over-discounted. This reduces profit margins unnecessarily.

METHOD: Historical data was collected for perishable and non-perishable items depicting shelf life, competitor pricing trends, seasonal demand variations, weather, holidays, including customer purchasing behavior (frequency, preferences and price sensitivity etc.).

Data was cleaned to remove inconsistencies, and machine learning models were deployed owning to their ability to handle large datasets. Linear regression or gradient boosting algorithm was employed to predict demand elasticity for each item. This is to identify how sensitive demand is to price changes across both categories. The models were trained, evaluated and validated to ensure accuracy.

INFERENCE: For perishable items, the model generated real-time pricing adjustments based on remaining shelf life to increase discounts as expiry dates approach to boost sales and minimize waste.

For non-perishable items, the model optimized prices based on competitor trends and historical sales data. For instance, prices were adjusted during peak demand periods (e.g. holidays) to maximize profitability.

For cross-category optimization, Apriori algorithm was able to identify complementary products (e.g. milk and cereal) for discount opportunities and bundles to increase basket size to optimize margins across both categories. These models were continuously fed new data and insights to improve its accuracy.

CONCLUSION: Companies in the grocery retail industry can reduce waste from perishables through dynamic discounts. Also, they can improve profit margins on non-perishables through targeted price adjustments. With this, grocery retailers can remain competitive while maximizing profitability and sustainability.

DM me to join the 1% of club of business savvy data professionals who are becoming leaders in the data space. I will send you to a learning resource that will turn you into a strategic business partner.

Wishing you Goodluck in your career.",6,6,1736976163.0,analytics
1i27wsy,Which database certifications should I get for working with databases?,"I am really doubtful since there are a lot and I see the database administator role in many of them. Could you please give me a hand out?

",5,5,1736975470.0,analytics
1i25z4m,Looking for Yearly Sales Dataset for Myntra,"I’m looking for a dataset that contains yearly sales figures for Myntra, preferably covering the past few years. 

I’m working on a project that analyzes trends in the Indian e-commerce sector, and this data would be incredibly helpful. If anyone has access to such datasets or knows where I can find them (free sources preferred), I’d really appreciate your help.

",2,2,1736970414.0,analytics
1i2aa2t,JD skills,"I came across this, “Build logistics plan and own logistical coordination for high complexity learning experiences including physical and digital requirements. Manage administrative data and processes for high complexity programs, including LMS entry, learner communications, enrollments, and assignments. Update master training calendar as needed based on program scheduling Monitor performance and effectiveness and perform quality assurance testing on high complexity in-person and digital content, identify and prioritize areas of improvement; design curriculum solutions to improve learner impact Facilitate various topics for high complex in-person and virtual content delivered to leadership and teammate audiences Other duties as assigned”

What do I need to learn to satisfactorily perform these tasks? ",0,3,1736981694.0,analytics
1i196lv,Frustrated as a Data Analyst: Are we just storytellers?,"I’ve worked in five different roles in the data field, and across most companies, I’ve noticed a common trend: data analysts are primarily tasked with producing dashboards or generating figures based on very specific business requests. However, when it comes to tackling broader, more open-ended questions, things seem to get more challenging—especially in companies where Python isn’t part of the toolkit.

In my current company, for example, we’re expected to find new insights regularly, but everything is done using SQL and Tableau. While these tools are fine for certain tasks, doing deeper data exploration with them can feel tedious and limiting. We’re also not encouraged to use statistical knowledge at all, since no one on the team, including our boss, has a statistical background. It feels like there’s no understanding or value placed on applying more advanced techniques. We just need to have exceptional data storytelling skills + put up some nice figures which confirm already known intuitions.

Honestly, I’m feeling a bit frustrated. I can’t help but wonder if this is common across the field or if it’s just the nature of certain industries or companies. Would things be different in a more tech-focused company or in a dedicated data science role?

What’s your experience with this? Is this a frequent issue in your work as well, or does it vary depending on the company or team? I’d love to hear your thoughts.",178,97,1736870187.0,analytics
1i1lu49,Where is the DS career headed?,"Just saw the Rogan / Zuck podcast on how AI is changing most tech careers. I’m just now transitioning in a DS career, getting well versed with the ML algorithms and Gen AI concepts. For the more experienced folks in the field, how is the DS career specifically going to change in the coming years? How can we try to stay on top of all the changes coming in? 

PS: This might be more of a question for the r/datascience sub, but unable to post question there. ",21,38,1736903448.0,analytics
1i1klda,YouTube channels for background noise?,"So for IT it's easy to throw on any tech youtubers video for ambient noise relevant to the field and occasionally pick up some useful information. I understand it's easier to make content for IT, but  I'm wondering if there's anything similar for analytics that isn't just a python tutorial or a how to on landing your first job.

Thanks for any suggestions.

Also, if there's a better place to post this I'd be glad to move it there",7,6,1736899832.0,analytics
1i1vjy0,Title: Should I Take an EPR Support internship While Aspiring to Be a Data Analyst?,"Hi everyone, I'm a bsc Computer Science graduate  in 2024. I want to become a Data Analyst. Despite applying to many roles, I haven't landed an opportunity in the IT field yet. I've received a 3-month offer for an EPR support internship and the company is related with marine industry. Should I take it or focus solely on upskilling and searching for data analytics roles?

They are giving me stipend of 10k per month for this internship.
",0,9,1736941190.0,analytics
1i1j7vr,Predictive Analytics Cert?,I'm curious if I should get a certificate in Predictive Analytics. No one on my team or in my organization currently offers reporting like this and I would like to start. I manage a small team of analysts specializing in financial and operational reporting and analytics. We do most of our analytics in Tableau & excel but I'm trying to think ahead and there are plenty of use cases for predictive analytics. Any suggestions on who to get certified through? Has it been useful/successful at your organization? Thanks in advance!,6,6,1736895947.0,analytics
1i1s73x,What is the best practice for number of events added to an ecommerce website?,,1,3,1736925795.0,analytics
1i1njjf,How can I create a function using values from two different data sources in Looker Studio?,"In my report, Data Source A is giving me the fields A, B and C. 

Data Source B is giving me the fields D, E and F. 

There's a formula behind each of these fields.

I want to create an additional field which would be pretty much (A-B) / D, but that is not possible because they come from two different data sources. 

If I select them and try to choose ""Blend data"", the option is greyed out saying ""You can't blend with an already blended chart"". So I'm currently lost if there's anyway to display this information to my client without manually calculating this.

Alternatively, is there any any to just use the fields as values, instead of replicating the massive formula that's behind each one of them?",2,3,1736908679.0,analytics
1i1efq3,New grad jobs,"Is January a bad time to look for jobs? Recently graduated in December but the issue I’m having is that there’s not that many jobs to begin with. LinkedIn is only showing about 20 - 30 jobs. Most of them are for senior roles too.

I’m not sure if I’m competitive enough for this job market tbh. I only have 1 internship utilizing sql, excel, and some data visualizations. The rest of my resume is some other unrelated job and a couple of projects on tableau public. ",7,11,1736883447.0,analytics
1i18boz,How do people progress from an Academic environment to real world? ,"I recently graduated from an MS in Business Analytics program and had classes in Data Analytics, Stats, Machine Learning, R and Python. The courses covered things but some things were pretty basic. Like we covered SQL but we did not do queries involving multiple joins or CTEs or complex stuff. Rather simple individual queries on a chosen dataset, things like that. It feels like we did learn but did not go too far or deep like people do in industry or real jobs. We did not work with things like Qlik or do ETL. For Excel/Sheets, we had no class and just did some basics, while I have seen some jobs require proficiency. All in all, I feel like classes and class projects might not be enough. Or is this enough to get started? Because I have seen data roles are individual contributor roles where you are kind of on your own. How can an entry level person manage this straight out of college? Is it possible? What did people with experience do or what did your journey look like? ",15,23,1736867902.0,analytics
1i19n03,Can I use Leadsnavi as a lightweight alternative to GA for web analytics?,"I have both GA and Leadsnavi on one of my client’s websites. We are using GA for analytics and Leadsnavi for identity resolution and lead generation. The web pages have gotten a little slow and I’m considering switching to a much lightweight analytics tool. I have tried MS clarity but there is not much difference there either.

Leadsnavi has analytics too but I’ve never used it for that, we just use it for identity resolution. I’m considering doing away with both GA and MS clarity and let Leadsnavi handle the analytics too.

Will it be enough or do I need to continue looking for alternative analytics tools?

Note: It was the client’s idea to add Leandsnavi for identity resolution and lead generation, my role is to set up the infrastructure, he uses the tools himself, that’s why I want to know if Leasnavi is good for analytics from a business point of view.

",11,2,1736871377.0,analytics
1i1osow,New to SEO: I need some help with deciphering my GSC graph!,"Does anyone have any experience with your page showing a sudden decrease in clicks and impressions? I optimised the blog on 26th December, which showed an increase for 1-2 days, but then hit a 0 on the third/fourth day. Position wise is one now.",1,1,1736912731.0,analytics
1i1akf5,Need help deciding which route to take for transition into DA,"Hi everyone ! 

I bet this is a pretty much always asked question and sorry for asking it again but i would like some answers specific to my situation. 

First lemme say i live in France for some context, so things are a bit different here. 

I have 2 masters in engineering, one in Material Science and the other in Space Systems, from 2 highly recognized schools (+ i did my final year at Imperial College in the UK). 

I have worked 2 years as an R&D engineer in microelectronics, doing 40% of theorical physics and the other basically doing the job of a data analyst. The firm i was in had no data person whatsoever so i kinda became it and built a whole application in VBA to extract, transform, load, analyse and dashboard data coming from our devices tests. Did some python and Power BI dashboarding while i was there. 

 I am saying all this because i keep reading posts where ppl say that a degree is the most imporrtant thing in the field and a bootcamp in case you have the diploma will help but not as much. 

So i have a degree, in a related field, but we kinda did everything you do as a DA (or even DS). A lot of proba, stats, machine learning, math, python and such...

I quit my job a few months ago now and i'm lost between doing a bootcamp (and pay 5k+ for it) to learn more DA skills and have the certification or going the self taught route and build a learning path to be as close as the bootcamp's one, using DataCamp or Maven analytics resources.

On the one hand, self-teaching would save me a lot of money, and there’s a ton of free or affordable resources out there. On the other hand, bootcamps offer access to career coaching and industry networks, which could be invaluable for landing a job. A structured curriculum might also keep me on track and ensure I don’t miss any key concepts, plus they often provide real-world projects that would help me build a portfolio.

So i woul really need your advice here and what you think would be the best choice considering my background and situation. 

  
TL;DR: I’m an engineer with two master’s degrees and two years of data analysis related experience trying to decide between an expensive data science bootcamp and self-teaching. Looking for advice on which route might be better for breaking into data analytics

Thanks a lot ! ",2,8,1736873736.0,analytics
1i19sg5,Drone Data Analysis Projects,I recently got a DJI Tello drone and I am very passionate about drones. Would analyzing battery performance over time or doing flight data analysis be interesting projects? I was thinking I could use the SDK to get data from the drone and put it in the database. Then from there use SQL and maybe Looker Studio to manipulate the data and create a dashboard or some visualizations. Could these be interesting passions projects? Any recommendations or has anyone done something similar? ,2,2,1736871766.0,analytics
1i0l0i5,Projects that got you A job,"If you don’t mind sharing, what project got you an entry level job? 

Background: I want to transition from teaching. I have a degree in math and computer science. I have completed Google Data Analytics on coursera. I currently have 2 personal projects completed. One is analyzing my finances using python to automate things. The other is analyzing student tests performance with excel. 

I want my 3rd project to be more business facing and impressive. Ive looked on Kaggle for data sets but the data seems basic. Like i can find average, increasing or decreasing trends, max and min but if i was a hiring manager i would not be that impressed. 


Tldr: 
I finished learning the basics and have 2 simple projects. I want to work on a project that would impress people but i am having a hard time finding interesting data sets. What project impressed your hiring manager enough to get you your first job? 

Thanks!",76,33,1736793429.0,analytics
1i0v2kv,How Big is Your Team?,"I’ve worked in analytics for a few years, starting off as a Sales Operations Analyst to now working as a Business Intelligence Analyst for a Fortune 50 company. 

Throughout the duration of my career, I’ve mostly worked on a team where I’m the only analyst and the only one responsible for data related projects and reporting. From the rhetoric I’ve seen on Reddit and having conversations with other analysts, there doesn’t seem to be many fully developed analytical teams within companies. 

Is this true for most businesses? Do most companies generally keep a small analytic team if not solely relying on one person? ",17,24,1736819726.0,analytics
1i15z8z,Need interview preparation sources,"For analyst interviews, can you suggest some good sources where I can practice questions on SQL, Python, Pandas, etc.?",2,9,1736861104.0,analytics
1i0u7q9,Analytics communication and writing style,"I've long struggled with writing style. I'm usually either too verbose or too concise. Rarely find a spot in the middle.

I've found some success with writing anything in a work/analytics doc as TLDRs, separating into bullet points, adding a table, markdown, having a consistent format like 2 sentence insight + chart + chart link, appendix section, using a general template for specific repeated projects. But in Slack or more detailed analysis docs, it's harder. The audience can be a large range, technical and nontechnical, executive and non-exec, sometimes only 3-5 people, sometimes 30-50.

Writing takes me a disproportionate amount of time. I'll spend 20 mins tweaking a Slack message and still edit it a min or two after I send it because I forgot this and that or I edited so much that I left an extra word in.
 
What to do? Any useful analytics writing guides or something that you can recommend?

I wish there was a leetcode for writing. I know I can use ChatGPT for suggestions but this is so niche and I want to be able to do this going forward with more care in the moment.",6,10,1736817199.0,analytics
1i1dxku,Is 74k too low for new grad?,"I got an offer from a company that I've been interning for 2 years. The offer requires me to move to a State that I don't really like. The job is quite boring, but the pro is that I get to work remotely. Everyone at the company is quite chill and nice. The job is not too stressful and the company really values wlb. They also offer tuition reimbursement

The only thing I didn't feel happy about was the pay and the fact that I have to move to a different state. I don't know why I have to move, if they let me work remotely. I've been applying to other jobs and in the interview process with couple companies. Any advice what I should do moving forward?

I know the job market has been really difficult, so I'm grateful for my offer but I still want to know if there's anything else I can do.",0,38,1736882181.0,analytics
1i0oyno,Can I work in analytics with Master’s in Computer Science and Bachelor’s in Business? ,"Hey yall. I’m a 23 year-old with bachelor’s degree in Business Management. I recently got accepted to masters in CS in the UK( I actually applied to Data but they rejected it and offered me CS instead). I am quite interested in working in IT business analytics or data analytics. The thing is I don’t know if I can start working in those fields if I get masters degree in CS. I’m scared that it can cause me problems because it is completely different from my bachelor’s major. Can anyone give me advice? If I study CS what are my career prospects in business related field? Will it affect my career negatively?
Is it better to wait for the next year and apply to Data analytics at other universities? ",6,19,1736803168.0,analytics
1i0jsim,MIS/CIS or Data Science Degree,Hello everyone! I am currently finishing up my general studies in a community college and plan on transferring soon but not sure what to go for. I was planning on going for a data science major but started learning more about MIS/CIS degrees. I have to say I really like the versatility option of that but I most likely would still like to look for a job in the data science field when done. Would it be a waste to go for MIS/CIS degree? Is it a wiser choice since It would give me more options when i'm done? Another thing is I'm not sure if the data science program is just a cash grab from the school since it is fairly new. Anything helps!,3,17,1736790483.0,analytics
1i0mmj3,I want to enroll in Analytics,"Hello! I have an undergrad in clinical psych in India and want to enroll in georgia tech masters in analytics, online or offline both should be fine, what should I do to be proficient for this course and what requirements will I need to fulfill and would I be eligible? Would someone like me get in? I have decent knowledge of statistics and have dabbled into SPSS and a little bit into R. And what prospects would I be looking at after the completion of the course? Any advice would be very much appreciated. ",2,3,1736797392.0,analytics
1i0l1yo,Need Resource for Speeding Up Power BI Data Refresh from SharePoint Excel Files,"Hello,

I'm a Jr. Analyst working with Power BI and SharePoint, and my manager tasked me with fixing slow data refresh times for one of our reports. Currently, we're connecting to Excel files stored in a SharePoint folder using a basic web connection, which seems to be the bottleneck.

My manager requires that the refreshes be done in Power BI Desktop, and the reports to then be uploaded to the Power BI Service. Additionally, we rely on Power Automate flows to handle some parts of our workflow. These flows open up websites, download data, and upload it to SharePoint. However, this setup requires our laptops to remain running for the process to work, which isn't ideal for a nightly refresh scenario.

My manager suggested exploring the SharePoint API as a potential solution to improve refresh performance. They don't mind how it's done, as long as the data refresh speeds up significantly.

I've already looked at a few tutorials and articles, but I'm hoping someone with experience can point me to the best resources (videos, articles, or guides) to optimize this process. I'd appreciate help in cutting through bad advice and finding the most effective solution.

Thanks!",1,2,1736793531.0,analytics
1i0ku21,Few questions ,"1.Is there any data analytics course or tutorial that is worth learning and that is free ?
2. Google data analysis course- how long does it take to finish the course- any experience?",1,8,1736793002.0,analytics
1i0jxzk,Transitioning from Accounting Specialist ,"Hello there,
I am transitioning from Accounting Specialist to Data Analyst. I have more than 8 years of experience in accounting and Bookkeeping. I am freelancer and have Top Rated Plus badge on Upwork. My hourly rate on Upwork is $8-10. I just got IBM Data Science Professional Certified from Coursera.
My question is:
-To be more competitive, what should I learn more and from where I should learn it? (I got Coursera certification through financial aid. So I have budget constraint.)
-What should I expect in my earning when I enter the market as financial data analyst?",0,2,1736790859.0,analytics
1i0059n,Entry salary expectations?,"I know there’s been a few post regarding the same topic but everyone’s qualifications are different. I am entering my last semester and am graduating with a bachelor’s degree in business analytics. I’ve done 2 summer internships (about 7-8 months total) during my time in college. Some qualifications I have are mastering excel (who hasn’t at this point), good experience with power BI, JavaScript, python, tableau, and sql. So with that being said what are realistic salary expectations I can have for entry level jobs giving my qualifications?",20,54,1736724334.0,analytics
1i0euyy,Quick question for everyone,How do you make sure your content stands out in a crowded niche?,0,11,1736777638.0,analytics
1hzx2fe,Any good resources for case study? ,"Any thing that helps regarding a case study or a case interview - be it tutorials, example questions, tips, websites, YouTube channels etc. 

Thank you! ",3,1,1736716218.0,analytics
1hzxb4s,Considering business analytics post grad - Biz Econ Major,"I am a recent UCI grad with a Business Economics degree. I recently got laid off at my job as a business development associate. I am looking to move to a more analytical role and am trying to learn about data and business analytics. I have many questions on the topic and would appreciate any advice. 

1. Is a BA in Business Econ enough to start applying to business analyst roles  
2. I am currently taking a Google certificate for data analytics, but am finding that it's not teaching me the tools I need like SQL and Tableau. What recommendations are there for these certificates.   
3. I am considering applying to a master's in business analytics, but as the applications are due relatively soon I am unsure if this is a smart decision to rush through these right now. I would also like to consider a flex MBA/ MSBA as I would like to be working full-time   
4. As I am currently looking for new jobs, what is the best way to prioritize my time, and the best roles I could apply to? (can provide more info if necessary based on previous experience in more management roles)  
5. I am also considering project management. However, I do like the idea of being able to guide business decisions for a company (specifically the games industry). What does the day-to-day as a business analyst look like? I do prefer the idea of traveling to conferences and meeting with other people, but I hate traditional sales. I love the idea of working on a project, but don't want to be sitting without interaction all day, and also find coding to not be my favorite thing in the world. I enjoy a PM position but I feel I would not be as impactful in the overall decision-making for a company.",1,3,1736716832.0,analytics
1hz9kpi,Just landed an internship interview at BMW! Any advice?,"Its in 2 days and I really want this internship, can you experts give me any advice?

Edit: its online btw ",47,23,1736639744.0,analytics
1hzb2mu,how to transition into healthcare analytics,"I graduated from Michigan this past May and started working at the Mayo Clinic as a clinical scientist. I seek to exit the lab and work more in Healthcare Analytics or anything adjacent to this position. Given my current position, can I get some pointers on how to be a good candidate? 

  
I am seeking to be proficient in Excel, SQL, among other things. Thanks!",7,8,1736644074.0,analytics
1hz53m9,"Is ssms, SSRS and powerbi out of demand? ","I'm in USA and have been working with the above 3 for 8 years in healthcare. I'm looking to make a move as the product will be sunsetting soon unfortunately in a year. I'm fully remote for 4 years and have been applying for the past 1 year. Haven't even received an interview with the techical person, which is very disappointing.

Should I pivot for remote opportunities to data engineering or maybe learn Tableau? 

 ",7,27,1736627510.0,analytics
1hyhkdj,Is College Still Worth It?,"Hello,

I am a Sophomore in College and was just wondering which majors are useful in the current market. I am currently a Data Science Major, and I like it for the most part, but the tech job market is super competitive right now. I want to eventually get a job in analytics or something in big data, however, I've heard so many horror stories that I'm worried about going on about college and not being able to make it out with a job. Please let me know.

Thank you.",40,57,1736549995.0,analytics
1hyy26u,Excluding internal traffic,"I setup internal traffic filtering by the following method.

Data collection & modification > Data Streams  > Configuration > Define internal traffic

Using the IP address is in range (CIDR notation): 5.565.3.226/337 (not a real IP)

I have confirmed with the IT department the IP and hence know it is correct.

I then activated the Data Filter.

However, I am still seeing hits from the very small town the business is based. Hence I am pretty sure those are internal hits.

Is there a ""better"", more reliable way of filtering out internal traffic? ",0,2,1736608759.0,analytics
1hz20tc,Are you looking to crack into data domain.,"Hello everyone are you looking for data domain career. Lot of people are struggling and have no proper guidance and resources.Almost from 4 years in data domain and i can say that with right skills set you will definitely crack the data domain whether you are looking for data analyst, business analyst, financial analyst, product analyst. Some of the skills are common in this but yes enough to crack. Feel free to guide you and help you if you want any guidance.",0,3,1736619342.0,analytics
1hyv7yo,Zuck,Did you guys hear that in meta ai will replace mid-level engineers sometime this year? How do you guys think it will impact analytics?,0,6,1736599850.0,analytics
1hyc3oc,"Is it true that the field of analytics is over saturated? If so, what are other options or roles for one’s interested in analytics?","
In one of my previous posts someone commented that analytics is over-saturated. If that’s the case, what are other roles someone who is interested in analytics can look into ? I’m an MIS major at my undergrad college and my coding skills or skills necessary for analytics are below the bar for a tech/analytics role and I was wondering if analytics is actually over-saturated what are other roles I can look into ?",2,51,1736535947.0,analytics
1hy3van,Switching to Marketing Analyst from PPC,"Hi all, I went through previous similar questions and got some answers, but I am starting a new thread due to my somewhat specific situation. I work at a big e-commerce fashion company as a PPC/media activation manager. I have solid experience in digital marketing, having worked in media agencies before this job.

I need the change and don't see a future in PPC and media, to be honest, I would like to switch to Marketing analyst. I don't have any formal education from that field in maths and tech, my degree is in marketing and public relations. However, I always loved numbers and stats and I believe I can interpret data to real business conclusions and, let's say, real-life use.

My main question is - Do you think that's the right direction and more importantly how I should do it? The reason I think this is very doable is that my current company encourages career change and I could proactively reach to the Analytics team to have me in mind for that role when they need it (ofc when I am skilled enough) What do you think would be the best way education-wise (courses, specific tools, maybe go back to some math first) and how I could leverage a lot of marketing and business data I have access to to make some projects for myself and gain experience? Thank you!

",6,7,1736514039.0,analytics
1hxyy3j,Required work logging/tracking,"Role: data analyst (sql reporting, projects, validation, research, etc)

Recently required to start logging everything to the half hour. Example of one day below:

Meetings: 2.5 hours 
Project A: 3 hours 
Emails/issues: 1 hour 
Project B: 1.5 hours 


Does this seem a bit excessive? I get the need for resource planning but I haven’t heard of other teams doing this outside of an agile environment. Looking for ways to make this more efficient if I’m doing it everyday.

Any thoughts, experiences, or advice welcome.",9,13,1736493236.0,analytics
1hy4jbu,Google Apprenticeship 2025 - Status Discussion Sub,,0,3,1736516176.0,analytics
1hxktf2,Freelance/Practice Data Projects,"Between jobs now and trying to keep my data skills sharp. I have tried working on a few sports and gaming data projects, but wondering if there is anything out there that has real life examples with prompts that can help me keep improving and keep my skill level up. Trying especially to stay up on excel and SQL as well as learn about PowerBI. Thanks for any help. ",9,2,1736450564.0,analytics
1hxd98p,Is it possible to transition to this career?,"I graduated with a degree in Computer Science back in 2023. I have not found a job related to my degree. My internship was only a position as a QA Analyst which mostly involved testing software.

The problem is I'm not really passionate about CS. I have tried working on side projects but quickly lose interest/motivation in completing them. I have not really tried to find a job in CS hence why I have not held a position related to it since graduating. The job market for CS new grads is also really difficult where I live right now (not saying data analyst is any easier, I don't know).

Data Analyst has been something I've been interested in and I'm not sure how I can get my foot out the door. What should I do before applying for entry level positions to increase my chances? How long of a commitment do I need before I have decent chances at landing an entry level position?

I know the obvious answer is to go back to school and get a degree for it, but that isn't something I can do.",24,29,1736430803.0,analytics
1hwntvr,Resources to Learn APIs,"Hello Everyone,
I’ve been working as a data analyst for a little over a year now and have never needed to know how to use APIs until now. Does anyone have experience learning how? Any recommendations?",59,28,1736352858.0,analytics
1hwydwt,is it overkill to be asked to provide 5 different references 3 mgrs 2 coworkers for a senior/mgr role at a non profit?,No judgements or assumptions pls kindly stick to answering the question I just want to know if this is common. Thank you!,9,25,1736378948.0,analytics
1hx1nqx,Live Gaussian Filter Line (Zero Lag/Loss Average) for Time Series Data,"I have spent the past few months developing a formula (using python and linear regression models) for my time series data to generate a ""live"" Gaussian filter.  This way I can apply it to incoming data and have a smooth, zero lag, average for readability / further analysis. The best I have been able to accomplish so far is 94% correlation between my line and the original gaussian line...

I am looking for at least 96-98% for it to be useful in my case. There is still information to be extracted from the features I have derived for this calculation, since they are between 20-30% correlated with that last 6% error, but I am absolutely stumped and tired... 

Does anyone know where I can hire someone, who to hire, or where I could put out a prize, to come up with some kind of equation/function that is correlated with the error?",3,4,1736388019.0,analytics
1hwtr8j,Analytics Communities - Thoughts?,"When you look for subreddits, LinkedIn Groups, Slack channels, etc. that you engage with outside of work/on your free time at work - what are you looking for?

Hoping to help members, mods & just really the whole analyst community grow. Lots of appetite to help each other and build relevance in our beloved profession, I thought it would be interesting to see what people have to say on this.

Some options to get the conversation started:

* Career Advice
* Educational Resources (certs, upskilling, etc.)
* Networking
* Fun/Memes
* Vetting About Boss/Executive Stupid Requests
* Showcasing Work/Portfolio Building",3,16,1736367379.0,analytics
1hx1dl5,"Is this survey biased, or am I overthinking?","I'm reviewing survey data for one of our products. We have two versions: a digital version, and a printed version. Another team ran a survey because the print version is very, very expensive to publish, and we want to know if it's worth it. They gave me the results yesterday, and it seems there's a preference for print (82% want the print one to continue, 81% prefer print) -- until I asked **who** they surveyed. **They only emailed print users**!! Mind you, these people also receive the digital version (our lists double up), but isn't that iffy? Like, asking the people who use the thing if they want that thing to continue?? 

I think this is valuable data, but it needs more context (like offering a split survey wih one path for people who only use digital, and one for people who get both). Tell me why I'm wrong. ",1,0,1736387225.0,analytics
1hwefma,I’m a statistics student looking for a internship ,"What would you guys suggest I do to be ready or even a worthy candidate of finding a data science internship?

I am a junior at my college and I am 31 years old. 

Any suggestions would help. ",9,17,1736321480.0,analytics
1hwkhx6,Pima Native American diabetes dataset,"I have a question regarding this dataset because I have seen logistic regression models created from it with varying degrees of success.  Specifically, there are two fields that I think may be collinear but I am not sure.  One is [body] weight, and the other is BMI, which is a function of body weight and height.  I think it would make sense to trsnsform the BMI column so that it only contains height, because body weight is already represented in the data.  Thoughts?

Thanks, 

K. S.
",3,3,1736343713.0,analytics
1hw55l5,Thoughts on Thoughtspot?,"I have been using PowerBI and Looker with almost all of my clients. Just got a new client and they use Thoughtspot for analytics...anyone have any experience? Supposedly the big selling point is ""search-driven analytics"". I didn't really see a whole lot of that.",6,6,1736291967.0,analytics
1hw2jfo,"Free SQL, Tableau, Python certs?","Are there any free SQL, Tableau, Python certs I can get to better my resume for a data analytics job? ",5,7,1736285271.0,analytics
1hvbzxl,Got my first Analyst job!,"Just got my first job in analytics! Some background I have been doing a little bit of data analysis for a relative who owns a small business. Nothing crazy, not a lot of data but I used Excel, sql and powerBi to analyze it and to refine my skills. After getting my degree in CS I have been applying to jobs for 4-5 months and it finally happened! 

Keep pushing!
",353,80,1736202888.0,analytics
1hw3pq0,Career Path Advice Needed: Navigating Between Software Engineering and Data Science,"I have been working at my current company for three years. During my first year, I was involved in designing database schemas, building our database, and undertaking a data science project focused on building a customer retention model using data analysis and machine learning algorithms like scikit-learn. 

Eventually, I transitioned into a software engineering team and now work primarily as a software engineer, specializing in Python and SQL. I build applications using AWS Cloud Services such as Lambda, API Gateway, Step Functions, and DynamoDB etc. My role involves various responsibilities, including designing and implementing APIs, writing API and general documentation for our product. Being at a startup, I also lead many project areas and manage timelines for external integrations.

I’ve been a software engineer for 2 years now and have a solid foundation in Python and engineering because of this. However, I still have a strong interest in data science and analytics. I have a Master’s in Data Science as well as an MBA. After working for three years at the company that I’m currently at, I am in a complicated situation because while I have an interest in analytics, data science, and machine learning, I do not have a lot of work experience in these areas. I’m at the point where I need to think about the future and figure out what my next steps will be.

One thing I enjoy about being a software engineer is being able to build new things. Each sprint I get to work on something new and contribute to a part of a feature for our application. One looming problem is the fact it will be tough in the current job market to get a data engineer or ML role with little experience. I am at a crossroads trying to decide if I should continue on my current path in software engineering or dive deeper into data areas and leverage my education.

Any suggestions or experiences you could share that might help me navigate this decision would be much appreciated. ",3,3,1736288244.0,analytics
1hvy6g9,Advice for technical interview for Analyst position,"So, this is the first ever Analyst job I applied to. It's in London (if at all it matters). 

My previous education is not directly related to data analytics as I did masters in Psychological research, where I learnt all the technicalities of research and also spent a lot of time learning SPSS, R etc. So I can do data analysis.

I have had 2 interviews with this firm before - one with recruitment manager and the other with firm partners. In the Partners interview we talked a lot about research and different methodologies, since this job includes research as well (not just data analysis)

Now, I have final interview scheduled this Friday (10th Jan) and I am told that it is with team members and team lead. So I am assuming it will be a lot more technical. Can you all please help me with what kind of questions I should expect and how to effectively answer those? I really want this job, so I am prepping my best for it.

Thank you so much in advance!",3,11,1736274455.0,analytics
1hw74ry,GA4: How to see which pages are triggering a specific event?,"I know this is a very dumb question, but I am fairly new to analytics, so please forgive me. :/

  
I am working for a not for profit organization that has hundreds of pages on their website. Their website has a Donate button on the header which will be present on every single page you visit. Once you click in this button, a pop up appears, and the the URL will be kept the same, but ?form=donate-(countrynamehere) will be added. 

The pop up will collect all the necessary data required to complete the donation, and once you finish it, you get redirected to a page named ""Thank you for your donation!"". I am not sure about the URL for this page.

I can see that all the donations are marked as a purchase event on GA4. I can also see that this page named Thank you for your donation is the page that generated most revenue/purchases, but this is just the thank you page - I'd like to see what the users were actually reading that convinced them to click on the donation button and complete the purchase event?

  
All the help is deeply appreciated. :)",1,5,1736297454.0,analytics
1hvyd5s,Not another question about how to get started as an Analyst…,"Or even about what certifications would
help.

I’m already a working Senior level implementation analyst (Adobe Analytics specifically), but one that doesn’t utilize LinkedIn as someone likely should. 

I revamped it slightly when I was on the job hunt in 2022/2023 but have done zero with it since. My previous role wasn’t exactly focused on Analytics so I’m not too sure where to start. 

Anyone have any tips, suggestions or tips whatever I’d love to hear from you!",1,6,1736274916.0,analytics
1hvs49j,Remote jobs in europe,Is it possible to get a remote DA job without live in Europe. I live in Asia.  Is anyone working in remote? Any tips are highly appreciated. ,1,3,1736258658.0,analytics
1hvcrr6,Company Changing My Job,"Hello all,

I recently got hired as an analyst for a consulting company (I am the only DA at the company). My job description was oriented around data analytics and so far I've been doing that consistently for half a year. Lots of programming, good amount of statistics, and data wrangling. I've loved it thus far.

Now the company is restructuring and they are changing my job title to consultant. They want to standardize roles and job descriptions for everyone. My new description is way more general now and includes a lot of non-data related tasks. To be frank, my role has been dumbed down. I don't want to get into too many specifics, but it's now a lot of project management/report writing/admin assistant tasks which are things that I wanted to avoid when I first got hired. Data is still a part of the job, but it seems less so.

The transition hasn't been made yet, and there will be individual discussions about the restructuring. My question is how should I approach this? Am I being too pessimistic/cynical? Should I start looking for a new job despite being here for a short amount of time? Essentially I am asking what you guys would do? I have about 2 years worth of experience so I am still entry-level I suppose.",5,6,1736204903.0,analytics
1hv22cq,Almost-40 and want to switch careers to analytics,I work in Advertising Operations (campaign management) and desperately want to switch to a career where I’m working less with clients and more with data. Where do I begin? What qualifications/certifications do I need for DA? I plan on looking to make an internal switch to get my feet wet before trying to apply externally.,21,24,1736178573.0,analytics
1huzss4,Senior Web Analyst to Marketing Analytics Manager,"I have been working in web analytics for roughly 4-5 years and have worked my way into a senior web analyst role. I have yet to manage any employees but have managed tasks, projects and driven insights/results.

I am wondering how to break out of the analyst loop and into a manager role. Making the leap into management without management experience is a conundrum. I have a bachelors in communication and am self-taught with all technical knowledge.

How did you break into the marketing analytics manager role?",14,8,1736172419.0,analytics
1hv8bze,Data Analytics Externship red flag?,"So I recently joined an Externship to gain more practical experience but saw AI was a big part of it. 

“Over the next 12 weeks, you'll work on projects that will sharpen your skills in analyzing both quantitative and qualitative data, understanding customer sentiment, and leveraging AI to generate actionable insights.“

“Synthesize your insights into a final presentation, where you’ll use AI to deliver data-driven recommendations”

The term AI is used throughout the whole curriculum. Maybe it’s just me but it seems a bit lazy like there is a section called generate presentations and insights using AI.  Am I overthinking it? I know AI is the future but program feels superficial with so much AI being involved. Would appreciate insights. 

",3,9,1736193891.0,analytics
1hutndk,I built a tool to get quick insights before data visualization,"Hello everyone,

I have been working on an AI tool which generates good visualization and gives quick insights on your CSV file. 

Hi everyone,

As someone who frequently works with raw data, I’ve always found the initial steps of analysis (the cleaning, exploring, and understanding phase) time-consuming and often repetitive. I wanted something faster, simpler, and more intuitive, so I decided to build a tool to solve this problem.

Introducing **CSVIZ**, a lightweight app designed to help you get quick insights from your CSV files before diving into full-fledged data visualization.

# Key Features:

* Automatically highlights trends, outliers, and key statistics in seconds.
* Provides instant previews of possible visualizations based on your dataset.
* Simplifies data exploration, saving hours of manual effort.

Whether you're a data analyst, business user, or anyone who works with CSV files, CSVIZ helps you move from raw data to actionable insights without wasting time.

I’m currently launching the **beta version** and would love feedback from this community! If this sounds interesting, feel free to check it out or share your thoughts. Your input would mean the world to me.

Let’s make data exploration simpler for everyone! ",16,17,1736148453.0,analytics
1hurqt0,Data Governance - What do you do on your day to day? ,"My company is creating a new dept for data governance, but I don't know anyone that has experience in it, so does anyone here have some exp regarding this topic? How's your day to day? What type of projects you have? 

Thanks! ",22,4,1736140949.0,analytics
1hv8my7,Andrew Ng's Coursera courses,"Does anyone have any reaction, positive or negative, to Andrew Ng's beginner-level machine learning courses?  I am a little peeved that they are not included in the Coursera Plus subscription plan.

Thank you,

K. S.",1,2,1736194622.0,analytics
1humrno,Bachelor of Science in Biology looking to transition into data analyst role.,"Like the title says I have a bachelor's in biology with academic research experience. I've been pretty involved in data analytics in that realm during my undergrad but I would like to transition into data analytics from a business perspective. I have obvious experience in excel but I'm not an excel master. What certs are professionally recognized that would help me become relevant when applying for an entry level data analyst position and what other tips would analyst have for someone like me? My current role is in a quality assurance department in food manufacturing. I definitely analyze data (material breakage reports, inspections, testing results) but I wouldn't say that I have professional data analyst experience. TYIA for any advice and guidance.

Free courses would be great, but I could afford to purchase one at a time. ",6,9,1736125523.0,analytics
1huw4lc,Statistics student here! Advice suggestions everything is accepted thank you,"I would like to dedicate myself to RPA development, where can I find online practices or tutorials? What advice do you give me? I'm also interested in the area of ​​production forecasting, I think it's called that. What do you recommend and thank you!",1,5,1736159569.0,analytics
1huvrmx,Looking for a more efficient way of collecting commentary on financial movements?,"Hi All. 

I work in insurance and I am responsible for running a report on a weekly basis which shows all movements greater than 100k. I am then required to reach out to the respective adjusters and ask for their comment on the reason for the movement. Currently this is done by me sending them an email and populating their reply into an excel sheet. Once all narratives are in hand, I check for any conflict of interest claims and then circulate to a wider audience. Is there any way this could further be optimised using Microsoft suite? Preferably not sharepoint, but perhaps a PowerBI related tool or otherwise?",1,2,1736158001.0,analytics
1huqkyz,Advice on data warehouse options,,1,2,1736137034.0,analytics
1hu2j8q,Which certificate course is most recommended for securing a data analyst job?,"Which certificate course, covering everything from beginner to advanced data analysis concepts, provides hands-on projects and practical experience to best prepare for a data analyst role and increase the chances of securing a job in the field?",28,34,1736068212.0,analytics
1hu80ei,Data Adjacent roles and positions?,"Hello all!

While considering the oversaturation in the entry level data analytics job market, I was wondering what other titles, roles, or positions outside kf ""data analyst"" one can search or fine tune ones resume to to increase their chances of being hired after learning analytical skills and tools? 

For example, I heard accounting shares many traits and skillets with data analysis? Am I right in this assumption?

Thank you all!",8,22,1736086775.0,analytics
1huoahg,"Seeking literature(books, articles, theories, etc) related to a specific data topic ","I would put that topic in the title but truthfully I am not sure what to call it. Basically, my company went through a transformation years ago and they decided to make dashboards for everything and they never ever revisit what has been created unless someone yells loud enough. 

As a data analyst it’s a bit frustrating because the customers in the company seem to complain certain things are not even reflective of current needs and are outdated metrics. 

I want to convince my management(who have zero experience in data) that there should be some kind of operating model where we revisit key information and dashboards at specific intervals(could be yearly, quarterly, etc) and see if it still satisfies business requirements. It does not need to be done at a micro level either. 

I think I’d have a better case if there was actual industry standards on how data and reporting should be managed. Data governance is a bit wider of a scope than I had in mind. Are there any key terms I should be searching for that explains what I’m seeking?",1,4,1736129958.0,analytics
1huigky,Aspiring DA/DS w/social science degree and holistic health experience,"Hi, my domain experience is in holistic health, massage/spa services, human services jobs, and learning statistics and social research methods. Will study Six Sigma, and help desk (and maybe social media marketing).

Where can I work? ",1,2,1736114052.0,analytics
1huos2a,MS in Data Analytics,"Looking towards a masters in Data Analytics, what are the general pre-reqs for these programs? ",0,13,1736131423.0,analytics
1huijov,What can I do with a social science degree in DA?,"Hi, I have a background in holistic health, massage, human services (disabled community), and endurance athletics. 

What are possible domains for me?",0,10,1736114281.0,analytics
1htv0z9,Tracking Tool for Personal Development,Anyone have specific software or best practices to track accomplishments and/or work completed? EOY I am wanting to present a short summary for a promotion and I would want to track it the most efficient way. ,3,3,1736043207.0,analytics
1htizsw,Profile/Role transition from business Analytics,"
Hi All,

Need your advice at this crucial junction of my career.

I am a business analyst(34 M) with 8 years of experience in a US bank.
So i did my Masters in Computer Applications from top NIT in india. I studied computer science in my bachelors.

I worked on data and business analytics roles (campaign analytics domain) in banks and was unlucky in terms of organization, clients and quality of work.

My current skill involves SAS, SQL, Python, Excel, case studies,credit cards use cases.

I was good at coding & data structures during my college time.
Now somewhere i feel very underpaid if i compare myself to my batchmates who are software developers.

I am looking to learn something like machine learning, data science and move to a more niche role.
As i see people in data science, machine learning, data modelling or risk analytics teams earning well.

I need your advice where shall i take my career & what is the right domain for me from here where my experience can help in transition.

I know it is late after 8 years but I am someone who believes that you can start at anytime from zero.

Please help in comments or dm me your advice. Open for conversations and to answer any further question.
",4,2,1736010593.0,analytics
1hsqtue,Senior Analyst but only Excel & power bi? ,"can someone actually make it as a senior analyst with only those two tools? 

as a current junior analyst,  i find myself caught up answering business questions and building case studies but only using advanced excel and power bi dashboards and grabbing data from our SQL server

i know the ordinary 
“ analytics isn’t about what tools you use” 
but what is that really true or is it just some LinkedIn corny hype up posts ?

edit 1 : clarification
",65,40,1735922920.0,analytics
1hsrfpj,Unsure about analytics job market,"Hi all,

I'm 26, working remotely as a supply chain data analyst at a small company. My role is diverse—I manage the entire supply chain and create/mantain Power BI and SQL reports for other departments—but there’s limited room for growth.

My original plan was to use this job as a stepping stone into data analytics. However, seeing constant posts about layoffs and oversaturation in the field has made me question that path. I got this job about a year ago and when I was job hunting, the market was terrible. I thought the market would improve but it seems worse now. I'm also worried about AI automating analytics roles in the future. I value job security a lot. 

I’ve considered pivoting fully into supply chain since it feels more future-proof and secure—companies will always need people to manage supply chains. But those roles seem more stressful and less likely to offer remote work, which I value. Tech jobs just seem more ""cushy"" in comparison.

Am I being delusional about the tech job market? I'm unsure if I should focus on data analytics or start working on supply chain certifications instead?",56,25,1735924391.0,analytics
1htj0kv,Recommendations,Hello I am looking for recommendations for an online masters in data science or data analytics that offers courses synchronously. Basically I want to attend classes via zoom at a set time every week live. Most of the ones I have seen have been just recorded lectures but that is not what I am looking for. ,0,5,1736010646.0,analytics
1hsmjx2,Career Pivot,"I’ve been working in data analytics for almost eight years now. At the senior level and starting to see a lack of advancement opportunities. How have people made a career pivot away from analytics? Data engineering seems like the logical next step, but idk if the pay is all that different. Would software engineering be attainable, or is that so completely different that I’d be starting from scratch?",45,30,1735911202.0,analytics
1ht1hwc,SQL,I know this is the golden language to learn here and i know how to use it. But how is SQL used in day to day analytics? Is it integrated with something else or im just kind of confused as to how it all plays together. Thanks!,3,7,1735950006.0,analytics
1ht2m7f,Tips to be a Data Analyst,"Hello Everyone,

Newbie here I just started going in the world of analystics which is taking courses, watching tutorials, reading things that people post in this community and most especially doing a lot of research before I do a change of career but I have one question. 

Let's say in the near future I learned most of the basic stuff which is Excel,SQL, Tableu, Power Bi etc. gain some Industry certificates would it be a nice idea to do free lancing first before I do the leap of faith and go straight in job hunting and if I'm fortunate take the interviews. 

Because I read a post in this community one time that learning from courses and video tutorials in the internet is not enough to get a Entry-level data analyst job. I read that you should have atleast an experience to land a job.

Any tips and advices would be much appreciated. I want to learn more and gain understanding. Thank you in advance everyone and God bless 🙏❤️",1,22,1735953157.0,analytics
1hsna5x,"I tried and I failed, what's next?","Hi everyone,

I've been working in this field for two years now, and after all this time, I’ve come to realize that it might not be the right fit for me. I feel like I haven't really learned much, and I'm thinking of exploring something else. The thing is, I’ve never quite figured out what I truly enjoy doing in life, despite trying career coaching and aptitude tests.

I was wondering if anyone has found themselves in a similar situation and what steps they took to move forward. Any advice or personal experiences would be really appreciated.

Thanks!

",6,36,1735913422.0,analytics
1hssen9,Any sas platforms out there that connect to ga4 and automatically build reports ,"Hi there, 

I work for a small agency, and analytics and reporting aren’t our main focus but we’d like to have something to add as a value prop for our clients.

Currently that requires that someone, usually me, looks through all of the data and builds a lookerstudio report based on some interesting insights.

Are there any sas platforms out there that look at your data and automatically model it into interesting visualizations based on standout data? Something like a looker studio dashboard but where the reports are prebuilt or use ai to create powerful insights.

Thanks all!

",3,2,1735926796.0,analytics
1hsghuu,Struggling to Transition from Startup Analytics to mid size or bigger company Roles,"I’ve been working in marketing analytics for a few years, in a small ecommerce startup environment. My role involved working with tools like SQL, Power BI, Excel, google analytics. While I’ve gained valuable experience in many aspects on business apart from learning these tools, I’m finding it challenging to get analytics roles at mid size to larger companies. Larger I mean not even big tech companies, something like 500-600 people. I’ve been working on enhancing my portfolio with projects showcasing broader more advance skills and aligning them with industry standards. And have GitHub account to showcase that. Also, I have advanced degree in statistics and mathematics.

In about 50% of my interviews with hiring managers, some looked up ecomm startup I worked at and commented that it seems like a small business with limited requirements for analytics, some just got turned off by finding out smaller product range and not having larger data to manage I guess or who knows.  And these has made me wonder if my startup experience is being perceived as insufficient or if there’s something else I’m missing. I wonder If getting certified with some tools might help. Though I am extremely tired after finishing up my masters degree, I somehow thought degree plus some experience  will be enough to show my credibility to get entry to 3+ years experience jobs. 

I’d love to hear from those of you who’ve successfully made the transition or have insights into how I can improve my chances. Specifically:

Are there skills, certifications, or projects that helped you stand out?

What’s the best way to position working at a startup when applying to roles at mid to larger organizations? 

Thank you in advance for your advice and insights.",22,10,1735887155.0,analytics
1hshrvp,My website is being spammed by a Singapore IP address,"They rotate between 3 different IP addresses, daily, every few hours click on my page almost 100 times in 2 minutes. 

My analytics doesn’t mention it as a bot, or any device for that matter.

Is this something I should be worried about or move on? ",4,4,1735892562.0,analytics
1hs5fp8,Are any AI Analytics Tools Actually Good? ,"Like are you using analytics tools with built in AI, or just giving ChatGPT, MS CoPilot, or some other model access to your data? If you are using an AI is it sanctioned by your company? ",18,35,1735854397.0,analytics
1hsdh4k,Anyone here Export data from Epic Medical Software to Excel?,"Hi all.  I do revenue analysis for a small hospital.  We use Epic to keep track of all of our data.

I generally pull data from other data warehouses but I am helping out for someone who is on leave and I have a big (well tedious actually) problem with one particular section.

There are these work queues.  Each work queue has a number associated with it.  There are 70 of these work queues that I have to export.

The only way I can see the data is look at a list of work queues and click on the particular work queue, and then a list of a few hundred lines pops up.  From there I have to export it (no copy/paste), having to specify the name and location, along with a password (HIPAA thing), then open up the data, then add the WQ number to each, then copy and paste that to my main list, where I will do actual analytics.  Each export takes about 5 minutes (longer if the software crashes), and like i said, I have to do 70 of them.  I cannot take the tedium.

I can deal with it for this month, but I am going to have to do this for at least 6 more months and I am trying to save my sanity.  There has to be a better way but all I get is ""that's how we've always done it""

Is there a way to pull all of the work queues at the same time?

A shot in the dark I know, but if anyone knows what I am talking about please let me know, or at least point me in the right direction.  Thanks.",3,4,1735876499.0,analytics
1hror9q,Is it as bad as that guy said?,"So I saw that post recently where OP was a bit frustrated with the influx of new people trying to break into data analysis and not understanding what they are exactly getting into. Seemed like frustration with expectations of ease and salary as well as availability with them noting a declining job market. 

Should I be tuning this out and driving or should I heed the alarms and go back where I came from? 

I ask because I’ve just chosen to go down this past. I’ve done a lot of research and the job does genuinely sound like what I want to do. I’ve been researching different jobs for almost 2 years now and this is the first thing I said I really wanted to do from deep inside of me. I know it’s not just some ‘easy fun remote gig shortcut to 200k’ BUT aren’t jobs just hard in general? Not to say anything about data analytics but millions of jobs deal with overcoming new challenges, struggling to meet deadlines, and the alternatives are destroying your body doing manual labor or losing all opportunities to see family and maintain healthy relationships. 

I’ve been working in hospitality for going on 7 years now. I’ve come to realize I can feel my body being worn down, almost everyone I’ve met more senior struggles to be even a little happy. I haven’t gotten a major holiday off in maybe 3 years? I would do a lot to be able to spend Christmas with my family or go to Thanksgiving. 

My understanding is it will be a lot of hard work to even get an entry level job. My plan was simply to work hard everyday, try to get some certifications that show I am capable of learning and working hard and maybe eventually I will get an entry level position. I expect no tech salary and that isn’t even a long term goal. I don’t expect it to be easy though and I do expect it to still be a ‘job’, only so enjoyable. 

I’ve chosen this route because going back to school for a degree in it in person would be almost impossible working full time and getting an online degree even would be at least 3 years and tens of thousands of dollars. Not to mention I fail to meet GPA requirements simply because I was too immature to apply myself as a kid. I did well enough sleeping through most classes and just passing tests that I never learned how to learn, I was not an idiot in any way except the fact that I was too short sighted to begin building my future. 

I’ve now learned how to learn and filled with drive to build these skills. I’ve seen what life is like in service and it’s not what I want and I believe that hard work can eventually make something.

Am I just another hopeful imbecile wasting his time or is there truth that I can get an entry level job with hard work and multiple certifications? 

Hope this post is allowed by the rules! I’m not seeking career advice or assistance but I DO want to hear it from the community directly whether or not this is some bleak industry not even worth anyone’s time or if there is hope. 

Thank you!
-A hopeful person",34,75,1735804393.0,analytics
1hsjm4i,Is data analytics an entry level position?,"
I’ve heard professionals like the data janitor say it is",0,8,1735900390.0,analytics
1hrocwx,Best Resources For SQL Interview Questions,"Hi everyone!

Following the positive response to my previous post, *“*[Best Practical Way to Learn SQL](https://www.reddit.com/r/SQL/comments/1hqyvuh/best_practical_way_to_lean_sql/)*”*, I wanted to share the next step in your learning journey: practicing real interview questions.

In a typical Data Analyst or Analytics interview, you’ll encounter these stages:

# 1. Technical Screening

* Often a HackerRank or similar test designed to filter candidates based on technical proficiency.

# 2. Hiring Manager Round

* A discussion with the hiring manager focusing on your work experience, problem-solving skills, and understanding of business concepts.

# 3. Technical Rounds (1–2)

* **SQL-focused round**: This tests your hard skills, such as query writing and problem-solving with databases.
* **Product Case Study (varies by company)**: A more in-depth exploration of your analytical thinking and approach to product-related problems (more on this in a future post).

# 4. Cross-Team/Stakeholder Round

* A soft skills round assessing your ability to collaborate with other teams and communicate effectively.

Look at this pattern we realise that SQL is the cornerstone of success in most Data Analytics interviews. While other factors like communication and business understanding matter, as a hiring manager, I’ve often observed that candidates are underprepared for the SQL round.

Just like coding rounds are essential for Software Engineering roles, SQL proficiency is crucial for entry-level Data Analytics positions. When you lack prior experience, technical skills often carry the most weight.

I’ve said it before, and I’ll say it again: the **technical screening round can make or break your interview**.

# Recommended Resources for SQL Practice

Here are some resources I’ve personally used and recommend to ace SQL interview questions:

# 1. LeetCode (Database Section)

* Start with the free SQL questions — no need for a subscription initially.
* Explore the “SQL 50 Questions” list as a structured starting point.

# 2. DataLemur

* A dedicated platform for SQL interview questions.
* Use this after completing the free questions on LeetCode.
* Again no need to buy anything, the free section is good to begin with

# 3. StrataScratch

* Offers SQL questions alongside other analytics-related technical problems.

# 4. HackerRank (SQL Section)

* Not the most comprehensive but worth completing for its free and limited set of questions.

# Pro Tips for SQL Prep

1. **Practice Consistently**: Go through the resources 2–3 times to build speed and intuition.
2. **Bookmark Tough Questions**: Track questions that challenge you so you can revisit them during subsequent practice rounds.
3. **Master Problem-Solving**: Aim to develop an intuition for solving SQL problems, which only comes with repetition and persistence.



Feel free to drop any questions in the comments below — I’ll do my best to answer and help you succeed in your SQL and analytics career!

Happy learning and good luck! 🚀",18,3,1735802663.0,analytics
1hr493m,What are peoples' reasons for trying to break into analytics still?,"Each day I see numerous posts about people attempting to break into analytics with the most random backgrounds that make them less than ideal candidates. They likely face a massive uphill battle to break into an analytics related role.

Why does this keep happening?

Do people believe there's still a huge boom in the job market for analytics?

It just confuses me to be honest given how saturated the field is and bleak the job market is right now. You have an exponentially increasing supply of labor and decreasing demand for it.

Edit: it appears that a few people are getting upset and think that I am gatekeeping. All I am asking is what are poeples' motivations to try and enter this field. It seems like many people think we're in a 2021-22 situation where you can complete a bootcamp or masters with no relevant experience or domain knowledge and then have the opportunity to jump right into the industry with a hybrid/remote role as a data analyst/scientist, etc. I personally think people are getting influenced by trendy influencer/youtube videos and universities creating these programs.

Obviously people can do as they wish. I don't care, it's just a job. However, I worry that many of the people posting about how they want to break in don't understand the true nature of the general job market and the analytics industry in particular. No shit most industries are saturated right now, but analytics is clearly at a higher level due to the combination of hype, off-shoring and cooling of the overall job market.

I feel bad for the individuals who have decided to complete a bootcamp, a MS in analytics or just graduated with an irrelevant degree, and possess zero domain knowledge with few analytical skills but want to completely jump ship and break into analytics. They're going down a path that'll likely lead to hundreds maybe even over a 1000 applications with most being rejections and ultimately making a failed investment.

They can do what they want, however, I worry that many people think the barrier to entry is much lower than it truly is and are making poor decisions.",158,119,1735742762.0,analytics
1hrjqpf,Data Analyst vs Business Analyst,My current title is a DA but the duties I perform tend to align under a BA title which pays much higher in my area. What is a quick difference between the two (generally speaking) I could point out to management when negotiating my yearly raise? Should I request a title change?,17,18,1735786144.0,analytics
1hrmkff,Are Business Intelligence (BI) and Threat Intelligence linked with data analysis?,"Forgive me for the question as I have just learned about these roles that have piqued my interest since starting my data analysis learning journey. 

I'd love for anyone with experience in these roles to help me understand these and what sets them apart from data analysis itself, and (if at all possible) how can I build a history/portfolio to help me land these types of positions.

Happy 2025!",4,4,1735795525.0,analytics
1hqyv1u,Best Practical Way to Learn SQL,"I have seen multiple posts and youtube videos that complicate things when it comes to learning SQL. In my personal opinion watching countless courses does not get you anywhere. 

Here's what heled me when I was getting started. 

* Go to google and search Mode SQL Tutorial
* It is a free documentation of the SQL concepts that have been summarised in a practical manner
* I highly recommend going through them in order if you're a total newbie trying to learn SQL
* The best part? - You can practise the concepts right then and there in the free SQL editor and actually implement the concepts that you have just learned.   

Rinse and repeat for this until your conformatable with how to write SQL queries. 

P.S I am not affiliated with Mode in any manner its just a great resource that helped me when I was trying to get my first Data Analyst Job. 

What are your favorite resources?",97,19,1735719826.0,analytics
1hr4j9n,How to decide what field to work in with data?,"25yo.

I currently work in data analytics position for last 2 years, doing mainly updating code, some analysis, and basic visualizations like in Excel. 

I learned some basic data science the past few months and ran some small machine learning models, but didn't really enjoy it. My impression was ""doing this well would take a bit of work and time. I could be interested if I keep working at it, but I'm not sure"". I did some machine learning in college too which I didn't particularly like.

I also considered doing more visualizations/business analyst stuff. It's easier to do but if I go down data science, it'll pay more. 

  
How can I decide if I should do more like data/business analyst, or if I should go toward things like data science/engineer and start to move my career toward it. ",11,19,1735743656.0,analytics
1hqulws,"NBA employee - Paul George - criticizes NBA analytics for weakening the ""product""","Interesting perspective from PG on his podcast - he's basically saying analytics have sucked the soul out of NBA basketball by turning every team into robots chasing the same three shots: three-pointers, layups, or free throws. He points out that while players today might be more skilled than ever, they're all being forced to play this cookie-cutter style that's making games boring to watch, especially for old-school fans who miss the physical, gritty basketball of the past. The kicker is that even though we've got all this talent in the league, teams are so obsessed with playing the ""analytically correct"" way that we're not even getting to see players show off what they can really do on the court anymore.

FWIW: TIcket demand and fees for casting rights appear unaffected...",65,37,1735701721.0,analytics
1hrez89,Career switch to data analytics from Behavior Therapy?,"Hi, I have my undergrad in psychology with a minor in business/accounting. I landed in medical billing/ insurance claims analyst. I also did applied behavior analysis classes and training. I did that part time and worked remote mostly for medical billing. Now I would.like learn data analytics and focus on just that. Is that doable? How is the market for junior data analysts? I don't want to do another degree. I am open to affordable courses and training.",0,13,1735772175.0,analytics
1hqfi0m,Trouble Fining Entry Level Marketing Analytics/Database Marketing Jobs as an MBA Student,"I'm currently an MBA student and I want to have a job lined up by the time I graduate in May. I'm having trouble finding entry-level marketing analytics or database marketing roles. I'm starting to wonder if this type of job is too niche and if I should be looking for EITHER data analytics OR marketing roles.

I mostly check linkedin and google frequently. I always apply directly on the company's website when I can. I think I have a well-structured resume, as I've had many professionals critique it.

For more context, I have a bachelor's in economics, and my concentrations in my MBA are marketing and data analytics. Furthermore, I am learning Power BI and SQL right now and working on some data projects to add to my portfolio.

Please let me know if I'm doing something wrong in my job search and what I can do to improve!

Edit: I forgot to mention my experience. I currently have a marketing internship at a startup, I'm a statistics tutor, and I was a consulting intern for a mid-sized accounting firm. ",15,23,1735654430.0,analytics
1hq3126,Uninterested in being more technical; what to do next?,"Hi! I've been a data analyst for several years. Over the years, I've gathered a variety of skills, including the tech stack (SQL, Tableau, Python/Spark), PM (general and tools like Jira), and design (general and tools like Figma), and I've improved my stakeholder/project management skills. 

I'm not excited to dive deep into the technical work, hence ruling out data scientist/engineer careers. I don't feel motivated to learn more Power BI/DAX or continue to upskill in new tech stack, for example... and I don't see myself doing side projects outside of work. Because of this, I'm nervous about finding other data analyst positions in a difficult job market (e.g. in case of a layoff, etc.) considering how saturated & talented the market can be. I like mentoring others, teaching, and being creative about solutions to help the business. I've looked into some career fields that hit on these topics while maintaining the data background, but some seemed stressful, which isn't what I'm looking for either.

Has anyone been in a similar position where they were a data analyst but transitioned into a different position/career based on similar experience? Would love to hear any advice or hear about what you ended up doing!

\----

As another way of looking at this, I'm curious if I can still be successful as a data analyst without being more technical. What are areas I can focus in learning, etc.?

  
",40,34,1735607712.0,analytics
1hpvjny,How has your organization effectively managed data quality?,"Hi everyone, we all know that data quality is typically very bad which creates problems for analytics. My question is: what has your organization done to effectively combat poor data quality? What type of data governance protocols did you employ that was useful? How did you ensure that the same data quality issues didn't keep showing up in the future? Thanks for your insight! ",18,30,1735587711.0,analytics
1hq9ilf,31st December Meme,"Pov: 31st December

Me: I've got 1 day left to meet my project deadline

Data: I've got 1 day left to crash and lose all your progress",0,2,1735629510.0,analytics
1hpl9hx,Why operational analysts should sit in finance...one opinion,"Operational analysts should sit in Finance, not IT or scattered across business units. Here's why:

Key Benefits:

* They develop better financial awareness and can tie analysis directly to ROI/bottom line impact
* More consistent methods and data quality across the company when centralized in Finance
* Resources get allocated based on company priorities, not department politics
* Better career growth path with exposure to senior leadership
* Analysts get broader business context vs being stuck in one department's silo

Common Pushback:

* ""But they'll lose touch with operations!"" - Nope, just set up proper rotations and communication channels
* ""They need IT support!"" - Modern tools reduce this need, and you can still partner with IT while focusing on business outcomes
* ""Won't it slow down urgent requests?"" - Not with proper service agreements and priority frameworks

You don't want your analysts becoming Excel jockeys in IT or getting buried in one department's bubble. Finance gives them the perfect mix of business context and technical opportunity while keeping them focused on actual value creation.",16,17,1735557994.0,analytics
1hpklqi,How do you look for jobs ,"Back on the market and want to refresh my skills in Job hunting. I’m prepared to dedicate a lot of time to this as my contract for my job is ending in January. I check LinkedIn every morning, but I just see a bunch of promoted and reposted opportunities that seem stale. Indeed is a literal shit-hole (At least for my city: Chicago). Google jobs I feel meh about. 

Any tips for how to diligently job hunt and what has actually worked for you all.

My main goal along with networking is to be one of the first applicants for these jobs postings to give myself a good shot. 

Any tips or life hacks will be welcome!! 

Btw: I have analytics masters with nearly 5 years of work experience. + coding / readership internships before that. 2020 grad. Mostly worked in retail / cpg analytics roles along with a research role. ",13,9,1735555265.0,analytics
1hpqtz8,How do you keep track of reports/insights?,"Hey all,
I was wondering how other people in other companies keep track of reports or insights you made for different stakeholders.

Lets say that the marketing team wants to know how well a certain campaign did and you do an analysis on their ab test. Next year they want to do a similar test, how would they find it back, where is it stored?

I'm super curious as I'm thinking about a small SaaS solution to build for this. In our company we self host a small website where Jupyter notebooks could be hosted.",2,7,1735575690.0,analytics
1hpjz9e,38M - Inquiry about a master in business analytics,"Im 38, living in Madrid working for a large multinational consulting and tech firm in the finance area for the past 13 years, in particular FP&A and Deal Shaping and pricing. 

  
Feeling quite stuck in terms of career opportunities and wanted to pivot to business analytics and intelligence. I feel like my train has long left the station but still want to give it a shot, any recommendations for programs that offer part time studies so I can keep working?",5,7,1735552624.0,analytics
1hptn8m,Free Database Tool for Novice Personal Project,"I would like to create a small-ish database to analyze trading card game decklists, similar to limitlesstcg.com. I am a novice data builder, and would like to use a free tool to build the data sets. The basics are as such:
1. An event will have an event type, date, location, players who use a discrete decklist (the number of players and therefore lists will vary by event), and placement in the event
2. Track pairings of players; the scale would be small as most results I am tracking would be among the top 8-16 players
2. Each decklist will have a minimum of 1 leader, one starting location, and 50 cards, with an additional sideboard of 10 cards maximum
3. Cards that are included in the decklists can be in multiples, and have characteristics that I would like to analyze (cost, power, aspect/affiliation, etc)

With these data sets, I would like to analyze:
1. Popularity of a particular leader (and occasionally the starting location) over an adjustable period of time
2. The most frequently used cards with that leader/location pair <- this choice realistically eliminates the majority of cards that could be included in a deck, and in what quantities
3. Identify the most common place of the most frequently used cards, either in main deck or sideboard
4. Characteristics of the cards chosen for deck data sets

Assuming I have a decklist creator that generates a file in a specific format with names, quantities placement of the cards; would like to ingest the data from those files for the raw data. The data can be stored locally, but would lean toward something that could eventually be stored online and/or entered from multiple users.

Is there a recommendation you would make on a tool to create the database?

Thank you in advance!",1,2,1735582893.0,analytics
1hpg68n,Production Level Custom Analytics,"What is your go to analytics solution ready for production?

E.g. some tools I have used in the past:
- Apache Beam
- Custom Python based framework

Generally, not happy with either so want to explore options. ",4,3,1735536900.0,analytics
1hpj2wn,Where do i start?,Hey guys! Hope its okay to post this. I was wondering if you could give me advice on where to start or recommend any courses to take to start learning data analytics? Im a complete beginner but im very eager to learn and have lots of time on my hands! Id really appreciate suggestions ,0,6,1735548649.0,analytics
1hpbqkm,Advice on breaking in to the field,"Hi everyone,

I hope you’re all doing well! I have a quick question about breaking into the analytics field as a student. I’m majoring in business analytics, but my university program has focused more on data analysis. I’ve put together a resume that includes a project I completed for a class (I plan to add more as I finish them).

Since I’m in my final year, I’m trying to land a data-related role that I can use to bridge into an entry-level analyst position, as I know the market is tough even for entry-level roles. If I’m lucky enough, I’d love to land an entry-level analyst role directly.

I asked for feedback on my resume from the resume subreddit, specifically about whether I should focus on my projects since I don’t have direct work experience. Someone suggested that I “lie” about having logistics coordinator experience (even though I don’t have that experience) and also said my formatting needs work, which I’ll be fixing.

I’m not sure how to feel about the advice to “lie” on my resume. How can I convince an employer to give me a chance if I don’t have direct experience? Should I rely on my projects to show my skills, or is fabricating experience really necessary? Would love to hear the advice and feedback. 
Thank you!! 
",2,25,1735522697.0,analytics
1hpfqvu,How to Combine Files with Different Headers and Sheet Names Using Power ...,"In this video, I'll guide you through the process of combining multiple files where the column names and sheet names are not unique, using Power Query in Excel. This tutorial is essential for anyone handling diverse data sets from various sources, helping you merge them into a single, cohesive dataset for more efficient analysis.  
  
",0,2,1735535415.0,analytics
1hpejs2,Would you get into this field if you don't know what to do?,"I believe a strong strait that leads me to this career is my strong investigative skills, also I like the money potential and schedule and self employment opportunities. Would this be a regrettable career in 5 years time ?",0,22,1735531442.0,analytics
1hoq4ao,Probability calculation relevance,"Just asking, how much do you have to deal with probability calculation daily (as an analyst, hobbyist,...)

Any case you have to calculate a large set of linked events (and, or, conditional events,...), and need to find the probability of event X. If so, which tool do you use for that task ?",6,4,1735453963.0,analytics
1hohi5u,Funnel analytics ,"Hi all

Complete newbie to data / analytics. 

Can someone recommend me a way of finding out how far people get into my conversion funnel? So every page is tracked and I can see where they drop off between hitting the home page and  checking out?

If that’s  even possible?

Many thanks ",8,8,1735426119.0,analytics
1hoifpj,Data analysis or cybersecurity?,"Hello all!

I am considering starting a new career path after years of stagnant career growth within trust and safety and GenAI. 

I have done much research and I have come down to either data analytics or cybersecurity.

Cybersecurity because what had motivated me to follow tech in the first place years ago was protecting users from harm (internal content and external). But since then trust and safety has been all over the place from content moderation to customer support (and of course lots of layoffs ans outsourcing).

Data analysis because I have some familiarity with analysis concepts and tools, mostly excel, and I have found trends and insights through large datasets. I thought this would be a better choice since I already have some experience, and just need to aquire more technical skills and create a decent portfolio. 

But something about cybersecurity has always intrigued me, and feels like something more meaningful for me in the long run. I do understand that the general consensus is that cybersecurity is no entry level job and requires some time in IT helpdesk roles (which I'm fine with) before landing a threat intelligence role, ethical hacking, red teaming, or anything or the like. 

I would really appreciate some guidance here on which is a better career path for me. Again. I am not asking what exactly to do as I can always do more research in this subreddit. Just advice from a pro or two on whether one or the other is the right career path for me.

Thank you all and happy new year!",0,28,1735428784.0,analytics
1hnrmz0,What analytical and statistical methods do you use in your job regularly?,"What is your job/role, and what statistic and analytic methods/tools do you use? What are the critical lessons/skills/in-house-protocols needed for your specific role? 

I’ve heard a good amount of general advice, but I’ve been looking for a more tailored advice to explore different roles/fields and steps to take to be competent in different jobs. I won’t be able to be a top candidate for every path, so I want to see tangible steps to a variety of roles. I’d then choose from there and make a career/education roadmap from there.

Some background:
I’m a first-year MS Statistics student. I came from a finance background and I’m currently specializing in medical statistics, but I’ve (until now) planned my coursework to make me a generalizable analyst between fields/industries.

Discerning between:
- Federal govt. statistician 
- Hospital/Pharma statistics
- Business Analytics (seems like most here)

Programming background, in order of competency:
- R (my main language since undergrad) 
- SAS (graduate classes)
- Python (Self-taught. I thought it’s not too dissimilar from R. I also enrolled in classes next semester for machine learning and a general ‘apply Python to projects’ class)
- also SQL, Tableau/PowerBI, and Excel

General statistical topics I know to a decent degree: 
- Sigma-algebras (for understanding what my computer is doing)
- Bayesian methodology
- Regression (logistic, linear, negative binomial, MLE vs OLS)
- Data importing, cleaning, analysis, reporting
- Handling issues like confounding, reverse causality, multicollinearity, etc.


",79,56,1735340809.0,analytics
1ho46ez,What are the best techniques for Anomaly Detection in Complex Datasets?,"What are some of the latest techniques you use for detecting anomalies in complex datasets, and how do you apply them in real-world production systems?",3,5,1735386699.0,analytics
1ho4bf9,Upcoming Interview for Associate Analyst - Data Governance,"I have my interview scheduled soon for this position. This role is for undergraduate candidate from INDIA. I’ve been doing SQL interview questions on hacker rank and Leetcode but if someone has some insight on what types of questions will be asked, I would really appreciate it!

some point from Job description

* **Data Quality Management**: Conduct data profiling, analysis, and quality assessments using tools like Informatica, SQL, PL/SQL and MSFT Excel analytics. Implement data cleansing, linking strategies, and automate processes using AI/ML.
* **Stakeholder Collaboration**: Engage with Data Stewards and Business Stakeholders to address data quality issues, prioritize resolutions, and ensure compliance with data governance standards.
* **Documentation & Improvement**: Maintain data dictionaries, document business rules, perform root cause analyses, and drive continuous improvement through periodic assessments and gap analyses.",0,2,1735387287.0,analytics
1hoc29c,I built Marketing Analytics tools in 7 Days – What's next?,,0,13,1735411117.0,analytics
1hnnbcs,What's your approach to historical GA4 data migration to BigQuery?,"Working with GA4 data migration and curious how others handle historical data syncing to BigQuery.

What techniques have you found effective for:

* Managing multiple GA4 properties
* Ensuring data consistency during migration
* Handling large historical datasets
* Maintaining data quality

Would love to hear about your experiences and best practices.",7,1,1735329205.0,analytics
1hnghde,"Pathway from an IC to Analytics Manager, Advice?","I’m currently working in FP&A as a lead analyst, (F500, Aerospace, Canada) while I like finance I figure that my trajectory isn’t as high as people with a CPA and I strongly prefer data analytics. 

Most of my job is building dashboards for the finance department and providing analysis for senior managers and SLT, lots of business partnering (as well as forecasting and budgeting etc). My job revolves around SQL and excel/power query. I’m comfortable using python although my job has no real need for it.

Im not thinking of changing roles for the next little bit as I just got this gig but I would like to know what it takes to get to the next level on the hamster wheel at some point. 

Edit: I should note that I do have manager/leadership experience when I was in the army but I find civilian managerial/leadership qualities are a bit different. ",12,24,1735311091.0,analytics
1hn528u,R or Python,"I'm considering learning R or Python and was wondering which would be better for me. I'm on the younger side and not set on a single career path yet, but I'm currently leaning toward becoming a data analyst and I'm hoping specifically to become a data analyst in sports. I feel like one of these tools will be essential for whatever my future career ends up being. Any advice? R or Python? Pros and cons of both for my specific scenario?

Thanks in advance",35,60,1735267689.0,analytics
1hng7ke,Independent consultants and analytics agencies,"Hi guys, I want to ask independent consultants and analytics agencies, how you guys are getting clients.

How cold emailing and linkedin outreach worked for you?

What industries do you target?

Your monthly emails volume?

And the amount you guys are spending on cold outreach.

I'd love to hear from you.",3,6,1735310288.0,analytics
1hnikng,Uncertainty in Hypothesis Testing Method,"For my research on judicial fairness, I used a chi-square test to compare how different groups perceive fairness in the justice system. However, I realized that the chi-square test is typically used for categorical data, while some of my data are ordinal. SPSSAU didn't explain why it recommended the chi-square test. Should I be using a different statistical method for my data? How do I know if the hypothesis test I am using is appropriate?",2,2,1735316820.0,analytics
1hmvtdj,Psychology - Data Analytics,"I just recently found out that there is a POSSIBILITY of transitioning from a psych background and work in data analysis as there is lot of stats being thrown around in psych. Ngl i am very interested but seems like huge decision to make so i would like to get some proper input about this first. I have used SPSS and looking forward to enroll in an online course in SQL if possible. Really be honest, will this be worth it for me?? ",15,8,1735241152.0,analytics
1hnasjt,Pair Programming and Mentorship,"Can anyone help me with this concept, like how 'pair programming and mentorship' really works and how It can help freelancers.",1,2,1735289331.0,analytics
1hml273,Anyone else works as a tech analyst in a non-technical team?,"I think this is the secret to be an over performer. I work for one of the top tech companies in the world, and I am the only analytics professional in a non-technical/business team. 

Recently I created a Power BI dashboard that summarizes and shows my team’s products performance in a more structured way. I have gotten so many awards and recognition on this, even though to me it was a simple project. 

Anyone else with a similar experience? What other examples of projects you have done that have impressed your non-technical teammates? 
",65,28,1735205783.0,analytics
1hmrlyh,Quick question for current Data Analysts ,"As someone about to do some interviews for data analyst roles at start ups as I wrap up my masters program (can be for anyone though)….(context is the United States)

What does a day look like for you? 

What are your biggest challenges?

With tasks in SQL do you work with existing code?

How often are you using tools like python and SAS for analysis compared to data viz tools?

What kind of accountability is asked of you? (ex. monthly reports, yearly reports…)

any other insights would be nice ",17,31,1735229762.0,analytics
1hn76am,Building a Freelancing Platform Exclusively for Data Professionals – Need Your Input!,"Hi Everyone, 

I was wondering that there is no specific freelancing products which are solely cater to data professionals and platform like toptal, fiverr and upwork seems like too scatter and just competing with the third world countries 

I am building a freelancing platform for data professionals and want to solve some of the problems which I face 

1. Giving the rate price for beginners and intermediates like from 30 - 50 for beginners and for the intermediates 70 -100 and professionals can set their own rates depending on the experience

2. Have some rigorous vetting process so that I can ensure that the clients are getting the genuine freelancers not the mocked ones

3. Want to build some features where you can rate clients as well so that we can block them if they are not working as expected

  
I want this platform to truly serve the needs of the data community. **What features or processes do you think are critical to include to make this platform successful?**

Some specific questions I have for you:

* What frustrates you most about current freelancing platforms?
* What would attract you to join a platform dedicated solely to data professionals?
* How can we balance inclusivity for beginners while maintaining quality for clients?
* Are there any must-have features or tools you’d love to see on such a platform?

I’d love to hear your thoughts and feedback! If this is a problem you’ve faced, let me know how you’d like to see it solved.

Thanks in advance for your insights!",0,4,1735274817.0,analytics
1hmrg6b,I want to start a freelance career but I only use Excel… ,"I work in public health, managing studies and program evaluations at a university. I don’t have extensive quant skills but I learned how to run regression analysis in Stata during my MPH. I haven’t used R or even written script in years. I took a workshop on data visualization and got pretty good at it, exclusively in Excel. 

I am frequently approached by colleagues who want me to work with them/for their team. I also landed a side job conducting evaluations for an old university partner. This is my only freelance contract.

I really want to grow a freelance analytics business, but are my skills in Excel enough? For folks who’ve succeeded in building a freelance career, what other skills/tools would you suggest I focus on? Any advice is welcome!",9,22,1735229314.0,analytics
1hmu9t1,Which level of those analytical tools you needs to become succesful in your job?,"My question is how deep you needs to know tools such Excel, SQL, Phyton, BI/Tableau/Qlik for your current position?

Or any other tool that can improve your productivity a lot.

I assume Excel and SQL are a given and Phyton may improve your productivity by a lot paired with ChatGPT if you knows about automatization.

Whats the tool that became a game changer in your daily job?",3,15,1735236977.0,analytics
1hmzlv3,What business questions do you address in insurance in a marketing/customer focus department as a data/business analyst? ,"I would like to know what business questions/ analysis/ dashboards one do in a business performance & intelligence analyst role (inserted in the marketing department). This role is more customer data focused/ campaign data / etc 

Thanks!! ",1,3,1735251434.0,analytics
1hmklur,Help me approaching this case study which I'm doing as a part of getting into Sales Analytics,"So, I have beein working in Sales, which my role is to generate the revenue for the company. I have worked for almost 10 years and want to move into Analytics. To begin with, I have working on Sales Data Analysis and came across this project which I'm clueless about it on how to begin with?

>The case study outlines the below instruction.  
Analyse the historic sales performance of Umbrella Corp keeping in mind the various parameters at play viz. Region, Industry, Business and Client, across the 3 Partners. Create a narrative from the trends and insights you identify, highlighting the focus areas for the next 5 years.

>Your supervisor has provided suggestions on questions to be evaluated:

>•The current Region categorization is to be reassessed•

>There will be no change in the Industry prioritization

>•Investments the firm makes to drive growth should be split equally between the 3 alliances

>•Understanding performance by Clients, and understanding alliance performance relative to Firm performance will provide further insights

>

>Additionally, your supervisor has learnt that leadership feels large size deals will be the driver of growth going forward, which she is unsure of. She would like to understand this better and wants you to include this is your narrative

I just want to know the idea on how to begin with? what should be my approach and how can I make a breakthrough.

Data Set is given in the comment",8,7,1735203619.0,analytics
1hmw11f,Need ideas on how to start freelancing in the data analytics field. ,"Have been working for 2+ years in the data analytics field, have experience in snowflake, data visualization (power bi and excel) and power automate. Don't have a lot of hands on experience in scripting (Python Or R). If anyone has any freelance experience in the analytics field, would like to know where and how to start, whether I need to be an absolute master of the skills. Any kind of details is much appreciated.",0,3,1735241720.0,analytics
1hmmsbo,Can i get a Data science if i had completed Bcom,As the title suggest im a bcom computer applications graduate and im about to join in an institution which provides me with data science course. I know data science is a more technical job but im keen intrested in getting into IT and can learn coding as i have intrest in that too but not more very advanced level coding what would be my carrer options also ,0,4,1735213649.0,analytics
1hmhjno,Advice needed: Remote/Hybrid Courses/Diploma/MSc in Health Data Science or AI in Healthcare/Medicine Upskill,"
I am ICU Specialist in UK and have no coding background but happy to spend time in it to upskill. Of late, contemplating this as an additional knowledge to work in it as a part-time (maybe divulging into it completely at some point). 
(PS: My job pays good and not looking to match that with added skill)

Can you suggest some or comment on these:

1. Imperial College London – AI in Healthcare and Medicine (Online Short Course)
Duration: 8 weeks, part-time.

2. University of Oxford – Artificial Intelligence in Health and Care
Duration: 6 weeks, part-time.

3. Health Education England (HEE) – AI for Healthcare (Specialized Programs)
Duration: Varies (short courses and workshops).

4. King’s College London – Digital Health and Machine Learning in Healthcare
Duration: Flexible learning options (short courses or part of MSc programs).

5. FutureLearn – AI for Healthcare (Multiple Courses)
Duration: 2–8 weeks per course.

6. NVIDIA Deep Learning Institute (DLI)
Mode: Online, with certificates available.

7. University of Manchester – Health Data Science (Online MSc)
Duration: Part-time (2–5 years)",0,3,1735190127.0,analytics
1hm8rjb,Is google trends accurate?,"I know Google is a big “trusted” company but when I was comparing shows on the tool I have insider knowledge that one show is more popular than the other yet trends showed the other show had more searches…so I ask…

Is google trends accurate?",3,4,1735159920.0,analytics
1hlpr7n,Is it normal to constantly work past 5:30pm?,"Landed my first analytics job a few months ago and I’m having a really tough time not only getting stuff done, but understanding the business. There are many concepts I’m just not understanding and it’s affecting my work and it’s not fair to my coworkers as well because I try to do stuff, but I end up constantly asking them for help. When I do go to them, they’re so busy they either respond late or just tell me to study the topic myself. The problem is when I do, I still don’t understand some of the concepts. 

I’ve only been here for a couple of months so I don’t know if it’s something I’ll start to understand over time but I’m feeling very overwhelmed and am missing due dates on some projects. It’s gotten to the point where I’m trying to catch up way past 5:30pm on stuff and it’s stressing me out a ton. Any advice?",42,40,1735085251.0,analytics
1hlvahc,Do you have any colleagues who were hired based on nepotism and how did they work out?,"I've seen several people hired from the same university the manager was from, and they performed well. On the other hand I've seen a couple of hires that got their analytics position due to being related to a senior coworker. Not surprising they were not qualified. It feels bad to see all the people struggling to find analytics jobs right now while these nepotism hires get a free ride. What are your thoughts?",7,6,1735107855.0,analytics
1hliq2d,AI and Data Analysts layoffs ,"Hey everyone, has anyone noticed layoffs in data analyst roles due to AI advancements? Just curious if it's affecting the industry and how people are adapting. Drop your thoughts!",58,46,1735062706.0,analytics
1hlmneb,2025 forcast...,"After much debate and self reflection, as well as asking questions on this subreddit, I decided to begin my data analysis journey (starting with an IBM coursera cert and focusing on Excel and SQL first). 

From a novice to the more seasoned data analyst professionals, how does 2025 look like for new analysts? I have heard of the oversaturation and of course the tech layoffs (I hope to try to be an analyst in a different industry that isn't social media or tech), but I still wish to push forward, network, and hopefully find a mentor (feel free to message me!). Is data analysis still worth the uphill battle in your opinion?

Thank you! 

Merry Xmas and happy near year to all of you!",23,17,1735074767.0,analytics
1hm262r,Actual work happening in Data Science roles in India,"I'm working towards learning and building my Data Science portfolio. I want to know what kind of work actually happens in companies for Data Analyst and Data Scientist roles. I've completed a one year course from GL and now using udemy to brush up on my skills. However I find the course content to be very similar. I lot of posts also mention working on building models which are more or less limited to around 7-8 models universally used plus visualization which is also just tableau, power bi and couple of other tools. 
Is this actually the way jobs are in companies? Am I missing something specific (other than stakeholder management) regarding the job roles which have to be learnt if i have to excel in a data scientist role?",0,7,1735138749.0,analytics
1hl2hpf,Interested in Data Analytics -- What would you advise a total newbie?,"Hello everyone, as the title suggests, I am interested in becoming a Data Analyst. I'm currently a first year at SJSU. I do not have any coding experience, but I am actively doing more research about this field and searching for good skills/certifications to achieve. Please feel free to offer any advice you may have. 

\- What exactly does a Data Analyst do?

\- If you could go back and start it all over again, what would be the first 3 steps you would take?

\- What certifications do you recommend?

\- What type of coding should I start out with?

Thank you so much! :)",18,40,1735003283.0,analytics
1hkzt8p,NLP folks - What tools do you generally use? ,"Familiar with NLP basics but new to deep learning. Data science folks that work on NLP projects, what models do you generally use? RNN’s? LSTMs? If it’s not too much, could you go over what your NLP projects look like?",9,13,1734994880.0,analytics
1hl8l6f,Need help,Can anyone suggest me a good certification course in data science and analytics? (Beginner here!!) ,0,9,1735025746.0,analytics
1hkzphs,Platform for DS practice problems,"What’s a good platform to practice DS problems? Aiming for more so traditional data science roles with model development/NLP focus. Somewhere I can practice data science python problems (pandas, functions, loops etc). Ive used hacker rank for SQL questions and have loved doing those. I’ve never leet-coded but have heard those are more so focused on SWE roles. ",2,2,1734994587.0,analytics
1hl7rko,I built a tool to save hours on data visualization,"Hello everyone,

I have been working on an AI tool which generates good visualization and gives quick insights on your CSV file. This process happens in less than a minute thus saving the process of preparing your datasets and planning the visualizations. 

What do you think about such tool? Would you like to use it?",0,19,1735022312.0,analytics
1hkmzmf,I'm seeking guidance from anyone who is a Business Analyst or studying Business Analytics.,"

What is the scope of business analytics, and can you recommend the best online sources to learn this skill? I’m a business graduate; how much time will it take to become proficient in this field?

Given my lack of prior knowledge and technical expertise in Excel and other tools, should I pursue this skill?",6,14,1734957888.0,analytics
1hkl34z,Experience using Adobe Target,"Hey folks, my company is looking at integrating a platform for A/B testing, has anyone here used Adobe Target and if so could I get some info from you about what it was like to use and integrate?

The other option on the table is an in-house solution, but we're not obliged to use it. It's a kind of a ""how long is a piece of string"" question but if anyone has an experience of using an in-house product vs Adobe Target that would also be really helpful. 

Thanks",5,7,1734950101.0,analytics
1hkqkw4,"If you want data cleaning automated, would you prefer using SDK (python api) or web platform?","Hey folks,

Dev here

Working on data cleaning platform, automating data cleaning & mapping using LLMs

If you have this platform, would you prefer to have it as API (for example, python SDK) or web platform (when you can connect your db/upload csv, explore and iteratively process the data)?",0,11,1734969057.0,analytics
1hkgert,Can you help me planning my 6 months before the start of my MSBA degree at UW?,"I recently got admission from UW MSBA, and have 6 months before the start of the program. I want to know what should I prepare to maximize my productivity and skills within the 6 months before the start of the program.  
I have no coding experience and would like to know the details of where should I start.

Thank you so much!",6,13,1734929524.0,analytics
1hkkngg,FBCLID can't be decrypted right ?,"So i'm helping a startup capture conversion rates on their webpage. i've asked them to add utm params to their ads if they want those numbers. but my employer is telling me that i should use fbclid to decrypt campaign and ad info. he thinks facebook passes campaign info through the fbclid cookie. 

i'm of the understanding that fbclid is just a one way hash and acts as an index for facebook to identify clicks in their db. am i wrong

and if i'm not wrong how do i put this through to my guy",1,3,1734948112.0,analytics
1hk5d1m,Need Help Choosing a Data Analysis Course to Land My First Internship ,"I’m currently doing my master’s in economics and really want to break into data analysis. The thing is, I know there are a ton of courses out there, but I’m struggling to figure out which ones are actually worth it. I don’t just want to learn theory—I need something practical that’ll help me land my first internship and stand out in the hiring process.  

I’m looking for courses that:  
- Teach essential tools like Python, SQL, or R.  
- Include hands-on projects that I can add to my portfolio.  
- (Bonus) Offer certification that recruiters actually care about.  

If any of you have taken a course that genuinely helped you get your foot in the door or build your skills, please share! Whether it’s on Coursera, Udemy, or something more niche, I’d love to hear your recommendations.  

Thanks a ton in advance!  ",10,23,1734894753.0,analytics
1hjxy8n,How to deal with information Overload to stay focused for landing a data job in this market? ,"Hello, initially this was supposed to be a Mavens Analytics vs Data Camp vs edX question but then I decided to generalize it. My issue is I’ve been coming across so much resources that it’s so hard to pick one, then I just end up doing nothing. Or as soon as I start one thing, another thing pops up and always seems more in line with what I want to do. This has affected me in so many ways but I don’t want to bore y’all with the details. 

Lately I’ve been landing several interviews and made it to final rounds for a few, but they always go for the more senior person or for whatever reason just say they found someone else they preferred. Companies now are being very specific with what they want in a candidate. 

Note: I don’t want judgement. I want advice. I have 12yrs professional working experience. The most years I’ve spent were in home healthcare and in retail pharmacy. My last role was a BA/BSA which lasted a few months. I have a bachelors in health sciences, an MBA in MIS & currently in Georgia Tech’s MSc in Analytics. Don’t ask me why I’m pursuing another masters. And no I’m not just qualified for an entry level position like some of y’all have been saying in previous discussions. I’m aiming for Mid level roles MINIMUM where either my BSc, MBA or both can be utilized. Thank you!!!!",16,17,1734872069.0,analytics
1hkaccu,Customer profiling tips please,"Wanting to profile customers.
 The data I have right now is sales data, emails, addresses and UK housing transaction price data.

So far for each email I’ve calculated:
- number of purchases
- AOV
- months since last purchase
- last product purchased category 
- Item style purchased
- A list aggregate of all styles purchased
- Type of home they live in (flat, terraced etc)
- Last known sale price of that home with a linear regression on the county to then extrapolate its value in 2024 prices. **
- the 2024 house price / 4.5 to get a estimated household income

**When someone purchases multiple items, I take the last known items shipping address and then join that to my housing table for the housing info. 

 Spent a few hours on this but I am now at a loss on what to do to take this to next level. The idea would be to use this table for marketing efforts and to improve our email marketing campaigns. Any tips or publicly available datasets which could enchants my profiling would be appreciated.",1,4,1734909169.0,analytics
1hjncm2,Data Analysts: Do you use Linear Regression/other regression much in your work?,"Hey all,

  
Just looking for a sense of how often y'all are using any type of linear regression/other regressions in your work?

  
I ask because it is often cited as something important for Data Analysts to know about, but due to it being used predictively most often, it seems to be more in the real of Data Science? Given that this is often this separation between analysts/scientists... ",57,56,1734827990.0,analytics
1hk3tg1,"When do people typically take out loans? At what time of the month do they usually do so, and in which month are the most loans taken? ",I'm trying to determine the best timing for my affiliate marketing efforts by understanding when people are most in need of affordable small loans and cash advances.,0,7,1734890341.0,analytics
1hiygjk,"In one sentence, how do you describe your job to strangers?",You meet someone and they ask you what you do. What do you say?,25,66,1734742539.0,analytics
1hiuarm,Feeling burned out with data analytics,"As the title says I am feeling really burnt out within the field of data analytic. I have been working in the field for over 4 years now but it seems to have drained me that I don’t want to do it anymore. Please advise to other possible fields to get into, I am really looking for a career change without having to go back to school. 
I am well paid in my current role, in the lower 100s so I am looking for another high paying field as well. 
Any advice will be appreciated. 

Thanks ",42,62,1734730261.0,analytics
1hj796g,Rejections from BA roles. Need advice to get interviews.,"
Hi. I am a recent graduate with masters degree and decent professional experience in data analyst roles(approx 1 year) I also have good portfolio websites with data analytics projects. I have been applying to data analyst and business analyst positions. However I straight up get rejected from all the BA roles. I understand I don’t have academic background as in business analysis, but I want to understand what am I lacking here? What can I do to make my profile better suited for BA roles?  As for skills, I do have the technical skills, may be I lack business knowledge. But there must be a way to crack it. People currently working in the industry, help me out here - what would it take for a candidate like me to get into entry level BA role?
I can DM my resume if needed reference, thank you in advance:)",3,15,1734777121.0,analytics
1hi6b3m,Employer is paying for my Master’s Degree,"I’m a business major with a minor in business analytics and information systems. After a long and grueling job hunt, I landed a decent gig at a huge finance firm. Still wanting to pursue Data Analytics, what would be the best pick? I’m between Information Technology, Statistics, or just a regular MBA",92,53,1734651148.0,analytics
1him1cu,Career/Job Advice Question,"Hello,

I am posting this to get some perspective on possible avenues I can explore after graduation. I am currently in an MS in business analytics program and have an MS in clinical psychology with six years of clinical experience. I decided to pursue a business analytics degree because I am interested in how impactful data and metric-driven data are on individuals and companies. I am unsure how I could possibly incorporate my psychology experience in my future in business analytics. Do you have any recommendations on positions or avenues to explore?",3,13,1734708018.0,analytics
1hiami6,Pharmacy Data Analytics,Hey I'm senior about to graduate soon. I'm curious how to get into Health Data Analytics or Pharmacy. I'm about to take tableau desktop certification and thinking about doing Comptia cert as well with data analytics. Any suggestions is SalesForce or AWS cert is worth doing? or am I doing too much.,19,22,1734664709.0,analytics
1hip9ce,The data in this preview has been truncated.,"https://preview.redd.it/j6cdhz78i18e1.png?width=1919&format=png&auto=webp&s=ed337ab37aadf89809392d417ae715d216903fde

Hi guys, have been battling with this for a week now, please how can I resolve this issues?  
I tried with ChatGPT, everything it said I did, yet it won't work. Please help.",1,19,1734716646.0,analytics
1hhw27w,Does a data analyst need to know about prepared statements in SQL?,"I'm learning SQL as prep for my upcoming job (switching from SWE to DA).

I learned about how to pivot a table in MySQL. But it's sooooo clunky....

In order to pivot you need to use CASE WHEN statements. So I looked up if there's a different way. Turns out, there is!

By using prepared statements, GROUP\_CONCAT, CONCAT and a variable.

This makes me wonder, do data analysts need to know about prepared statements or did I go too far in my prep?

",34,33,1734624024.0,analytics
1hi0dtb,Highest Value Certification/EDU?,"I am looking for some help as my boss let me know they will pay for just about any additional education/certifications I am seeking, but they won’t pay for my masters. What should I look into?

Background: I am 2 years in as an Account Analyst in the Automotive Aftermarket industry. 

I learn better in person but open to online programs/courses if the value of it is worth it. 

I could use a refresher in BI but want to see what is out there to get the most bang for my (company’s) buck. ",12,8,1734635126.0,analytics
1hhzv1n,Course Recommendations?,"I’m wanting to keep my skills sharp while I’m job hunting because it’s taking forever. Unfortunately my current job doesn’t have any opportunities for me to use my data skills so I was thinking about taking some online courses. Anyone have recommendations? Udemy, Coursera, other? Or am I better off doing small projects and keeping them on GitHub? In particular I’d like to focus on SQL, Python, and PowerBI, but I’m not opposed to other suggestions!",7,10,1734633812.0,analytics
1hhyjbw,Master's in Data Science,"I recently obtained my BS in Electrical Engineering. I work in a manufacturing support role and am looking to further my education. I find myself liking using test data to analyze trends, see where we can improve in our process, and predict failures using existing data. I want to further my knowledge on methods I can use to help support the efficiency of production and testing of our product.

With this said, would y'all recommend me getting a Master's in Data Science? If not, is there something better that you might suggest I take a look at?",8,13,1734630429.0,analytics
1hhw29e,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",9,20,1734624027.0,analytics
1hib7s9,What should my job title be?,"**Here are my tasks:**

• Conceptualize, implement and build data models from ground up in both SQL & Power BI

• Automate various tasks via python

• Manage a team of outsourced software engineers to automate more tasks

• Process improvement projects

• Work with data engineering to have data show up in database / data lake ",1,8,1734666710.0,analytics
1hh56t7,Is it reasonable of my bosses to expect us to be data analyst and an economist? Unsure of what to learn anymore ,"For some context, my current team is very small and my daily work unfortunately involves churning adhoc data requests internal stakeholders than data projects. When i mean data projects, i refer to dashboards and playing around with data on a specific topic.

Lately, my bosses also expect us to do econometric modelling but they are not trained ij economics. I have undergraduate background in economics but I feel that this is always insufficient as many theoretical stuff are only taught in graduate school — as confirmed by my teammate who has graduate school knowledge in economics.

On a related note, my teammate also have extensive knowledge in programming and database including creating test suites, reading SQL scripts and API calling. All these were not part of my job scope and job description at all. Worst part is I have zero clue on how to begin them. 

So now I'm wondering, 
1. Is it reasonable for my bosses to expect us to do data projects, do research and/or econometrics project and do adhoc data requests with just the two of us? 
2. How can I improve my knowledge in econometrics (I use R) without graduate school? It's too expensive for me and my company cannot sponsor me. 
3. Should I be worried my teammate is clearly more qualified than me? The issue here is all these value-add they bring in were not what I was expected to do. Half the time i feel like an imposter with no clue on what's out there.
4. How can I improve my data analytics skills, e.g., using SQL in the real world, web scrapping, API etc? 
",34,38,1734538910.0,analytics
1hhjl86,Am I being asked to do too much?,"I’m sitting just a little over 60k (~60-61k) and I’m also at the lowest grade role that the company offers. Still an analyst, but there’s other analyst roles, admin, reporting, etc. that are higher grades.

I’m building out an org chart model. I need to be able to

- build an org chart from our server (doing through PowerBI, data from sql)

- compare that org chart to a should-be chart

- there’s multiple charts for different operating segments, and they all have different nuances and numbers

So…here’s a few “overrides” I’m doing to that allow us to make changes to our chart will still ingesting data directly from source:

- need “manual overrides” for employees whose managers are incorrect (fixing the source data is not an option because another incorrect record will show up before we can fix the first one…cycle repeats)

- need manual overrides for certain jobs, in the event a “standard job” shows up as multiple titles, codes, senior vs non-senior

- standard structures (things that don’t change like every operating division will have 1 accountant)

- changing structures (things that might be based on number of sales locations in a given area, ingested from another table)

- overrides (things that vary by region and don’t follow the “rules” outlined above)

Then I need to build out position_ids for these charts, which is complicated enough but I’m basically just planning to take “how many of this role are there” and index it the unique role so we can have (e.g.) salesperson-1, salesperson-2, and then I would need to index the actual chart and join the employees to the should-be chart

My actual big concern in all this is that it’s getting hairy. At first I thought it would be pretty simple conceptually, and it was, but now I’m adding a lot of levers to a tool that I may not even own long term. I’m thinking about how I might have to make a “data dictionary” to explain this thing and it just concerns me at the thought of someone else having to own this and figure out what the f*ck this thing is. It’s going to be used by non-data stakeholders so I need it to be at least moderately user friendly.

My other issues mainly have to do with the fact that it feels like I’m basically building out an application/software/tool. This isn’t just analysis, it’s not just pulling data, it’s not even some of the cool data modeling I did/have done as an intern. It’s messy, but I can’t figure out a more robust way to do this with all the “it should work this way but there might be some exceptions.” Does this at least sound like it matches my pay? I can’t tell if this is a lot to ask for me or not, I can see the whole thing in my head because I’m good at this stuff, but that makes it harder for me to gauge if this is something an entry level analyst with a good head on their shoulders might be able to do vs something that you might be pushing your luck for.

I know 60 is low, but the company isn’t reputed for their crazy salaries. 60 is low for my company though for an analyst role, but it’s the same “low” as another company’s low by relativity’. 60 is slightly above median for my pay grade. Anyways, So like I know I’m underpaid but I’m just struggling to gauge if this genuinely is a pretty cumbersome ask, or if it’s something you’d expect a fresher to be able to figure out by themselves. Frustrated because I feel like I’m setup to fail at something by being asked too much, I’m not allowed to consult with other analysts/data scientists because it’s confidential work.",4,22,1734578470.0,analytics
1hhq779,Is this a possible tracking issue?,"Greetings,

I have a big drop in my data, a frequency of a certain event that is triggered in distinct users... Now I notice a few weird things in the specific drop. 

* It's all browser data
* it's all Google Chrome
* it's all a specific browser version
* It's all from one city (Amsterdam)

What could this generally indicate and how would you generally perform follow up action to get a deeper idea of the cause? 

This data is in BigQuery. ",1,3,1734604831.0,analytics
1hhd1x5,how to do less,"How do you decide what NOT to do on your product roadmap?

I’ve been thinking a lot about how ambition can derail teams. Every new feature we add isn’t just more work—it’s more complexity: dependencies, testing, and the risk of things going sideways. Instead of delivering value, we end up managing chaos.

Take Google+ as an example. It tried to be Facebook, Twitter, and LinkedIn all at once. The result? A product that didn’t excel at anything and confused its users. Imagine if they’d focused on solving just *one* problem well—would the outcome have been different?

I’m curious how others here handle this. How do you make the tough calls to prioritize one big initiative over everything else? What’s your approach to saying “no” without killing momentum?",6,7,1734559302.0,analytics
1hhc9j6,What KPIs should I be measuring for a free-to-use productivity (time-keeping) mobile app? Is this a good retention rate?,"I launched an app this past year and it's sitting around 50% for 30 Day Retention and 29% for 90 Day. I'm seeing different benchmarks posted online, some say the average 30-day rate ranges from 27% to 43% and can be as high as 32% to 66% for high performing apps. Is this accurate? I'm seeing wide variation on the numbers listed (I know it's kind of dependent on industry) I think Todoist would be a good company to measure myself against but they don't publicly disclose those numbers. So it seems the app is doing well? But I am noticing user engagement is low, most users are only using the app a few times a month, on average <10 times / month.

Are there any other KPIs I should be paying attention to? I currently only have access to a report that only shows me who logged into the app in the past month and how many times (no timestamps or dates on their logins, just an aggregated number).

",4,4,1734557232.0,analytics
1hhluw4,"Hi, Can anyone suggest some good countries where analytics jobs are relevant, where market is good. I am thinking for Nov'25. I am thinking Newzealand. Any suggestions are welcome. 🙏 ","Rn I am a fresher getting difficult to find any relevant jobs, I am thinking to get some experience by end of 2025. And then go for masters outside for better exposure. Thanks ",0,9,1734586317.0,analytics
1hguv7e,what was your first data analyst career and how did you manage your fear and anxiety as entry position?,I'm just curious how life is going to be like as I go into data analyst jobs if I WFH or Hybrid. Did you manage your time well? Did you use mostly Zoom and Microsoft Teams as projects meetings? just curious as I graduate next year. Worried about laying off?,16,20,1734500186.0,analytics
1hh97dl,Transformation Step Involving Text Change - Best Practice?,"I have a dashboard, and the data is being extracted using a custom SQL query. There's a column that the business wants modified based on text, example:

Values of DBP-1, DBP-2, DBP-97, should all be trimmed to DBP. ATRP-2, ATRP-7 should all be trimmed to ATRP, etc.

My question is, what is the best practice for where this change should take place? Should I adjust the custom SQL to pull it in trimmed, or should I pull in the full data and make a calculated column in the dashboard to handle it?",1,3,1734549290.0,analytics
1hgzdyg,Requesting Laptop Recommendation for Data Analytics and Data Science (ocassional photo/video editing) folks.,"My budget is 1k to 2k USD. What's the best VALUE for money? 
I'm okay with both windows and mac (I'm leaning towards mac this time as they provide the best overall experience).

If I opt for mac should I choose MBA M3 15"" (16gb + 512gb) for 1300 USD or MBP M4 Pro 14"" (24gb + 512gb) for 1800 USD considering the additional benefits and longevity?

Your honest suggestions will be sincerely appreciated. 

Cheers guys.",4,34,1734520364.0,analytics
1hgcoit,"As an experienced data analyst, what are some of your best practices? ","Over the years of working in this field, what are some of the best practices (1) you think every data analyst should observe, and (2) you would have done in the beginning of your career in your first work (if you could go back in time)? ",111,39,1734448214.0,analytics
1hgy2wj,IB and PE role!?,"As a new investment analyst trainee with a role that combines investment banking and private equity, what daily tasks should I expect? 
Since I'm more interested in pursuing a career in investment banking, what specific responsibilities should I look for to ensure that I am aligned with the right domain?

Thanks 
",1,2,1734514365.0,analytics
1hgxy81,Help me pick my MSBA University,"Looking for help in deciding where to attend for a 16-month in-person MSBA program. I have been awarded merit scholarship at all of these schools. Please provide insight on any experiences or knowledge about these programs.

* Northeastern (D'Amore-McKim)
* Babson College (Olin School of Business)
* Brandeis University (International School of Business)
* Bentley University (McCallum School of Business)",0,6,1734513727.0,analytics
1hghjmu,ship faster = ship better,"Hey, I write a blog on product analytics (why number go up) and was curious to get feedback from some fellow analysts. Does this resonate with your experience? 

**the perfection illusion**

Have you fallen into analysis paralysis in hopes of finding the perfect answer? Endless dashboards, pristine PRDs, and perfectly aligned roadmaps can feel like progress but they’re often just distractions. You don’t learn about user pain by sitting in meetings or refining models. You only get there by shipping.

The longer you wait, the further you drift from reality.

**plans fail, products evolve**

No plan survives contact with the real world. Here’s the hard truth: No matter how much you analyze, you will never predict exactly what users want. Take Slack. It started as an internal communication tool for a game studio that failed. What they thought was the perfect plan for a game became irrelevant. By shipping fast and pivoting, they built a communication product millions now rely on.

Iteration always wins because user behavior is complex and assumptions break under real-world conditions.

**why shipping wins**

*Validate your assumptions*

Every product decision you make is a guess until users validate it. Shipping quickly gets those guesses into the wild and allows you to measure their impact. Analysis might help prioritize what to build, but only feedback tells you if it works.

Example: A team spends months improving a sophisticated search algorithm based on internal debates and assumptions. After launch they realize users don’t want improved search, they are looking for better content. If they had shipped improvement incrementally, they would may have seen this in their metrics sooner.

*Bet small to win big*

Shipping quickly isn’t about cutting corners; it’s about reducing risk. Smaller, faster releases help you make “small bets” instead of doubling down on a single, high-stakes feature. Small bets let you adapt to what works. Jeff Bezos calls this “two-way doors.” Small decisions can easily be reversed or improved. Ship them, learn, and iterate.

*Speed is good for morale*

Teams that ship quickly build momentum. They’re learning constantly, compounding improvements over time. When speed is prioritized, every small improvement adds up to better products and stronger teams. Teams chasing the perfect launch move slowly, get frustrated, and second-guess their (likely good) intuitions.

**how to ship faster**

1. Think small - Break large projects into atomic components that can validate hypotheses.

2. Stop chasing complexity - Prioritize simple projects that solve for a known pain point over complex projects that solve a suspected one.

3. Shipping as a metric - In the same vein of Elon's ""what did you get done this week"", anchor your team on readily measurable indicators of throughput and celebrate wins.

Shipping fast doesn’t mean cutting corners. It means getting real, messy data from the only people who matter: your users. You’ll never find the perfect product through analysis alone. You can only iterate your way there and speed is what makes iteration possible.

**tl;dr**

Stop overthinking. Start shipping. Iterate faster, learn faster, and you’ll build better products faster.",8,6,1734460916.0,analytics
1hgtqpf,Need Advice: Applying for a Business Analytics Internship Without Experience,"Hi everyone,

I’m starting my second year in a Master of Business Analytics program and planning to apply for a university placement next semester. The challenge is that I don’t have any experience in analytics yet—my bachelor’s degree is in a completely different field, and I’ve never worked in analytics before.

I know it’s okay to not have much experience as a student, but I expect the competition will be tough, and I really want to make my application stand out enough to land an interview. What do you recommend for someone in my position?

I’ve read that showcasing projects can help demonstrate your skills even without formal experience, and I have about a month until the application deadline. Are there any specific tips to do that?

Here are three sample placements I’m considering to give you a better idea of what’s expected:

# Placement 1

**Key Tasks**:

* Collect and organize data from various sources.
* Understand how data supports business functions like memberships, marketing, and partnerships.
* Perform ad hoc analysis and extract actionable insights.
* Present findings to internal stakeholders.

**Selection Criteria**:

* Passion for AFL and the sports industry.
* Strong analytical and data management skills.
* Intermediate to advanced Excel skills.
* Excellent communication and ability to present insights.
* Bonus: Experience with SQL, databases, Tableau, or Power BI (not essential).

# Placement 2

**Key Tasks**:

* Analyze and organize internal and external data.
* Learn how data is applied to business operations.
* Provide insights through reports and visualizations.
* Present findings to stakeholders as needed.

**Selection Criteria**:

* Strong analytical, data handling, and Excel skills.
* Clear communication and presentation abilities.
* Proactive attitude and eagerness to learn.
* Bonus: Familiarity with SQL, databases, and BI tools like Tableau or Power BI.

# Placement 3

**Key Tasks**:

* Collect and analyze data to generate actionable insights.
* Support business functions by applying data solutions.
* Create reports and dashboards for stakeholders.

**Selection Criteria**:

* Analytical mindset with data and Excel proficiency.
* Great communication and problem-solving skills.
* Proactive and self-motivated.
* Bonus: Experience with SQL, Tableau, Power BI, or databases.",0,1,1734496060.0,analytics
1hg3t42,DAE gets worried about the oversimplification of Data analysis?,"As the title says, lately I feel like becoming a data analyst is being treated as a ""get rich quick"" scheme, and honestly, it really concerns me. Let me explain why.

First of all, let me preface this by saying that I don’t think this is the hardest career to get into. Heck, it probably wouldn’t even crack the top 10 of hardest career paths,nor do I think it should. I genuinely believe everyone should be able to earn a decent, livable wage without having to study for 10+ years (Kudos to the ones who do tho).

That said, my main concern is how oversimplified data analysis is being portrayed. Everywhere I look, it feels like people are being told they can become a data analyst practically overnight. The number of certifications and bootcamps has exploded in the last years, and there’s no sign of it slowing down. Just Google “data analysis” right now, and I guarantee most of the top results will be courses promising to turn you into a data analyst in three months, one month, or even just a couple of weeks.

It honestly breaks my heart to see people signing up for these courses, because I really don’t think they’ll get what they need to actually become data analysts. Instead, they’ll probably just end up poorer and more frustrated. Heck, in a one-month certification, you might not even get a proper understanding of the difference between measures and calculated columns.

So, what do you folks think about this? I know we could just laugh it off, but I hate seeing people get scammed out of their money and watching my career path get devalued in the process.",31,45,1734413721.0,analytics
1hg2qou,Is a job in data science possible with a degree in Info Systems & Tech? Masters?,"Hi all. Hope all is well. I am in my 4th year with a degree in Information Systems & Technology with a concentration in Analytics. I am recently interested with pursuing a career in Data Science in the future but was not sure if that was even possible. After doing some research it seems math/stats, CS, or pure Data Science would be a better suited degree choice for an aspriation of a Data Scientist. The reason why I am not enrolled in a CS or more statistical/math based program is due to switching from another degree (a CS wouldve been impossible time & financial wise). So, I am graduating with my current and was also wondering what other jobs would possible aside from Analyst, Business Intelligence, and Cyber/Network Admin positions. I currently have experience working hands on with Data Analysis using Python, R, and SQL as well as a decent portfolio. However, im not sure if that would even cut it to being close enough to becoming a Data Scientist. ",7,10,1734409877.0,analytics
1hfxtgf,Best way to approach the job search for an entry level data position?,"Hi everyone,

  
Thanks for taking the time to check out my post here. 

  
Little bit about me:

  
Graduated from a target school for business and engineering in the NYC area with a bachelor's degree in Information Systems and management in May of 2023. My coursework consisted of a few Excel classes, an intro to Python class (which was taught extremely poorly), and a SQL class (also taught poorly with a professor we had to report for harassment among other things). 

  
The classes that were the most important to me were made obscenely difficult due to the poor communication, teaching environment, and my mental health. This is supposed to be one of the greatest schools in the area for this kind of thing, and I feel as though I've gotten absolutely no where in my degree. 

  
From December of 2022 to December of 2023, I applied to thousands of jobs, all well within my qualifications, and got no where. Spoke to recruiters, worked with my school, and really tried my best to land anything. 

  
I ended up getting a job as an IAE in sales for a VAR with an incredibly toxic work environment. I took this just to get my out of my retail job and was promised I'd make commission in 3 months, which turned into 6 months, and then a year. Started this place in January of 2024. I figured I'd try out sales to see if I like it, but I want out. 

  
My problem is my skills were never the best and now that it's been almost 2 years since I've graduated, I feel like I'm starting to miss the boat for this kind of thing. This has been absolute hell to sit and go through the application process knowing nothing will get returned. 

  
All I ask is the following:

What skills should I focus on sharpening to better myself as a candidate, and which courses would you recommend taking to sharpen said skills? 

  
What approach should I take to applying to jobs? I've tried recruiters, my school, LinkedIn, and looking for jobs on company websites. 



This is the most dehumanizing experience and I'm feeling defeated at this point. Then my parents get on my case as to why they helped pay for such a phenomenal degree and that I can't do something with it. 

Sorry for venting, this just sucks.

  
Thanks in advance.",12,15,1734394485.0,analytics
1hg58de,Analytics Career Progression,"Data science career progression, go for PGD Business Analytics (IIT B or IIITB), or learn on own

Hi. I am a working professional with 8+ years of experience in analytics (expertise in multiple tools). My_qualifications B.Tech from NIT. I am looking for a job switch as my growth and salary have stagnated, but honestly on various job boards, I have been unable to fetch even an interview. I am seeing lot of job profiles asking for ML (and some AI) and statistics. I have never had a certification in statistics. Higher management roles require MBA/ PGDM. Now I am exploring 3 options:

1. IIT B newly launched ePGD in AI and DS. It has got statistics as well in curriculum. It is an extensive 18 months course but the good part that it is online.

2. Similarly IIIT Bangalore with upGrad is offering Executive PGD in AI and DS. 12 month course, seems to have best reviews among other 12 month programs.

3. Learn stuff online on own and get certifications on Edx, coursera or Udemy. Tutort DS program seems legit too.

Now, first 2 options give me a degree (kind of certification) which I am reading online is not considered as an eligible degree by employers.
I can’t do MBA at this point of time, and I feel I won’t be able to do it later as well due family and MBA programs taking 2 years and so much money.

What is the best way to take my career forward?",3,1,1734419456.0,analytics
1hfkxaf,Mismatching numbers in different dashboards - how much time do you lose on this?  ,"In my company there's far too many dashboards, and one of the problems is that KPIs never match. I am wasting so much time every week on this, so just wondering if this is a common problem in analytics. How is it for you guys? ",43,35,1734361086.0,analytics
1hg6kb0,"Question for Data Analytics and Industry Professionals RE: ""The User Experience"" (From: a user)","So, serious question - do any of you work, or know anyone who works on UIs/Databases/User Experience designs specifically for one of the various streaming giants: Netflix, MAX, Prime, Disney+, Hulu?  


I apologize first-hand if this sounds overly-angry, but this problem has gotten EXPONENTIALLY WORSE over the past 7 years or so, in particular and will only be exacerbated by BULLSHIT AI TRENDS.   


Because I HAVE A NITPICK.   


Why do you make your service impossible to index properly to the average user? Occasionally, I may be able to sort a genre or sub-genre alphabetically, such as with MAX; however, the default sort is ALWAYS a smattered, mish-mash of ""here's shit you're likely to click on"" or ""more recent/promoted"" results...JUST LIKE THAT HORSE-SHIT THAT GOOGLE'S been pulling for the past five years.   
  
Look, I just want to know exactly WHAT IS THERE, and I DO NOT want it appearing in multiple categories thereby increasing the time it takes to scroll through each category, NOR do I want a NEVER ENDING category based on some bullshit, algorithm that tries to generate results on the fly via applied \[TAGS\], popularity, or god KNOWS however other ""hidden metrics,"" that we as users cannot see, nor control. (as is the current trend)  
  
From a design/user-experience standpoint, IT'S A GODDAMNED NIGHTMARE. IT'S FUCKING STUPID.    
  
I realize this may save the labor of having to curate and sanitize your records, BUT IT IS MASSIVELY UNFRIENDLY TO YOUR ENTIRE USERBASE.   
  
Look, here's what I'm talking about. I walk into a Library Today, or a Blockbuster Video a decade ago, titles aren't spontaneously being shuffled from one shelf to the other, aside from NEW RELEASE to OLD SHIT. The STATIC LOCATION of said item helps ME, the USER, keep track of what's available and what's no longer available. It makes BROWSINGS a MILLION TIMES EASIER, not to mention more satisfying.   
  
This is WHY people LOVED GOING TO THE STORE. The sort MADE SENSE, instead of whatever logarithmic HORSE SHIT you software developers are fucking with today.   
  
And it's not just limited to the Streaming services. This is what Amazon does, this is what STEAM does, hell - this is what FACEBOOK has been doing with your posts, instead of just giving a ""TRULY"" chronological timeline of your contacts.   
  
I   
DO  
NOT  
NEED/WANT    
YOU   
MAKING   
DECISIONS   
FOR   
ME   
  
FUCKING FIX IT. The amount of autonomy that's been STOLEN from users by the modern data informatics and analytics paradigm because of current data trends is FUCKING CRIMINAL.   
  
It intentionally obfuscates ""what is actually going on"" to ANYONE who doesn't have direct access to the individual data-records, and is ethically OPENLY HOSTILE TO THE END USER.   
  
In other words, YOU NEED TO MAKE IT POSSIBLE FOR ME (THE USER and YOUR CUSTOMER) TO DO A ""REAL"" not ""LOGARITHMIC"" search query of your records.   ",0,12,1734425589.0,analytics
1hg34fu,Data analytics,"Hey! I want to develop skills essential for data analytics, what skills I should start working on? Let me know best platform for that",0,15,1734411256.0,analytics
1hfwj6x,Examining the residuals when conducting a negative binomial regression,"Hey, what do I look for when looking at the residuals for a binomial regression? No matter what transformation I do, the residuals vs expected values are heteroscedastic. Is this supposed to be be this way for negative binomial regression?  What other assumptions of this model must I test?  I'm really struggling here so any help I much appreciated",1,4,1734390882.0,analytics
1hfl9il,What’s the first step in the thought process w.r.t analysis?,"Hello folks,
It’s been year since I translated to Data Science and working as a data scientist. This question is basically to learn how to go about dynamics of analysis. How do you all go about analysis? What’s the 1st step in the thought process of analysis to final model building you take? How do you decipher some aspects in the initial steps which translates to a model that effectively answers business questions and needs?

I still finding my feet in Data Science world. I switched my career from dentistry to Data Science. LOOKING FORWARD TO LEARNING! Thank you.",2,2,1734361997.0,analytics
1hf84z6,How can I effectively show employers my data projects?,"I'm currently working on a few personal data projects to show employers that I am actually capable of the skills which are listed on my resume (I'm a senior in university with no industry experience). I'm wondering if anyone has good advice on how to best display these projects so that employers don't just think I'm another person essentially copying and pasting code. 

  
One idea that I've had includes the links on my resume to some app or website like Kaggle or GitHub where I would have: a summary of the project (goals, outcomes etc.), the code that I've written, visualizations, a PowerPoint presentation, and step by step breakdowns of my thought processes throughout the project sprinkled intermittently. Essentially a more professional blog post of sorts. I think this way I could really touch on all of the phases of the project and properly display all of the work that went into it. The project would involve python (web scraping), SQL, and a visualization tool, and possibly some implementation of cloud services/databases. 

Does anyone have any recommendations? Is what I'm looking to do even necessary or effective? ",10,14,1734313405.0,analytics
1hfcm8k,Getting Masters in Information Systems and Analytics. Where Do I Go From There?,"So I'm currently enrolled in a master's program in information systems and business analytics and slated to graduate 2025. I'm also a senior financial analyst at a mid tier hospital system. I mainly do excel based data analysis, as requested by my supervisor, basically so that my work is easier to pass on if I leave (or get canned). For a few years, the forecast for the clinical side of the hospital system all hinged on my spreadsheets... we've since moved onto using a big name company cloud based application. As far as day-to-day goes I do forecasting, budgeting, and budget system admin for end users in smaller departments. I have a bachelors in finance, 7 years of experience, and make 95k a year.

In school, I have a decent handle on SQL and getting better with Python. Worked with powerbi, azure machine Tableau. And NLP, and tableau.

If I just applied to all IT and data jobs as much as possible, what type of jobs could I reasonably expect to get callbacks for that would pay more than what I make right now? I have enjoyed pretty much everything I have studied so far with cybersecurity being the least (but still fun). I ask this because I see the job market is tough right now and worry I won't be able to break in without taking more than just a haircut to my salary.",2,1,1734328620.0,analytics
1hfbdri,Australia - Post Grad options - DS/DA,"Hey all, got a few offers to study a Master of Data Science/Analytics online across a few universities here in Australia, but I’m having a bit of a hard time deciding on who to go with as I value the quality of education over the prestigiousness of the institute. So far, I have offers from UNSW (M Analytics) and UniSQ (M Data Science), whilst still waiting on a response from Griffith (M Data Science) and UNE (M Data Science). Can anyone speak of their experience with these degrees, or can comment on their learning experience at these institutes? Cheers!",2,1,1734324133.0,analytics
1henwfn,Data Teams Are a Mess – Thoughts?,"Do you guys ever feel that there’s a lack of structure when it comes to data analytics in companies? One of the biggest challenges I’ve faced is the absence of centralized documentation for all the analysis done—whether it’s SQL queries, Python scripts, or insights from dashboards. It often feels like every analysis exists in isolation, making it hard to revisit past work, collaborate effectively, or even learn from previous projects. This fragmentation not only wastes time but also limits the potential for teams to build on each other’s efforts. Thoughts?",79,29,1734249314.0,analytics
1hez1lc,Sales vs marketing vs analytics?,"
If you are comfortable sharing:
1. What industry and what background did you have?
2. Where were you happiest?
3. What was your pay in each and progression as you aged/advanced? 
4. Looking back, what do you wish you looked into or did differently?


Background- currently in med device. Got in with a great rotational program post grad and got experience in marketing, analytics, education, and now field sales. I’m struggling to find out next steps. Company outlook isn’t super positive, my current role is draining me, and I liked the aspects and lifestyle of marketing but like the idea of more reward in sales. I also see the ortho industry is dying a bit. ",13,21,1734287794.0,analytics
1hfcr9r,Do I need to get my BS/ Masters,"I am already a Pharmacy Data Analyst, and am trying to transition out of healthcare into a more finance related role or FAANG eventually. 

Majority of applications I apply for requires a BS/Master, but I don’t have it. Is it worth it to back to school for these pieces of paper? I have 3YOE already.",0,12,1734329137.0,analytics
1hf3ea6,Starting a new career with data analytics,"I just started going back to school for Computer Information Systems. My main focus is Data Analytics and Networking. Is Programming a good path with Data Analytics vs Computer Science? Quick background, I work in Healthcare and want to apply both discipline to become a Healthcare Information System/Data Analyst. Thank you Guys for any input",4,35,1734299539.0,analytics
1hf8lrk,☎️   Tracking calls after an ad click   ☎️,"UPDATE: This is not about working out stats, it is about assigning a Google Click ID to a person we have in our CRM. That way we know that person responded to a specific ad and we can attribute the conversion correctly.  
  
How can I track a call from a website if someone clicks on a Google Ad, ends up on the landing landing page, but gets distracted. They then disappear for a few days and call directly from the website?

The click would not be attributed to the call and subsequent conversion.

I am using the following platforms just in case that helps:

Google Tag Manager

GA4

High Level

Zapier",1,5,1734314864.0,analytics
1hf7yym,"Are There Any Worthwhile Certifications Concerning Novice Experience in Business Analytics/Data Analytics, Excluding the CBAP?","Hey all, I know this probably gets asked ad-nauseum here but I'm having a hard time getting a straight answer online. I'm just looking to see if there are any well-known certifications (as in, a recruiter for a Business Analyst or Data Analytics role would likely know what the certification is and it could give me a leg-up in the job market) for Business Analytics or Data Analytics. The only one that I could find like that is the CBAP, but I don't have the years of experience for it (3 yrs).

Any comments are appreciated! Thank you!",0,3,1734312863.0,analytics
1hfdice,How AI affect your workflow and actions,I am a data analyst in a gaming App. I just wonder how AI stuffs like chatGPT affect your workflow? eg: you will do some correlation analysis or reporting analysis just based on AI tools nowadays?,0,2,1734332328.0,analytics
1hek1ny,Is econometrics essential for business analysts? ,"I’m considering between two masters. One is informational technology. This includes a bit of everything regarding tech including analytics. The other master’s is strictly analytics which includes econometrics. It also includes prescriptive and predictive analytics (which actually is also offered in the informational technology master’s). 

They both share other classes like R, python, Tableau and such. Oh. And big data.

I am lost. 

",14,37,1734233832.0,analytics
1hegre2,makes decisions (not dashboards),"I started a Substack - whynumbergoup.  I'm only two posts in but would love to hear early feedback!

*It’s a common pitfall for data teams: leadership has a constant stream of questions, and you respond by building a 50-chart dashboard nobody checks. Product wants segmentation? You deliver a sophisticated clustering model that never sees production. Marketing wants acquisition tracking? You perfect a complex attribution model that fails as soon as a new channel appears.*

*Too often, we trade tangible impact for false sophistication. Sometimes these projects even earn praise, fueling a cycle of busywork. You’re working hard, getting recognized, but can’t tie your hours to real results.*

*In a data-saturated, attention-addled world, there’s no end of well-rationalized but low-impact analytics work. The key is to identify what moves the needle—the 20% of effort that yields 80% of the outcomes.*

*The solution? Break the loop. Focus on decisions. Your role is to help the organization make better, faster decisions. Everything else is secondary. If your dashboards, analyses, or models don’t directly inform a next step, pause and rethink.*

*You’ll never build a perfect product, but you can iterate toward one. Analytics truncates the distance between question and decision, letting you learn and improve with each cycle.*",11,7,1734222758.0,analytics
1herlzm,"Hi people, need some help with PayPal interview","Hi all, I'm having the third round of interview for PayPal for the role of Data Analyst - 2. It is in the collections strategy team and it is with a senior manager. In the last two rounds I was asked normal questions, cases, some SQL questions. I think this third round will be the final round. I wanted to know how the third round generally is, what can I expect from the interviewer. If anyone has gone through the process, please shower some knowledge, thanks!",0,7,1734265766.0,analytics
1hf2268,Should I study date analyst? ,"Hi everyone! Can someone tell me a bit about working in data analysis? I’m considering studying it but feeling unsure if it’s the right fit for me. I’m not very strong in math, so I’m worried about whether I’ll be able to handle the subject if I pursue it. Any advice or insights would be greatly appreciated—thanks in advance! #dateanalyst ",0,5,1734295927.0,analytics
1hef40h,Should I enroll myself in university again?,"I want to break into tech. I moved to USA last year and I have masters degree in information technology majoring in data analytics which is an international degree. I’ve worked in different industry and nothing related to my field. After relocation i lost my previous job and now im out of the work for a year. I’ve been applying to jobs in data analytics but had no luck whatsoever. I also tried to apply for different roles like IT tech support, project coordinator, etc. I was able to get 3 interviews out of those but none of them worked out. I’m confused about what do to now. Enrolling in a college would be a better option to get opportunities? 
Any suggestions is welcome 

Edit: I Graduated 4 years ago.",3,6,1734217763.0,analytics
1hdz2sz,Picked up a free Statistics book. Worth the time?,"I’m a college student with about 6-8 weeks off. I’m currently taking Google Data Analyst course and it suggest few books about big data etc. Obviously im gonna try to skim them and learn as much as I can but what about the textbook on STATISTICS: learning from Data? 
Really want to make sure I invest my time wisely, and my GE classes giving me an option to skip Statistics and take Econ for Data Science major. 
How much value can you really get out of studying a textbook on statistics?",28,12,1734166840.0,analytics
1hes3bj,Any recruiter who is willing to hire a data analyst for free please reach out me ,"I am looking for data analytics/ data science opportunities in the US. As the title says, I am willing to work for free. I have around 1 year of experience in data analytics and also a masters in CS. Please reach out to me if you have any relevant opportunities. I am open for internships also.",0,3,1734267602.0,analytics
1he0rap,supply chain to business analyst? how to make resume better w/o business analyst experience? ,"hello. i’m a senior in college, majoring in supply chain. I have experience analyzing supplier lead times, and performing cost and category analysis in excel from internship. i also have  tableau experience through a class project. I would like to be a business analyst but my major did not teach me the hard skills needed for it. SCM is an analytical field, which could maybe help me transition into the field. I would like to get a certificate (not sure which one) and create a portfolio that I can submit alongside my resume as a supplemental. do u think this would benefit in getting interview or making me a more competitive applicant or no? what are some ways i can improve my resume to increase chance of landing business analyst role? Ik there are supply chain analysts, but i still do not have enough technical experience and knowledge for these roles (most require Power BI and SQL, and sometimes coding languages). I’m looking at general business analyst roles and pricing analysts. let me know :) i know it’s unlikely I get a role like this anytime soon, but I wanted to ask ",3,6,1734174559.0,analytics
1hdldpd,Anyone know of a publicly available Grocery Dataset?,"Right now groceries are a hot topic item.  I also know that grocery prices vary depending on where you are located in the US, but figured there might be something out there that has that data to at least get some baseline change over the next few years.

  
I checked Kaggle, but all the datasets were old.

",21,9,1734121430.0,analytics
1hd6j5h,The guy who wanted to take his own life that posted in this sub,"Remember the guy threatened to off himself if he couldn’t get a job in analytics even if he is overqualified. Where is he now?

It’s been a month. Did somebody reported him to suicide prevention?

Even though you’re an asshole to everyone I hope you’re still alive somewhere.",75,21,1734072324.0,analytics
1hdp1pa,Meta Product Growth Analyst technical screening next week! Please help!!,"Hi, I have my technical screening (2nd round of 60mins video call) for Product Growth Analyst at Meta next week. The prep doc mentions it’ll contain SQL + a product sense question. Can somebody please help with what kind of product sense case study questions should I be expecting? Any prep materials or question banks would be super helpful? I’m kind of freaking out as I don’t have much experience with product case studies. I’d appreciate any help! TIA!!",2,4,1734131533.0,analytics
1hczj9b,Hit the 4 year mark as an analyst at my company. Looking for a reality / sanity check as to what is my actual role?,"Hey everyone,

As the title says, I’ve been a data / business analyst on the marketing team at my company for just over four years now and am considering looking for a new role, however the issue is I have no idea what I even truly can call what I do-- I do a bit of everything but feel I'm a ""master of none"" in the eyes of roles beyond what I'm currently in. Including my last position, I have about six years of experience doing various work with SQL and Tableau.

(for the record, I didn't run this through an AI program or anything, so if it reads weirdly, apologies!)

Since starting, I have done (and continue to do) a variety of things:

* Tableau
   * The marketing team had no BI/reporting prior to me, so I got us into the Tableau ecosystem
   * I currently create and manage multiple dashboards for 15-20 people, am the administrator for our Tableau portal, handle all Tableau Prep Flows, and the SQL pipeline used to power the viz layer
   * All imposter syndrome aside, I would say I'm pretty good with Tableau. Various people on my team have said they've never seen somebody do in Tableau what I can do as quickly as I can do them, including using Tableau for live table QA
* SQL and Data
   * I am responsible for a large chunk of financial and virtually all surfaced marketing data at the company. My job is heavily SQL based and touches on many different types of data, from performance marketing to user retention to in-product analytics using log-level Mixpanel data
   * I work with various teams to spec out important metrics, where to source from platform portals, working with devops to develop connections to our database, mapping out how this should look at the table and reporting level, and all QA– from the digital channel to the database to the dashboards
   * Various team members have told me that my SQL readability is ""unparalleled"" and window functions and stuff are pretty easy
* QA
   * Our data is very dirty, so a huge chunk of my time is spent doing QA – unit tests on data, finding discrepancies and putting in tickets with our data team to rectify
* Reporting
   * I’m responsible for putting together all marketing reporting for c-suite and the board of directors
   * Due to a large chunk of our data not existing in the database, and because marketers will never leave gSheets, I also maintain every gSheet report
* Things I wish I had more opportunities to explore:
   * I help surface insights when I can, however given my bandwidth it’s not feasible to do in-depth exploratory analyses for and/or alongside subject matter experts on each team
   * I feel like I’ve been funneled into a role building and maintaining reports, and while I can and do offer insights into how campaigns perform, I don’t have a hands-on look into how our performance marketing is being tweaked by our agencies
   * While I know how to use Python, I haven’t done even a fizzbuzz in years– I mainly use it for very basic API pulls in Databricks but haven’t had much experience with using python for stats
   * My math isn’t the strongest. I have a degree in Econ and haven’t done a linear regression in over a decade at this point, hah

For full transparency, doing the above, I an full time and currently make $133k a year and have had a consistent 20% bonus every year. (Bay Area) 

Thanks in advance for any insights :)",53,24,1734048952.0,analytics
1hdhlyj,Should I Accept McGill University’s MS in Management Analytics Offer or Wait for Other Options?,"I recently received an offer from McGill University for their MS in Management Analytics program, along with a CAD 10,000 scholarship. The tuition fees are CAD 70,000, and with living expenses, the total cost will be around CAD 80,000 (\~₹50–55 lakhs). I have savings of around ₹20 lakhs and will need to take a loan for the remaining amount.

For context, I have five years of experience working in an analytics role in a government organization in India. My long-term goal is to transition to an international organization. I initially applied to McGill because of its reputation and the program's alignment with my career aspirations, but now I’m reconsidering due to the financial burden.

Additionally, I’ve applied to the Rotman School of Management (University of Toronto), and their decision will be released on December 23. McGill requires my decision by December 20.

Here are my questions:

1. **Is McGill’s MS in Management Analytics worth the investment of CAD 80,000?** What is the ROI, especially for international students with prior work experience?
2. **How does McGill compare to Rotman for analytics-focused programs and job prospects in Canada?**
3. **Does McGill allow extensions for decision deadlines?** Has anyone successfully requested one?
4. Alternatively, would it make sense to continue upskilling in India (through certifications, online courses, etc.) and apply abroad later when I have more savings or a clearer plan?

I’d appreciate any insights from current students, alumni, or anyone familiar with these programs or the Canadian job market. Thanks!

  
Edit: I also got into Rotman. But as mentioned in the comments I think it is better to consider doing it in India.  As there are pleothra of courses on data science these days , will be happy if someone suggest a reputed course from IIT or IIMs for working professionals. 

",2,15,1734111487.0,analytics
1hdk3n1,Choosing between graduate programs,"Hi y’all,

I’m looking for some advice on grad school decisions and career planning. I graduated in Spring 2024 with my BcS in statistics. After dealing with some life stuff, I’m starting a job as a data analyst in January 2025. My goal is to eventually pivot into a data science or statistical career, which i know typically requires a master’s degree.

I’ve applied to several programs and currently have offers from two for Fall 2025:

1: UChicago - MS in Applied Data Science
* Cost: $60K ($70K base - $10K scholarship)
* Format: Part-time, can work as a data analyst while studying.
* Timeline: 2 full years to complete.
* Considerations: Flexible, but would want to switch jobs after graduating to move into data science.

2: Brown - MS in Biostatistics
* Cost: $40K ($85K base - 55% scholarship).
* Format: Full-time, on-campus at my Alma mater.
* Logistics: Would need to quit my job after 7 months, move to Providence, and cover living expenses. My partner is moving with me and can help with costs.
* Considerations: In-person program, more structured, summer internship opportunities, and I have strong connections at Brown.

My Situation
* I have decent savings, parental support for tuition, and a supportive partner.
* I want to maximize my earning potential and pivot into data science/statistics.
* I’m also considering applying to affordable online programs like UT Austin’s Data Science Master’s.

Questions
1. Which program seems like the better choice for my career goals?
2. Are there other factors I should think about when deciding?
3. Any advice from people who’ve done graduate school or hired those fresh out of a masters program?

Tthanks in advance!",0,2,1734117985.0,analytics
1hcv0b1,Analytics Director Salary?,"I'm actively applying to director level roles at larger, non-tech organizations and the salary range on the job description is usually pretty wide (e.g., 140K - 300K base salary at one place and 180K - 320K at another).

I'm currently a Senior Analytics Manager making approximately 175K base + 25K bonus with 3 years of experience in this role but more than a decade of experience in analytics.  Looking for the next step in my career and a pay bump.  

Three years ago I was in the running for a director role and was told they were looking to hire at 190K, so I imagine it would be more now.  What's a reasonable base salary for a director level role in the analytics field?

Would 210K - 220K be pushing it given the wide range (140 - 300K)?  220K is the mid-point of the aforementioned range.",35,24,1734036650.0,analytics
1hd3pp4,Will a masters in Data Analytics help me get into becoming a ML/AI engineer? ,"Am I delusional for thinking that a data analytics masters sounds like it can help me pivot into a Machine Learning role? 

Are these types of degrees cash grabs that have no real substance to them? I am looking specifically at the degree from University of Maryland Global Campus


Background: 

I am currently a IT system administrator working for an employer where they will cover the cost of any graduate degree completely, so I would like to take advantage of this opportunity.

I would like to eventually take my career to possibly becoming a Machine Learning engineer away from general IT. I have a bachelor in IT and one in cybersecurity (dual major). I know python pretty well as I use it a bit at my job for automation. Have used pandas and numpy for projects in school but that’s about it. ",3,15,1734061958.0,analytics
1hd3osz,"From your experience, what are the best practices to really learning your org's data?","I will speak objectively to focus on the question. The org has some DEs who have deep knowledge in our product data, sources of truth, linking data together, etc. Assuming you want to get to the same level for the purposes of analytics and be self-reliant, what would your approach be? Of course, DEs are busy with their day job and you can only meet with them so much - how would you explore your data tables and understand your environment? There are a ton of tables and variables within - what’s a best practice approach to exploring these elements and starting to document and track variables in unfamiliar territory?

The data are in Google Cloud Platform and can be accessed  with SQL BigQuery. Thanks!",2,5,1734061877.0,analytics
1hbxgm9,Director of Data Science & Analytics - AMA,"I have worked at companies like LinkedIn, Pinterest, and Meta. Over the course of my career (15+ years) I've hired many dozens of candidates and reviewed or interviewed thousands more. I recently started a podcast with couple industry veterans to help people break in and thrive in the data profession. I'm happy to answer any questions you may have about the field or the industry.

PS: Since many people are interested, the name of the podcast is Data Neighbor Podcast on YouTube",576,251,1733934727.0,analytics
1hcaiyr,Job Search Vent,"I know I’m not alone in this, but I am so frustrated and beat down right now. After over 200 applications, over half of which resulted in absolutely no response whatsoever, I landed an interview. And advanced round after round. All in all over the course of 2.5 months (yes, months) I completed 7 interviews. Yesterday I found out I didn’t get the job and received no feedback as to why. 

Seriously- anyone who has landed an entry/lower level remote analytics job recently, how? What did you do to stand out? ",32,22,1733969196.0,analytics
1hckimc,Will this lead the career to  data analysis?,"Will this role lead to data career?

Will this lead the career to  data analysis?

Started with the product support where I was more kind of helping with conversion tracking for users online stores and help issues related to it and some errors by google.

After 1.6yr I got offered a role in other company as Product support/Data analyst whose JD is this:

“Provide support to the e-commerce and logistics web projects by examining and optimizing integrations and configurations. Resolving incoming client issues through the ticketing system to maintain a seamless operational process.

Analyze processes to identify areas suitable for automation and working with the team to implement automated solutions. Contribute to data enrichment initiatives and develop skills to optimize business logic and improve overall efficiency.”

Hr confirmed me that this role will lead the way and according to interview a team member also asked me interview question on etl, advance excel and whether I know tableau. 

After landing this job mostly I am doing product support, however, they say find the area suitable for automation. Further, there is as such no tool involved as of now don’t know if I get to use data analysis tool but wanted to know is this career path will lead to data? Or will stuck to product support any advice would be appreciated!! :)",2,7,1734008247.0,analytics
1hcir6c,Looking for product&marketing&revenue analytics tool,I have been wondering whether there are analytics tools which have all of these analytics at the same place. Do you know anything?,2,5,1734001561.0,analytics
1hc8j3e,Data Help,"Hello! I am currently in need of help with some HR analytics excel regression models. I was wondering if this subreddit would be right one to ask for help, or if anyone from this subreddit would be willing to help? Thanks!",3,4,1733963210.0,analytics
1hbhr1o,Data Analytics worth pursuing in 2024 with no experience?,"Hi everyone,

I’m in a bit of a crossroads in life and looking for some honest advice. I’m a 30-year-old with a degree in Statistics from a university in the US, but due to personal struggles (including some mental health challenges I’ve worked hard to address), I’ve been out of the workforce since graduating 8 years ago. Unfortunately, I have no professional experience in my field—or any field, really.

That said, I’ve recently gotten serious about starting a career, and data analytics has always intrigued me. I enjoy problem-solving, working with numbers, and tools like Python make me feel confident and capable. However, I’ve been struggling with doubts because the job market is obviously brutal right now. Every time I hop on Reddit or LinkedIn, I see posts about people with years of experience and polished portfolios struggling to get interviews. It’s discouraging, to say the least.

I’ve been researching and planning to relearn SQL, Python, Tableau, and Excel over the next few weeks. My goal is to build a strong portfolio, apply to small-to-mid-sized companies, and avoid big tech giants where competition seems overwhelming. But I can’t shake the fear that my lack of experience and the 8-year gap on my resume will make this a lost cause.

So here’s my question:

* Is it realistic for someone in my situation to break into data analytics in the next year, especially in the current job market?
* Would upskilling and building a portfolio be worthwhile, or should I pivot to another career path altogether?

I’m open to honest feedback, even if it’s not what I want to hear. I don’t want to waste any more time if this path is a dead end. Thanks so much for taking the time to read this—I’d really appreciate your advice or any words of encouragement (or a reality check).

**TL;DR:** 30, degree in Stats but no work experience, 8-year gap. Want to pursue data analytics but worried the market is too saturated. Is it worth it?",47,18,1733879627.0,analytics
1hbj0ns,"NYC Area Analysts, please reach out, I may be hiring for full time at a we’ll known company","We haven't opened the headcount yet. But I expect within 3 months I will be hiring for a business analyst role or 2. We have not yet decided on seniority, there's some flexibility at this point. Would be hybrid in nyc. Bonus points for experience in Media and / or Data Visualization. ",9,25,1733883483.0,analytics
1hbehl3,Entry job,"I'm searching for a data analyst  job, I've completed my master degree in applied maths, did internships, even small freelance jobs, but now I'm really struggling getting an entry job, my moral is getting lower by the day, I'm not the type of guy that barely code in a jupyter notebook, I can actually code with a more than intermediate python and SQL, I have some github projects, know a bit of aws, databricks, yet it seems lost, how can I pivot, is all hope really lost, entry jobs in data seems to just not exist anymore.",15,21,1733870474.0,analytics
1hb8zk2,Math degree but 15 years in unrelated field - transitioning to data analytics?,"Hi everyone, I’ve been browsing this subreddit for a while, and have found it helpful. Would appreciate some insight on my situation. Some background: I have a quantitative (Math) undergrad degree and did well. However, this was 15+ years ago. I’m in my late 30s and, long story, ended up doing a few things that are completely not relevant (think legal/compliance) after graduating. I hate what I’m doing now and want to pivot to data analytics. But it’s been a long time since undergrad and I have no even remotely relevant work experience, nor am I in a field of work where I even have datasets to play with.

Realistically, how difficult would it be to make this transition, especially relative to someone fresh out of school? Has anyone here made a similar change? I’m willing to start from scratch as I haven’t progressed nearly as much as I’d like in my current field anyway, but would rather not go back to school for too long although I will if I have to. Thanks so much for any insight / advice.",6,8,1733856438.0,analytics
1hb1ukh,HR Data Analyst: Where should I go next with my career?,"Hi everyone, I am currently an HR Data Analyst in the UK. I have been in HR for 5 years now and the last 2 years in HR analytics. I am in a situation where I identified 3 possible routes to further develop my career, but none feels right at the moment, so if any senior data analyst or hr data analyst can give me some advice, it would be very much appreciated.

1. **SQL & Python**: This is the path I find most appealing technically, but because the last time I used SQL and Python (Pandas) intensively at work was already in 2022 and at the moment I mostly build things in Workday, I don't have practice space and things get rusty. I can still self-learn using Leetcode, and building my projects, but I am not sure if it would be enough to get my expected salary if I don't use it at work every day. Due to personal responsibilities, I am not open to a pay cut for learning at the moment.
2. **Advanced Statistics & Regression**: The good thing about HR data is that even though the size is smaller than product data or sales/marketing data, I have room to play with most of them. If I can do some data science work I think I can unlock a lot of insights for the organisation. The problem is this is way harder to self-learn compared to SQL & Python. I tried Google Advanced Certificate for a bit but after finishing the courses, I still feel I am not ready to initiate an A/B Testing or Multiple Linear Regression for a Turnover project in HR at all without guidance. I have an assumption that only a Data Scientist would be the proper person to run this kind of project.
3. **Workday Data & Reporting**: This is the path that technically and financially makes the most sense to me, as I am quite strong in building things in Workday already and can look for Senior/Lead Workday Reporting & data roles in the future. However, whoever knows Workday will understand that its syntax is quite rubbish, with a lot of repeated 'code' and unnecessary manual work; and I feel once I go deep into Workday it is hard to get out and learn any industry-wide skills like SQL and Python anymore.

I do want to get better in my career, but at the moment all 3 options are not so clear to me. Very much hope for some advises and thank you so much in advance!",10,5,1733837660.0,analytics
1hbeyjg,Level 4 data analysis qualification without apprenticeship (UK). Is this possible?,"Hi I am interested in learning more about data analysis but I don't want to an apprenticeship scheme. I'd rather just study for it independently. Is there a way to do this?

Sorry people outside of UK :)

Thank :)",1,4,1733871733.0,analytics
1hb6f2q,Business Analytics major vs MINS,"Hi all, I will be attending Chico State Univeristy next sem to major is Finance but have been really interested in either **Business Analytics** or **Management Information Systems** as a double major (or double concentration). Im a little confused on what the main different is between BA and MINS. According to my program descriptions it seems like BA is more managing data for business decisions with integration of machine learning, AI, and programming languages for processing data. MINS is more designing and maintaining the data bases. Both concentrations have a lot of overlap in the description, and I'm honestly surprised that the Business Analytics option would have techinical skills such as machine learning. However, it seems that MINS would cover the same topics.

In the description of the MINS program it states ""The MINS option is the bridge that connects business with technology by focusing on innovative uses of technology to address business needs or to harness business opportunities"". However, isn't Business Analytics kind of the same thing? I have looked at the courses for both, and they seem similar maybe just more data analytics for BA. I have also done plenty of research online and it seems that the difference varies between universities.

**Here are some of the classes for each option (for reference):**

**Business Analytics:** Data Analytics, Database Application Dev., Business Intelligence & Data Warehousing, Cloud Computing in Business

**MINS:** Business Programming, Data Communications & Networking, Enterprise Resource Planning- Systems Configuration and Use, Business Systems Analysis

 I just wanted to get some opinions from folks who have majored in either of these?



(Yes, I understand that Finance is a very different field, but thats why I'm interested in BA and MINS. While these concentrations may not aide in finding a finance related job/career, it would make me feel confident knowing I have the skills if I ever wanted to change paths... and they just seem fun!! I also posted this on my colleges' reddit forum but have not got any responses yet so I figured this would be the next best place. Thanks!!!)",2,6,1733850015.0,analytics
1haj086,What are some data-adjacent jobs I could do with my skills?  ,"Hey everyone,  

I’m 24 and graduated in May 2022 with a bachelor’s degree in math (stats emphasis). Right now, I’m working as a substitute teacher, but I’ve been trying to break into a data analyst role for the past couple of years. 

I’ve had a few interviews, but most of my applications end in rejections. So now, I’m wondering—what are some data-adjacent jobs I could look into with my background and skills?  

Here’s what I’m working with: Python, SQL, R, Tableau, Excel, Git  

Any suggestions for jobs or areas to explore that could help me pivot into a data-related career would be super helpful. Thanks!  ",26,33,1733774887.0,analytics
1hakmp7,Looking for a data mentor,"I am a data professional with over 5 years experience and a strong background in statistics and policy work. I’ve spent the last few years in UK government roles, leading fraud policy projects,creating automated data pipelines and publishing statistical commentary on behalf of government. While I have experience in tools like R, SAS, and SQL, I’ve been in more policy-focused roles recently and want to dive back into hands-on technical work.

I’m looking for a mentor in data analysis from the private sector who can help me sharpen my skills, stay up-to-date with industry tools, and guide me in aligning my public-sector experience with private-sector opportunities. If you’re passionate about data and willing to share your insights, I’d love to connect!",8,8,1733778940.0,analytics
1h9uhug,I do analytics for a college basketball team - AMA,I love sports and I love analytics. Ask me anything - and I’m also trying to learn more about non-sport analytics ,95,62,1733696468.0,analytics
1hafm2r,Analytics a Skill vs Career,"I saw this post and thought I would see what others think about the idea that analytics is more of a skill than career. I believe it’s both. It is a skill but to understand how to interpret and use the data is what leads to it being a career. 

https://www.reddit.com/r/dataanalysiscareers/s/pzgaodIVvr

",2,14,1733766578.0,analytics
1hah6yp,Do you need to know Javascript?,"Hi everyone. New member here but I have been working in Digital Analytics for over 15 years. I used to be a web developer before and I have been coding in Javascript since 1998. I am also the dude who's been to 58 MeasureCamps and I always present.

If Javascript drives you nuts, I can help. I have a monthly Javascript course specialised for Digital Analytics practitioners. I have just released lesson #6 and I have the code for another two dozen lessons already! Drop me a PM if you want to know more!",0,3,1733770420.0,analytics
1h9kodp,How good do you need to be at SQL query optimization?,"Incoming DA here, from a CS background. Currently relearning SQL.

How good do I need to be at query optimization? Do I need to know that databases are implemented with a B-tree, and reason from the data structures? Or is it more like: apply indexing effectively and you're good to go?

If I need to be good at it: any courses you'd recommend or books? I'm fine with going deep.",13,17,1733670195.0,analytics
1h9umlj,Anyone switched from Analyst > SWE?,"I'm a Senior Data Analyst and have been accepted onto a Conversion MSC in CS at Uni of Liverpool (part-time study whilst I work my analytics job).

Is it worthwhile spending 2.5yrs doing this? It would be 20hrs pw on top of my full-time role.

I earn £45k now at 28 years old, and would be 30 when I finish this masters. l've done python courses etc in the past and enjoyed it at the time.",2,4,1733696818.0,analytics
1h9bqjp,What can I an Analytics Engineer (Laid off) do to get hired,"As noted I'm an Analytics Engineer laid off but there is more story to my career:

Been in the Healthcare industry since 2014 in various 'Data Analyst' positions using SQL mainly.  
First Job 2 years: SQL + BizTalk rules composer to automate client revenue cycle systems

Second Job 3 Years: SQL + SSIS + Various Internal tools to do audits, create reports, and work with State Government on Medicaid.

Third Recent Job - 5 Years:  
\- Did 3 Years without any SQL, mostly using the system to create reports, work with our clients to set up the product,and create automation using the system's internal tools.

\- About 1.5 years ago was promoted to our Data Team, and became ""Product Analytics"" but in reality did mostly Analytics Engineering stuff, b/c of internal politics/BS. Here I used dbt, snowflake, CRMA (salesforce visualization), and Metabase to create reports, automate audits for internal teams, and a few KPI dashboards for our products sold to clients.

Got laid off 2 weeks ago along with half the data team, the company just wasn't mature and ready for it, especially leadership. Since then I have been learning Python hard to up my skills. Did some courses on Looker as it seems that's the other big thing right now.

Analytics engineering is definitely the career path I want to be on, I don't want to go back to 'Data Analyst'. I could really use some experienced advice on what can I do stay on this path? I feel like I was kind of shafted, with less than 2 years of ""Analytics Eng"" exp and online all the jobs postings are asking 3-5 years.

Been getting rejected within 1-2 days for any job I apply for. Its rough out there :/",26,35,1733635333.0,analytics
1h9bbiz,CTV measurement ,"Outside of platform pixels/identity graphs within programmatic, how are people measuring CTV (non clickable) media. Any proven 3P parties that have a strong methodology? In particular looking for solutions that work for sensitive categories.",6,2,1733633846.0,analytics
1h9uu1n,PROJECT SOS,I have a group project due in 9 hours and I need help on a question. can anyone offer their assistance? it’s really urgent. ,0,5,1733697389.0,analytics
1h8wzlf,Marketing to Analytics ,"Who has made the jump from pure marketing to Analytics? What was your path, why did you switch, and are you still doing marketing or solely analytics?",25,13,1733591289.0,analytics
1h95t22,Do I have Data Analyst experience?,"Hey all,

My first job out of college was an Analyst at a PR/Communications agency, was there for nearly 5 years. 

It became more quantitative as time went on, but all the data/calculations were done/delivered in Excel, so I didn't get any experience in SQL/Python. I also did some web analytics and social media reporting (trends off of social reporting dashboard services).

I was then on a web/software development team, but now wanting to get back into data related jobs.

I have a bachelors, but it is not STEM (BA in New Media Communications).

I definitely see that many jobs ask for a stem-related degree, but I know experience is valued- do you think my experience is close enough to ""data analyst experience""?

I have been thinking of getting an MS in Data Analytics--and since the market is competitive, do you think the experience I have negates needing an MS?

Thanks!",6,10,1733615818.0,analytics
1h8w0g4,Looking for SQL Project Ideas and Resources,"Hey everyone!

I’m trying to improve my SQL skills and thought it would be a good idea to work on a project that I can also add to my GitHub portfolio. The goal is to learn while building something useful and showcasing what I’ve worked on.

Does anyone have ideas for interesting projects I could try? Or maybe recommendations for websites or platforms where I can find databases to practice with? I’m open to anything that challenges me and helps me get better.

Thanks in advance for your suggestions! I really appreciate any tips or resources you can share.",8,8,1733588626.0,analytics
1h8lm2n,What Project helped you land your first Analysis Job?,"Hello all,

I want to transition careers into Data Analytics. My background is in teaching Mathematics. I have a degree in math and have programming knowledge from college however i am rusty as i graduate 8 years ago. 

I’ve lurked this sub for a while now and understand that having a portfolio with projects is the best way to develop skills and showcase them. 

I currently am almost complete with google data analytics on coursera and i am starting to think about how to develop a well rounded project that is interesting but i feel like I don’t have enough business sense to make a complete project.

I feel like my a lack of business acumen is making it difficult for me to plan out a project. I know the fundamentals of sql and excel and I can play around with the data but is there a process to follow that will guide me towards proper analysis. I am not interested in following YouTube tutorials because i learned better from diving straight into data with tasks and questions to guide me. What are common analytics task you all do at work? 

With that being said, how did you all decide your beginner projects? What type of analysis did you do? How do you come up with questions that are important and interesting? How can i showcase data cleaning? 

It would be cool if y’all can share a link to your projects that helped you land an entry level job. I am curious to see what original projects look like.

Thanks for the help!",49,23,1733549469.0,analytics
1h8wvki,Data Science Learning and Career,"Hi Everyone, I'm a b2b market research professional looking to learn data science from scratch. I've completed a course in data science from Great Learning couple of years back and haven't been able to use the skills. I have beginner level knowledge but now want to brush up on my data science skills to move up to the next level. What is the best way to do this in quick time, say couple of months time? Where can I get access to projects to learn from so I can move to a level where i can do lot of freelancing projects? I'm doing this to build a freelancing career and not be dependent on a salaried position.",4,6,1733590992.0,analytics
1h86m0h,Finance in Manufacturing? What concepts are critical to know?,"Hey Guys,  
I really need help in figuring out how to prepare for this interview, it will be a case study focused on the finance side of things in improving Manufacturing processes. I have to present my findings based on a dataset they will provide.  
  
The industry is food mfg, the job desc mentions financial insights and operational knowledge to create reports, develop business models, and provide recommendations. The goal is to drive continuous improvement.  
Report KPI improvement and the financial outcome. 

I am well versed in analyzing mfg data for improvements, but never did financial analysis based on them. So any help would be highly appreciated!

If you could also give me an idea of what features could be in the dataset, (Planned Downtime, defect quantity??)",3,30,1733505908.0,analytics
1h8iwbv,Veteran - thinking about transitioning to data analytics but hesitant if oversaturated,"Ok my fellow redditians. I is a veteran. I is a smarty (not really lol). I gots me 2 bachelors degrees, one in Pyschology (useless), and one in Information Technology. I never got really far in IT, because basically my school sucked, I had the Comptia A+ cert and let it expire because with all the studying I did I could never get a job that wasn't a call center/help desk. I can't do those jobs well bc I is also a deafy boi from big boom boom in sand land. I have some somewhat relevant SQL and Excel experience, and have reviewed a few Tubers talking about blah blah, you need excel, sql and tableau and you can get a job if you do my course and network blah blah. I am trying to see if I actually put the time into this, make my resume look shiny, don't list my crappy employment hx bc of my disabilabuddies from the military if I stand a chance after 6 months of study and maybe that google cert. I think I can be a shiny turd on paper, but looking for opinions from those that have tried, those that have failed and those that are lucky enough to have succeeded plz. Thankee. ",0,34,1733539987.0,analytics
1h7qgv6,A government job in NYC.,"do you think this job offer is a good deal?

job title: IT Software Developer.

employer: Government of NYC.

location: lower Manhattan, around the financial district.

work schedule: 2 days onsite, 3 days remote.

work hours: around 35 hours a week at the most, most weeks are around 30 hours of work.

Salary: $110,000. city government pension after 22.5 years of employment.

benefits: 12 days of paid vacation a year, health insurance, and 13 days of federal holidays.

culture: very relaxed as there are no hard due dates and work is fairly easy.

Job security: It is fairly secure, insulated from layoffs, and hard to let go of as it is unionized. 

for context, I am 35, 5 years or so of experience in IT, if I take this job then I am settling down for at least 10 years because after 10 years you get what is called a healthcare pension, healthcare for life basically.",31,28,1733450882.0,analytics
1h7vvf5,How should I prepare for my internship?,"I’m going to be a summer analyst at GS next year on a data analytics team (it’s really a data science team though). I’m a computer science major, but have very limited knowledge of SQL, Python, and Tableau which is what GS mainly uses along with their own proprietary language. How can I prepare myself best to be successful? I’m especially looking for good resources for learning SQL or projects that cover a lot of key concepts.",13,16,1733469395.0,analytics
1h7jw1i,Google Advanced Data Analytics Certificate?,"Hey everyone I have just recently completed the advanced Google data analytics certificate via Coursera and am in the process of building a portfolio on Kaggle and Github of my various projects in Python, Tableau, SQL, R and Excel/Google Sheets. I'm still in the process of fine tuning it and hope to be applying for jobs fairly soon here. I'm just a little worried as I don't have much experience in this field yet since I'm only 24 years old. I do have a college diploma in chemistry which utilized lots of Microsoft Excel. I do not have a degree in this field and my only certifications are the Google data analytics certificate and the advanced one. Is there any other skills I should try to add to my tool belt? What are my chances of actually landing a job in the future with my current skulls and experience? I'm open for any advice as I really enjoy this field of work. ",11,24,1733432896.0,analytics
1h7hflg,What’s a Normal Application-to-Interview Ratio for Analytics Roles with 2-4 Years of Experience in current market?,"Hi everyone, I’ve been applying to analytics roles, and I’m curious about what others have experienced when it comes to the application-to-interview ratio.

For context, I have 2-4 years of experience in eCommerce analytics and a Stats/math degree. Out of 100 applications, I’ve received about 5 interview calls. I’m wondering if this is normal for someone with my background or if there are ways I could improve my approach. FYI, I am tailoring resume for each job application, and have projects/github etc on resume. Also, applying outside of ecommerce domain. Applying to new job postings only so within 3-5 days max. 

If you’re in analytics (data analyst, business analyst, BI, etc.), how many applications did you have to submit to land an interview? 

Also, any advice for improving response rates would be greatly appreciated.",8,14,1733426749.0,analytics
1h74to7,"First year done, What to do now? ","Hello,

I ve spent the last year working as a health data analyst for a health insurance company. We mostly use excel, SSMS,T-SQL, Python and we are getting into using snowflake and salesforce. We are a small team that supports the account management team. Recently our manager took a new role and our most senior analyst has also left, Making myself with one years of experience in the field the most senior on our now small two man team. I find myself now answering for our department on my own making manager level decisions, and have very little direct over sight.  I also now  only report to the director who reports to the CMO. I m probably due to get a 2-3 percent Cost of living raise this year that will put me right over 60k per year. I live in a Medium to high Cost living, for example a average 2-3 bedroom apartment here for rent is around 1500-2k a month. A house 2k plus. Our small team is putting out around 50-70 ad hocs a month and another 130 or so reoccuring and we are responsible for side projects that take serious development time. I m also asked to provide custom reporting solutions, write  complex sql with builds in the thousands of lines, and edit and make changes to our different python applications that automates hundreds of reports.  I regular manage stakeholders, mostly clients and account management with the occasional work done for another department. We are expected to complete all this work between 40 and 44 hours per week, I m getting burnt out, Is this a fair wage in this economy for this type of work? I know the market is bad, How do I make the most of this? In this economy I dont really feel like I can jump ship with only one year worth experience in. On the other hand Given the work I feel like I should be making close to double what I m paid, But with my limited time in, nobody is going to offer that. ",11,20,1733388395.0,analytics
1h7a5bn,Pressure on work,"Im currently working 3 months as data analist consultant. My background is that i came from a fabric-operator. So im new in the field data analytics. 

My experience is good so far but i feel some some pressure from my senior colleague to finish some tasks. Also i get monitored. I have done 2 months training and get rolled into the real world usecase. 

Over time its getting frustrated that they expect to much of me. They know my situation and hired me because they want to train people from the scratch. I got a bachelor degree en the guys whom i work got master degree. 

Sometimes i think : is this the right choice to choose this field. ",1,19,1733408208.0,analytics
1h7gpj4,"Is to become data analyst, strongly required any certifications","Learning Data Analytics from Alex... The Data Analyst. If anybody could tell me, whether any certifications required to land a job. ",0,19,1733424946.0,analytics
1h6up56,Manager questions the value of historical data,"What is everyone else's opinions on historical data reporting and analysis?

Note; my team is a dev/tpm team with myself as the sole analyst.

So I work in SaaS looking at enterprise customer tickets and analysing the relevant data etc. We've had a past migration of systems in 2022 and a lot of my initial work (under a different manager) as an intern was stitching together the data to keep a continuity so we have several years of historical data for analysis and forecasting. Now, obviously internal processes change over time so. So in my current manager's eyes, this historical data is not important for new data visualisations and projects as it doesn't represent the current situation. While this is true I don't believe we should be casting aside data just because it doesn't fit a certain narrative.
Thoughts?

",4,13,1733355169.0,analytics
1h72mz7,Tableau with LLM,Has anyone integrated LLM/gen AI with their tableau dashboards? Is it helpful and what are the steps to it? ,1,10,1733379120.0,analytics
1h6rwdl,"Presentation to execs, what to expect","Hi, I just started my first full time job 6 months ago and haven’t had much experience presenting to leadership. For some context, I developed and deployed an operations tool that my boss wants me to present and demo to execs including my SVP.

I’m super nervous for it and presenting isn’t my strong suit :/ 

Could anyone give me some tips on how to present and what I should focus on, as well as what kinds of questions I can expect to be asked. 
I have a script drafted and a couple of use cases on the business impact, but is there anything else I should focus on. I’m also scared that I’m going to freeze on the spot so don’t know if the script will even help lol 

",4,17,1733348118.0,analytics
1h6wpfj,Should I ask for a title change?,"About a month ago, I started a job at my college’s Career Design Center as a Career Peer. I’m a sophomore studying Data Analytics with a minor in Computer Science, and I was told my role would involve a little bit of data work (relevant to my major) but would mostly consist of administrative tasks like answering phone calls, reviewing resumes, and setting up appointments.

However, after my training period my responsibilities turned out to be mostly data-focused. Here’s what I’ve been doing:

	•	Collecting and cleaning data to optimize our social media content types and post timing.
	•	Analyzing data on previous career fairs, including trends in attendance by major, which companies are hiring specific majors (both for jobs and internships), and associated salary data.
	•	Using this analysis to suggest which companies should return, which new companies we should contact, and how to address underrepresented majors at our fairs.

Basically I’ve been using Meta Business Suite to create excel files or cleaning existing excel files from career fairs to then import them into an SQL server, analyze the data and create Tableau reports to share insights with my supervisors. I definitely am not an expert at any of this and am only using what I’ve learned the past couple years in school. I could definitely be doing things more efficiently but I’m just working with the knowledge I have for the time being. If I would’ve known I had these responsibilities when applying for the job I don’t think I would’ve qualified haha. 

Either way, given these responsibilities, I think that my current title, “Career Peer,” doesn’t reflect what I’m actually doing. I want to ensure this experience stands out on my resume, especially as I apply for summer internships in data-related fields.

Do you have suggestions for more accurate job titles I could propose or use on my resume? I’m not entirely sure what the best and most accurate data related title I could use. Should I even change my job title? I’m not doing it for a pay bump (even though I’m paid minimum wage), I just want to make sure my work is reflected properly. Thoughts?",2,3,1733360636.0,analytics
1h6d3ep,How Much Math and Programming Do You Actually Need for Data Analysis?,"I’m curious how much you actually need to love math and programming to work in data analysis or ICT.

For data analysis, is it all about Python and SQL, or do you really need to dive deep into stats and math?

For ICT, how much programming (like Python) do you really do day-to-day?

What kind of tasks should you enjoy to thrive in these fields?

Would love to hear from anyone working in these areas!",26,36,1733308523.0,analytics
1h6k97z,Survival analysis with a twist,"Hello. 
I have a problem that is close to survival analysis, but not quite.

The analog to my actual problem is this - 
let’s say I have an ad campaign through FB, IG and Tk - specifically targeting 1000 known user in my members list. If one buys the product the campaign in the other platforms automatically closes for that user.

I can use survival analysis to see which campaign is faster and more effective by testing time to buy.
But it’s not classic. The three campaign “eat” the same population. Lets say instegram get 10 buys on the first day. These users can only be counted as censored for the other platforms.
But it feels too simplistic and makes it seem like the losing campaign “succeeded”.
I read about “competing events”, but it doesn’t sound right eather, as one event doesn’t logically block the others (like death event and intubation event), they can randomly “block” each other.


So to summarize - 
Is there a version of survival analysis where multiple risk factor with different half-lives and different plateaus “pray” on the same population? And can the parameters be estimated in the live observation with all risks running, without conducting a different test for each?",6,5,1733329802.0,analytics
1h6qe4l,Cheap Embeded Dashboard?,Any affordable products to get an embeded dashboard connected to my database in the Backoffice website of my business?,0,4,1733344427.0,analytics
1h6ng89,Need suggestion for web scraping ,Can I scrape.content from a web page which has restricted access? It's showing 403 forbidden when I try to user bs4. And I am.thinking of using selenium but it's not working out. I need to access a client portal and scrape data from the website / link. Any suggestions 🤔 ,1,13,1733337338.0,analytics
1h6lqno,Excel Copy/Pasting Issues,"Scenario:

 \- I have a table, where I create a bullet point list (using \[SPACE\] ""-"" \[CONTENT HERE\] \[ALT + ENTER\]

Example cell in a table:

 \- Item A

 \- Item B

  
When I copy this cell in one spreadsheet, and paste it into another cell in another table, it breaks it into 2 separate cells, and breaks everything. Also, i'm collaborating with others, and if I know how to prevent this, I can't prevent others from accidentally doing this.

What is going on?

How can I format my spreadsheet (multiple lines in a cell) without this happening?",1,7,1733333328.0,analytics
1h6ja1u,AB testing - observed difference higher than MDE without collecting minimum sample size,"In the AB-test summary dashboard results are shown as follows:
- If the minimum sample size has not yet been collected, it shows how many more days are needed to collect it (to avoid stopping the test too soon).

- If the minimum sample size has already been collected, it shows whether the result is statistically significant.

This approach can sometimes be problematic, let's say my data is:

baseline conversion -1.05%

assumed MDE - 5% relative

minimum sample size on this basis: 596 k sessions per variant

So after 2 weeks of the test, I still get information in the dashboard that I need data for several hundred more days. Now 2 examples of the results on the dashboard:

a) ver A: 1.05% ver B: 1.24% (18% diff) - difference not statistically significant

b) ver A: 1.05% ver B: 1.41% (34% diff) - difference statistically significant

So I'm aware that I haven't collected enough traffic based on my assumptions, but I see differences much higher than the assumed MDE, even significant for (b). My questions are:

-How should i approach this? Should i adjust my initial assumptions?

- Can i trust the result b) if it shows significance without collecting enough traffic? What if these results are observed after 2 days, should i still trust them or can assume it's due to random noise? Where is the line?

I have read the What if the Observed Effect is Smaller Than the MDE? | Analytics-Toolkit.com article. I remember coclusions that MDE and observed effect shouldn't be compared, but with such big differences it doesn't seem to be intuitive.
I would be very grateful for any help",1,2,1733327465.0,analytics
1h5qewt,Is analytics a young person's game?,"Have you seen fewer older ICs in analytics than in other technology fields? I work for a non-FAANG tech company, and I realized that there are essentially no older analytics ICs in the entire org. I'm in my late-thirties and recently realized that I'm the pretty much the oldest person in my entire analytics department. Is this an industry-wide thing or a company thing?

Part of that is definitely due to tech generally skewing younger, but analytics seems to skew even younger when I compare it to SWE, DE, and DS. Those departments seem to have more older folks with families while DA is pretty exclusively younger people. 

What do you think?  None of what I said applies to management paths - I'm talking about specifically IC tracks.",29,37,1733240772.0,analytics
1h5z9ix,Data Analytics Project Ideas,"Hey everyone! I’m getting started in data analytics and looking to build some projects to sharpen my skills and grow my portfolio. I’m aiming for roles like Data Scientist or Business Analyst. I work with SQL, R, Excel, and Tableau. what are some project ideas that make good use of these tools? Also, what kinds of projects do you think stand out most to employers? Appreciate any advice!",0,7,1733262674.0,analytics
1h4zxe6,Math & Statistics in Data Analytics,"I've been doing a bit of researching when it comes to moving into a data analytics The usual 3 things you are told to learn is: Excel, SQL and a data visualization tool (which I'm going to work on). But one thing I've been seeing mixed responses is needing to know math and/or statistics.

So I'm here to ask how much math/statistics should someone dive into if you are looking to aim for a entry level to mid analytics role? I've seen others say it varies from job to job. But I'm thinking it might not hurt to learn some of it. I was looking at taking an intro to statistics course (took a stats course back in grad school but that was many years and never used it) and maybe a basics/fundamentals algebra course.  I'm not looking to get into data science or engineering right now.

Would love to know others thoughts/ideas. Also if you have suggestions on courses/books? Something relatable as I'm not good at math at all and it can take me awhile (along with repetition) to understand things. ",67,22,1733159357.0,analytics
1h5p7rb,How Can Flask Help in Data-Related Roles?,"Hi everyone,

I'm starting an internship in about three months as an Analytics Engineer. My mentor mentioned I'll be using Flask during the internship. I want to train and be fully prepared before I begin.

I have a few questions:

1. How does Flask help in data-related roles like mine?
2. What kind of resources should I explore to get better at Flask?
3. What types of projects should I try to build with Flask to improve my skills?
4. Do you have any ideas on where I can find project datasets or examples? I’ve checked Kaggle, but it doesn’t seem to have anything Flask-specific.

Thanks in advance for your suggestions and advice!",1,5,1733237720.0,analytics
1h5m5du,GA4 Question: How Do See Previous Page Path/Referrer (Which Pages Lead to Conversion Pages),"Hello from the Azores! I have a question about GA4. I used universal analytics somewhat and could find my way around somewhat. But GA4 Is a black box to me.

The answer to my question should be intuitive and clear but I can't figure out how to determine the previous page path for our conversion pages.

For example, I want to know which blog articles drive the most traffic to the contact us page (a conversion goal).

Can someone clearly and concisely explain how I would see that?

I'm mostly interested in organic traffic user pathways here. So I can optimize the top of funnel/ entry points and optimize them as needed.

Obrigado!",1,2,1733228428.0,analytics
1h57yxz,Offered a promotion - how much should I ask for?,"There’s a lot of detail here but I’ll try to keep it as short as possible.

I’m on contract right now as a manager-level IC at a growing company that does not have a formal data analytics function yet.

Rather than just converting my contract, I’ve been offered the opportunity to take a Director level position and head up the new DA team.

I’m interested in and qualified for the position, my only hesitation here is wanting to land the right compensation.

A few pertinent points:
- total analytics experience is 7 years
- industry experience is 5 years
- I have been in my contract role for 4 months but have previous experience at another subsidiary of the parent company
- I work in a field that does not attract strong data talent, and hiring an outsider would likely cost the company industry knowledge
-company is around 250 employees but experiencing rapid growth 

My current contract rate is $60/hr ($125k annually, no benefits while on contract).

The new role’s incremental duties include P&L responsibility, managing 3 direct reports, scaling out a non-existent centralized data function.  I’ll be reporting directly to the president of the company.

What are everyone’s thoughts on what I should come in asking for?",8,3,1733178986.0,analytics
1h58o8t,What Experience is Worth Getting?,"I’m currently finishing my Master in HI. I’m actively a Physical Therapy Assistant. I just applied and have an interview for a Medicaid Analyst position for the state of Louisiana. I’ve had interviews for jobs involving HI that have interested me. One was for a children’s hospital that was collecting data on the effects of consistent wear of braces for kids that had varying spine diagnoses. I want to work in the healthcare and contribute to projects like this. The hesitation is the Medicaid Analyst job is a significant pay cut than other jobs I could be taking in the PT world. I know in order to make the transition I will need to take entry level jobs, but will this experience translate towards my career goals if I do accept the Medicaid job?",2,5,1733180803.0,analytics
1h5da10,Minor in Communication as a DS major,"Hi everyone, I'm a freshman majoring in DS at a top 50 university. I have decided to minor in Statistics and now I'm considering about Economics. I am also an ISTP and not so good at talking 🥲 

I think I'll work in FinTech, but I still do not really know what I should do. How is FinTech for a DS major?",1,6,1733193863.0,analytics
1h5b0gy,Cross Domain Tracking,"Hi, I'm working on cross-domain tracking between a hotel website and a Mews booking engine. The setup in GTM is done but:

* GTM preview mode often asks me ""Unknown domain  The domain mews.com is not enabled for debugging. Would you like to enable it?"" when I do I can see the tags firing with the page location and page referrer parameters but it keeps showing that warning.
* Debug View works on the hotel website but breaks on app.mews.com transition.

Both domains have GTM implemented. The main goal is tracking the user journey properly between these domains. What might be missing?  


Current setup:  


* GA4 Cross-domain Linking listed as contains mews.com
* Conversion Linker tag configured: enable linking on all page URLs, enable linking across domains and auto link domains mews.com

Thanks in advance.",1,1,1733187194.0,analytics
1h4zrs4,Got fed up with traditional analytics tools… Any suggestion for warehouse-native?,It just so much time to import-export the data. We tried reverse ETL tools as well but its still so overwhelming. does anybody have experience with warehouse native tools? We woild like to test one,3,7,1733158974.0,analytics
1h524nx,non inferiority testing in A/B testing,"Heya,

I work as a product analyst and one of my task is doing A/B testing.

However, sometimes the goal of the A/B test is not so much is A better than B (or vice versa) but is B not worse than A. In normal terms; they have put out a change, and mainly want to know if it isn't performing worse than the first change.

In my general statistics courses I've only learned the many techniques for rejecting null hypothesis rather than proving them...

Any of you got experience with this?

Currently this is mainly for binary variables",2,8,1733164644.0,analytics
1h4ry83,Guys i dont know if i made a mistake on a call from a recruitment team,So here i am on a good morning and a recruitment team called me askasking about my current job i was speaking and he asked what does my company do usually i said they take in project and sometines outsource staff members they asked where was i outsourced to ? I said i have signed an nda can't say that eventhough i could say i dont know what came in to me then later they said they would call me after 2 weeks did i just get myself fuckedwith that answer ?,8,9,1733135015.0,analytics
1h55y1b,Looking for Projects to Build Experience as an Aspiring Analyst,"I have around 7 months of experience in hotel operations and a few months in business management. I also have an undergrad in Finance and HR and am pursuing a Master’s in Applied Business Analytics at Boston University. Since most employers require direct experience, I’m looking for ways to gain hands-on practice outside of traditional jobs. What projects, platforms, or volunteer opportunities would you recommend to build relevant experience and strengthen my resume for analyst positions? Any advice or suggestions would be greatly appreciated!",0,15,1733173954.0,analytics
1h55afx,How can I apply statistics to my job?,I work in corporate finance. I want to start creating tools for my team to help give us better insight into the reports we process each month. What are some useful statistical techniques I can apply that can be applied universally?,1,5,1733172370.0,analytics
1h4rz44,I need ur opinon please help 🥲,So here i am on a good morning and a recruitment team called me askasking about my current job i was speaking and he asked what does my company do usually i said they take in project and sometines outsource staff members they asked where was i outsourced to ? I said i have signed an nda can't say that eventhough i could say i dont know what came in to me then later they said they would call me after 2 weeks did i just get myself fuckedwith that answer ?,2,6,1733135128.0,analytics
1h4pjs2,Sales data analyst case study ,"I have been asked to work on a case study for sales data analyst position for a firm which deals with staffing for nursing homes. They have asked me to give five recommendations to sales teams, providing the graphs and tables from the query I would write along with how I achieved it. I have never work with sales data, any recommendations on what I can focus on? 
",3,3,1733124160.0,analytics
1h4cmfb,Help! Cognos Report list view,"I have a current report in Cognos that calculates sales made by contractors. Report gives the contractor names in rows while the columns have the sales they made that day (sales count, sales volume, net). This report is executed daily and exported as html. I am adding a YoY view also to this. Initially I had created a second table below the main one to show the YoY view but I want to have them in one table. When I try to combine them all into one list table it changes the data from previous year to current e.g moving sales count may 20 2023 right next to sales count may 20 2024 in the Cognos design view changes the count of the 2023 data to 2024. How can I avoid this?",2,2,1733084819.0,analytics
1h3m6hc,Data analysts! What was your college major?,What did you study in college? And did it prepare you well for your current role as a DA?,141,392,1733000221.0,analytics
1h468dh,Working at Amazon as an Applied Scientist,What’s it like? Pros and cons? I have an interview there soon ,4,2,1733068388.0,analytics
1h3yqp1,Unemployed for a year with 1 YOE. How hard will it be for me to get a new job?,"I was laid off last year and spent about three months actively applying for data analytics roles. During that time, I noticed a lot of news about layoffs and saw some of my friends go through similar challenges. Around January 2024, I decided to take a 9-month sabbatical to travel. I had always wanted to experience extended travel, and I knew a few people who did something similar after college. Considering the tough job market at the time, it felt like the right decision.

Now that the job market seems to be improving, I’m looking to transition back into analytics. Has anyone here navigated re-entering the field after a year-long break? How challenging was it? And is it advisable to be transparent about taking time off to travel when discussing the gap with potential employers?
",12,11,1733041177.0,analytics
1h3gyiu,How did you get your remote job oppurtunity?,Hi dear data analysts how did you get your remote job oppurtunity?,12,86,1732986034.0,analytics
1h3tpvu,Manufacturing Data Reality. What do these datasets typically look like?,"Hey Guys,  
So I have an interview coming up for a food manufacturing company and they are going to give me a case study on Excel to work on. The job desc is focused on continuous improvement on sugar lvl.  


Does anyone here work in manufacturing and help give me an idea of what a typical dataset could look like?

I would love to start practising on some fake datasets, I asked ChatGPT but it isn't giving the most realistic datasets.

Any help us much appreciated!!",1,7,1733022518.0,analytics
1h3xgmb,What are the skills I should focus on to move on to the next level?,"I am working as a DA for 4+ yrs but I am not getting any good projects and even when they come it's very rare I get to work end to end. I want to switch into a senior role based on my experience.  Since mainly I am getting involve in non-da work and whenever I get da it's not end to end which kin of is effecting my overall experience. 

What skills would you suggest me to learn?
Also as for my DSA it's like 5/10 and it's been long since I practiced.

These are my current skill sets:
Tableau (not prep)
SQL (not advanced like procedures or windows)
Python (pandas mostly)

What should I be focusing on? Top 4 skills.
",0,4,1733035799.0,analytics
1h31azc,How much is worth to learn R outside of academics & research?,"Hey all. In a professional environment working in Data Analytics, or as Business Analyst or in a related field how much is worth to learn R? Outside of Research & Academics there are advantage of learning and then using R over say Phyton.
Phyton seems much more versatile and valuable and a requirement for many companies along with Excel, SQL + dashboarding.",13,23,1732930127.0,analytics
1h35qkq,How Do I Find An Analytics Internship As A College Student?,I'm currently a college student that is studying marketing analytics and I'm wondering where I can find internships for the summer. I'm honestly worried about finding one. I got invited to an interview where I record my answers but I completely bombed it and now I'm worried I won't be able to find anything. I seriously need help.,2,12,1732945217.0,analytics
1h30a1t,Querying multiple large dataset ,"We're on a project requiring to query multiple large dataset & multiple table using GPT to analyze the data (postgresql). Some of the tables have like 2,000 words text or more.

Any recommendations to tackle this issue?",2,18,1732926978.0,analytics
1h2rcfn,Freelancing as a Marketing Analyst,,4,2,1732902422.0,analytics
1h2ak46,Which age group or generation is more prevalent in Reddit more than any other social media platform ,It could apply to today or all time,10,6,1732843482.0,analytics
1h2ctbb,Any Posthog users here?,"I just watched Posthog's demo video and was mind blown. If you have used it in production, please share insights.

* It offers 8 different products, does it become confusing at some point? Is there a real advantage to having all those tools in one place?
* It seems to be feature rich, but do you see any major gaps compared with mixpanel/amplitude for analytics?
* How is the pricing compared to what you were using before?
* Any reasons why you wouldn't prefer such a tool over individual tools for analytics, experimentation, session replays etc?",2,2,1732851461.0,analytics
1h1vqx9,Microsoft Clarity: Heatmap Download doesn't work correctly ,"Hello everybody,

I'm using Clarity for a few weeks now and wanted to download some heatmaps as PNG. Unfortunately everytime I download it it doesn't load the CSS and/or JavaScript. But in the application itself everything is fine.

Do I have to configure something that I miss?",1,2,1732799837.0,analytics
1h1fslx,If you could automate one thing when analyzing data what would it be?,"If you could automate one thing when working with your data, what would it be? Cleaning up messy data? Creating dashboards? Finding insights faster?",12,39,1732743802.0,analytics
1h1bmp6,What data do you consider most essential for judging the success of social media content?,Title says it all. What do you consider the most accurate measure for success when it comes to running a company's social media account? Would you prioritize engagement or sales?,4,10,1732733182.0,analytics
1h15h17,Sessions attributed in Direct channel when consent is denied?,"Following a spike in Direct traffic in GA4, I’ve been told by our cookie banner support that when a user refuses cookies or makes no choice in Consent Mode, this traffic falls into the Direct channel.

Has anyone ever truly experienced this? 

I’m somewhat doubtful about that as I always thought sessions were not processed when a user refuses cookies (except through modeling, which is not active in the property anyway).

Please note that navigation is blocked prior to consent and the page is not reloading on consent.",4,4,1732717461.0,analytics
1h17sul,How to Build an In-House Tool for Tracking EMV and VIT?,,2,1,1732723643.0,analytics
1h0ty6j,"New grad, entry level job responsibilities ","Hi,
Can anyone let me know if this is normal for an entry level new grad data analyst role?

For context, I joined a new grad rotational program and I feel like I’m being over worked and also doing a lot more product/management work than data work.

I’m on a team of all product/non technical people and I’m the only data analyst/technical person on my team. Because of that I’m in charge of developing dashboards for my team, as well as requirements from cross functional teams. Getting data from different teams takes weeks of communication and negotiation, often escalating to their supervisor and it puts me in a really awkward position since, as a new grad, I feel uncomfortable demanding/requesting said confidential data from different teams . 
Since I’ve started I’ve designed and am now maintaining 4 different dashboards. My boss wants me to start writing requirement docs and also leading product meetings with different teams. 

I guess I thought that being in a new grad entry role position, I would have a mentor to gain data analyst/bi skills but instead I feel like I’m overwhelmed with responsibilities and since it’s my first time doing bi and there isn’t a mentor/another data analyst on my team to bounce off of ideas and get help etc . there’s also an added learning curve with bi tools and cloud data management, which is hard to learn while also having to keep up with deadlines.

I don’t mean for this to come off as complaining, I’m super grateful to have a job and have people that trust me to develop these reporting tools, but I’m wondering if this is normal ",21,11,1732674953.0,analytics
1h0cb7x,"I hate python, should I give up the pursuit of this career?","Can't stand the language, tried it many times over the course of my life, with many different platforms/websites. Absolutely hate it, makes no sense to me whatsoever.

I like SQL though, but I see that python is a reocurring skill being mentioned for DA's so I'm just asking, how important is it? and should I honestly just give up the pursuit to transitioning to this space if I can't learn it?

Thanks",51,106,1732629007.0,analytics
1h0z4ft,Is Tableau Desktop Specialist Certification worth it?,"Hi everyone,

I’m graduating next May and looking to start a career in **Business Analytics**. Would you recommend the **Tableau Desktop Certification**? Has it helped you professionally?

I appreciate your time and insights!",1,3,1732693458.0,analytics
1h0s8w6,Trying to get back into Analytics for a DA job,"Since graduating college, I've been searching for a DA job for a year and half. I've learned R through college. Learned SQL, Tableau, and a little bit of Python and Power BI. Even got a certification in Google Data Analytics. I settled for a accounting job and eventually went up to Pricebook Analyst (current job). Since my current employer has changed a lot, I've been seriously been getting back into learning programming again and seeking a Analytics role. Problem is: It's way harder now than it was a couple years ago.",3,2,1732669814.0,analytics
1h0lhf8,Data Quality vs Data Architect,"Hey everyone!

Being direct:  
Data Engineer with 10y experience, trying to do a move to Data Governance side. I want to be part of the DG Team, somehow in the future. Ideally I'll be a CDO.

I've got three proposal in hands:  
(1)  
1x for a Data Quality Analyst role, working very close to the Data Governance Team.  
Business: bank industry  
Tasks:  
Maintain DQ  
possibilities to evolve the DG framework, but it's not the focus. but I'll work close to the team.  
Work with MS Purview  
Work as a team in a ""agile"" methodology  
Concerns: business limited scope, since I'll be QAing data for reports.  
Pros: I'll work exactly on the field I want

  
(2)  
2x for Data Architecture  
Both companies (Industry Innovation and Telecom)  
Both are trying to follow a Data Mesh Architecture.  
Both don't have a strict DG framework implemented ; I'll have contributions to it, but will not implement it directly.  
Don't know exactly which tools I'll work with.  
Both don't work in agile methodology, since I'll work ""alone"", defining something and not necessarily build projects.  
Concerns: tasks are not directly related with DG tasks.   
Pros: Strategic position. Possibility to work with DG. Working on a Data Mesh centric idea, possibly designing data products (oh sh\*, developing data products without a DG framework already implemented)

  
What brings me here:  
How the f\* do why choose? I really need to decide. All the three came to me on linkedin, are good companies with good salaries. Bank industry is in general more ""close"", no big progress, I think. But it's DG. However working as a Data Architect I'll be more strategically positioned to help on DG tasks, I believe.  
Bank pays a little less.  
All have good facilities and perks.  
The thing here is essentially the tasks.

  
Any Data Architects/Data Quality/Governance Analysts here?

Thanks for your inputs!  
",5,3,1732651867.0,analytics
1h0e8lv,Industrial Engineer Looking to Enter the Analytics Field,"I’m a final-year industrial engineering student, and I’ve recently started learning Power BI, Tableau, and Excel. I’ve been enjoying the field of analytics so far, but here’s the thing—I absolutely dislike coding. Even for VBA in Excel, I rely on ChatGPT to write the code for me. Luckily, my professor thinks it’s okay to use tools like that, so it hasn’t been an issue.

I’m curious to know more about the nature of work in analytics roles, such as Business Intelligence (BI) or Business Analysis. What does a typical day look like? Are these roles heavily reliant on coding, or can they be more focused on tools and insights?",6,12,1732634282.0,analytics
1h0cxo0,How to get into analytics?,"I have a BA in Psychology and Gender Studies and for the past 8 years I've been working as a welfare caseworker for the state, before that I worked for 2 years at a crisis center as a service coordinator for clients. 

I'm interested in finding a job as a data analyst but I'm 37 and I don't have relevant experience or education for it as far as I understand. I did take some statistics classes in college and was an RA for 4 years in college but that was a really long time ago.


Would I benefit from a bootcamp program? There is one offered at a local college but I think it's like 10k. Or do I need to go to grad school in data analytics? Or is this something that can be self-taught? Any help/advice would be appreciated!",5,8,1732630756.0,analytics
1h04rih,Is it possible to become a data analyst without a degree or relevant work experience?,"I know this topic has been answered many times before, but I wasn't finding enough relevant answers to my situation. For context, I'm 19 years old and I live in the United States. As the title states, I don't have a degree as I dropped out after a year because I felt the vast majority of my classes were useless and not applicable. I've been running a 3D printing business for the past 1.5 years and it has been pretty successful, but also inconsistent. Before the business, my only work experience was a couple of busser roles at restaurants.   
  
Anyway, I wanted a good job to fill in for slow months in the business. I decided on data analytics because it was interesting to me, and it seemed like there was a lot of room for growth/learning. So I did the data analyst career track on Datacamp and really enjoyed it and want to continue my learning.

I'm just trying to figure out if this is possible, and if so, what my next step should be. I would prefer not to go back to a university, but I do like structured learning. So I have been considering potentially doing a BootCamp. I know that it will be harder for me since I'll probably get filtered out for not having a degree a lot of the time. I was thinking it might be better to try to get an internship before a job. Thanks for reading all of this, and hopefully some of you can give me some guidance on this.",9,35,1732599815.0,analytics
1gzt3ln,Getting masters in Data Analyics while having a BS in Computer Science?,"Firstly, I apologize since I know this is a FAQ, I just wanted to ask it given the context around my circumstances. For context, I'm 22 and graduate with a BS of Computer Science & Engineer from OSU back in May. I've been applying to a lot of jobs(mostly software dev), since my senior year had started but am still unemployed and living with parents. I've had this thought in the back of my head the whole time though about how I didn't really know if I wanted to do general software development. It just always continuously bored me, but I guess I was sticking with it out of ""expectation"" and not wanting to deal with feeling like my life plan was falling apart. I ended up finally thinking it over lately and realized I mainly enjoyed the handful of classes I had focused on databases, cleaning up data, analyzing data, etc. 

So, I've been starting to go through some decent tutorials I've found online to refamiliarize myself with the processes I learned in class.(mainly Python and Jupyter Notebooks since that's what school used). I plan to learn about using other tools as well, such as Tableau, and I have a personal project I'm planning as well. Tbh, I've had way more fun doing this compared to even just thinking of writing code for things like apps and websites. 

That's gotten me thinking about doing a Master's via something like WGU or Georgia Tech's online program, since I can afford them(I have money saved up). But I also don't want to make a decision to drop that much cash lightly, so I wanted to ask here: How worth it would it be to get a Masters, and should I just focus on finding free/cheap courses online to take and doing personal projects instead?

Edit: alright yeah yall have definitely convinced me to not go for it rn, i’ll keep working on projects/my self-learning and finding a part time job. Thanks for the responses! :)",24,31,1732566876.0,analytics
1h05yi8,Reason for rejecting an offer,"I interviewed in a company in 2021 and rejected the offer since i was not super happy with it and was getting promoted in my current company. The offer basically helped me negotiate my ctc in the existing one.

Now i am reapplying to this company and i really want to get in! I am 100% sure i will be asked why i declined the offer earlier. 
What can i say to let my previous decision sound reasonable?",3,6,1732604483.0,analytics
1gzozxk,Advice on Preparing for Healthcare Data Analyst Interviews,"Hi,

I’m currently applying for Healthcare Data Analyst positions and want to prepare for potential interviews. While I don’t have an interview scheduled yet, I’d love to get advice on resources and tips to prepare for it. I am looking for an entry-level position. I am based in the USA.

Some details about my background:

* I have an MD degree and a Master’s in Health Informatics.
* I have intermediate SQL skills and Excel proficiency.
* I’m familiar with healthcare ontologies like ICD-10, RXNorm, and SNOMED.
* I’ve worked with data cleaning, analysis, and visualization (using tools like R).

Interested to know the following:

1. Key skills or concepts I should focus on 
2. Common interview questions for Healthcare Data Analyst roles.
3. Recommended resources to practice technical and analytical skills and behavioral interview questions

If you're a healthcare data analyst or have recently interviewed for a similar position, please share your insights. Any advice is greatly appreciated!",13,13,1732557084.0,analytics
1gzfybj,Starting on data analytics journey,"What do you guys recommend the best way for  a beginner to proceed, I wish to learn python, sql, power bi and tableau. I have very basic exposure to sql since I worked on zoho analytics.
1. Should I undertake projects to make it stronger and then move to python?
2. Should I enroll in bootcamps that span over 6-7 months and do a proper course with certification? (I have done a professional course in accounts and finance so basically I have no prior knowledge in using these tools, nor do the typical job profiles in my line require the same, however I wish to switch streams, get into more technical roles as this looks more fun and intriguing)
3. I know there isn't an end to the knowledge you acquire but still What should be the level of various tools (even if other than above) I should acquire to be able to freelance maybe and unskilled myself.

If possible please guide me with the best source of acquiring such knowledge as well.",11,26,1732530893.0,analytics
1gz5k1c,"I work in analytics, but need recommendations on a new job title based on my new role","My company is looking to change/update my job title (including a raise) because I started as a data analyst but I’ve taken a larger role over the past year. Problem is, leadership has left it up to me to decide first on what I think the job title should be and then they would discuss it with me. 

Here are some of the things that I do:

- SQL Reporting
- Create Power Bi and Tableau Dashboards
- Create ETL processes using SSIS and C#
- Manage database and data governance
- Part of the team designing internal applications

Would appreciate any recommendations! 



",45,22,1732493139.0,analytics
1gz6wrn,"In your opinion, has the optimization pendulum swung too far?","What I mean by this is have we gotten to a point where companies are investing way too much for way too little gain?

For example, demand forecasts can be useful. And they might even be pretty damn accurate with 5-10 variables. Is searching for and applying those next 5-10 variables really helping that much. Is the team dedicated to optimizing inventory and merchandise layout in stores really worth the ROI? 

I am not at all saying no analytics is useful. I think data is useful in some industries and extremely useful in others. But have some companies gotten to fixated on data/optimization/forecasting to the point there’s an excess of analysts who are not providing any additional value?",19,10,1732497013.0,analytics
1gzehik,Is there any Analytics for BlueSky?,"Is there any out-of-the-box solution similar to what Twitter has but for BlueSky? I see more and more hype around BlueSky, but I need help finding guides or descriptions on ready-made analytics for this social network.",3,2,1732524324.0,analytics
1gyus70,Need some help with HR analytics,"Hi, all,

I'm working on a project with some HR data. I'm trying to create some useful dashboards (using Tableau) from this information, but could use some guidance on what to look at.

I have three excel spreadsheets.

The first is employee\_data. The fields are as\_of\_date,	employee\_id,	employmentStatus,	location,	division,	department,	tenure (months),	tenure\_in\_position (months).

The second is hires. The fields are as\_of\_date,	employee\_id,	employmentStatus,	location,	division,	department

The third is terminations. The fields are as\_of\_date, employee\_id, employmentStatus, location, division, department

So far, I've thought of the following metrics: Total headcount per division/department/location, average tenure per division/department/location, and average tenure of terminated employees (again per the usual dimensions).",6,11,1732465443.0,analytics
1gyvqu3,How to train a multiple regression on SPSS with different data?,"Hey! Currently I'm developing a regression model with two independent variables in SPSS using the Stepwise method with an n = 503. 

I have another data set (n = 95) in order to improve the R squared adj of my current model which is currently around 0.75. 

However I would like to know how I could train my model in SPSS in order to improve my R squared. 
Can anyone help me, please?",3,2,1732467834.0,analytics
1gykfd8,How should I go about  making things more efficient?,"Some context

I interned at this organisation for a year and now that I am expected to graduate, I have been contracted by the organisation to help out with making the data analysis and validation process efficient and as automated as it can be. The oganisation uses Microsoft 365 license and hence has all the access to the Power apps. Unfortunately, the team is too busy with their portfolio (it is not a data team) to really find the time for improving efficiency.

The expectation is that I will be able to help them out, make things more automated and use my data analytics skills to provide them with monthly insights. It is a government organisation, and hence, will not allow the usage of Python/R as they deem it to be unnecessary and potentially dangerous. I do not mind that as I have had good experience with working on PowerBI and Excel. The issue however is that the data is not on a SQL server or a datalake which will allow PowerBI to perform the required ETL and allow me to analyse. 

What is the problem statement ?

The data comes in as a standalone Excel submission from the service providers. These files are then individually validated, analysed, and insights gathered. This can get really inefficient and overwhelming very quick as the project keeps moving forward. The Excel file has data that would literally be nightmare for PowerBI to work on (It is not in a Tidy format). 

What I intend to do ?

I have this idea in my head where I could potentially automate the data cleaning process using Power Query. Our service providers submit the data in the exact same format each month. So, using Power Query would allow me to convert the data into a tidy format, allowing me to feed it into PowerBI and analyse/create dashboards. As the Excel template does not change, a simple refresh should update the data each month. The Microsoft 365 license means that we also have Sharepoint access. I also intend to make good use of this, so that everything is now centralized, easily accessible, and updated on the go. I also hear a lot about the automation benefits of Power Automate but have never used/experienced it. I am more than willing to learn this and implement

Your inputs in this which would help a lot:

Firstly, how I could use Power Automate as a resource to help streamline the process?  Do you think I have the right approach to the problem? What are some of the pitfalls I could fall into, considering I will be the only ""data"" person in the team and will need to contact the business intelligence team of the organisation for any specific help (my manager is willing to support me and get help from other departments but this is usually easier said than done)?



",7,3,1732428892.0,analytics
1gy464m,Ask me anything: 3+ YoE and Just Accepted a New Offer,"I'm still fairly new in my career as a DA but I recently went on the job hunt for a new role and want to share some stats real quick!

**Total Duration:** 1.5 months  
**Applied:** 137 companies  
**Interviewed:** 12 companies  
**Interviews Held:** 27 interviews  
**Final Stage:** 4 companies  
**Offers:** 2 companies  
**Accepted:** 1 company

It seems like we have a lot of people in this channel asking for career advice and while I'm not an expert, feel free to ask anything! Happy to share what I can.

EDIT: This is US based and in the SaaS space.",62,61,1732381067.0,analytics
1gyh8iv,Preparation tools and resources,"Hey Community , 

  
I am actively looking to change and would appreciate if you could send me the links , references to the training , preparation material  that you might have.

  
Thank you very much in advance. 

  
",0,5,1732417417.0,analytics
1gy9x19,Learning Excel as a CS student ,"I’m currently back in university majoring in computer science (post bacc program) with a concentration in data science/big data analytics. But I already have a BA/MA. My MA was paid by my previous employer and I pursued a certificate in data analytics which was how I was able to convince them to offer tuition reimbursement for the program (I worked in data entry). I graduated but I was never able to find a data analyst. So I started looking elsewhere and I was able to land a remote role in digitization. 

However, I’m still interested in finding a data analyst role as I believe that work experience as a data analyst and a CS degree can help me stand out for a data engineer role once I graduate. Ideally I would like to work as a data analyst while I’m still in school for CS. I’ve been focusing on improving my skills in SQL and Python, but I wonder if I should focus on improving my skills in Excel? I was thinking about taking a few of Maven Analytics Excel courses and create some Excel projects to add to my resume. Are there any other resources that help with learning Excel? Thanks in advance! ",2,10,1732396124.0,analytics
1gx83uu,Rant: Companies don’t understand data,"I was hired by a government contractor to do analytics. In the interview, I mentioned I enjoyed coding in Python and was looking to push myself in data science using predictive analytics and machine learning. 
They said that they use R (which I’m fine with R also) and are looking to get into predictive analytics. They sold themselves as we have a data department that is expanding. 
I was made an offer and I accepted the offer thinking it’d be a good fit. 
I joined and the company and there were not best practices with data that were in place. 
Data was saved across multiple folders in a shared network drive. They don’t have all of the data going back to the beginning of their projects, manually updating totals as time goes on. No documentation of anything.
All of this is not the end of the world, but I’ve ran into an issue where someone said “You’re the data analyst that’s your job” because I’m trying to build something off of a foundation that does not exist.
This comment came just after we lost the ability to use Python/R because it is considered restricted software. I am allowed to use Power BI for all of my needs and rely on DAX for ELT, data cleaning, everything. 

I’m pretty frustrated and don’t look forward to coming into work. 
I left my last job because they lived and died by excel. I feel my current job is a step up from my last but still living in the past with the tools they give me to work with. 

Anyone else in data run into this stuff?
How common are these situations where management who don’t understand data are claiming things are better than they really are?",240,127,1732282899.0,analytics
1gy4yhx,Blindspots as a CS student who pivoted to DA?,"US Citizen, graduated January 2023 in CS and wasn't able to find a job for a year mostly due to poorly managed ADD getting in the way of me actually applying, so I did a basic DS bootcamp end of that year  through simplilearn (I know) to get certified and try and pivot out or just show that the time wasn't entirely wasted. Even though the bootcamp was pretty crap, I learned from it and was able to start using some of that knowledge this year in some of my jobs.

This year I've been doing very simple webpage design, some digital marketing/SEO stuff, had an IT job doing hardware repair and data analysis on server failures + technical writing for that until the company fell apart a few months ago, and some claims and referral data analysis for a bunch of small health practices in my area. I'm coming up on a year of experience overall, specifically in those web design and data analysis areas since I've been freelancing the whole year on the side, and I'm hoping to use that to get a role that pays more than $18 an hour once hiring season hits, but I know for a fact there are areas I need to improve in because I've never been hired as a data analyst with a team to work with and learn from. The only thing I've really been asked to do is make reports for different clients in excel, and I ended up using the opportunity to gain experience with both Power BI, more excel knowledge like pivot tables, and Tableau. The problem is outside of specific instances, I really don't know what it is that I don't know.

Here's what I'm current doing to get ready for interviews: I'm working on getting my SQL back up to par by going thru sqlzoo as well as a GitHub SQL course that someone posted here a month ago, since my previous data engineering mentioned my SQL skills specifically needing work. I'm also trying to learn react & typescript for web development instead of just using basic CSS with one of those pagebuilder services like Wix. Planning to build a portfolio site with those skills to showcase my freelance work as well as a site for my own music/visual art career and use that specific project to strengthen my knowledge on database design and front-end/back-end development. At some point I also want to try and train some image generation model on my own visual art and make it generate new art just to see what it spits out, but that's something I would do after my own portfolio site is ready.

I've done a bit of research on how to prepare for these interviews like watching one or two mock interviews, but I'm still a little lost on what my next steps should be. What other things should I try and do for getting ready for interviews or just strengthening my knowledge base? Are there any specific resources similar to leetcode that people in this industry use?",1,5,1732383099.0,analytics
1gxvent,Looking for advice,"Not sure if this is the right subreddit but hi, I'll make this quick.

I have a degree in mathematics - applied statistics and I want to get my foot in the door of the Data Analytics industry. I'm a very quick learner and self-teacher but I have no experience and about to turn 30. What would help my resume the most? Certificates? Portfolio of projects? ",1,4,1732352114.0,analytics
1gxhkvb,How Can I Push Through and Advance My Career?,"Hello everybody, I really need your help because I'm struggling so much mentally on how to continue my data analytics path. First of all I have a bachelors in Economics and a masters in Data Science, I graduated this March from my MSc, so as you you can understand I have a very big problem on finding a job, it's taking a huge toll on my mental health and I've forfeited for the last 3-4 months. 

  
Main thing I struggle with is that I don't really know where to focus, what tech, what programming language, etc. I know Python (not so well but with the help of ChatGPT I can do everything, coming from economics I don't really know how to code good, but throughout my studies I know how to spot mistakes and adjust code to where it needs to be or tune models or anything, so the knowledge is there but not the coding, ikr its bad I will try to work on that). I also know SQL and have done dashboards in PowerBI and Tableau, I could really easily learn Excel too if needed. What should I work on? should I for example try to master Python and SQL? Then choose either PowerBI or Tableau and work on them too?

  
Also second thing the industry is using too much different tech, for example I search for the limited jobs that currently exists and everyone needs something different. For example I saw ads asking for AWS, Azure, MLOps, from the data engineering side that I'm also looking for to break in data analytics, snowflake, mongodb, Apache Airflow, databricks, SSIS, and all that chaos. 

  
Like literally what could you suggest me? I don't know how to continue with meaningless projects with toy data or what tech to focus to guarantee me a job as a starting point. I'm feeling so lost and devasted that I studied for 6 years to deal with this.",8,9,1732307484.0,analytics
1gxpigx,Is analytics for me?,"I’m a freshman and wondering what to major in college.  I’ve always had an interest in numbers and math looking at charts etc, but no so much theoretical math.  Physics is cool but it’s not really my thing so probably not gonna in any type of engineering.  With CS the classes doesn’t seem to interesting to me and I heard it’s pretty theoretical.  After doing some research I heard analytics might be good for me I’m good at math, it’s practical, and it’s businessy which I’m also interested in. 
1. Hows the pay?  And maybe in comparison to other tech roles like software engineering
2. Work life balance? 40 hrs? WFH? Stressful? Etc.
Saturated? (I plan on doing internships and a lot of outside stuff other than grades)
3. Career progression/exit opportunities?
4. Anything else many people overlook
",0,12,1732329754.0,analytics
1gx8b8t,Final class for Master's - Enterprise Data Management or Machine Learning / AI?,"I have one semester left in my master's program, and I'm only able to take one more class. I can't decide between them! Can any experienced analysts help me out? I know analytics jobs are diverse, so your experience will likely be different from mine, but it would be helpful to hear from you anyway.

The two classes are Enterprise Data Management and Machine Learning / AI Applications with Python. My current job entails wrangling data from SQL databases and creating dashboards in Tableau, so the Enterprise Data Management class sounds more relevant to my current role. However, I personally love working with Python, and while I think an AI class may not be as relevant, it sounds very fun and might be more future-proof.

What do you think? Leaving the descriptions of the classes below if that helps!

""Enterprise Data Management: Explores how the data warehouse provides the foundation for analytics within the enterprise. Topics include: dimensional models, design and creation of data warehouses and data marts, ETL process and the extension of the data warehouse concept to big data.""

""Machine Learning / AI Applications with Python: Machine learning is pervasive, with high-stakes applications spanning all business sectors, including fraud detection, high-frequency trading, and highly personalized and relevant marketing campaigns. Machine learning requires interdisciplinary techniques to create algorithms that sift through large volumes of data to support business decision-making. This class will equip students with the analytical techniques and skills to build and evaluate machine learning models using Python. In addition, students will use Python for a hands-on exploration of a broad crosssection of algorithms for machine learning, including linear models and dimensionality reduction. Students will gain additional familiarity with deep learning models such as artificial, recurrent and long shortterm memory neural networks. Cloud-based resources and the open-source frameworks TensorFlow and Keras will be leveraged. At the end of the course, students will be prepared for accurate, effective and ethical research or industry application of machine learning techniques.""

Thanks in advance!",2,3,1732283504.0,analytics
1gwl77t,Anyone notice lower salaries for analytics roles?,"I'm currently interviewing with 3 companies for roles that require 3-5 yoe in a HCoL area in the US and their salary range are around 70-85k. Some even have an analytics manager title but the pay is 70-80k. Anyone else notice salaries being lower while also requiring more experience?

PS: they're more focused on marketing analytics but require (again ,3-5 yoe) in analytical and BI tools",63,45,1732210871.0,analytics
1gwae96,"Analytics market is rough. Officially checking out, changing careers","As above. Every job had 100 plus applicants, tech is evolving fast. Employers have an ever increasing amount of skills they want for less pay.",111,111,1732172429.0,analytics
1gwmw0a,Tracking KPIs for an AI Video Maker Launch,"Hi analytics enthusiasts,

I’m working on the launch of Calvin AI, a video maker app for creators. It automates video creation and uploads to platforms like YouTube and TikTok.

Which metrics or KPIs would you prioritize for evaluating the app's performance in the market? Are there any specific analytics tools you'd recommend to track user behavior and engagement with the app?

Looking forward to your expert opinions!",1,3,1732214805.0,analytics
1gwcm64,Trying to get a Pulse,"I’m thinking about switching careers and was thinking about going into data analytics. I’m trying to get a pulse on the market. From what it seems the sentiment is looking a little grim from some of these posts. Examples being tech layoffs, over crowded market, bloggers saying this field is the best thing since sliced bread etc. 

Do you think sentiment is a byproduct of a difficult job market overall or is this field really having challenges for the foreseeable future due to companies not valuing the position or thinking AI can solve their issues? I was going to enroll for a class to learn SQL as a starter. Any advice and opinions welcomed. 

Thanks!",5,14,1732182336.0,analytics
1gwkq6j,Advice and opinions on getting Magister's degree for Data Analyst role,"Hi everyone, I am in my 3rd year of BCs in Marketing and communications. Looking for a change to Data Analytics. Throughout my time at uni I have realized that I want to work in a field related to analytics. I am wondering if it is worth to get Magister's degree. I am also currently learning SQL with Data camp and planning to learn Power BI to get PL-300 cert. After reading some threads I guess it is better to get a degree in Statistics?
Maybe you could give me a relity check.
Any advice or guidance are appreciated. 
I am currently located in Prague, Czech Republic.",1,1,1732209796.0,analytics
1gwd8e1,Help- best practices for Analytics and tracking strategy for a website in multiple regions ,"Hi everyone,

Hope I can grab some help here. 
I’m currently working on a tracking strategy for a website that operates in multiple regions (e.g., France, Japan, Singapore) with plans to expand further. 

The main goals are to:
1. Maintain accurate, region-specific insights for stakeholders while also having a centralized view of global data.
2. Comply with local privacy laws (e.g., GDPR in Europe).
3. Optimize costs and resources while ensuring the system is scalable for future expansion.

Some initial thoughts and questions I’m
- I’m thinking of creating separate properties or data streams for each market?
- with this in mind, how do I ensure event consistency across regions while allowing for region-specific tracking?
- Has anyone implemented server-side tracking for such use cases? How did it impact costs, implementation complexity, and reporting?

Any insights, examples, or resources would be greatly helpful. 

Many thanks ",3,5,1732184934.0,analytics
1gw33di,is it normal for me to have no cloud knowledge ,Hello  im a data analyst intern here i have been in an internship for about 8 months (still ongoing about to end in 4 months ) and now im looking for a job as an analyst for some other company but they are asking for cloud knowledge is it normal for me to have no prior knowledge on cloud eventhough i have expereince for 8 months ? ,12,8,1732149120.0,analytics
1gw6r3r,Career Advice,"Hi all. I’m looking for career advice as I’m entry level into data analytics/science and I’m unsure if I want to continue to be in this field or not. Although I love data and performing analyses and building machine learning models, this unforgiving market has me rethinking things.

Some context, I have a BS in Statistics (theoretical maths minor), a couple internships, and am currently doing a MSCS with a concentration in ML. My BS and MS are from fairly prestigious universities, and hold a decent amount of influence. I like to believe they’ve helped me get some interviews, although I’m not certain.

As for the job search, I’ve been trying to get entry level data (fill in the blank). From healthcare, tech, finance, etc. I’ve probably applied. I’ve gotten a couple interviews, and earlier on (about June) I know I fumbled due to self inflicted poor interviewing skills. However, I spent the time to learn how to interview and have stepped it up quite a bit. However, recently I’ve either been rejected due to: HM doesn’t like me, budget cuts (final candidate after numerous rounds), or ghosting (after numerous rounds and/or projects). The latter seriously make me reconsider everything I’ve done up until now. 

Tl;dr, I’m lost and looking for something. Any and all advice appreciated.

Edit: more context. Originally, I had planned on pursuing a PhD in Stats (hence minor in theory maths) but my GPA wasn’t competitive enough. It’s above a 3.3 but no Latin honors.",3,4,1732159606.0,analytics
1gwbbhz,somebody please help ,"
is bachelor of science in business data analytics a good degree especially in pakistan and which universities should one go for? what’s the scope and is maths and cs compulsory for it? can’t one take foundation or extra classes for these subjects if one hasn’t took these subjects ever before?",0,1,1732176426.0,analytics
1gw9hp3,random tool stack question,"I have a new project coming up and need to build a simple back end and front end to enter and update some data. My company pays for every tool under the sun. for those that have used them, would you rather use snowflake + streamlit or mssql server + power apps? ",1,1,1732168883.0,analytics
1gvwseu,Looking for Recommendations: Best Sources for Analytics News and Updates?,"Hey everyone! I’m on the lookout for great accounts, websites, or blogs that regularly post about analytics. Ideally, I’m interested in sources that cover a range of topics, like:
• New product launches in the analytics space
• Emerging startups worth keeping an eye on
• Cool tools or fresh features from existing products
• Unique implementation ideas or case studies with existing analytics tools",5,4,1732128235.0,analytics
1gw0feu,Small DA Gigs,"What are some DA projects that can readily be offered to local small businesses or individuals, where the data is already there?

For example, helping a small business optimize social media postings, or a restaurant's staff numbers and sales to better overlap.

Any ideas or proven experience selling small gigs to locals?",1,0,1732139492.0,analytics
1gw0eve,Small DA gigs,"What are some DA projects that can readily be offered to local small businesses or individuals, where the data is already there?

For example, helping a small business optimize social media postings, or a restaurant's staff numbers and sales to better overlap.

Any ideas or proven experience selling small gigs to locals?",1,3,1732139450.0,analytics
1gvcp6w,Sports Analytics Career,"Hey everyone!

I am very interested in getting into the sports analytics industry. I have a bachelors in psychology, but sports is my passion, and I also love math. Do you have any recommendations on what route I should take?",13,7,1732062777.0,analytics
1gvg0r5,How important are degree titles?,"I’m currently in my undergrad studying Data Analytics with a minor in Computer Science. I’ll be applying for masters programs soon and my school offers a Data Science masters and Data Analytics: Statistics masters. These programs are nearly identical depending on what electives you pick, but the Data Science program has a required thesis whereas the Data Analytics: Statistics program has a required capstone project. I’m wanting jobs as a data scientist, machine learning engineer and I know my coursework in my undergrad and either graduate program will prepare me well, but will employers really look at the degrees differently? I’d rather not do a thesis, but would do it if Data Science looks better than Data Analytics: Statistics on my resume. Thoughts?",6,5,1732072588.0,analytics
1gvc68f,Does anyone else find being able to apply the 'basic' stuff they learned in education to a job fun?,"TLDR ; enjoyed using Monte Carlo esque methods for an experiment at work, despite the seemingly miniscule business value.

Context - I work in SaaS as a data analyst looking at customer tickets, things like time to resolution (TTR) etc. 

I thoroughly enjoy the freedom I have in my job to explore data and not just deploy XGBoost models and create dashboards, and one question my manager (a very open minded person) had was ""given our distribution of TTR samples, if a particular customer was to have 20 or 70 tickets, how does our expectations for their average TTR change?"" 

Feel free to critique my methods (recent grad), but what I did was take this distribution (heavily right skewed) find the mean and SD, and use those values to solve for the rate and shape parameters of a Gamma distribution (I felt it was most appropriate). Once I had this approximate distribution I did a loop of taking random samples for n=1 to 300, many times to get a distribution of sample means.

Now that I have my different distributions of means, when you plot them you see most are approximately normal (CLT) but what I was interested in was how the tails would become shorter as n gets larger (a customer having more tickets), so now what we did was compare that to our observations and see which customers have a TTR that fall outside of our 95% range of means for the given n, hence better highlighting the customers that received an especially bad service.

While I believe the applicable business value of this is quite minimal (a customer doesn't care about probability distributions, just their own individual service, and just looking at the data before this experiment would tell you who has been receiving a poor quality service) I did find this to be quite fun, especially for a work environment. So maybe this could serve as a message to those that don't enjoy their job enough that maybe we can create our own opportunities to do fun experiments.

Thoughts?",10,4,1732061305.0,analytics
1gv4ift,Deploying models,"Guys i have a couple of questions about deploying models:

1. is it difficult for someone with a ds background to learn how to deploy a model? i mean can one or two courses/certificates teach that or a strong swe background is needed?

2. do data analytics/data science master degrees (for example MIT MBAn) teach how to deploy models and other MLE stuff or they generally only teach how to analyze data and build models?",10,3,1732041723.0,analytics
1gvcqax,Looker Studio - Event Count w/filters different from calculated field,"Made an event in looker studio called ""error"" and I wanted to create an error/user metric using calculated fields, but my issue is that I have error defined in two different ways. One is using ""Event Count"" (I'm using GA4) and filtering the event name to ""error"" and get a count of 5. I then tried to make a calculated field like this 

  
COUNT(IF(event\_name = 'error' then 1,0 END)) or something to that degree and the field itself works! But I get a count of 1 instead of 5 like my event count field. The calculated field needs to work because I am trying to make that error/user metric. Any reason why I'm running into this? Thanks!",3,2,1732062862.0,analytics
1gv0ice,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",10,12,1732032016.0,analytics
1gvfjdn,Bachelor's in Education to Masters in Business Analytics or Business Administration?,"Hello,

I'm currently working on my Bachelor's degree in Early Childhood Education, but I know I want to pursue a Master's degree right after I'm done. I don't know if I want to pursue business administration or business analytics or if there is a better route to take. I don't even know if business analytics is a good route to take but I am willing to do the study time and work for it. I wanted to stay in education but the way the fields going and the fact I want to provide for a family weighs heavy on my shoulders and I feel like I have to do what's best for my future. I have my next job locked in with career counseling when I'm finished with the bachelor's if that helps with experience in analytical work. I'm m/23 with five years of educational field experience under my belt. Any advice? ",1,8,1732071109.0,analytics
1gv4d8g,Google Analytics 4 finding referrals from job boards we don't post to,"I've recently been digging into the last year of GA4 data to see where our candidate traffic is coming from. There are several sites and job boards we don't post our opportunities to that are showing up as referral sources. This also includes some ""AI-powered"" job boards. On some of those sites, the information isn't even correct.

Are there companies scraping popular job boards for openings and posting them on their own sites?",1,3,1732041377.0,analytics
1gvbubj,HS senior son considering analytics - recommendations for or against?,"My son is a HS senior, and, based on his strong background in math - finished AP Calculus AB & BC with A's as a junior - he is considering analytics and/or applied math as his concentration in college. He will either be attending Johns Hopkins University or Harvard University - has an offer to play football at JHU in-hand and is currently applying restricted early action to Harvard, where he would be a walk-on football player, so needless to say, he's going to have an opportunity to get a great college education.  

All that being said, we don't really know a lot about the actual analytics profession, so would any of you analytics professionals please be willing to share some advice for him at this point in his path?  Is there a specific major that would best prepare him for the field?  Is there anything that you would have done differently on your path to your current job?  How will AI impact the profession in the near future?

I appreciate your help!",0,29,1732060403.0,analytics
1gupieb,How long do you keep your web analytics?,"I'm running a WooCommerce store with Matomo for web analytics. I'm trying to decide how long should one keep web analytics for?

I was thinking at least 18 months to see trends from prior year, but beyond that, are web analytics needed?",7,10,1731992327.0,analytics
1gubrrn,"How Important is Linear Alegebra, etc. Truly in Data Analytics?","Pretty much the title. I'm someone who came from a business background (finance/accounting) and have a good amount of experience transforming/analyzing data from large/disparate sources and presenting key findings to executives across a range of business problems. While I'm certainly not THE most technical or quantitative person on an analytics team, I do have a relatively strong, albeit limited, background in certain data skills, such as Python/statistics, such that I was able to solve problems or do some of the work myself when more technical folks were busy or otherwise unable to help.

I want to keep building on my data skills because I frankly enjoy analyzing and explaining data/generating insights moreso than I do the regular cadence of reporting that I am forced to do in finance/accounting roles. I also want to analyze and solve problems beyond just profit/loss metrics. 

When I look online, I keep seeing that fairly advanced math (i.e. Linear Algebra+) is often seen as foundational knowledge for data science/analytics. My question is how correct is this outside of the highest levels of data science (i.e. FAANG or other very data-centric organizations)? To be blunt, I've found the following to be most useful in my career so far:

1. Being able to transform or build data models that aggregate/generate reports that a business partner/stakeholder can understand quickly and without error. To me, SQL/Python are generally good enough to solve this as you can use these tools to ETL the data and then Excel to put it into a spreadsheet for folks to see trends or create their own ad-hoc analyses

2. Once step 1 is done, simple definition of KPIs that are meaningful, being able to track them, as well as some visuals, dashboards, etc. to slice and dice data. To be honest, I can solve for this via PowerBI, maybe even Excel using pivot tables. The first part of defining business requirements, etc. mostly comes from having good business sense or domain knowledge. Don't really see a use case for linear algebra, etc. type of math here either

3. Strong communication skills and being able to present the ""so-what"" in plain english. Again, I'd almost argue that using really complex algorithms or advanced math will confuse the average business user. Candidly, I've never found much use for executives to present anything beyond some regressions, which I don't believe requires a ton of advanced math (correct me if I'm wrong here). 

So can someone help me understand where the major use cases for really advanced algos/math come up within the data world? I feel like there's something I'm missing, so would really appreciate some insight. Further, if anyone has good resources that explain practical use cases of linear algebra, etc. when coding, that'd be great. I find trying to pick up linear algebra by studying the theory hasn't been helpful, and I'd love to understand more practical examples of how I can apply it while furthering my education.

Thanks for the help!",37,33,1731954685.0,analytics
1gueinu,"Currently in cloud administration, debating switching to data analytics or marketing?","I'm a cloud admin thinking of switching careers to data analytics or marketing. The interviews in tech seems really intense even after working in tech for a few years as a system or cloud admin. The interviews feel like tests where they want you to memorize multiple applications, processes, and steps. The hiring for the last year has been ruthless too, and I've had less responses from jobs even though I have more experience. 

I thought of data analytics first because it relies less on programming like powershell, javascript, or cisco commands. It also is more interesting analyzing charts. I'm interested in investing so observing patterns and seeing how changes can improve company earnings interests me because you actually see a result from your work. I feel the charts are less abstract than random powershell scripts that you would use as a cloud admin. 

Idk if it'd be possible for me to switch to data analytics? I don't have a tech degree. I do have 4 cloud certs and CompTIA. I've been in a few tech jobs over the last 4 years. Would I need an MBA or to go back for another bachelors?

My last option is marketing. Because I like the analytical nature similar to data analytics. The different advertising creative ideas interest me as well. I also like that it's not as technical. However, I'm an introvert, so idk if it would require a lot of direct facing customer work. I've heard some say the pay isn't great and it's like a sales job, is this true?

From my experience, interests, and qualifications. Should I stay in tech as a cloud or system admin or switch to marketing or data/business analytics?",6,34,1731961395.0,analytics
1gu7zjt,Whats the perfect career path for being a Business Analyst,"Hi everyone. In your experience, whats the best course of education one should take to land a job as a business analyst(BA) in the Tech sector? Some say do your undergraduate in BA and then an MBA, or the other way round?

CONTEXT: Im a fresh high school graduate and I have a huge creative side. I like to sing, film edit funny videos and extroverted too. I talk alot too💀. I took Business Studies in High-school and loved it so much I got the highest score.  I like the idea of applying business knowledge in an environment such as Spotify, Netflix, or any other major tech corp to make it more innovative and accessible.",8,6,1731945507.0,analytics
1guftnv,Guidance neeeded: leading bank Data scientist interview in 6 days,"Hello Everyone! 

I have an interview schedule for Data scientist at a leading US bank

Job role requirements: SQL, PYTHON , TABLEAU

my skillset: SQL problem solving( writing SQL queries)

To what I have understand and got the information is that 
The first round will be technical 
Consisting of python ,sql and case studies 

Please guide me on 
1.theoretical and hands on problems for python how I can watch yt videos if any and solve python problems 
2. Theoretical question on DBMS 

Any and all suggestions are welcome",2,3,1731964651.0,analytics
1gu5wda,Question on presenting multivariate categorical data,"Hello! I have a dataset with people who answered multiple (five to be exact) questions on disabilities in their families, and turns out that many of the types of disabilities co-occur. I wanted to show this in a report somehow, but I really struggle to find an appropriate way of presentation. I would like to show how many people have co-occurring disabilities, and which disabilities co-occur. I do not want to use an alluvial graph or parallels sets, I would rather have something like a Venn diagram, but I don't think anything like this is used for presenting data.

Could you please help me?",2,3,1731939966.0,analytics
1gtg9pt,WHAT DO I DO. If I can't land a job NOW and the market is only going to get worse THEN WHAT DO I DO,"I cannot continue to be poor. I cannot enter my 30s with no career making shit money living paycheck to fucking paycheck. Not after all the hard fucking work I've put in and all the suffering I've had to experience just to get my fucking education. 

  
**MA Mathematics, Certificate Computational Linguistics - A** university

**AS Data Science and Computer Science - A** community college

**Certificates in Java and SQL/Database Development - A** community college

**Data Analysis:** Python, SQL, Excel, Snowflake, PowerBI, Tableau, Data Visualization, Natural Language Processing, Large Language Models

Why isn't this enough to get an entry level job? Even with relevant work experience? I get interviews, sometimes I get deep into the process. One job interviewed me SIX TIMES. NO OFFER. WHAT DO I DO. I cannot continue like this with no future and no job prospects.  
",56,245,1731858157.0,analytics
1gtjumd,Is it worth it to get a MS in Data Analytics?,"Hi everyone! I (23F) have wanted to pivot into data analytics for a while now... Is it worth it to get a MS in data analytics with my current credentials or will my path be enough?

As background, I graduated last May of 2023 with a BFA in Industrial/Product Design (STEM-certified major) and a minor in Business from UIUC. I have 2 internships under my belt, one being with a non-profit where I did social media marketing and 2. at a audio electronic company as their HR/Marketing intern. After graduation, I took up a HR sales consulting role for a year where I was super client-facing and managed my own book of business. I did NOT like this role, as I had to serve as an admin for the team, an EA for our CEO, all while handling all of the incoming website leads.

Since leaving that role, I started to self-study with Alex the Analyst beginner SQL tutorials on Youtube as well as making my way through the Data Analyst in Power BI track via DataCamp. After I finish this course, I was planing on taking the PL-300 MS test to gain a certification. I've built one project so far and have posted it live on my GitHub portfolio, and this went through my process of merging in SQL, data cleaning in Power Query, and visualizing in Power BI. I found that I really like my creative side when visualizing and am interested in a Power BI analyst role. 

We all know how saturated the job market is and transparently, I haven't had much luck cold applying to entry-level roles. Even internships, they require you to be enrolled in a master's program for institutionalized benefits (ugh). Considering that I don't have experience with hands-on data, I am in the position where I have an unrelated bachelor's + no experience.

Wondering if I enroll in a master's program to gain education + ability to apply to internships? Is this my best bet?",30,27,1731867493.0,analytics
1gtrs03,Tips or help for Business Analytics Internship Interview,"Hi everyone,

I’m currently interviewing for the **Business Analytics Internship** at Comcast, it’s an 11-week paid internship that offers exposure to work on thought-provoking projects involving data analysis and reporting.

Here’s a summary of the role:

* Work with large datasets to conduct analysis and derive insights that drive change in KPIs.
* Utilize tools like SQL, Tableau, Power BI, and Excel to create visuals and reports that align with business strategies.
* Collaborate with teams to improve processes and adopt best practices.
* Present findings and recommendations through storytelling and visuals that resonate with business goals.

**Preferred skills include:**

* Basic SQL/relational database querying
* Experience with Tableau or Power BI
* Understanding of data models
* Proficiency in Excel and PowerPoint

I’m looking for advice from anyone who has experience with **business analytics, internships at Comcast (or similar companies), or interviews for data-related roles.**

# Specific Questions:

1. **Interview preparation:** What are the key technical and behavioral questions I should prepare for?
2. **SQL/Tableau:** Any recommendations for resources or practice projects to refresh these skills?
3. **Storytelling with data:** How can I effectively present data insights during the interview if asked?
4. **General tips:** What’s the best way to stand out in a competitive internship like this?

I have a background in **MIS**, experience with SQL, Tableau, and dashboard design, and a genuine interest in leveraging data to drive decision-making.

Any tips, advice, or personal experiences would mean the world to me! Thanks in advance for your help 😊",9,8,1731888827.0,analytics
1gtp17o,FP&A to Analytics?,"I’m hoping to get a bit of guidance on what my potential next steps would be career-wise. I took an FP&A Manager position about 2 years ago. This position is within our Analytics team. I’m able to get involved in projects that are both financial and analytics in nature. I’ve gotten experience with SQL, PowerBI, and soon to be Python. I’ve had quite a few projects that have been focused around process improvement and restricting teams. The analytics portion and process improvement/restructuring have been very enjoyable for me, and I seem to be fairly good at it.

I find myself wanting to do more analytics focused work, opposed to finance. I’ve got a natural talent for finance, but I’m much less interested in it. I’m self-evaluating on where exactly I should go from here. For those that have made a jump from finance to analytics, how did you find it? I feel like the skills are very closely aligned. I’m likely behind the curve for others in my peer group that have been purely in analytics, but my finance skills may fill a bit of the gap. The general consensus pay-wise, seems to be that analytics has a higher floor, but finance has a higher ceiling, due to executive opportunities.

Any feedback would be appreciated!",9,6,1731881157.0,analytics
1gtb4xl,Christmas gift for a budding data analyst,"Hi all,

A friend of mine wants to switch careers and become a data analyst. I'd like to support them by giving a Christmas gift that will help them along the way.

Are there any books, productivity accessories, or paid resources you recommend that will help with their learning journey?

Thanks in advance.",2,9,1731840924.0,analytics
1gtgrjc,"I have completed my bachelor's in Statistics, Is it mandatory to complete masters if I am looking for entry level job in data analysis? ","I live in India, In 2023  I completed my bachelor's in statistics with CGPA of 9.72  ,For period of one year I worked in a BPO just to support my family and some personal expenses, Now I am persuing Full Stack Data Science Course From Ed Tech institution which is providing this course in association with IIT Guwahati ( A prestigious institute in India ).
I know python and Sql to the level that I am comfortable in solving leetcode Database problems, I invested 2 months in prepration for Power BI Data Analyst Associate Exam of Microsoft and Successfully Cleared it ,Now I am preparing for AWS machine learning Speciality exam, My question is , considering my educational background, Courses and certificate what are my chances of securing entry level data analyst job as a fresher , I just have 3 years degree in Statistics without any master ,So guys  just want to know your  view on Persuing Master's ",0,7,1731859503.0,analytics
1gt4jhp,Sports betting analytics/modeling,"I recently made a post in 
r/algobetting showing the results of some deep analysis I've been doing on a sports betting moneyline model. If anyone wants to comment or bud in on the conversations about expected value I would appreciate some other opinions from experts in this sub since that sub can be full of pessimist sometimes. 
Here's the link to the post: https://www.reddit.com/r/algobetting/s/RrZ23ovl3o",1,1,1731813873.0,analytics
1gsvg8i,bootcamps for a beginner?,"any advice or reviews for bootcamps for someone looking to get into data analytics? i haven’t been in school since i was i believe 19 or 20, i’m now 25 and living in the US, recently received a SSN so looking to get a job (in anything) or start some form of education again. data related things interest me, although my last education experience was with computer science (i left before taking my college exam.. fuck up on my part, so my highest level of education is high-school).

i’ve been reading negative things about generalassemb.ly or bootcamps in general, but i’m struggling to see how to get my foot in the door with something that isn’t being a waitress or cashier 

any advice is welcome, thanks for reading",3,14,1731786786.0,analytics
1gsonp6,Looking for feedback!,"I am an analytics professional with over 10 years of experience in Marketing and Product Analytics. One of my biggest pain points has been spending hours every week checking multiple dashboards to monitor trends.

To solve this, I built a Chrome extension that automates insights from Google Sheets and delivers them straight to your inbox on a weekly basis. All you need to do is pipe your data into Google Sheets, and the extension takes care of the rest.

I’m genuinely looking for feedback and would greatly appreciate your time and thoughts. Thank you

Here is the [link](https://chromewebstore.google.com/detail/octo/obloifdoiohdfcnimbahlphceecmjoci).",8,6,1731768085.0,analytics
1gstq1f,Time Series for Business Forecasting or Multivariate Analysis,"I'm a student in college studying mis/ business analytics. I have to pick one elective between these two then I graduate. I'm looking into careers in data analytics. Which one would be more beneficial? 

Time series is more interesting to me and multivariate analysis seems very useful. 

  
",2,2,1731782096.0,analytics
1gsg5s0,Has anyone had experience with data engineering academy? ,I keep getting an insane amount of ads the past few months from them ,0,1,1731734113.0,analytics
1grplu5,Proficient in SQL,"For a data analyst how proficient in SQL should we be ? 

When applying to job they usually say knowledge or proficient in SQL. I get nervous applying to them because I don’t know if they expecting 100+ lines of code or just being familiar with the six SQL clauses. 

This is my second data analyst job. And I still have a beginner-ish knowledge of SQL. I want to get an analyst job that uses SQL frequently however I am nervous applying to them.  ",57,37,1731649712.0,analytics
1gs737r,Help Tracking Leads to Phone Calls?,"What's the best way to track leads from a website to a phone call? This client I just started working with doesn't have any kind of shopping cart system- merely a call to action to call their office. I know that's not great, but- that's what we have to work with. I'm guessing I need to just setup a special tag in GTM and apply it to the call to action buttons so I can at least track conversion rates. Any other suggestions on how I can track/manage this data and get some good insights on the marketing we're starting to do? Any way to track actual phone calls, and gain some metrics to tie that to the sales software (Housecall Pro, Quickbooks, etc)?  
Thanks!",1,7,1731705769.0,analytics
1gs4ige,Connecting Tableau To US Census Data / API,Has anybody on here had success connecting Tableau directly to the US Census data / API? Looking to get as close to real-time as possible without having to export .csv files.,1,3,1731698910.0,analytics
1gs0kmn,Podcast Database Auto Update in SQL,"I created a database for a podcast I host to keep track of shows and movies we have discussed, the topics within that episode (or more so main topic), guest we have had on, (if so, their name and profession) whether the episode was a re-releaes or not (have only  had that happen a couple of times).  This is to keep track of our content and so that we are not covering the same thing twice or multiple times. We use Buzzsprout as our hosting service.  What I would like to do is somehow have that table in SQL updated each time we upload an episode to Buzzsprout and it is published, to update the table in SQL. with the fields needed (I haven't worked with Buzzsprouts API and not too sure how to ago about doing this or if it is possible).",1,1,1731688943.0,analytics
1grzzei,How to track conversion in mobile app for campaigns that forwarded to landing page?,,1,1,1731687414.0,analytics
1gridcq,How do you quantify the value of Analytics?,"Do you come up with a value add metric for analytics at your organization?  Is so, how? 

The clearest way would be the incremental lift from predictive models etc.  Which is great. 

I'm asking from a different angle... The overall value of being ""data driven"" as an organization.  So for example, enabling people to use self service analytics tools vs submitting a request and waiting.  Or ensuring that analytics are built into workflows so that people don't need to ""do the math"" in addition to their day jobs.

I hope that makes sense.... And clearly not an easy answer, but would love to hear different ideas or approaches.",12,19,1731626648.0,analytics
1grcvec,How much easier is it to get the next job after your first analytics job?,Just wondering if anyone had personal experiences or thoughts on this. ,24,35,1731612184.0,analytics
1grmppy,how to transition to different industry; From e-commerce/marketing data analyst to health or finance or supply chain/operations industry ?,I have been looking for a job within data analytics or business analytics field and I see where I live currently has lots of jobs within healthcare or finance sector although all these jobs somehow ask for industry experience. Is there a way to get my resume selected in different industry by putting 2-3 portfolio projects in that industry even though my only experience(4+ years) of data analyst is in e-commerce? Any suggestions will be helpful. I was also thinking that job market is tough so therefore I am not getting response from those jobs but also in the requirements/ preferences they always mention experience in healthcare etc. Those people who transition successfully or in hiring positions can give any advice will be helpful.,3,1,1731639681.0,analytics
1grlr5b,Predict Red Flags in Healthcare Insurance Claims (Fraud),I was asked to do a dashboard showing possible fraud in healthcare insurance claims. Can you give me any advice how to start on this. ,3,4,1731636666.0,analytics
1grjfim,Entry Level Job with no College Degree,So I am pretty(intermediate level) well versed with Python's data science/analysis libraries and have done a lot of smaller projects. I also know a little bit of SQL. Are there any entry-level jobs I can get without any college degree? Any feedback would be great. Thank you.,1,40,1731629648.0,analytics
1gr3wu8,Is It Worth Considering an In-House Content Creation Team for Your Store in 2025? Or Just a Waste of Time?,"**Looking at the trends in content creation, we see a major shift starting from 2020.** Yes, creators existed before then, but in much smaller numbers and without the level of influence they have today.

In 2020, with global lockdowns and the explosive growth of TikTok, **the number and significance of content creators skyrocketed.** TikTok in particular was initially dismissed by many brands as a passing trend for teenagers, yet it’s now a *dominant platform in online engagement.*  
*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# TikTok Demographics (April 2024)

* **Women 18-24:** 17.3%
* **Men 18-24:** 18.9%
* **Women 25-34:** 14.6%
* **Men 25-34:** 19.3%
* **Women 35-44:** 6.9%
* **Men 35-44:** 8.9%
* **Women 45-54:** 3.7%
* **Men 45-54:** 4.2%
* **Women 55+:** 3.1%
* **Men 55+:** 3.1%

**63.8% of TikTok users** fall into the over-25 age group, with **49.7% in the 25-44 range**. These users are not only *digitally savvy* but also *trust online transactions* and actively engage with social media, seeking information from influencers who play a significant role in shaping opinions and driving purchasing decisions.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# A Four-Hour Audit with a Jewelry Store

Yesterday, we spent four hours auditing a jewelry store that launched in 2018. This store had effectively used influencers and content creators to drive sales until mid-2023.

Our audit focused on data from 2022 onwards. Together with the owner, we analyzed **current sales numbers and past marketing activities** to understand the impact of various strategies.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Snapshot of Online Performance and Advertising Costs

* **Until Summer 2023**: The store maintained a network of relevant influencers and content creators who consistently converted traffic into sales.
* **Since Mid-2023**: The store reduced its partnerships, retaining only a few key creators from earlier years. Since then, **performance has gradually declined**.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Performance Highlights from the Audit

**2023 Summary:**

* **Sessions (GA4 data):** 541,203
* **Contacts in Database:** 13,935
* **Orders:** 6,538
* **Average Order Value:** 248.58 RON
* **Revenue:** 1,625,216 RON
* **Unconverted Contacts:** 7,397
* **Potential Losses:** 1,838,746.26 RON due to weak communication with existing customers

**2024 Year-to-Date (through Nov 13):**

* **Sessions (GA4 data):** 323,374
* **Contacts in Database:** 20,023
* **Orders:** 4,491
* **Average Order Value:** 258.79 RON
* **Revenue:** 1,162,225.89 RON
* **Unconverted Contacts:** 15,532
* **Potential Losses:** 4,019,526.28 RON from poor engagement with the existing audience

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Key Insights and Takeaways

1. **Organic Reach Builds Long-Term Value:** Organic engagement is crucial for *building a durable, profitable brand.* Brands that establish a strong online presence through organic channels stand the test of time.
2. **Content Creators Are Essential:** Content creators don’t just put your products in front of an audience; they *lend credibility*. Their influence builds trust and can make a significant difference in purchase decisions.
3. **A Well-Defined Strategy Drives Success:** Brand strength isn’t just about ad spend but also about *market relevance.* Consistency, visibility, and perception are powerful elements in brand growth.
4. **Customer Neglect Can Be Costly:** Neglecting your audience, especially those who’ve already shown interest by engaging with your promotions, can cost you. *Failure to nurture these relationships can lead to substantial revenue losses.*
5. **Understanding the Digital Landscape is Key:** The internet works through specific mechanisms, particularly those focused on capturing attention. *Understanding these can unlock new opportunities, increase sales, and open doors for growth.*

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Platform Performance Analysis (2022)

From the analysis conducted on **Meta, Google, TikTok, and Email Marketing**—platforms the store has utilized since 2022:

* Out of **6,872 total orders on the site in 2022**:
   * **Meta** generated: 3,466 orders
   * **Google, TikTok, and Email Marketing** combined: 20 orders
   * **Organic (Influencer-Driven)**: 3,406 orders

*Key Insight*: Nearly **50% of total sales came organically**, largely driven by influencer collaborations established years prior to 2022. When the store reduced these partnerships, **performance began to gradually decline**.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Conclusion and Next Steps for 2025

This audit revealed core issues facing the store. During the four hours of analysis, we focused on **hard data and critical metrics** that used to be strengths but are now missing. We didn’t even look at creative elements like ad copy or target audiences.

*Thanks to the store owner for dedicating time to this audit.* We’re excited to start working together and unlock the store’s full potential in the upcoming year!

Picture of Store Performances from 2022 to 2024, in Comments.",10,7,1731588196.0,analytics
1grc9ad,Interview: take home assignment,I have an case study coming up next week where I’ll be given several CSVs to analyze and come up with recommendations using any tool i choose. Does anyone have recommendations on what to use besides excel?,0,20,1731610647.0,analytics
1gqmtiz,Some days I want to scream into the abyss,"The boss wanted to create his own metric to calculate user engagement across several platforms. He came up with his own methodology and everything.  

He put the results in a PowerPoint.  

A few days go by and I just spent the last half hour explaining his OWN metric and methodology to him because he didn't know what it was or how we got that number.  

Why",47,14,1731530087.0,analytics
1gqw0rt,Interviewing into new job,"Background: I was laid off in June but had a solid severance package, which gave me some time to find the right next step. After a lot of rejections and auto-responses, I finally accepted an offer for a director-level role in my industry with a small company. I’ve been in the role for a couple of weeks now. The pay is decent (a bit below my previous package), but compensation wasn’t my main concern. The smaller company size means less corporate red tape, and the people seem great—but I do have some concerns about long-term stability.

Now, just as I’m settling in, a wave of interview requests is coming in for roles I applied to earlier. Most are with larger companies offering strong compensation potential, LTIs, and career growth, though they’re hybrid, while my current role is fully remote.

Question: Would it be unprofessional to keep interviewing for these other roles? I’m not applying to new positions, but I want to make sure I’m making the right call before fully committing. For context, I was with my previous company for 10 years (so, not a job-hopper) and I’m in my mid-40s.",10,10,1731555942.0,analytics
1gqw864,Analytics student project,"Hi! 
I hope everyone is well! I’m a student close to finishing my degree and trying to build a portfolio of projects to show off my analytical skills since my experience so far is mostly in retail. I’ve included a uni project on my resume (even though there were some errors in it when graded), and I’m hoping to work on more projects during my break.  I found a Kaggle dataset I’d like to use for market analysis via sentiment analysis, but my questions and approach are pretty similar to what’s already there. Is it okay to use these datasets even if my analysis overlaps a bit? I really want the projects to be my own—not just for recruiters, but for my growth too. To summarise, is it okay to use kaggle datasets and just analyse from there even if the questions are quite similar? 

For background, I’ve mainly been using R during my degree as that’s what has been taught. I know SQL and PowerBI is something that’s highly regarded or required and it is something I am working on learning through out my break and last year of university. Any advice on standing out with these projects or anything else is super appreciated!

Thanks!",4,6,1731556618.0,analytics
1gqf6us,What are fields that analytics is not very prevalent in ,"I was reading about sports analytics and someone mentioned that certain sports are less explored with analytics.

This made me think. Outside of sports what are some fields that have a lot of untapped potential in terms of analyzing and using data?",24,31,1731511061.0,analytics
1gr2zim,Most common ad hoc (on demand) analytics tasks for e-commerce?,"Hey guys,

I built an AI agent for ad hoc data analysis. The process is simple: the analyst uploads data, types in a question, and the AI writes custom code and iteratively extracts insights. Analysts can then validate the code & results and ask follow-up questions, creating a human-in-the-loop workflow for deeper analysis.

Now working on cold outreach and aiming to personalize use cases for potential clients.

**Question for data analysts in e-commerce:** what are the most common inquiries you get? Some of the most common (weekly/daily) custom analytics tasks, the most time-consuming, if applicable.

Thanks!",0,14,1731584806.0,analytics
1gqglzc,Can I realistically get hired with a degree and internship in Accounting?,I graduated with an Accounting degree this year and finished a 3 month accounting internship. Can someone with my resume realistically get hired as an entry level data analyst? ,3,15,1731514686.0,analytics
1gqcyid,Missing data in Google Analytics,"Hi

I have a WordPress site which is notoriously missing data in Google analytics.  
Generally the setup seems to be correct, we're using gtm4wp plugin to connect everything.

However, it seems that sometimes even 30-40% of data is missing. There is evidence pointing to google consent mode v2 blocking some data.

How would you go about it finding where the problem is? What would be the order of steps to take?",2,2,1731504762.0,analytics
1gqg1o1,"JuicyAds data is showing up as ""not set"" in GA4","We’re using JuicyAds for PPC, but GA4 is categorizing traffic coming through UTM-tagged links as ""not set,"" so I’m unable to see this traffic in the analytics.

Additionally, JuicyAds offers various tracking options, such as for registrations and payments. Have you had experience setting up these kinds of measurements?

Please let me know if you can help with the above.",1,1,1731513279.0,analytics
1gq7tm5,I get data out of customer systems and into monitoring systems. Where do analysts come in?,"Splunk etc, we create visibility of complex systems and especially issues. 

At what point can I call an analyst /data engineer/ data scientist in and they'll be able to be helpful?

If I were to come into a computer that has analysts, what would I need to give them to be relevant?

",1,1,1731484080.0,analytics
1gq6but,How And Should I Start To Learn,"Hello I’m 16 and i want to get into this field whenver i’m older. My first question, Is it even worth getting into becoming a Data Or Financial Analyst at this time in the job market. Secondly where should I start learning and what should i learn to be able to get entry level jobs right out of college with little competition against experienced people. Thank you for your time.",0,9,1731477744.0,analytics
1gp7a68,Boss Gifted me a SQL Course,"So I checked my email today and my boss gifted me a mastering SQL for data science course via Udemy. However, I have a certificate in data analytics from a bootcamp I attended for a few months and took somewhat of a similar course in Datacamp during that time. I use SQL almost daily and I am not an expert by an means but feel I am atleast decent.  My most recent performance review was meeting expectations. However, this has me a bit worried, that I am going to get let go and this is there way of telling me I am not good enough. Though there have been no other warning signaling this. Am I just being paranoid?

  
I have been in my role for about a year and this is my first data analytics job.",71,24,1731369615.0,analytics
1gpn664,Working in Logistics with an MSBA,"When I graduated college with a Bachelor's in Business Administration I got a job at a consulting firm doing what they called ""project steering"" (basically junior PM). While I was in this role my company paid for half of my MSBA and I planned to make use of it, but ended up being comfortable in my job even though I wasn't making very much money. I switched projects and roles several times and got a promotion during that time but still hung on because of comfort and the hope that I could make the leap into project management at some point. 

During the summer of 2023 after 5 years with the company I got laid off and had to quickly find another job. The firm I was with did packaging planning for a large automotive manufacturer and so I was able to land a packaging role with one of this manufacturers biggest suppliers. Huge pay increase from my previous role but now 1 year in and I'm unsure of what I want to do with my career. I recently got a promotion to Logistics Planner but haven't started yet. 

For some background, we worked in Tableau and briefly in R during my MSBA but other than that learning hard skills was on our own time. I know that I would obviously need to dive into SQL and Python, but my question is if I wanted to explore business analytics again what would that look like? I understand it's different for everyone but I cant help but think I would basically need to start over given that I don't have any work experience in analytics. I fear that I would take a sizeable pay cut (in my new role that I haven't started yet I will be making $82k). In your opinion would it be worth it for me to start looking at making the switch at this point in my career? I understand the job market for these jobs can be brutal.

",3,8,1731425020.0,analytics
1gpm67o,"Zip codes labeled as Urban, Suburban, and Rural","Does anyone know of a good datasource for labeling zip codes by the categories (Urban, Suburban, and Rural)? 

Thanks ",3,4,1731422340.0,analytics
1gpum4c,Sample size for weighted sample?,"I'm working on a project where we want to do a weighted sample of contracts. Each contract will be weighted according to its dollar-value.

Our goal is to collect some data from each contract and estimate a total number for the entire population. Its a dollar value, not a proportion.

I'm struggling to find any resources regarding sample size or confidence intervals for weighted samples. Does anyone know how to handle this, or could recommend a resource? ",1,5,1731443412.0,analytics
1gpg37g,How useful is inferential stats for a data analyst?,"So I've just finished a university course in descriptive stats, and now I can do the more advanced module. Descriptive stats has been very helpful for my day to day job as a data analyst (digital analyst). It took me about 9 months to finish, but was quite useful.

For the new course I'll again have about a year. I'm curious how useful the advanced module will be and which specific techniques I'll most likely use, and should pay the most attention to:

  
• Study Task 1: t-test for independent observations / Mann-Whitney U test

• Study Task 2: one-way ANOVA / Kruskal-Wallis test

• Study Task 3: Pearson correlation / Spearman correlation

• Study Task 4: Regression analysis

• Study Task 5: t-test for paired observations / Wilcoxon signed-rank test

• Study Task 6: Repeated measures and GLM (General Linear Model)

• Study Task 7: Chi-square test",6,10,1731399677.0,analytics
1goy7ph,28M Regretting My Move to Tech Sales—How Can I Rebuild My Career in Data Analytics?,"Back in 2021, I landed a data analytics role through a grad scheme at a Big 4 firm. It was a great start, learning SQL, Power BI, Python, and gaining consulting skills. But over time, the repetitive tasks and limited pay progression made me consider other options, so I switched to tech sales, hoping for better earnings.

Unfortunately, sales wasn’t the right fit. My first company lacked proper training and direction, leading to layoffs. My second role also struggled with product-market fit and management issues, and I eventually decided to leave.

Now, I’m looking to rebuild my career in analytics. Has anyone here navigated a similar career switch or returned to analytics? Any tips on re-entering the field or insights on interviews would be amazing.

**Additionally....**

A friend of mine, who’s a founder, suggested that I consider *“enhancing”* my CV by adding experience I don’t have in this field, to improve my chances of landing an analytics role. The only challenge would be preparing well enough to handle any specific questions during the interview.

Has anyone else faced similar advice or have thoughts on the risks and benefits of this approach?",14,25,1731347226.0,analytics
1gpcehg,Medical Science graduate looking for a career in data analytics,"Hey guys, I'm about to graduate from a Medical Science from the University of Sydney with a major in pharmacology.  I understand its hard to find a job with solely a science degree however I am still hoping to get one as I do not want to pursue postgraduate study as of yet due to financial and time constraints.  My plan during the summer break is to complete the Data Analysis with Python Certification on freecodecamp and complete some projects.  I am hoping to find an entry level job in data analytics in the health or pharma industry.  Any recommendations or suggestions? Thanks. ",1,6,1731385029.0,analytics
1gp4nwe,Topic Modeling - Support/CSAT/NPS ,"So I've done some topic modeling in college but I'm not an expert at it. We use intercom as our supporting system and I've been peaking at there topic feature set. The cool part about it is that they do all the heavy lifting where you can create your own custom topics, and it also identifies emerging topics. 

The challenge then is when you have support conversations with multiple topics. How do you identify the most pressing? If you have someone mention billing once but then mention videos ten times, intercom doesn't weigh the topics per conversation. 

I've thought of dusting off some ole python scripts I've written in college, but now with AI I wonder how beneficial it would be to just pipe these all into GPT and have it create topics and weigh them for me. 

So question: How have you successfully implemented topic modeling in your orgs for either Support, NPS, etc?",0,1,1731362811.0,analytics
1gp4e42,Are you changing Career to Analytics?,"This post is for you!

I have seen a lot of post lately for people who are planning or already switched to analytics.

I would like to know your progress and see if I collaborate with you.

How much time it took you to fully change your career path and secure your first job? 

During the switch process did you worked with any mentor or trainer to help you? 

Why do you find analytics interesting?

I run a growth consulting and have 14 years of experience out of which 9 years is digital analytics. 

I am happy to work with people who are genuinely passionate about data, digital marketing, analytics or AI to assist and share my insights.

Please comment below with the answers to the questions above. 

EDIT: I am not charging any fees!",0,8,1731362128.0,analytics
1gotyxy,"assigned to a pricing project, no prior experience help !","Hello everyone,

I was recently hired as a corporate financial analyst in an energy sector company. For context, I previously worked in banking as an analyst securities portfolio valuation. I wanted to move into the corporate world to pursue a career in strategic finance, which greatly inspires me. I have a decent academic background in finance that helped me get through the technical questions in the interview (DCF valuation, investment decision-making, Excel, stress testing, etc.).

Currently, the company offers a natural gas appliance protection plan with a monthly fee, and maintenance services are provided by a third-party contractor. Recently, this contractor raised their prices, which has reduced the current margin to nearly zero. I've been assigned the task of reviewing the pricing, but I have no prior experience in service pricing and don't know where to start. also, this position was created and there is no significant work done before.

Logically, I believe I should list all the costs related to the service, calculate a cost price, set a margin considering the competition, and then decide on the new price. However, how is this practically done? Do we project costs and revenues over a certain horizon? If so, how do we determine that horizon? How do we calculate the overhead allocation ratio for this service? How do we assess profitability? Where exactly do we apply the desired margin? Should we stop at EBITDA or continue to net profit?

Can anyone provide an overview of the method and model for a beginner in corporate pricing? is there any resources or website where I can go to learn ?

Thanks in advance for any insights!",3,1,1731336772.0,analytics
1gotcrq,Intelligently Calculating Return on Ad Spend,"Am looking for a way to project return on advertising spend (ROAS). Does not need to be a complex model (in fact would much prefer it if it isn't!).

We have all the data coming in through our SQL pipeline (revenue / subs by campaign, source, adset etc, and based on the attribution date have ROAS after 3 days 7 days 21 / 30 days etc).

What I want to be able to surface in our dashboard is some kind of prediction which is smart enough to update the predicted roas for (e.g.) 30 days, but as time goes on this prediction converges on the actual amount. I.e.. your prediction is calculated from the present performance, so the 30 day roas prediction made for customers converting on (e.g.) 1st October would have some value x\_1 predicted on the 8th October, but you would expect x\_2 from (again, e.g.) 28th October to be much closer to the final amount.

Anything that can just be calculated in a Python script or something on a daily run would be absolutely perfect. Any help v much appreciated!",2,2,1731335108.0,analytics
1gotdhb,Qlik data ingestion suite ,"I worked on a data migration project for a major financial institution and would like to discuss one of the powerful tools they used for data movement

It’s called Qlik Enterprise Manager and Qlik Replicate. I wrote an article on how it works.

I worked extensively on the infrastructure setup for Qlik Enterprise Manager and Qlik Replicate in a public cloud environment. Let me know what you’d like to learn about it, and I can cover it in my next article

you can find me on medium (@rasvihostings)",0,1,1731335166.0,analytics
1goizfd,Harvard For Data Science,"hello gang, after much thinking, i decided I want to join harvard extension school master of data science program, whole thing with the MIT Micromaster will cost me around $35,000 or so. what do you all think? any one has experience with this program before? any one can share experience on how good the program is? thanks.",3,21,1731296136.0,analytics
1gohe2s,Where to progress from a Business Systems Analyst job title specifically ,"For those of you who were BSA’s in the past,   what roles do you hold now? 

- I’ve done some research and my target roles seems to be Analytics Engineer, Data Scientist, Data Engineer. I have 11 years overall working experience but made the pivot to tech in 2022. I graduated last year with an MBA in MIS & currently doing the OMSA at Georgia Tech. (Pls don’t ask me why I’m doing a 2nd masters & why can’t I just get a job first. Short answer is that I come from a non traditional background in healthcare & didn’t have proper guidance. That’s all you need to know thank you!)

- Although I prefer having to do less presentations or managing people, I still welcome your responses and advice on how to advance.

- For this job title,  I want my tasks to be confined to 9-5 I don’t want to do anything related to work outside of that. 

- I’m talking BSA not BA or SA. Yes I know they’re interchangeable but I want to know how you fared after you moved on from the BSA title specifically.

Thanks in advance!",5,3,1731291014.0,analytics
1govbid,"l’m Looking for a Remote Internship in Data Analytics, but My Applications Haven't Been Successful. Any Advice?""","I graduated from physics, then I worked in the Front-End field. This year, I decided to focus on developing my skills in data analytics, a field where I feel more enthusiastic and motivated. I've learned Python, Numpy, Pandas, and Tableau, and I’m continuing to learn and work on projects. I am currently looking for a remote internship. My English level is between B1 and B2, which sometimes makes me feel less confident. Although I have applied to many positions, I haven’t yet found a remote internship. What steps should I take?

",0,6,1731340216.0,analytics
1goaj11,Business Analytics Engineer,"Anyone ever heard of this job title or know someone who does? If so, may I ask what your day to day looks like and your salary especially in the USA? 

I think this may just be another buzzword for the specific analytics engineer they want but I want to know if $90-$110k is a good range for it. There’s no indication that it’s a mid level or senior role but the job description includes  expectations to lead etc",8,19,1731271771.0,analytics
1gojgd3,What does a good data analytics project look like?,I'm having a hard time finding quality data analytic projects out there. Most of the projects on YouTube use simulated data with no real world impact of their findings and its super basic like cleaning in Python and then visualizing in a BI software.,2,3,1731297719.0,analytics
1goa9ml,Tools for A/B Trsting?,"Been out of the dev game for a few years. What’s the best way to split test landing pages, overall site design and individual elements? Mainly looking at conversion rates. 

Free is good but not required. I read that Google’s optimize is going away- is that still a good option for now? Optimizely? Anything else?

Thank you!",2,10,1731271100.0,analytics
1gnqsh5,How does your work impact business you are working for ?,"I aspire to be a data and BI analyst, I wanted to gain vision of how analytics work that data analyst like you do impacts the business you are working for.

If you were to crisply describe what your work achieves and why exactly it is critical to your particular firm and its operation, it will give valable insight in developing my thinking about analytics as well as better me skillset.





",14,18,1731206781.0,analytics
1gneqkt,Do you feel you are responsible for EVERYTHING?,"I am business side Power BI developer for last 5 years, but I found myself not only doing the typical front-end stuff, but also
- stakeholder management,
- creating adoption frameworks,
- being product owner,
- running team of data engineers, BI developers and business analyst 
- responsible for WHOLE data quality in the domain
- doing simple data engineering stuff
- conducting business analysis
- creating roadmaps for future analytics development

The scope creep is real and I kinda envy external consultants „do my stuff only” and getting even better rate and overtime, whereas being employee while having more security it means I do unsaid Data and Analytics Manager work. Do you have similar experience?

I seriously thinking about going consultant route, moving to IT department with goal of having less scope and more focus. I am not sure that being covert manager is way to go.",39,28,1731172166.0,analytics
1gnlgah,How do you like to do your process of ETL?,"I’m currently learning about data analytics and curious to hear how people like to Extract, transform, load the data. 

What’s your process for it? What advice do you have? 

",8,12,1731190494.0,analytics
1gn7tnt,Business Project - Weather dataset,"I have a group project in my business analytics class that requires using data to get meaningful information from the set. 20 hours of work must be put in to receive full credit. 

I decided to get 5 years worth of datasets of the weather in florida. What are some good data visualizations to choose from? would excel or power BI be the best option to work with? honestly any suggestions would be awesome, as my 2 other group members are not helpful ",10,11,1731150528.0,analytics
1gn2emv,A Fresh Data Analyst Aspiring To Meet Others & For More Opportunities,"Hey, everybody! Hope you all are well and safe.

I am a new Data Analyst - I transitioned into this field last year, earned some certifications and did some full-scale projects, and thankfully landed a full-time job as a Data Analyst. Now, I am working in a Data Analytics role full-time and also continuing my academic education.

I just wanted to post here and welcome anyone who is willing to connect and exchange ideas and grow in the professional world together! If you are interested in discussing and helping each other and growing in the field and networking together, please do DM me. I am also on LinkedIn.

Also, it would be great if I can get some consulting work - entry level consulting projects would be great and I am now trying on Upwork to earn some. Please do connect me to anyone who can help my chances of getting beginner-level consulting projects so I can build my experience and forte there as well.

Thank you! Welcoming responses.",6,5,1731127707.0,analytics
1gn1ijl,Does Google penalize sites that don't use Google Analytics?,"I used to get signficant search traffic from Google. I removed Google Analytics. Now very little. It's wierd, all my sites are open source, super fast, no ads, no trackers, have sitemaps/accurate titles and descriptions, useful original content, inbound links. I can't figure out what's different, other than I no longer have GA on the site.",4,1,1731124612.0,analytics
1gn6i6n,I am presently doing an MBA in India and want some advise on career options : I want to hear my career towards analytics field,I am presently doing an MBA in India and want to hear my career towards the analytics field. Our internships are going on. Is it prudent to take an internship in sale/marketing or some other domain( based on brand name or salary). Would it affect my original goal ?,0,2,1731144663.0,analytics
1gn4fzu,I made a Customer Journey Tracker & Analytics for WordPress. (Plus chat with data) AMA,Designed for Martech.,0,1,1731135625.0,analytics
1gn2fle,American Data Scientists With International Degrees?,"hello gang, I am an American, I work in IT and I see people all the time with international degrees, from India, from China, other times they have degrees that have nothing to do with IT at all, like my team lead at my company has a master's in counseling. so here is my question, theoretically one with an Indian bachelor's degree in computer applications, and a master's in data science, with us citizenship or a green card, can be someone perfectly qualified to work as an American data scientist right? Indian data science can't be much different than American data science am I right?",1,5,1731127804.0,analytics
1gmjnzn,How would you go about this?,"Hi all,

So I recently started a job this week working as a game master for a business that just opened up this past month. 

Looking into the business, I can see that the way they collect and store customer data is very basic, and leaves me the opportunity to create a DB for them. It also gives me the opportunity to do DA for them as since it's a new business, DA can exponentially make a difference in business decisions.

However, I don't want to just create all these and still get the title Game Master. I'd like to negotiate with a title change as Data Analyst and if possible an increase in wage, as it is a field I want to get into, but with the market right now, can't.

How would you go about this?",3,13,1731075795.0,analytics
1gluqt0,I want to start my first portfolio project as a beginner data analyst,"I wanted to create my first portfolio project as a beginner data analyst. I am a person who learns by doing and not studying for 5 hrs straight. I am having difficulty in choosing what is the best data set i should focus for my first project that can be showcase in my portfolio website for applying a job.

Any insight from you guys who started also from the bottom will be very helpful and i know its too much to ask but i hope someone who has the time can comment here an exact roadmap what to do. There are guided projects on youtube but i think following along will not make me learn what i needed to do. Its like i’m just copying somebody’s work. 

I hope someone can help me 🙏",35,34,1730997575.0,analytics
1gm53au,Looker Studio - Report level filter for session duration?,"
I am looking for a way to filter traffic at the report level by session duration. I want to remove any traffic with a duration on the site of 0s. So far, the only way I have found to do this is to view a table of cities by average session duration, look for cities with an average session duration of 0 seconds, then create a report level filter removing those cities. 

I feel like there has to be a better way to do this. Any ideas?",4,2,1731024004.0,analytics
1glv0dq,Should I attend the Gartner Data & Analytics summit?,"4 YoE in analytics, manage a small team of analysts. I am interested in attending but know the cost is not insignificant when factoring in travel and accommodations and don’t want to blow a massive hole in our professional development budget if it’s just meh. Has anyone here attended previously? Are there other conferences that you’d recommend as an alternative? Interested in any feedback - positive or negative. ",3,3,1730998241.0,analytics
1gm00w2,Coursera/ google data language courses ,"I’m a undergrad currently pursuing a degree in sciences but want to move into data analytics after I graduate. I’ve been using coursera courses, including the Google ones that give certificates, to learn the basics, but each course is around £38. Are these courses/certificates worth it? Or is it better to do some bootcamp instead? ",1,3,1731010750.0,analytics
1glxwyc,looking for a partner to make a data bank,"I'm working on a personal data bank as a hobby project. My goal is to gather and analyze interesting data, with a focus on psychological and social insights. At first, I'll be capturing people's opinions on social interactions, their reasoning, and perceptions of others. While this is currently a small project for personal or small-group use, I'm open to sharing parts of it publicly or even selling it if it attracts interest from companies.



I'm looking for someone (or a few people) to collaborate with on building this data bank. 



Here’s the plan and structure I've developed so far:

# Data Collection

* **Methods:** We’ll gather data using surveys, forms, and other efficient tools, minimizing the need for manual input.
* **Tagging System:** Each entry will have tags for easy labeling and filtering. This will help us identify and handle incomplete or unverified data more effectively.

# Database Layout

* **Separate Tables:** Different types of data will be organized in separate tables, such as **Basic Info**, **Psychological Data**, and **Survey Responses**.
* **Linking Data:** Unique IDs (e.g., `user_id`) will link data across tables, allowing smooth and effective cross-category analysis.
* **Version Tracking:** A “version” field will store previous data versions, helping us track changes over time.

# Data Analysis

* **Manual Analysis:** Initially, we’ll analyze data manually but set up pre-built queries to simplify pattern identification and insight discovery.
* **Pre-Built Queries:** Custom views will display demographic averages, opinion trends, and behavioral patterns, offering us quick insights.

# Permissions and User Tracking

* **Roles:** We’ll establish three roles:
   * **Admins** \- full access
   * **Semi-Admins** \- require Admin approval for changes
   * **Viewers** \- view-only access
* **Audit Log:** An audit log will track actions in the database, helping us monitor who made each change and when.

# Backups, Security, and Exporting

* **Backups:** Regular backups will be scheduled to prevent data loss.
* **Security:** Security will be minimal for now, as we don’t expect to handle highly sensitive data.
* **Exporting and Flexibility:** We’ll make data exportable in **CSV** and **JSON** formats and add a tagging system to keep the setup flexible for future expansion.",1,1,1731005470.0,analytics
1glwo9w,How to pull all ads data for Meta?,I am working to build a data pipeline to get metrics for all Meta ads. The problem is with Insights API you need to pass ad id to get the data and doesn’t allow more than one ad id. I want to system to be dynamic meaning it pull all the ads data for provided campaign ids. I am using Make to build the pipeline. ,0,8,1731002385.0,analytics
1gl3vea,Data Camp Premium is free this week!,"Good for learning & showing certifications, if anyone's interested in transitioning to data/product analyst. 

It lasts for 4 more days i think. ",20,15,1730913999.0,analytics
1gl4hu2,Struggling to break into data analytics—any tips ,"I'll cut to the chase: I’m looking for guidance on securing a job or internship as a Data/Business Analyst. I’m in my final year of undergrad, with skills in Python, SQL, Power BI, and Tableau. I’ve also explored basics in deep learning and AI, and I have experience with Unreal Engine, Android Studio (Java), and am currently studying data mining and image processing. For my final year project, I’m working on a hand gesture-to-speech app. I've completed at least one project in each of the areas mentioned.

Despite these skills, I’m struggling to get responses to my applications. I’ve created an ATS-friendly resume with a strong score, and I’m actively networking on LinkedIn. However, I’m still not seeing progress, even for internships.

I’d really appreciate any advice on what I might be missing or areas I could improve to stand out in my job search. Any feedback on skills or experience I should build on would be a huge help. ",8,19,1730915581.0,analytics
1gle2z4,Best election analytics resource for interesting visualizations? ,Does anyone have any recommendations on websites that do in depth election analysis with visualizations? Preferably interactive. ,0,1,1730940357.0,analytics
1glamma,Job Hunt Guidance ,"Hello!!

I am a recent Masters graduate from a Canadian university with 1.5 years internship experience in Power BI and Power Platform. (2 years old experience in SAP ABAP). I cleared my PL-300 certification and have some projects.

I am searching for data analyst jobs for the last 2+ months but no luck for interviews. My resume should be fine as I am using the FAANG path LaTex format and GPT 4 for increasing ATS chances.

I am a bit down on the networking game and I would appreciate any feedback you guys might have if I am doing anything wrong.

Thanks <3",0,3,1730930978.0,analytics
1glgtvg,"Given a take home assignment, I'm clueless as to where to start. Can i get a little guidance?",I was given 5 different data sets and i was to choose one dataset to analyze. The goal is to create a question and find an answer to the question using analytics. I was given 4 hours and it's due in 2 days... Can someone give me some guidance as to where to start?,0,11,1730948704.0,analytics
1glbm7j,Crazy Question!,I'm a Senior Claims Adjuster wanting to go into Data Analytics (working on a grad cert in Business Analytics) ... Possible?,0,7,1730933579.0,analytics
1gkplvb,"I am trying to decide between a business analytics masters or information systems and analytics masters. BA is in person while IS is online. Which degree will allow me more job opportunities and growth, or will the job pool be about the same? ",Which degree? ,0,12,1730865160.0,analytics
1gk4e5a,"CS to DA career path switch, where to begin?","Hi! I'm a CS graduate, more specifically cyber security, but i've had little to no luck in landing any jobs in my field, I've been applying relentlessly for exactly a year now, not even internships where I live!:(

Market is super saturated for anything remotely related to tech, even for DA, however, I find it still has good opportunities where I can land at least an entry level position.

I do have some background on analytics, I've done some research on SAS, completed the Microsoft Power BI course for data analysis, and I also have basic knowledge of SQL, but I still don't feel as though I'm ""ready""- I don't have any work experience in DA, I see many posts about how courses are not enough until you apply DA on real work, and how recruiters won't take your certifications/courses into consideration and really, will only look at past experience in Data Analysis, so how am I supposed to start?

I currently have a job in sales, it's all over the place as I do supply chain and some sales reporting here and there and I've tried to incorporate my own version of ""analytics"" into my reporting, but I do not see excel reporting as enough, and for the life of me I wouldn't have a clue on how more to incorporate DA into my work/life.

So, dumb it down for me, tell me where to start, what to focus on, what certifications I should do, what has helped you, what tools you find best personally, what suggestions and example sets should I try to work on at my own pace, all of it.

My issue is I need to follow a set of rules, but I can't even begin to make myself a schedule to follow with the end goal being a DA job, because I truly have no clue where to begin.

Sorry to sound like a child, but it's really one of those moments where my brain is insisting I get into it, but is not leading me towards any clues or starting points.

EDIT: I forgot to mention, I'm open to learning more about any other analytics-related field, as long as I feel like I am a good fit to understand it and I can follow through, but I don't think I'm knowledgeable enough on other analytics fields to make that decision, but I would like to give myself enough credit to note that if I really put my mind to something I'm more than likely to excel by all means, so feel free to suggest anything for me! Thanks a lot",8,12,1730805539.0,analytics
1gkbs9j,Looking to get into data analytics.,"Hello,
I recently had some life developments which pushed me into looking for a new career route. For the past 6 years I've been a stay at home dad, before this I was an analytical chemist. I received my masters degree in analytical chemistry and minored in mathematics in my undergraduate years. I've always been analytically minded and thought data analytics might be a good fit for my re entry into full time work. I've seen some data analytics boot camps advertised, however don't want to invest 8-15k into a program if there aren't good job prospects considering my background. I am a fairly good student and believe I could do a good job in any boot camp. Can anyone give me any idea whether this is a good idea or not? Thank you so much!",0,17,1730826818.0,analytics
1gjgr2o,How do I convince my c-suite that fish eaters won’t eat chicken?,"I’m a lead analyst at a late stage fintech startup, but for the sake of privacy I’ll be changing the products to chicken and fish. 

My company’s main line of business is selling chicken - roast, fried, grilled, you name it. That’s our specialty, and we were doing pretty decently too. 

One day, we decided to try out selling fish, and we hit a gold mine. Customers were crazy over our fish. There was only one problem - as fishes aren’t our main product, the margins were nowhere close to chickens. Hence, my c-suites tasked me to grind the data and find a way to cross sell chicken to these fish eaters. 

I tried everything - tons of experiments, analysis, prediction models, all leading to the same conclusion - fish eaters just want to eat fish and not chicken! But they won’t take that as an answer, and thinks that I’ll eventually find and answer if I keep digging. 

TLDR: C-suites wants me to find a way to sell chicken to fish eaters, and won’t take no for an answer. What do I do?",78,47,1730733617.0,analytics
1gjy85w,Scorecard,"Trying to get a clear idea of what a good business scorecard looks like. I’ve found a bunch of examples, but most don’t really make sense in a business context or lack clear metrics, which is confusing. From what I understand, a business scorecard would work like a dashboard that presents key metrics and data points, correct me if I’m wrong. I’d love any resources or examples that really show how they’re used and what they should include as a visual learner.",2,7,1730779680.0,analytics
1gjk0tl,From data analyst to swe,"Is it possible to go from data analyst to software engineer?

I dont have a strong cs background, i took cs courses but i am not a cs graduate, i am graduating in management and engineering with a specialization in analytics for business (it is more towards business intelligence or data science).

I want to know if it is possible to transition to swe and if there is A LOT of stuff to learn or the passage is not that difficult.
",13,12,1730741579.0,analytics
1gjperj,help prepping for Sales Analyst panels ,"Hi folks, I have panel interviews this week and wanted some advice from anyone who has experience with these. I signed an NDA and while it doesn’t explicitly state it, it seems like we may touch on internal data and potential future strategies/launches. 

These will take place over 3 days, totaling to 4 hours, with 5 different senior members of their team. So far I’ve completed 2 hour-long interviews that focused on my past experience, knowledge of trends in gaming and game analytics, some technical case study questions, and how I would function as part of the team. My main questions for you all are: 

* What types of questions are generally asked in panels for sales analysts or similar roles, especially in gaming? 
* Are there any technical skills I should practice? For example, could they ask me to do something like open excel, evaluate the impact of pricing changes in the game using multiple regression, and watch me do that in real time?",7,12,1730754726.0,analytics
1gissfz,Can you recommend me Data Analytics and Business Intelligence Books that really helped you throught your Career?,"If you add how many years you was data analyst or business intelligence, or what is your current role it will help more helpfull
Thanks",85,19,1730656464.0,analytics
1gjayr5,Has anyone here transitioned from a Testing role to a BA/ DA role?,"Am trying to transition from a software testing role to a Business Analyst or Data Analyst role...Need suggestions on how ya' ll did it...
",4,4,1730715309.0,analytics
1gipyc4,"Got the job, now what? (Your thoughts on continuous learning on the job)","Recently got a job as a DA / DE coming from behavioural science. My most pronounced skills so far are  analysis, hypothesis testing, low level data transformation, data visualisation with R and Python. In my current role I am responsible for a power bi datamart and the pipelines / transformations leading to it as well as our attribution model (e-commerce), which is all on the google cloud platform. 

**Now to my actual question, how do you structure your learning ONCE you already have a job.** Almost all posts I saw on here are for people trying to get a job or if they are professionals they just ask should I get certificate xyz. 

E.g. at the moment I try to work on all my weak spots in my skills for my current role, reading books on general topics such as Fundamental of Data Engineering and try out tools with youtube tutorials that might become relevant for me such as neo4j , docker and airflow. 

So in summary I would say 50% of my learning is reading online articles, documentations and yt tutorials on things that I need right now, 35% reading general books on the topic (next up are: designing data intensive applications and the data warehouse toolkit) and 15% is getting practical experience with tools. 

I shy away from power bi and gcp certifications because they are tool specific and at the moment I feel like broader information is more helpful. What are your thoughts and how do you as professional structure your learning?",12,7,1730649160.0,analytics
1gj5v00,Im currently working as junior analyst(my first job) in a cyber security firm it's more of tech support allied with data analytics. I want to switch to pure DA role. I'm doing certifications on courseera and I'm building a portfolio can someone please suggest me if the switch is possible? ,What else can be done to make this switch safe? ,0,5,1730693275.0,analytics
1gi8z01,Landed First Analytics Job,"Hello All finally landed my first analytics job.

A little about me; I primarily work on heavy equipment as a repair/maintenance technician. Decided to go back to school since I already had credits from my previous life. I was a CS major then, but decided to pursue a degree in information systems. Learned alot, especially foundations in coding. Really had a passion for analytics, so i continued the coding skills for that position. Graduated in August.

Created 2 projects that showed excel, python, sql, and R capabilities. One was a completed end-to-end project the other was more on linear regression model and focusing on the statistics

Since Finishing in August I decided to pursue a master's in data science/Analytics. I know people do not recommend this. But this actually helped me in my interview process. In addition, the program is dense and has some recognition. I feel that a Masters Cannot hurt you, it can only help. 

I just wanted to say thanks to this subreddit, I've read tons of advice and other peoples opinions that really helped with this process. But I know the journey does not stop here, im sure imposter syndrome will creep on me, but im confidence ill over come it.

I hope everyone continues to pursue their career goals and share their experience on this subreddit. It really helps the community.",90,16,1730588723.0,analytics
1hr6zga,Monthly General Discussion - Jan 2025,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",13,19,1735750833.0,dataengineering
1h47qv8,Quarterly Salary Discussion - Dec 2024,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",49,33,1733072430.0,dataengineering
1i6xclc,How many of you are self-taught Data Engineer?,I really don't think it is possible to become a self-taught data engineer in current job market...,138,98,1737503964.0,dataengineering
1i76qsx,"When your boss asks why the dashboard is broken, and you pretend not to hear 👂👂... been there, right?","So, there you are, chilling with your coffee, thinking, ""Today’s gonna be a smooth day."" Then out of nowhere, your boss drops the bomb:

**“Why is the revenue dashboard showing zero for last week?”**

Cue the internal meltdown:  
1️⃣ **Blame the pipeline.**  
2️⃣ **Frantically check logs like your life depends on it.**  
3️⃣ **Find out it was a schema change nobody bothered to tell you about.**  
4️⃣ **Quietly question every career choice you’ve made.**

Honestly, data downtime is the stuff of nightmares. If you’ve been there, you *know* the pain of last-minute fixes before a big meeting. It’s chaos, but it’s also kinda funny in hindsight... sometimes.",23,20,1737535195.0,dataengineering
1i70n8t,"Snowflake users, what are the reasons that have made you stick with Snowflake despite of the higher cost?","I'm using Snowflake for my small to medium company. It is actually a business group consisting of four LLCs, selling products and services, and we are in the process of choosing our DWH.

I was a BigQuery user and for me it is harder to set things up from zero. I tried Snowflake since Fivetran recommended it to me. My impression is SF is easier to use and navigate than BQ. Moreover, build things in SF is quite simpler and straight up. That means, I can learn and build SQL pipeline using tasks and snowpark; I can use Python in it. Lastly, it has been gaining some popularity among companies.

However, many people have said to me that SF is more expensive than BQ. I can see it from data analysts perspective where as you scale your Looker Studio dashboard the compute price can go up exponentially. Therefore the day to day cost will be higher.

But if the company decides to buy Tableau Online, surely this compute issue can be avoided by using *data extract* feature. So, for a company that want to fully become data driven, SF is still a good choice.

  
Well that's just my opinion.",27,36,1737513394.0,dataengineering
1i6y6pb,I built a free job board that uses AI to find relevant jobs,"**Link:** [**https://www.filtrjobs.com/**](https://www.filtrjobs.com/)

For ML roles title matching is a bad way to look for jobs. so many Data Engineering jobs are just called ""Software Engineer"" (e.g. airbnb does this) which makes it tough to find data engineering jobs

My job board ranks all job postings matching filters based on the contents of your CV and the JD to rank jobs

It's **100% free. No annoying sign up emails, no auth, no paywall, no ads.** My infra costs are well within free tiers so this will remain free forever

Features I added I really wanted (Under additional filters):

* Top companies filter, I used salary posting info online to curate \~50 well known high paying companies
* H1B sponsorship -- if a company doesn't sponsor in the posting, it is filtered out
* ATS chips: helps me avoid workday postings bc theyre so annoying

I've been through the job search and I know its so brutal, so feel free to DM and I'm happy to give advice on your job search

P.S. the job board is only for US rn, working on expanding it to other locations

",38,14,1737506227.0,dataengineering
1i76bsr,"For those who love Spark and big data performance, this might interest you!","Hey all! 

We’ve launched a Substack called **Big Data Performance**, where we’re publishing weekly posts on all things big data and performance. 

The idea is to share practical tips, and not just fluff.

This is a community-driven effort by a few of us passionate about big data. If that sounds interesting, check it out and consider subscribing:If you work with Spark or other big data tools, this might be right up your alley.

So far, we’ve covered:

* **Making Spark jobs more readable**: Best practices to write cleaner, maintainable code.
* **Scaling ML inference with Spark**: Tips on inference at scale and optimizing workflows.

This is a community-driven effort by a few of us passionate about big data. If that sounds interesting, check it out and consider subscribing:  
👉 [Big Data Performance Substack](https://bigdataperformance.substack.com)

We’d love to hear your feedback or ideas for topics to cover next.   
  
Cheers!",8,0,1737533207.0,dataengineering
1i6zy6e,DeepLearning.ai Coursera course (Joe Reis) or DE ZoomCamp? Or both?,"I have a bachelors in CS and a PharmD. Currently work as a data analyst/analytics engineer, but also working to move some data pipelines (already made by previous devs and our CTO) away from local machines and into AWS. Currently our pipelines are all in Python and SQL. My boss already had me take Andrew Ng’s Machine Learning Specialization on Coursera (paid for by work) as I prepare to build some ML models at my job. My boss seems to be supportive of my goals to learn more data engineering principles and is okay with having work pay for it (at a reasonable price). Would love to get a masters (OMSCS or OMSA), but due to family life and possibly being too much for work to pay for, I’m gonna hold off for a couple years.

Anyways, I see ZoomCamp recommended a lot as a free resource, but I also know Joe Reis recently came out with a course, and didn’t find a comparison to each other in the subreddit. If having to pick, which would be a better investment of time, if not both?",12,11,1737511317.0,dataengineering
1i77ahn,How to Start Learning Data Engineering and Data Science Using Kaggle?,"I'm looking to dive deeper into data engineering and data science, primarily using Kaggle as my learning platform. So far, I've completed the **Intro to Machine Learning** and **Pandas** courses provided by Kaggle.

I’m wondering:

1. **Should I complete all the Kaggle courses before getting started with projects, or should I start applying what I've learned now?**
2. **What’s the best way to start hands-on practice?** Should I work on competitions, datasets, or build my own projects?
3. **Any advice on combining data engineering skills with data science?**",2,1,1737537716.0,dataengineering
1i6mfo5,"How we use AI to speed up data pipeline development in real production (full code, no BS marketing)","Hey folks, dlt cofounder here. Quick share because I'm excited about something our partner figured out.

""AI will replace data engineers?"" Nahhh.



Instead, think of AI as your caffeinated junior dev who never gets tired of writing boilerplate code and basic error handling, while you focus on the architecture that actually matters.

We kept hearing for some time how data engineers using dlt are using Cursor, Windmill, Continue to build pipelines faster, so we got one of them to do a demo of how they actually work.



Our partner Mooncoon built a real production pipeline (PDF → Weaviate vectorDB) using this approach. Everything's open source - from the LLM prompting setup to the code produced.

The technical approach is solid and might save you some time, regardless of what tools you use.

just practical stuff like:

* How to make AI actually understand your data pipeline context
* Proper schema handling and merge strategies
* Real error cases and how they solved them



Code's here if you want to try it yourself: [https://dlthub.com/blog/mooncoon](https://dlthub.com/blog/mooncoon)

Feedback & discussion welcome!

PS: We released a cool new feature, [datasets](https://dlthub.com/blog/datasets), a tech agnostic data access with SQL and Python, that works on both filesystem and sql dbs the same way and enables new ETL patterns.",33,6,1737476720.0,dataengineering
1i77xyu,Career Advice: Transitioning from BI Developer to Data Engineer,"Hi Redditors,

I’m currently a Senior Business Intelligence Developer with 10 years of experience. My background includes designing and building data warehouses (DWH), ETL processes, reports (primarily Power BI), and gathering requirements. I’ve worked in both consultant and in-house roles, mainly using Microsoft’s on-premises stack, but I’ve also had some exposure to Azure projects. Occasionally, I’ve written C# to fetch data from APIs and handle similar tasks.

Lately, I’ve been feeling a bit lost and wondering about the next step in my career. While I’m interested in transitioning into a data engineering role, I’m also open to exploring other roles that might align with my skill set and experience.

Specifically, I’d love advice on:

• Whether I should double down on Python, as it seems essential for many data engineering roles.

• Sticking with the Microsoft stack or focusing on something like Snowflake or other modern cloud-based solutions.

• Other roles I could pursue, such as solutions architect, cloud engineer, or even BI lead/manager positions.

With my current experience, can I already market myself as a data engineer, or would I need to gain specific skills first? And are there other career paths I should be considering that align with my mix of technical and business skills?

I’d really appreciate any advice or insights from people who’ve made similar transitions or have experience in these areas. Thanks in advance!",1,2,1737540714.0,dataengineering
1i6mfke,"Building a Minimalistic BI Stack with PostgreSQL, FDW, and Superset – Looking for Feedback!",,22,19,1737476713.0,dataengineering
1i6ys1g,How do you best handle local units in a global data warehouse?,"If you have a data warehouse with hundreds of tables, some tables +1 billion records serving members of your users in different countries - how do you handle local units of measure eg acres vs sqft, feet vs meters etc when your users want what’s right for their locale?

The data warehouse is used by many different applications.

Do you replicate the data in the warehouse, convert on fly (performance impact? Esp on time series data)?

",5,5,1737507888.0,dataengineering
1i75ufj,Anomaly Detection project - with data engineering pipeline,"Hi, I am looking to do an end-to-end project in machine learning. Now I know this is a data engineering channel. I am not sure how long it would take but I know data science and want to learn and combine data engineering and have a solid project for my CV - I am looking for a real-world dataset if possible or if somebody worked on a project like this any guidance or tips would be good either the data engineering part or even machine learning part.",1,1,1737531079.0,dataengineering
1i6qoi5,First DE proj,"Hey everyone, 

I'm building a mini weather station pipeline using an ESP32 + BME280 for data collection, sending the readings to a flask server on a dedicated Raspberry Pi. 

The end goal is to move this data to Azure, use Databricks for processing/ML (maybe predict weather patterns or anomaly detection), and create real-time visualizations. As someone coming from data analytics but wants to continue learning DE, what other components should I add to make this a solid data engineering portfolio project? Or perhaps things to focus on in the future?",9,5,1737487114.0,dataengineering
1i75jr8,Looking for Quick Study Resources for Google Professional Data Engineer Certification,"Hi everyone,

I need to get the Google Professional Data Engineer certification for work, but the official course provided by Google is quite lengthy. I was wondering if there are any good Udemy or Coursera courses that can help me study for the exam and prepare for the certification more quickly.

Any recommendations or tips would be greatly appreciated! Thanks!
",1,1,1737529781.0,dataengineering
1i74b8z,Has anyone used AWS Entity Resolution? Is it worth the money? How good is it compared to GCP Big Query Entity Resolution? Any recommendations on Entity Resolution Solutions is appreciated.,Any advice on building an Entity Resolution Pipeline is greatly appreciated.,1,0,1737524814.0,dataengineering
1i6he47,Looking for tips on migrating from SQL Server to Snowflake ,"Hello. I lead a team of SQL developers pon a journey to full blown data engineers. The business has mandated that we migrate to Snowflake from our Managed Instance SQL server. My current plan is to inventory all of stored procedures and sources, determine what is obsolete and recreate them in Snowflake running in parallel until we're confident the data is accurate.  What else would you suggest? Thanks in advance. ",22,30,1737462191.0,dataengineering
1i73ane,Need Help in Databricks (PySpark),"Hello everyone, I have just started my journey as a Data Engineer in small company. Currently, the firm has a all projects on Databricks so I need to learn that but I am from Web Development background so learning databricks and pyspark looks some boring. Does anyone suggest the gamify approch, material or source to learn it with engagement and also suggest any projects (basic to advance types real-word) which can help to gain some interests in this fields !",1,1,1737521162.0,dataengineering
1i72b39,AI Use Cases,What are some AI use cases being used in your companies with your data? The only use case I see for my company is a slack app that uses snowflake cortex with semantic yamls to manage ad hoc data questions. Like Natural Language to SQL. Anything else? ,0,0,1737517953.0,dataengineering
1i723va,I created a free no-code tool for building data pipelines.,"I developed a free no-code tool for building automated data pipelines. I did it because my team of multi-discipline engineers wastes hours trying to analyze data from multiple sources with python or excel without having the skill sets to do it. I think it could be useful in way more applications and the no-code drag and drop interface makes it accessible to wider audience. I'll likely add paid packages in the future for more advanced functions like data acquisition but you can already connect to and combine databases, csv & excel files with this free version. 

I'll be submitting it to the ubuntu and windows stores tomorrow but can share a zip file if you'd like to try it out a bit earlier. 

If you'd like to give it a go, let me know here: [www.lazyanalysis.com](http://www.lazyanalysis.com)

",0,1,1737517398.0,dataengineering
1i6ht1u,Need an azure data engineer study partner !!,"
Hi, I’m a Data Engineer with 3.9 years of experience working with technologies like Azure, Azure Data Factory, PySpark, Databricks, SQL, and Python. I’m currently planning to make a career switch and am looking for a study partner with similar or more years of experience.  

I’m flexible and open to learning new technologies as well, and I believe collaborating with a like-minded professional can help us both achieve our goals efficiently.  

If you’re interested, let’s connect and support each other in this journey!  ",16,45,1737463633.0,dataengineering
1i6vnqk,Freelancing tips,"Hi!
I know there are here many successful data engineers working as freelance.
I'm willing to take that direction this year as well, and my goal would be to be full remote (because locally the daily rates are low, around 400$/day (european country)). I also would like to target short durations (~3months) because they offer better rates and I'd like to learn as much diverse tech stacks as I can this year (personal goal).

What's your advice on :
- How to find good paying freelance jobs? Any must know websites?
- How much should I expect for daily/hourly rates? Given I'll only target full remote offers, I don't care if it's in another timezone.
- For those who work only with hourly rates, how hard is it to maintain stability and get new work?
- Is posting on Lkdn helpful to get freelance jobs? I never did but I see a lot of people posting a lot there.
- any other advice? 
Edit : I have 7 years of experience in DE, last job I was senior DE. Never was a ""networking"" person as I focused only on my full time job and getting the work done.
Thank you very much!",2,3,1737499430.0,dataengineering
1i67kc9,Snowflake sent 6 behavior change emails in the last 3 weeks. Am I the only that find it annoying?,"I usually read these emails and flag them to engineering as an additional layer to ensure nothing breaks. Folks have been slowly ramping up since the holidays, and I have a mountain of backlogs and emails to go through.

Then Snowflake sent us these emails. In the past, similar behavior changes have broken our pipeline, causing significant headaches. These changes are often poorly documented online, requiring us to comb through behavior change emails like this to figure out what went wrong.

Now, whenever Snowflake sends these emails, our support team sees them, our partnership team sees them, I see them, and engineering sees them. To avoid multiple people reading unnecessary emails and trying to decipher their meaning, I usually open a ticket and explain what’s going on and how it impacts us.

Every email means I have to open a ticket.

They sent us six emails—three for a service we don’t use (Streamlit) and three that probably (but unlikely) impact us and require investigation.

Is it just me, or is this incredibly annoying? 

",66,35,1737424435.0,dataengineering
1i6fjqs,"People who work in data, what did you do?","Hi, I’m 19 and planning to learn the necessary skills to become a data scientist, data engineer or data analyst (I’ll probably start as a data analyst then change when I gain more experience )

I’ve been learning about python through freecodecamp and basic SQL using SQLBolt. 

Just wanted clarification for what I need to do as I don’t want to waste my time doing unnecessary things. 

Was thinking of using the free resources from MIT computer science but will this be worth the time I’d put into it?

Should I just continue to use resources like freecodecamp and build projects and just learn whatever comes up along the way or go through a more structured system like MIT where I go through everything?",14,44,1737454726.0,dataengineering
1i6xi1e,Looking for advice,"Professionally I'm a DevOps working in data environment, on my leisure I'm currently working on a data related project and am looking for some advices.

There is this podcast I love and I would like to try making new synthetic episodes. Here is how I see the pipeline actually:

- download the episodes
- use ml to get speakers diarization
- split into chunk using the diarization to keep phrases complète
- transcript each chunks
- split each speaker voice in isolation 
- finetune llm
- train ml with voice
- generate épisode script
- generate voices
- assemble

And I eventually would like to make funny data analysis like the most used word, longest sequence etc...

There is around 100 one hour episode.

I'm currently working with dagster for a client project so I tried to use it but it seem not be really suited for multiple batches. I was thinking about switching to airflow, but yet I'm wondering if this is really suited or if this is over engineering a ""small project""

I plan to start on few episodes, maybe 3 to 5 as I'm using replicate for all the ml parts and it can become costly pretty quickly...

Any advice wanted, would be happy to hear what you think",1,0,1737504368.0,dataengineering
1i6os41,How do I calculate Databricks job costs?,"I am completely new to Databricks and need to estimate costs of running jobs daily.

I was able to calculate job costs. We are running 2 jobs using job clusters. One of them consumes 1DBU (takes 20 min) and the other 16DBU (takes 2h). We are using Premium, so it's 0.3 per 1h of DBU used.

Where I get lost, is do I take anything else into account? I know that there is Compute and we are using All-Purpose compute that automatically turns off after 1h of inactivity. This compute cluster burns around 10DBU/h.

Business wants to refresh jobs daily, so is just giving them job costs estimates enough? Or should I account for any other costs?

I did read Databricks documentation and other articles on the internet, but I feel like nothing there is explained clearly. I would really appreciate any help

  
Edit: to add that these daily refreshes will be done on prod. Right now there is nothing on prod so I am using dev data to estimate",3,6,1737482512.0,dataengineering
1i6wkz0,"Cursor for your analytics stack, starting with dbt Core","Hey Reddit,

  
We're building Cursor for building end-to-end data products, starting with the most important (and overlooked part) - data transform.

  
Traditional code editors like Cursor and GitHub Copilot struggle with data workflows across systems. Turntable is different. It understands your project's metadata, lineage, and downstream usage, offering smarter suggestions for code modifications, tests, documentation, and even DAG refactors.

  
Unlike many clunky, slow cloud editors -Turntable feels like a local editor. It's familiar, and every panel is configurable. New projects can be spun up in seconds and changes are managed by an easy-to-use git UI.

  
Set up takes \~5 minutes and out of the box you'll get the IDE, column-level visibility from source -> dbt -> dashboard and a lightweight orchestrator. You can even run it inside your cloud for added security.

  
Leave a comment below if you'd like to try it out and I'll get you early access!



https://reddit.com/link/1i6wkz0/video/9eee9r4tkfee1/player

",1,1,1737501838.0,dataengineering
1i68gxc,35k euro in Paris as a data engineer is it good or bad? ,"I have 3 years of experience before Masters and graduated from a FRENCH B SCHOOL. 

Got an offer of 35k location Paris. Is it according to market standards?

How much salary I should ask.

What's the salary of an entry level Software Engineer/Data Engineer in Paris",42,71,1737427159.0,dataengineering
1i6vxn0,DP203 Exam Strategy,"Since the DP203 exam is going to be retired soon in March 2025, Is it worth trying to take the exam before the end date or is it better to prepare for DP700 as it is the replacement 

  
I am trying to improve my knowledge in Azure to make my profile better for job search and have to think about keeping the certification renewed in the future (not sure how DP203 renewal will work)

Any tips on choosing the right exam, and resources for preparation will help",1,2,1737500151.0,dataengineering
1i76cys,What is this error how to fix it,,0,5,1737533368.0,dataengineering
1i6bgim,How to stay relevant as a sata engineer,"Hey everyone! I’m a relatively new Data Engineer (been learning for less than a year) and I want to make sure I stay relevant in this fast-moving industry. My main goal is to expand my skillset so I can do more than just traditional data engineering tasks.

I’ve been thinking about learning data science concepts and even diving into building models or RAGs. Do you think this is the right direction? Are there specific skills or tools you recommend that would complement my data engineering expertise and keep me adaptable as new tech emerges?

I’d love to hear your thoughts or advice!",20,7,1737436982.0,dataengineering
1i727f9,interested in twitter automation?,"hey, i had an exciting hackathon this weekend, and built a bot that browses twitter and comments on other people posts and gives likes, completely on autopilot, based on the goal that you give it. 

under the hood it uses gemini 2 flash exp model and some browser automation techniques, plus vim

demo 1: [https://youtu.be/YBi\_itmZpQ4](https://youtu.be/YBi_itmZpQ4)  
demo 2: [https://youtu.be/E5ZgyeVg1vA](https://youtu.be/E5ZgyeVg1vA)",0,2,1737517668.0,dataengineering
1i60fx8,Postgres is now top 10 fastest on clickbench,,56,16,1737405619.0,dataengineering
1i6o7ta,Train My Own ML Model or Fine-Tune GPT?,"I am a cs major in my 3rd year of uni and Im working on another project to put on my portfolio.

I’m working on a project that involves **text classification**, and I want to optimize it for my **portfolio**. My dilemma:

 **Train My Own ML Model** (TF-IDF + Random Forest or similar):

* Requires collecting & labeling training data.
* Full control over the model, deeper ML experience.
* More effort but potentially more impressive?

OR 

**Fine-Tune GPT-4 or Use Prompt Engineering**:

* No need to manually collect/labeled data.
* Faster, highly accurate, and more industry-relevant.
* Less hands-on ML work, more about integration.

I’m aiming for **software engineering roles** but want to showcase **ML/NLP skills** as well.

# What’s more valuable on a portfolio?

Would training a custom model stand out more, or is fine-tuning GPT more practical?

Would love to hear your thoughts.",0,1,1737481160.0,dataengineering
1i6iwcu,Variable number of associated information for a base table record,"I have a base (or central) table:

**Subjects**:

\[subject\_id, subject\_name, .....\]

There are variable number of associated info related to each row (subject).

For subject\_id (or name) **Physics**, 4 associated attributes needed: { lab location, lab instructor, availability start date, availability end date } from the *Labs* table. Ofc, this table has a subject\_id column for joins.

For subject\_id (or name) **Maths**, 2 associated attributes needed { lecturer's name, teaching experience in years} from the *Lecturers* table. This table also has a subject\_id column for joins.

**Question**: Simplest or most elegant way to capture or model this sort of association between a base table record with variable number of attributes from different lookup tables.

I could think of a **attr\_association** table: \[id, subject\_id, lookup\_table\_name, lookup\_table\_column\] and forming a dynamic query and executing it to get desired result. But I believe there might be a much simpler and elegant solution for this scenario.",2,0,1737467144.0,dataengineering
1i69s15,What is the best path to go from a Business Intelligence Analyst to a Data Engineer?,"Hello all, So I am 6 years currently in the role of a Business Intelligence Analyst and I want to advance my career. I have a MBA in Business Analytics but in my actual job, all I really do is simple SQL pulls, reviewing data trends and pivot tables for sales trends. I want to advance my career to be more technical and I want to dive into the Data Engineering side of things, more back end database building and management, but I don't know what I need to do to or what are the best steps to take towards the Data Engineering side of things? I have intermediate SQL knowledge, Basic Python knowledge and I know I need to expand those areas, but what would you all recommend in regards to additional training or other areas I can expand upon? ",12,9,1737431267.0,dataengineering
1i6mvcy,Azure Synapse: Is it possible to parameterize a Notebook name/path the way it is with ADF pipelines and Databricks?,"To be clear, I'm not talking about passing parameters from a pipeline to a notebook, I'm talking about parameterizing the notebook reference itself.    
   
This is no big deal in ADF, probably because Databricks handles the notebook cluster configs, as apposed to Synapse notebooks where the cluster details come from the pipeline. But it still seems like I should be able to launch any notebook I want from the Synapse pipeline and have it spin up the cluster based on independant configs.   
   
However, it looks like the notebook is not allowed to be dynamic. Even my attempts to customize the pipeline JSON to force it to use a parameter have failed. Has anyone found a solution for this?",1,3,1737477808.0,dataengineering
1i6m5tf,Are practice tests a valuable tool in preparing for a certification exam?,"Quick poll to see what you all think about this method of preparing for certifications.

[View Poll](https://www.reddit.com/poll/1i6m5tf)",1,0,1737476017.0,dataengineering
1i5tmep,"What do you consider as ""overkill"" DE practices for a small-sized company?","What do you consider as ""overkill"" DE practices for a small-sized company?

Several months earlier, my small team thought that we need orchestrator like Prefect, cloud like Neon, and dbt. But now I think developing and deploying data pipeline inside Snowflake alone is more than enough to move sales and marketing data into it. Some data task can also be scheduled using Task Scheduler in Windows, then into Snowflake. If we need a more advanced approach, snowpark could be built.

We surely need connector like Fivetran to help us with the social media data. However, the urge to build data infrastructure using multiple tools is much lower now.

",77,77,1737389211.0,dataengineering
1i6kbwv,How to fix error: externally-managed-environment in Python 3.12 the Wrong Way,"Quick video I made a few months back on how to set the pip\_config file to allow you to pip install packages in Python 3.12 without having to specify ""break-system-packages""

[https://www.youtube.com/watch?v=g2TDfWDgwkE](https://www.youtube.com/watch?v=g2TDfWDgwkE)",0,0,1737471260.0,dataengineering
1i6k43u,The Modular Architecture Is the Way to Go for AI?,"What’s the general consensus on modular architecture for AI, especially in big organizations? 

Here’s the thing: AI projects often start with one use case—automating a process, optimizing something small, etc. But when it’s time to scale, everything starts falling apart: 

* The architecture can’t handle new use cases. 

* Data silos creep in. 

* You’re stuck rebuilding the whole thing from scratch. 

So, I’m trying to avoid that mess. The idea is to start with pilot projects but build with scalability in mind—something modular, flexible, and not a massive upfront investment. 

My questions: 

1. Is modular architecture actually the way to go, or are there better approaches to scaling AI across a company? 

1. How do you keep things modular without accidentally siloing everything? 

1. What have you learned about designing AI platforms to be resilient and scalable long-term? 

Curious to hear what’s worked (or what hasn’t) for those of you tackling this kind of challenge. ",0,2,1737470672.0,dataengineering
1i6snb1,"Thinking about migrating from Electrical Engineering to data engineering, anyone has been there?","Just wanted to know if anyone that went through this process has any tips, I have experience with back-end but not as complex as a data engineer needs to be.

I want to do remote work and in the country that I live in there is small demand in this area.",0,10,1737491936.0,dataengineering
1i6fh66,Unified API vs Data Integration tools,"I'm trying to understand the different use cases of Unified API platforms like [Merge.dev](http://Merge.dev) and [Unified.to](http://Unified.to) compared to data integration tools like Airbyte, Fivetran, and Meltano. Both can extract data from SaaS products like CRMs, but are Unified API platforms specifically designed for data ingestion within ETL workflows? Three key requirements that stand out to me in the context of ETL are support for extracting custom objects from sources (Salesforce for example), near real-time data ingestion, and integration with multiple data sources.",2,6,1737454416.0,dataengineering
1i6eyyh,EMR Serverless Spark Structured Streaming State Store,"Hi everyone, just wondering if anyone has any experience with configuring the state store in EMR Serverless Spark Structured Streaming jobs to anything other than the HDFS backed state store.

These days spark ships with a RocksDB implementation which sits outside the JVM reducing GC overhead when your stream contains state over many keys. I'm interested in using the RocksDB implementation for some near real time feature engineering I'm doing on EMR Serverless.

I configured
 ```
spark.sql.streaming.stateStore.providerClass=org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider
```
When I open the Spark UI I can see it configured in the environments tab, however on the SQL tab, when I click for further information below the query plan, it says it's still using the HDFS backed state store.

I've read through the EMR Serverless docs and don't see anything related to this (funnily enough I recollect reading something about it but can't for the life of me remember where!). I'm concern that maybe AWS disabled this due its Serverless nature and lack of access to appropriate disks for running RocksDB maybe?

Anyone any experience with this?

Thanks in advance!",2,3,1737452087.0,dataengineering
1i6i12f,How AI Agents & Data Products Work Together to Support Cross-Domain Queries & Decisions for Businesses,,0,0,1737464392.0,dataengineering
1i67wt2,Need help with my data model,"Apologies, I know this is probably a simple question, but data is not my main job and I'm stumped. 


My data set is not large, 300-400 rows, 122 columns. My data is [natively] already in a single table. The issue is the dates are in 116 columns (on the x-axis.) Six remaining columns are a hierarchy of forecasts from highest level down to man hours. 

Structure looks like attached pic #1. 


In my data model, (pic #1, right hand side), in the AllData_1 table, I created a unique ID for each row by combining Job+Trade into a Primary key column.  In the DateKey table, I transposed the axes, creating a date column, with the Job+Trade on the other (y-)axis.


Even though they are technically matching data sets, they don't have matching *columns* so currently, I can't link the two tables in the model. Wondering if I can create some kind of bridge table, or if I have to create separate transposed tables for each column? (Seems unnecessarily redundant.) Or is there a simpler way to do this.


My pivoted table (pic#2) looks amazing, everything is where it needs to be... Except (without linking the tables,) is obviously missing the actual data.


Constraints:
1. If this has already been answered in this sub (or another), I can't figure out how the response applies. I know this is probably a stupid question, please and thank you for helping me.


2. I don't have access to PowerBI for several more weeks. Just PQuery, PPivot & Excel. All are 365 versions  


3.CEO etc are accustomed to this layout, so original table & end result display can't change much. 


4. Multiple users access sheet each month, I have it protected, but would prefer not to store interim tables in the workbook or elsewhere. I don't see any reason why this can't be done with PQ, PP & Excel.


5. I can do some DAX, M, & Excel formulas, but I'm trying to stay away from VBA for this workbook. (But I can't see where VBA would be needed here anyway - this is just my lack of data modeling knowledge + my lack of skill in Power pivot. (Just started using power pivot 3 days ago.) 

If this is not the correct forum and you can direct me to a better one, please let me know. Thank you, thank you!!",8,7,1737425462.0,dataengineering
1i68uc6,Ethical Issues in Data Science,"Hello everyone! 

I'm currently pursuing an MS in Data Science and taking a course on ""Ethical Issues in Data Science"". 



I’m looking for a volunteer (Data science / Computing / Statistics professional) to discuss their experiences with ethical challenges—both technical and workplace-related—and their thoughts on how these situations were handled. 



All personal details, including names and companies, will remain anonymous. The chat would ideally take place via Zoom or any platform that works for you and would take about 15-20 minutes. If you prefer we can do it over DM.



If you're interested, please comment below or send me a direct message. Thanks in advance for your help!",3,14,1737428309.0,dataengineering
1i6vndn,First 100 sign ups free,"Im currently building out a series of Udemy courses to help those from an Excel background to move beyond spreadsheets. As a way of thanks to you good people here on r/DataEngineering i wanted to offer the first 100 people who use the below coupon, the ability to sign up to my initial Udemy courses for free:

Beyond Excel: Facilitating Data Change in Organizations

Beyond Excel: Structuring Data Outside of Spreadsheets

Coupon Code:

BEYOND-EXCEL-1ST-100

Keep Growing Your Potential!

PS. Udemy reviews are always welcome.

:)

Direct Course links:

https://www.udemy.com/course/beyond-excel-structuring-data-outside-of-spreadsheets/?couponCode=BEYOND-EXCEL-1ST-100

https://www.udemy.com/course/beyond-excel-facilitating-data-based-change-in-organisations/?couponCode=BEYOND-EXCEL-1ST-100

If the coupons are all gone, but you would like to check out the courses, the paid links are below:

https://www.udemy.com/course/beyond-excel-structuring-data-outside-of-spreadsheets/?referralCode=9B4DC47FE7DF043C853C

https://www.udemy.com/course/beyond-excel-facilitating-data-based-change-in-organisations/?referralCode=3CCEB63823C031E5AFBC",0,10,1737499403.0,dataengineering
1i6e9oa,Data stack for a startup,"Currently working at a startup, our BI stack is a Django server running queries directly against prod replicas and displaying reports. 

It kind of works, but generating a new report or analyzing a new business question takes anywhere from a few days to a week. Executives reach out to data engineers with a broad question, then engineers frame questions analyze data, and get back with results. It takes even longer when executives want something changed. I want to make it more easy for business people to understand and query data.   
  
I was considering proposing the use of a BI tool to get answers faster and make business people kind of self-serve some of their questions.  

Our main application database is in MySQL. Most of the tables have around 50-100 GB of data, some in TBs. I expect a lot of joins for analytical queries. In the future, I expect to also connect CRM and other internal DBs. I don't expect to get a lot of budget. 

The stack I was considering was Postgres + dbt + airbyte + airflow + metabase. 

I am new to the data engineering space and would love to hear your suggestions and thoughts. Thanks!",1,7,1737448801.0,dataengineering
1i5ijat,This chapter from the book Homo Deus,Reading my first book of 2025 - Homo Deus. Can relate to everything in this chapter about **Dataism**. Have you read it? What do you think about it?,164,32,1737348879.0,dataengineering
1i68dy1,nerd neck prevention (opencv2-posture-corrector),"Hey all,

As a remote worker, I have found myself slouching and getting nerd neck. This is an attempt to get rid of that. This creates a toolbar application that allows you to change intervals of how often your webcam tracks and recommends a posture check. Any and all feedback is appreciated.

[https://github.com/wtbates99/opencv2-posture-corrector](https://github.com/wtbates99/opencv2-posture-corrector)",3,0,1737426897.0,dataengineering
1i6n8l4,Bringing the scientific side to workplace,"PUM!!! BANG!!!

My head just exploded while talking with my wife.

Ok, so what if we (data professionals) align more with the scientific people (researches, PhDs, academic people in its essence) and ""drink"" their knowledge to apply on the professional side?

I honestly don't know if you're doing this but for me... These were two world that would never cross anytime, anywhere. Two different minds, two different goals.

But I think it makes sense, specially because now I need to do some research in my job and I have no idea from where to start (architecting data products in a financial institution, focusing in governance and quality) and thought ""hey, what if the academic side has researched about this topic somehow?"".

  
At the end two questions for all of you:

1 - does this make sense? (crossing this two worlds)

2 - any reliable scientific database from where I can search about my topic? (I thought about scopus, acm, ieee)

  
Thanks!",0,4,1737478746.0,dataengineering
1i6dt8n,Why Table/Stream Duality on Apache Iceberg Will Redefine the ETL market in 2025!?,,1,1,1737446662.0,dataengineering
1i6j38b,Show /r/dataengineering: ,"Heya! In the process of working on stress testing [bacalhau.org](https://bacalhau.org) and [expanso.io](https://expanso.io), I needed decent but fake access logs. Created a generator - let me know what you think! 

Readme below:  
\----

# 🌐 Access Log Generator

A smart, configurable tool that generates realistic web server access logs. Perfect for testing log analysis tools, developing monitoring systems, or learning about web traffic patterns.

## Backstory
This container/project was born out of a need to create realistic, high-quality web server access logs for testing and development purposes. As we were trying to stress test [Bacalhau](https://bacalhau.org) and [Expanso](https://expanso.io), we needed high volumes of realistic access logs so that we could show how flexible and scalable they were. I looked around for something simple, but configurable, to generate this data couldn't find anything.  Thus, this container/project was born.

## 🚀 Quick Start

1. Run with Docker (recommended):
```bash
# Pull and run the latest version
docker run -v ./logs:/var/log/app -v ./config:/app/config \
  docker.io/bacalhauproject/access-log-generator:latest
```

2. Or run directly with Python (3.11+):
```bash
# Install dependencies
pip install -r requirements.txt

# Run the generator
python access-log-generator.py config/config.yaml
```

## 📝 Configuration

The generator uses a YAML config file to control behavior. Here's a simple example:

```yaml
output:
  directory: ""/var/log/app""  # Where to write logs
  rate: 10                   # Base logs per second
  debug: false              # Show debug output
  pre_warm: true           # Generate historical data on startup

# How users move through your site
state_transitions:
  START:
    LOGIN: 0.7             # 70% of users log in
    DIRECT_ACCESS: 0.3     # 30% go directly to content
  
  BROWSING:
    LOGOUT: 0.4            # 40% log out properly
    ABANDON: 0.3           # 30% abandon session
    ERROR: 0.05            # 5% hit errors
    BROWSING: 0.25         # 25% keep browsing

# Traffic patterns throughout the day
traffic_patterns:
  - time: ""0-6""            # Midnight to 6am
    multiplier: 0.2        # 20% of base traffic
  - time: ""7-9""            # Morning rush
    multiplier: 1.5        # 150% of base traffic
  - time: ""10-16""          # Work day
    multiplier: 1.0        # Normal traffic
  - time: ""17-23""          # Evening
    multiplier: 0.5        # 50% of base traffic
```

## 📊 Generated Logs

The generator creates three types of logs:
- `access.log` - Main NCSA-format access logs
- `error.log` - Error entries (4xx, 5xx status codes)
- `system.log` - Generator status messages

Example access log entry:
```
180.24.130.185 - - [20/Jan/2025:10:55:04] ""GET /products HTTP/1.1"" 200 352 ""/search"" ""Mozilla/5.0""
```

## 🔧 Advanced Usage

Override the log directory:
```bash
python access-log-generator.py config.yaml --log-dir-override ./logs
```

## 📈 Analyzing Logs

The generated logs work great with standard analysis tools:

```bash
# Quick stats with DuckDB
python logs/test_duckdb_queries.py
```

## 🤝 Contributing

Contributions welcome! Feel free to:
- Open issues for bugs or suggestions
- Submit pull requests
- Share your use cases

## 📜 License

MIT License - feel free to use in your projects! 
",0,3,1737467737.0,dataengineering
1i5s6pt,Databricks or MS Fabric,"Hello,
I currently work at a company where we have a data warehouse (DWH) hosted on an on-premises SQL Server database. Every day, we execute a series of pipelines from Azure Data Factory, which are responsible for extracting information from the sources (OLTP databases and some CSV files in Azure Blob Storage) and loading it into staging tables in the DWH. Afterward, we use stored procedures to create dimensions (DIMs) and facts.

The issue is that the volume of data has grown significantly, and nightly loads are taking much longer to complete. I believe this is happening because we are using an OLTP database rather than an analytical database.

The decision to build this DWH on SQL Server was made many years ago, before I joined the company.

Now we are considering migrating the entire DWH infrastructure to something more modern. I have been researching, and I like the Databricks medallion lakehouse architecture using a Spark cluster. However, I’ve also seen that many people are beginning to adopt this architecture using MS Fabric with a SQL dedicated pool, which could also be implemented here.

Which option would you recommend for this migration to achieve the best possible performance and cost? Also, what about development time?

P.S.: Our DWH currently weighs 8TB.",20,32,1737385600.0,dataengineering
1i5wsea,Learning go,"Hello everyone Happy new year. I have started learning Go but i dont have direction to have advanced Level. Someone can we give advice about that?
",4,3,1737396900.0,dataengineering
1i5v4ir,Dataform tools VS Code extension,"Hi all, I have created a VSCode extension Dataform tools to work with Dataform. It has extensive set of features such as ability to run files/tags, viewing compiled query in a web view, go to definition, directly preview query results, inline errors in VSCode, format files using sqlfluff, autocompletion of columns to name a few. I would appreciate it if people can try it out and give some feedback

[Link to VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=ashishalex.dataform-lsp-vscode)

[Link to GitHub](https://github.com/ashish10alex/vscode-dataform-tools)

[YouTube video on how to setup and demo](https://www.youtube.com/watch?v=nb_OFh6YgOc)",6,0,1737392892.0,dataengineering
1i5w2lk,Recommendations for a Low-Code Data Transformation Tool Similar to Power Query?,"Hey All,

I’m looking for a low-code solution for performing data transformations—something that works like Power Query in Power BI, where you press buttons and use an intuitive interface to clean and transform data without manually writing code for each step.

Ideally, I’m hoping for:
	•	A tool or plugin that integrates with Python (or works standalone).
	•	A low-code or no-code approach for data cleaning and transformation.
	•	Compatibility with PyCharm or VS Code as a plugin would be a bonus.
	•	Free or paid options—I’m open to either, as long as it’s efficient and user-friendly.

Does anyone know of any tools or plugins that fit the bill? Any recommendations or insights would be greatly appreciated!

Thanks in advance!",3,3,1737395208.0,dataengineering
1i5zeub,"Teaching a machine to read, how LLM's comprehend text
",,4,0,1737403163.0,dataengineering
1i5kv4o,DP-203 Retired. What now?,"Big news for Azure Data Engineers! Microsoft just announced the retirement of the DP-203 exam - but what does this really mean?

 

If you're preparing for the DP-203 or wondering if my full course on the exam is still relevant, you need to watch my latest video!

 

In this episode, I break down:

 •  Why Microsoft is retiring DP-203

 •  What this means for your Azure Data Engineering certification journey

 •  Why learning from my DP-203 course is still valuable for your career

 

Don't miss this critical update - stay ahead in your data engineering path!



[https://youtu.be/5QT-9GLBx9k](https://youtu.be/5QT-9GLBx9k)",24,13,1737357999.0,dataengineering
1i5yy3h,"Self-Service Analytics Grounded in Reality - The Good, The Bad, and The Ugly",,2,0,1737402053.0,dataengineering
1i6duzs,Would you like / use a Cursor (AI IDE) for SQL?,"Trying to find project ideas to work on with my friends for a university hackathon, we want to build something useful. Also hated to work with Oracle SQL Developer, it feels like it has never been updated since Windows Vista",0,5,1737446884.0,dataengineering
1i5pp7f,Review and Finger Exercises for Networking in Docker and Docker Compose,"The first 14 minutes are probably a bit painful as I stumble through making my diagrams.   
  
However, if you are completely new to networking in Docker I think it's worth the time to watch and get the overview of how Docker, Docker Networks and you Local Machine's Network interact.   
  
Don't worry, it's not all drudgery. There is a simple Repo in the description so you can get hands on keyboard and actually practice. So if you are just using this as a quick refresher go ahead and jump to timestamp 14:00, clone the repo and follow along

[https://www.youtube.com/watch?v=jE7aRk6uZT4](https://www.youtube.com/watch?v=jE7aRk6uZT4)",7,6,1737378267.0,dataengineering
1i5sdbv,Is AWS experience mandatory for Data Engineer roles at Amazon?,"Hi Folks, I wanted to ask this question to people who have received opportunity from Amazon for Data Engineer roles.
Is AWS cloud exposure mandatory for getting shortlisted for Amazon Data engineer role? I am experienced in Azure cloud and tried applying to Amazon plenty of times and never got any reply from Amazon. I have 3 years of experience in data engineering domain.
I would be grateful for any opinion.",4,5,1737386063.0,dataengineering
1i5xwm2,Make Iceberg table data available to other systems,"I have an AWS Glue catalog with multiple Iceberg tables stored in S3. I want to make the data of these iceberg tables available to other applications inside the company which will consume the data. 

The data volume is low, so solutions like redshift are too expensive for us. Currently our main query engine is AWS Athena, but it has a bunch of limitations like the number of queries it can start within some interval of time. 

Also, I don't want to mantain an ETL pipeline to replicate the data from the iceberg table to a relational database. I want to consume the data directly from S3.

My main attempt was to use duckdb inside a postgresql container, but duckdb iceberg support is still in development.

What would be a possible tech stack in this case?",2,2,1737399567.0,dataengineering
1i5jng3,"AI agent to chat with database and generate sql, charts, BI",,10,8,1737352962.0,dataengineering
1i5vmg2,Sql dialect,"If one has experience with specific sql dialect can he pick up another sql dialect easily?

",1,17,1737394116.0,dataengineering
1i5uaem,Best technology for interacting with a new Dataware House ?,"Hey guys,

We are building a new Data Warehouse with my team.

The data is self-hosted. I looked around what are the best tech choices in terms of database. I think PostgreSQL makes a lot of sense, you know, open source, good analytics tools and good for large amounts of data. But really, what are the other choices, care to share some experiences?

https://preview.redd.it/1kbg9onte6ee1.jpg?width=1400&format=pjpg&auto=webp&s=32a22c9e21ff01279862bfa804c9c778038e719f

",1,1,1737390859.0,dataengineering
1i50mw6,Pinterest Data Tech Stack,"Sharing my 7th tech stack series article.

Pinterest is a great tech savy company with dozens of tech used across teams. I thought this would be great for the readers.

Content is based on multiple sources including Tech Blog, Open Source websites, news articles. You will find references as you read.

Couple of points:
- The tech discussed is from multiple teams.
- Certain aspects are not covered due to not enough information available publicly. E.g. how each system work with each other.
- Pinterest leverages multiple tech for exabyte scala data lake.
- Recently migrated from Druid to StarRocks.
- StarRocks and Snowflake primary purpose is storage in this case, hence mentioned under storage.
- Pinterest maintains their own flavor of Flink and Airflow.
- Headsup! The article contains a sponsor.


Let me know what I missed.


Thanks for reading.",70,8,1737299676.0,dataengineering
1i52gfe,"Data Engineers, What Business and Technical Challenges Have You Faced?

","currently working on projects. However, I don't have job experience yet, so I want to make my solutions realistic and have added value

Could you share some  **business problems** and **technical challenges** you’ve faced while implementing data solutions? 

Thank you in advance 

",36,33,1737304450.0,dataengineering
1i54dy3,Getting Foot in the Door as a Data Engineer,"I'm looking to make the switch to full time data engineering position. I find the field fascinating. I have working over last 7 years spending 1/2 my time as an analyst and 1/2 work non data related tasks. I want to make the switch to a permanent role. Pretty good at SQL, data visualization software's. Got a bit python experience as well. I'm close to south bay Silicon Valley and am open to working in office. Is that enough to get my foot in the door with the current scope of the market? Should I be contacting recruiters or is there a better method here? I feel a tiny bit intimidated, but at the same time, am confident I can learn anything. Is there an approach that you would all recommend?",25,18,1737309309.0,dataengineering
1i56pk8,Any advice for new Data Engineer?,"I just graduated college 6-8 months ago and will be joining a startup as a junior DE along with other junior, senior and staff DE. Any advice for me? I’m very nervous and having a bit of imposter syndrome. I’ll be using AWS, Databricks, Spark, Airflow and dbt.",14,19,1737314983.0,dataengineering
1i4asfr,Life of a Data Engineer,,870,37,1737216596.0,dataengineering
1i4yvgd,Azure Data Factory Data Flow vs SQL script in Synapse ,"Hello guys!  
  
I would like to ask for you help in order to decide whether the current data pipeline usage is good or bad.  
  
In my company we use Azure stack. We gather the data from source systems (mostly Postgres and MariaDB) using Copy activity to acquire and store source data into Azure Data Lake Gen2 in parquet format.   
After that we use Data Flow activity to read the data from Azure Data Lake, do some transformations and then load the data into the Data Warehouse (Synapse Analytics Dedicated SQL Pool).   
What I experienced is that, by using Data Flow activity it takes some minutes (2-3 minutes) to accomplish a very basic data transformation task (such as loading fact data without having to calculate anything). Spark cluster has already been started, and TTL option is set, so it is not about instantiating a Spark cluster. The parquet files' size on the other hand are little (most of the time it does not reach 1 MB). The interesting fact is that when I monitor the Data Flow activity, the processing time is just about some seconds, but when I monitor the whole data pipeline it shows that the Data Flow activity ran for minutes, which is not effective and I think it should be run much faster.  
  
I tried out a new approach, which would be loading the raw data into Synapse Analytics (As Is), and then use SQL script activity to do the same transformation logic but this time the source data would be a staging table in Synapse to be able to execute SQL.  
It was much faster in terms of Data Pipeline execution time and also it became much cheaper approach, because i did not have to use Data Flow activity and Spark cluster.

I'm curious about your opinion about the data pipeline setup. Is there any better approach than using Data Flow activity to do this data transformation? Is it better to use Azure Databricks or Synapse notebook in pyspark for this job? Or what else could you recommend? I would like to learn from your experience. Any suggestion or opinion would be much appreciated!

",15,5,1737294701.0,dataengineering
1i4njkr,Are most Data Pipelines in python OOP or Functional? ,"Throughout my career, when I come across data pipelines that are purely python, I see slightly more of them use OOP/Classes than I do see Functional Programming style. 

But the class based ones only seem to instantiate the class one time. I’m not a design pattern expert but I believe this is called a singleton? 

So what I’m trying to understand is, “when” should a data pipeline be OOP Vs. Functional Programming style? 

If you’re only instantiating a class once, shouldn’t you just use functional programming instead of OOP? 

I’m seeing less and less data pipelines in pure python (exception being PySpark data pipelines) but when I do see them, this is something I’ve noticed. ",117,71,1737252276.0,dataengineering
1i4xbdk,How to handle open ended design questions for Data roles and way forward?,"Recently I have seen that in the second or third round companies are asking open ended one liners, how should we approach these? Apart from the Data model, they are also expecting that we make some low level code and APIs and then also explain them about why we chose one service over the other, explain about latency, throughput stuff etc.  
Examples:

1. Design a Rider Management Platform which will provide riders to different e-commerce/quick-commerce websites

2. Design DQ Framework.

3. Design whatsapp/instagram/twitter/Jira etc.

As someone who wants to grow more into this field, I mean like as I grow I want to be on the tech side only like Data Engineer - > Senior Data Engineer - > Lead/Staff DE -> Principal DE. How should we proceed? Because these questions are similar to the System design rounds for Software profiles. Also, I feel that next 5 years DE profile is going to be merged in to Software roles only, like we see in some companies with designation such as Data Software engineer or Software Engineer (Data).

Seeking guidance from Senior folks here.  
",18,2,1737289436.0,dataengineering
1i50j2w,Data/System Design Resources,"Any one know where I can find comprehensive resources for Designing an end to end data ingestion pipeline. I want to ensure all things like designing, choosing tools, and scaling. Most YouTube videos aren’t that good and say generic stuff.  Thanks in advance. ",10,3,1737299391.0,dataengineering
1i4url5,What do you think of Talend?,"Qlik acquired Talend and has integrated it into their data studio for ingestion (currently limited to databases afaik).

Since our organization already uses Qlik, this was introduced to us last year. One of our team members spent a few weeks exploring it and concluded that it had its rough edges and was difficult to troubleshoot if something went wrong.

Now, it's being showed down our throats as it has been decided that Talend will be used for ingestions, because management believes UI-based tools equate to faster results. To counter this, I built a few ingestion pipelines for sources that Talend cannot handle (such as Rest APIs) using [dlt](https://github.com/dlt-hub/dlt) that have been working just fine since ~October of last year, but it seems that has fallen on deaf ears.

Has anyone here used the Qlik-integrated version of Talend? What’s your experience with it?",18,33,1737278654.0,dataengineering
1i4vij8,"We added parquet support (shoutout to the great hyparquet package) to our data management/app building tool, interested in feedback on workflow and anything else. Here working on the flights-1m dataset.",,9,3,1737281986.0,dataengineering
1i4m272,Why are DSA questions being asked in interviews if it's not so extensively used on the job?,I'm trying to understand how DSA concepts are actually used on the job.,39,26,1737247717.0,dataengineering
1i5fgc8,Hey I am looking for someone who can tutor me in Azure DE and Fabric,"I know there are lots of courses out there but I would love to talk to someone in realtime and work in some mini projects. If anyone is interested please dm, or also I am open to suggestion what other ways you guys reckon I can learn. ",0,1,1737338588.0,dataengineering
1i4qg8h,Annual learning credit,"I’ve got an annual learning credit through work (about $500) that I can use for any courses, certifications, etc. 

Would love recommendations for how to use this. I’m in more of an analytics engineering role currently, but want to expand more into the true data eng/platform/infra side. Some of the topics I’d be interested in learning more about include: 

- Kafka -> Spark Streaming -> DataBricks Delta Lake
- AWS for Data Engineering 
- Data Platform/Infra - I’d really love to learn the end-to-end required with deploying an Airflow instance onto a Kubernetes cluster. Would love anything that combines Airflow config with Terraform, EKS, etc. 
- Dev Ops - CI/CD, IaC

EDIT: adding some more info on my background. I’ve been in a few DE/analytics engineering positions in big tech for the past 5ish years, so looking for some more advanced courses beyond the basics/intros. I’ve toyed around with most things in my list above on my own - I know the best way to learn is through projects, but I’m specifically looking for how to spend this learning credit ",8,16,1737261447.0,dataengineering
1i4x07k,Need Help Finding resources for transactional data modeling,"I want to utilise a supervised ML model to score users based on their transactional data (the data contains both raw transaction data and analysed data - done by the third party ). it would be a supervised model, I have decided to use 30+ dpd in the next 6 months from the last transaction date as the indicator for good/bad behaviour. Any resources or Suggestions would be helpful. posted this on learnmachinelearning as well, but hoping to get resources from here too. Thanks in advance",2,1,1737288202.0,dataengineering
1i4klij,Real-world patterns for creating medallion workspaces and ingest data in Fabric,"Hi, I've read several articles about those topics, however I would like to ask Fabric practitioners what is the best approach to these 3 issues. I need to create medallion architecture where I create seperate Lakehouse for bronze and silver layer and Data Warehouse (or Lakehouse) for gold layer. Here are my questions:

**1st - creating separate workspaces for bronze/silver/gold layer in Fabric**

It's recommended to create separate Lakehouses in separate workspaces for each medallion layer - bronze, silver and gold. I'm wondering how it corresponds to another quite common pattern to create separate workspaces for Development, Test and Production (deployment pipeline). How should I combine the two approaches? In my company we split workspaces into DEV/TEST/PROD. I thought about 2 approaches:

*1. create 3 workspaces for bronze/silver/gold layers and within each create Lakehouses for DEV, TEST and PROD.* Here we follow the recommendation of having 3 separate workspaces for each medallion layer. For example: 

BRONZE workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD (in separate folders for example)

SILVER workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD

GOLD workspace which includes: Lakehouse DEV, Lakehouse TEST, Lakehouse PROD

*2. create 9 workspaces for each medallion layer combined with dev/test/prod architecture.* For example:

first workspace: Lakehouse BRONZE Dev

second workspace: Lakehouse BRONZE Test

another workspace: Lakehouse BRONZE Prod

another workspace: Lakehouse SILVER Dev

another workspace: Lakehouse SILVER Test

etc...

Here we also follow recommendation of having separate workspaces for each layer. However, as a result we have 9 workspaces. I'm wondering how those 2 approaches works in case we would use deployment pipeline to manage DEV/TEST/PROD environments. Please advise which approach is best here.

**2nd - data ingestion to bronze layer**

Let's say I created Lakehouse in bronze layer. Now I would like to load data efficiently to this Lakehouse. When it comes to data source it would be SAP data (to be precise data coming from SAP BW Application Server, de facto OLAP Cubes). I can connect to SAP via Dataflow connector. The issue is that I don't want to use Dataflows which are slow are generate overhead (I load huge amount of data). So please advise me how to efficiently load those data directly to Lakehouse Bronze layer from SAP. I have 2 options on my mind:

1. using data pipeline and Copy data activity to ingest data. However, SAP BW Application Server isn't available for data pipeline so I guess this option is about to be dropped

2. using PySpark and Notebooks - I could directly retrieve data from SAP BW Application Server and load it to Lakehouse as .parquet files. Question is if I could make connection to this particular SAP Server from Notebook (PySpark) or not? As far as I know Spark works much faster that Dataflows and is better cost-wise, that's why I think about this option.

**3rd - incremental data load to silver layer**

Now I need to load data from bronze to silver layer. Initial load to bronze layer would embrace, let's say, data for 2 years. Then I would like to upload data to silver layer incrementally for last 3 months. So now as a first step I should load data for 2 last years to bronze layer and then load it to silver layer. Next, delete all 2 years data from bronze layer. In next step load latest data for 3 months to bronze layer and then refresh last 3 months in silver layer. So in bronze layer we would always have data for latest 3 months and in silver layer data for last 2 years (from now) where last 3 months are updated and up-to-date.

My question is if it's good approach to incremental refresh and MOST importantly - should I make it in PySpark or use another approach?",15,4,1737243368.0,dataengineering
1i4k0z0, Beginner seeking advice: best learning resources to build data platform from scratch?,"Hi! Noob looking for advice. My goal is to train myself to be able to build a data platform and analytics platform from scratch. I'm not looking to build something too complicated - think of use cases for small businesses like a saas startup. 

I'd like to hear your suggestions on good learning materials. I'm looking for courses that can help me understand:

* What a data infra typically looks like, what's different components, how they are connected, mainstream tools, etc.
* How to make decisions on what tools / framework to use for each component, what's the trade-offs to be considered, best practices for scaling, etc. (i.e. I don't want to just build something that works, but want to understand the implications behind it)

I work in analytics so am very comfortable with python and sql, but have almost no knowledge on data engineering. And sadly this isn't something I can learn on the job as the data infra in my company seems way too advanced for my use case. Any pointers is appreciated. Thank you in advance!",7,10,1737241692.0,dataengineering
1i4nnb9,Help with orchestration[Airflow/Dagster],"We are revamping our scheduler which are cron jobs with Airflow/Dagster.
The requirements are 
1. All piplines across projects should be visible in a single UI.
2. The pipeline code will be a docker image that the orchestrator should pull and run as scheduled.
3. The individual tasks within a pipeline, i.e python functions, dbt models etc. should also be visible in the UI.(lineage within the pipeline)
4. The orchestrator will be in a seperate instance and should execute the docker images in other seperate instances.
5. The orchestrator code should live in the orchestrator instance and must be separated from pipeline code.
6. All logs should be stored in the orchestrator instance. 

We currently use Python for EL and dbt for T.

Questions:
1. Are these requirements appropriate and make sense?
2.Are these possible with Airflow/Dagster? 
3.Which of the two orchestrators would be more suitable for the requirements?
4.What is the best way to go about setting this up that fulfills all these requirements?
5. Are there better ways to go about this that I am overlooking?",4,11,1737252600.0,dataengineering
1i4adbr,What is wrong with Synapse Analytics,"We are building Data Mesh solution based on Delta Lakes and Synapse Workspaces.


But i find it difficult to find any use caces or real life usage docs. Even when we ask Microsoft they have no info on solving basic problem and even design ideas. Synapse reddit is dead.

Is no one using Synapse or is knowledge gatekeeped?",25,48,1737215460.0,dataengineering
1i43q2y,If i want to learn data engineering in 2025 from scrap what would be your suggestions?,"I have a strong foundation in Python, as I have been working with Django for the past two years.
But now i want to shift into data suggest from your learning experience what would be better for me.",90,35,1737191374.0,dataengineering
1i5307p,What's the Data Architecture being used at your Company at present.,"Hello Guys,

I am new to Data Engineering and would like to know or explore what are all the different architectures being used in different companies at present. Feel free to omit the Company name. 

Kindly mention the **size of the data being handled** at source and at different stages if applicable to make the understanding easier

Kindly mention the following (Feel free edit as per your need)  
1) Data Sources - Mention atleast 3 sources from where the data is mostly pulled from  
2) Data Ingestion - Tools used and in which format data is pulled and which format they are saved  
3) Architecture used - Medallion or any other architecture  
3) Orchestration and Data Transformation  - Tools used  
4) Data Warehouses if any being used and how they are updated  
5) How the data is exposed for Analytics and other purposes  
6) Security and Governance  
",0,8,1737305851.0,dataengineering
1i48phw,Building Real-time Analytics for an AI Platform,"Hi r/dataengineering!

  
So... my manager just dropped a ""small task"" on my desk yesterday: ""We need real-time analytics for our AI platform"". I've spent the last 24 hours diving deep into data architecture patterns, and I could really use some guidance from folks who've been there.

The situazion is this: I'm working at a company that has built an AI platform managing both containerized model deployments and API integrations (OpenAI, etc.). Every model interaction is stored in MongoDB, with our main collection ""modelCall"" containing prompts, responses, and usage costs. We've accumulated about 3M documents over two years (\~1.5M calls annually).

**Current System:**

* Platform manages AI models (both custom deployments and API integrations like OpenAI)
* MongoDB as primary OLTP database
* Main collection ""modelCall"" stores every model interaction (prompt, response, cost)
* \~3M documents collected over 2 years
* Other collections store user data, budget allocations, etc.

**The Challenge:** I need to add real-time/near real-time analytics capabilities, and I'm trying to figure out the best architecture. Here are my questions:

1. MongoDB seems suboptimal for analytics workloads - am I right about this?
2. Should I:
   * Implement dual writes to both MongoDB and an OLAP system?
   * Use Kafka as an intermediate layer?
   * Use Kafka + data lake format (Iceberg/Delta) + OLAP engine?
3. What's the best practice for write ordering? OLTP first or message queue first?
4. How should I handle potential inconsistencies if OLTP writes succeed but OLAP writes fail?

I'm new to designing data architectures at this scale, and I want to make sure I'm following best practices. Any advice on:

* Architecture patterns
* Technology choices
* Common pitfalls to avoid
* Resources to learn more

Would really appreciate your insights! Thanks in advance!",19,22,1737210637.0,dataengineering
1i4d4nl,[AIRFLOW] How to run one script after another?,"I've always wanted to use Airflow to manage pipelines.

I want to manage several scripts in a dependency flow, but I can't find any answers on how to do it.

I thought it would be a continuous series of script dependencies, like a flowchart, but I can only find answers that it can only be done through Tasks.

If I put my scripts in the task it will be huge and impossible to maintain.",5,6,1737222840.0,dataengineering
1i3n37a,data engineering? try dating engineering...,,292,21,1737138440.0,dataengineering
1i4bkuv,"I'm Torn Between Data Career Paths

","At the beginning of my journey, I aspired to become a data scientist and started self-learning the field. I made some progress by learning statistics and basic machine learning concepts, including supervised and unsupervised techniques. However, I don't feel like I achieved significant expertise.

Later, I joined a data analyst internship, followed by a data engineering internship. After completing these experiences, I was encouraged to pursue a data engineering career. While I’ve worked on improving my skills, I recognize that I still have a lot to learn in terms of tools and concepts to truly excel as a data engineer.

Recently, I was offered a data analyst position, and I’m about to start. I feel that gaining experience in data analysis could help me build on this career path, as data science seems like a natural progression from analysis. However, I’m also drawn to data engineering. Are all these roles interconnected in the end, or should I focus on one specific path to grow my career more effectively?",5,11,1737218757.0,dataengineering
1i4cv8z,First DE Project as Self Taught Developer: Seeking Tools & Best Practices for IoT Data Pipeline,"After completing ""Fundamentals of Data Engineering"" by Reis & Housley, I'm building my first real DE project and looking for guidance on tool selection and best practices.

**Project Overview:**

* Building pipelines to collect IoT sensor environmental data via API
* Cleaning data and engineering features
* Storing in TimeScaleDB (Postgres) with hypertables
* Generating automated weekly reports comparing 2-year running averages with current week's data
* Using Python and SQL (intermediate Python, learning SQL)

The book emphasizes building custom solutions only when they provide competitive advantage, otherwise leveraging existing tools. This leads to my questions:

1. How do you identify suitable open-source tools for a project like this?
2. Any tips for finding and adapting similar GitHub projects?
3. What are some recommended books/blogs/resources for someone learning DE independently?
4. How would this type of project typically be structured in a professional setting?

I'm self-teaching and using LLMs to help with coding, but I'm more focused on understanding proper frameworks and approaches. Any guidance on professional best practices would be greatly appreciated!",2,1,1737222147.0,dataengineering
1i3jqy0,Book Review: Fundamentals of Data Engineering ,"Hi guys, I just finished reading Fundamentals of Data Engineering and wrote up a review in case anyone is interested! 

**Key takeaways:**

1. This book is great for anyone looking to get into data engineering themselves, or understand the work of data engineers they work with or manage better.

2. The writing style in my opinion is very thorough and high level / theory based. 

Which is a great approach to introduce you to the whole field of DE, or contextualize more specific learning.

But, if you want a tech-stack specific implementation guide, this is not it (nor does it pretend to be)

https://medium.com/@sergioramos3.sr/self-taught-reviews-fundamentals-of-data-engineering-by-joe-reis-and-matt-housley-36b66ec9cb23",183,33,1737130033.0,dataengineering
1i44qxb,Question for AWS RDS to Redshift ,"
Hi kinda new in AWS.
Question:
When data is in RDS, can we use DMS to have a pipeline that is set to be transferred inside Redshift? Or theres better approach?
Like RDS > DMS > S3
or RDS >Glue > S3

Given we want to have data quality checks /  transformation like schema compliance, converting some zero columns to null basically simple transformations.",3,3,1737195942.0,dataengineering
1i3o0el,"They say ""don't build toy models with kaggle datasets"" scrape the data yourself
","And I ask, HOW? every website I checked has ToS / doesn't allowed to be scraped for ML model training. 

For example, scraping images from Reddit? hell no, you are not allowed to do that without EACH user explicitly approve it to you.

Even if I use hugging face or Kaggle free datasets.. those are not real - taken by people - images (for what I need). So massive, rather impossible augmentation is needed. But then again.... free dataset... you didn't acquire it yourself... you're just like everybody...

I'm sorry for the aggressive tone but I really don't know what to do.",67,43,1737140791.0,dataengineering
1i42m1k,Mongo-analyser,"Hi,

I made a simple command-line tool named Mongo-analyser that can help people analyse and infer the schema of MongoDB collections. It also can be used as a Python library.

Mongo-analyser is a work in progress. I thought it could be a good idea to share it with the community here so people could try it and help improve it if they find it useful.

Link to the GitHub repo: [https://github.com/habedi/mongo-analyser](https://github.com/habedi/mongo-analyser)",5,3,1737186369.0,dataengineering
1i419au,What keyboard do you use?,"I know there are dedicated subs on this topic, but the people there go too deep and do all sorts of things.

I want to know what keyboards data engineers use for their work.

  
Is it membrane or mechanical? Is it normal or ergonomic?",5,63,1737180635.0,dataengineering
1i44wqj,How much effort should I (and our team) spend helping other teams write queries?,"So I'm working at a mid-sized tech startup and we have a small data team. Our core responsibility is to prepare the data warehouse so downstream users (mainly business analysts in commercial teams and finance) can use the data to generate performance charts etc. 

You can imagine the commercial bosses are always not satisfied with the data views we have prepared so the BAs take on some heavy lifting to make a lot of ad hoc charts and tables. The problem is, most of them are not systematically trained to write efficient queries (we use BigQuery) or to properly debug them. So a lot of colleagues reach out to us and ask for help. I'm usually quite ok with helping someone, but there are cases where they have a tight deadline while I also have a deadline to meet for my own work. 

Our team has had some internal rants and discussions about this problem, but we see no way to either increase our HC (in the end, we don't believe it's a junior DE's role to serve the commercial bosses directly either) or make our downstream users better at using our platforms--their job is not sorting out BigQuery but sorting out ""insights.""

What are your thoughts on this?",2,3,1737196658.0,dataengineering
1i3pa1s,"Simple Python ETL job framework? Something that handles recording metrics, logging, and caching/stage restart. No orchestration needed.","I'd like to find a Python batch ETL framework that I can inherit from that has opinionated defaults. I'd like to be able to run something like the code below and have the metrics (run time, failures, success, etc) written to postgres, sensible logging, and a way to cache data to restart a job at the transform/load steps.

    class MyETLJob(ETLJob):
        def __init__(self, file_path):
            self.file_path = file_path

        def extract(self):
            with open(filepath) as file:
                data = file.read()
            return data

        def transform(self, data):
            lines = data.split(""\n"")
            return lines

        def load(self, lines):
            for line in lines:
                write_to_database(line)

    job = MyETLJob(""data.txt"")
    job.run()

I don't want any chaining, orchestration, job dependency management, GUIs, etc.

Does anything like this exist?",21,19,1737144083.0,dataengineering
1i3y6bt,Moving from GRC to Data Engineering,"I'm a GRC supervisor but have been learning Data Engineering in my off time. I'd like to make a switch since I really enjoy being able to move Data and learning new things.

I am steeped in cybersecurity but have reasonable skill in linux, SQL, some python, and have Google Associate Cloud Engineer certification. 

Any thoughts on starting a foray into DE would be greatly appreciated.",4,8,1737169496.0,dataengineering
1i41neh,Does Debezium cap out at 6k ops?,"I have been benchmarking some tools in the Postgres CDC and was surprised to find Debezium cannot handle 10k operations per second from Postgres to Kafka. 

https://preview.redd.it/ftu7xh0j7pde1.png?width=1490&format=png&auto=webp&s=f2f16a1c2820a4cb0ac351abe2fb4bc16193698a

The full terraform for Debezium on AWS MSK with MSK Connect is [on GitHub](https://github.com/sequinstream/sequin-vs-debezium-benchmark/tree/eeb8b108e92f90268c3c6e600f3a4bc213c28dab/terraform) and I have a comparison with my company's tool in [our docs](https://sequinstream.com/docs/performance).

Would be very interested if those who know Debezium or have it running more quickly could let me know if there is a way to speed it up! TIA",2,4,1737182249.0,dataengineering
1i3rs4y,AI support bot RAG Pipeline in Dagster Tutorial,,7,2,1737150738.0,dataengineering
1i3f2pe,Should Power BI be Detached from Fabric?,,29,11,1737116242.0,dataengineering
1i3dvqc,How to Approach a Data Architecture Assessment?,"Hi everyone,

I recently joined a firm as a junior data engineer (it's been about a month), and my team has tasked me with doing a data architecture assessment for one of their enterprise clients. They mentioned this will involve a lot of documentation where I’ll need to identify gaps, weaknesses, and suggest improvements.

The client’s tech stack includes Databricks and Azure Cloud, but that’s all the context I’ve been given so far. I tried searching online for templates or guides to help me get started, but most of what I found was pretty generic—things like stakeholder communication, pipeline overviews, data mapping, etc.

Since I’m new to this kind of assessment, I’m a bit lost on what the process looks like in the real world. For example:

What does a typical data architecture assessment include?

How should I structure the documentation?

Are there specific steps or tools I should use to assess gaps and weaknesses?

How do people in your teams approach this kind of task?


If anyone has experience with this type of assessment or has any templates, resources, or practical advice, I’d really appreciate it. 


Thanks in advance!


",24,7,1737111600.0,dataengineering
1i3ni9g,Airbyte on Docker and local CSV,"I am running Airbyte OSS locally on a windows laptop using Docker for Desktop. I was able to configure it and run a job/connection where it is reading from a Oracle table and writing to a local CSV. I can see that my execution was successful, but am not able to locate the CSV file created by Airbyte. As I am running Docker with WSL2, I though the docker folders would be available under `//wsl$/docker-desktop-data`, but the folder doesn't exist. Appreciate any input on this.",5,1,1737139514.0,dataengineering
1i3qi87,Need help with proper terminology around different ways to display a percentage,"I work with data, and in my data i have two columns ""Rate at origination"" and ""Rate (current)"".  
In my example, they both are, in the real world,  1.25 percent (1.25%)

But, in my table, ""Rate at origination"" is stored as 0.0125, and ""Rate (current)"" is stored as 1.25 (they come from different systems).

I want to explain to someone this difference/problem, but i'm struggling due to lacking the proper terminology.

Basically, I need to explain that they both should be stored in the same ..\_\_?\_\_.. format??  But, I think there's probably a better more precise/accurate term for this.

Help!",3,5,1737147334.0,dataengineering
1i3sr2y,FCMSA or Safer API ,Has anyone worked with the safer or FCMSA API? There is the ability to hit the endpoint by DOT for a snapshot or live data. The snapshot data appears to have less fields than the historical data and there are thousands of fields with nested json. Is there a smarter way to get all three fields and nested fields other than looping through. I am think of having different tables to store the data but the mapping exercise and how to hey all the data and fields seems extremely inefficient. I was going to use python and a RDMS. Any suggestions?,2,5,1737153303.0,dataengineering
1i3h048,Delta Live Tables opinions,"What is the general opinion about DLT? When to use and not use DLT? Any pitfalls? The last threads are from years ago.

I can see the benefits but I am honestly bothered by the proprietary nature of it and I am afraid it is going to move more and more into a low code solution.",8,6,1737122543.0,dataengineering
1i3935p,ActiveData: An Ecosystem for data relationships and context. ,"Hi r/dataengineering

I needed a rabbit hole to go down while navigating my divorce. 

The divorce itself isn’t important, but my journey of understanding my ex-wife’s motives are. 

A little background:

I started working in Enterprise IT at the age of 14, I started working at a State High School through a TAFE program while I was studying at school.

After what is now 17 years of experience in the industry, working across a diverse range of industries, I’ve been able to work within different systems while staying grounded to something tangible, Active Directory. 

For those of you who don’t know, Active Directory is essentially the spine of your enterprise IT environment, it contains your user accounts, computer objects, and groups (and more) that give you access and permissions to systems, email addresses, and anything else that’s attached to it. 


My Journey into AI:

I’ve always been exposed to AI for over 10 years, but more from the perspective of the observer. I understand the fundamentals that Machine Learning is just about taking data and identifying the underlying patterns within, the hidden relationships within the data. 

In July this year, I decided to dive into AI headfirst. 

I started by building a scalable healthcare platform, YouMatter, which augments and aggregates all of the siloed information that’s scattered between disparate systems, which included UI/UX development, CI/CD pipelines and a scalable, cloud and device agnostic web application that provides a human centric interface for users, administrators and patients. 

From here, I pivoted to building trading bots. It started with me applying the same logic I’d used to store and structure information for hospitals to identify anomalies, and integrated that with BTC trading data, calculating MAC, RSI and other common buy / sell signals that I integrated into a successful trading strategy (paper testing)

From here, I went deep. My 80 medium posts in the last 6 months might provide some insights here 

https://osintteam.blog/relational-intelligence-a-framework-for-empowerment-not-replacement-0eb34179c2cd

ActiveData:

At its core, ActiveData is a paradigm shift, a reimagining of how we structure, store and interpret data. It doesn’t require a reinvention of existing systems, and acts as a layer that sits on top of existing systems to provide rich actionable insights, all with the data that organisations already possess at their fingertips. 

ActiveGraphs:

A system to structure spacial relationships in data, encoding context within the data schema, mapping to other data schemas to provide multi-dimensional querying 

ActiveQube (formally Cube4D:

Structured data, stored within 4Dimensional hypercubes, think tesseracts

ActiveShell: 

The query interface, think PowerShell’s Noun-Verb syntax, but with an added dimension of Truth

Get-node-Patient | Where {Patient has iron deficiency and was born in Wichita Kansas}

Add-node-Patient -name.first Callum -name.last Maystone 

It might sound overly complex, but the intent is to provide an ecosystem that allows anyone to simply complexity.

I’ve created a whitepaper for those of you who may be interested in learning more, and I welcome any question. 

You don’t have to be a data engineering expert, and there’s no such thing as a stupid question. 

I’m looking for partners who might be interested in working together to build out a Proof of Concept or Minimum Viable Product. 

Thank you for your time 

Whitepaper:

https://github.com/ConicuConsulting/ActiveData/blob/main/whitepaper.md",40,9,1737090586.0,dataengineering
1i3q4sn,I have mixed data types in my JSON source data (strings alongside numbers) which is causing HIVE errors when querying in Athena. Not sure best practices on how to address it,"I have a pretty simple table with a column for quantities along with time stamps, units and sources of those quantities. The majority of my data are double with some int values as well. Initially there wasn’t too much of a problem with those two existing in the same column. The reason why they aren’t all double for example is that the type of the data is described in another column and that may dictate that there are whole number counts. That worked for a while but I did a large (compared to the amount of existing data) data load and now some quantities are strings. Those strings map to a limited set of ordinal rather than the cardinal values that the existing doubles can take. Now I’m getting HIVE errors in Athena. The data is also partitioned by date even in raw form. I suppose I’m wondering why in Athena it seems that there is an error because in the table schema I defined quantity to be strings but when glue crawls and partitions the backfill data it decides to detect the column in that partition as double if there are no string cardinals in that day of data. 


Another question is how to move forward. I get intuitively that rigid SQL rules will not allow a string to be in the same column as a double. Should I drop the string from the float at the source level of ingest? Should I split quantities into columns by type with one being for strings and accept lots of null values in my table? Should I map the strings to int and keep a dictionary somewhere else to know what those Int values represent? Or something else",2,1,1737146330.0,dataengineering
1i3k6f9,DP-203 Cert vs new DP-700 certification for new Data Engineer?,I am new to Data Engineering field. I just passed DP-900 Azure Data Fundamentals exams. I found out today that DP-203 being phased out in March 2025. Should I rush into taking it before it expires since thats the current industry standard or do you recommend me taking DP-700 Microsoft Fabric cert to future proof myself assuming the industry moves in that direction. Thanks for all your feedback!,6,2,1737131139.0,dataengineering
1i3o76i,Schema for transform/logging,"Ok data nerds, who can help me.

I am fixing 60,000 contact records
I have 3 tables: raw, audit log, and transform

My scripts focus on one field at a time 
E.g. Titles that are Mr or Ms 
- Load to table.transform as Mr. or Ms.
- table.auditlog gets a new record for each UID that is transformed with fieldname, oldvalue, new value
- table.tranform also gets a RevisionCounter where every UID new record is incremental so I can eventually query for the latest record 

This is flawed because I'm only querying table.raw 

should I copy all records into transform and just run scripts against max RevisionCounter per UID in transform? 

I'm worried about this table (mySQL) getting so huge really fast - 60,000 records x 30 transforms.... But maybe not?

Clearly someone has figured out the best way to do this. TIA!
- ",2,0,1737141287.0,dataengineering
1i2z15i,Anyone here switch from Data Science/Analytics into Data Engineering?,"If so, are you happy with this switch? Why or why not?",108,74,1737060719.0,dataengineering
1i3fzn0,Which filesystem do you use for external data drives?,"I am someone who constantly switches between Linux, Mac and Windows. I have a few crawlers running that collect a few gigabytes of data daily and save it to the disk. This is mostly textual data in json/csv/xml format and some parquet/sqlite files. All of my crawlers run on my Linux pc running Fedora but later the saved data should be ""read-only"" accessible on any os via the local network.

The saved data often has a large number of empty files, and it needs to have support for unix file permissions and git support. I was using nvme ssds till now but recently bought a few 16tb hdds as it was a lot cheaper than the nvme and I don't need the speed. 

Which filesystem should I use on the new drives to ensure my setup works fast and well across all my devices?",4,2,1737119413.0,dataengineering
1i37a2e,What a frustrating day,"My company has been in the process of migrating payment platforms for the past month or so. Today I went through the data and got hit up from a couple of stakeholders stating every looks off...then I learned they imported not just the customers but all the transactions from the old provider into the new provider, causing a ton of duplicates and all dimension tables pretty much became useless.

All the logic used to build the models is out the window now and none of the models are accurate because we have the same data in 2 different schemas now...and the only identifier at this time is to search on the prefixes of the customer IDs.

Since the data was imported to the provider, data from the APIs also contain these duplicates. I asked  the backend engineer why they even did this and it was ""leadership"" that decided on this.

I pretty much shut down for the day. I was so livid that I couldn't focus. I told my coworker that we should purge the records immediately and convert old customer IDs to the naming convention the new provider uses.

TLDR; rant over ",28,28,1737084408.0,dataengineering
1i3hmeq,Inner ADF pipeline return value expression is not evaluated ,"Hello all, 

I have an inner ADF pipeline that is supposed to give me an output variable name (string)

The set variable is inside a foreachloop connected to a get meta data 

The variable returns @item().name 


But when I look into into my variable that should capture the inner pipeline output I see 
value: ""@item().name""

The set variable uses this expression 

@activity(""InnerPipeline').output.pipelineReturnValue.latestFile


Which.... Should be correct but it's not evaluating the expression ",4,1,1737124345.0,dataengineering
1i3bw97,How to implement efficient incremental loading for a complex aggregation view in Snowflake or data build tool ?,"Hi everyone!, I'd like to share our Current Project Architecture in Snowflake Data Platform and seek advice on handling incremental loads.

Current Project Architecture in Snowflake:  
\- We have about 50 source tables in Snowflake  
\- These feed into 50 transformation views (with complex SQL operations)  
\- Finally, everything consolidates into a single wide table (\~800 columns)  
\- BI team uses this final table for all their dashboards

Current Snowflake Setup & Resource Usage:  
\- Running on Snowflake Medium size warehouse with 10 max clusters  
\- We reload the final table every hour  
\- Each full reload takes about 15 minutes to complete  
\- We only receive new records (no updates to existing data)  
\- Each view has complex operations (GROUP BY, ORDER BY, RANK, aggregations like SUM, COUNT)  
\- Then truncate and reload the final table  
\- This process is consuming significant warehouse credits

Challenge: As our data volume grows, this hourly full-reload approach is becoming expensive and time-consuming. We're looking to implement incremental loading, but many of our transformations require historical data for correct calculations.

Here's a simplified example to illustrate the type of transformations we're dealing with:

\-- Source tables

CREATE TABLE orders (  
order\_id INT PRIMARY KEY,  
order\_date TIMESTAMP,  
customer\_id INT,  
order\_status VARCHAR(50)  
);

CREATE TABLE order\_items (  
item\_id INT PRIMARY KEY,  
order\_id INT,  
product\_id INT,  
quantity INT,  
price DECIMAL(10,2)  
);

\-- View  01 

CREATE VIEW daily\_order\_metrics AS

SELECT   
DATE(o.order\_date) as sale\_date,  
o.customer\_id,            -- Adding this for uniqueness  
o.order\_id,              -- Adding this for uniqueness  
COUNT(oi.item\_id) as items\_in\_order,  
SUM(oi.quantity) as total\_quantity,  
SUM(oi.quantity \* oi.price) as order\_total,  
MIN(o.order\_date) as first\_order\_time,  
MAX(o.order\_date) as last\_order\_time  
FROM   orders o  
LEFT JOIN order\_items oi   
ON o.order\_id = oi.order\_id  
GROUP BY   
  DATE(o.order\_date),  
  o.customer\_id,  
  o.order\_id;



\-- view 02

CREATE VIEW customer\_daily\_summary AS

SELECT   
 sale\_date,  
 customer\_id,             -- This is our unique key along with sale\_date  
 COUNT(order\_id) as orders\_per\_day,  
 SUM(total\_quantity) as total\_items\_bought,  
SUM(order\_total) as total\_spent,  
MIN(first\_order\_time) as first\_order\_of\_day,  
MAX(last\_order\_time) as last\_order\_of\_day  
FROM  daily\_order\_metrics  
GROUP BY   
sale\_date,  
customer\_id;

  
final target table T\_customer\_daily\_summary

CREATE or replace TABLE T\_customer\_daily\_summary (  
sale\_date DATE,  
customer\_id INT,  
orders\_per\_day INT,    
total\_items\_bought INT,          
total\_spent DECIMAL(15,2),       
first\_order\_of\_day TIMESTAMP,     
last\_order\_of\_day TIMESTAMP,   
dw\_load\_timestamp TIMESTAMP DEFAULT CURRENT\_TIMESTAMP(),  
PRIMARY KEY (sale\_date, customer\_id)  
);  


Questions:   
1. How would you approach incremental loading in Snowflake when dealing with transformations that need historical context?  
2. For the simple example above, could you show how you'd handle just the first view incrementally?   
3. How do larger companies typically handle such scenarios at scale in their Snowflake implementations?

  
I believe many other Snowflake & dbt users might be facing similar challenges, so any insights would be valuable for the community. Thank you in advance for your help.",7,20,1737102520.0,dataengineering
1i3o1t6,Schema Issues When Loading Data from MongoDB to BigQuery Using Airbyte,"I am new to data engineering, transitioning from a data analyst role, and I have this kind of issue. I am moving data from MongoDB to BigQuery using Airbyte and then performing transformations using dbt inside BigQuery.

I have a raw layer (the data that comes from Airbyte), which is then transformed through dbt to create an analytics layer in BigQuery.

My issue is that I sometimes encounter errors during dbt execution because the schema of the raw layer changes from time to time. While MongoDB itself is schemaless and doesn’t change, Airbyte recognizes the fields differently. For example, some columns in the raw layer are loaded as JSON at times and as strings at other times. Sometimes they are JSON, then numeric, and vice versa.

I am using the open-source versions of Airbyte and dbt. How can I fix this issue so that my dbt transformations work reliably without errors and correctly handle these schema changes?  
Thank you!",1,1,1737140897.0,dataengineering
1i30v9r,Is SQLMesh multi engine support offering us an easy path out of engine vendor locking?,"I just read [this article on SQLMesh support for multi-engine projects](https://tobikodata.com/support-for-multi-engine-projects.html), and it feels like the industry is finally taking the right steps to enable users to switch between different data processing engines easily.

About a year ago, my company began integrating Iceberg into our data lake. We've been using Spark on AWS for about 10 years, and now we can also read data from Athena via the Glue Catalog. Currently, we mainly use Athena for data exploration since we haven't set up any dedicated project with it yet. 

We've been discussing creating another dbt project for Athena (we already have one for BigQuery), but I'm thinking that with SQLMesh, we could potentially create a single project with Spark (using Python or SQL) and swap out parts with Athena as it would be easier to leverage federated queries on our RDBMS in Aurora and finally use BigQuery through BigQuery Omni to bridge both cloud providers. All of this could be orchestrated in one inferred DAG! And a single SQL dialect (thanks to SQLGlot in SQLMesh)!

Has anyone tried something similar or is planning to?",37,4,1737065549.0,dataengineering
1i3jfoy,What to study?,"Currently in the application process for an entry-level data engineering consulting position. I have a possible technical coming up and I was just wondering would be some of the key things to study.

Asking because I have a degree in computer science and have done mainly backend work.

Skills I have that I think are relevant: SQL, some MySQL experience, python, some AWS, some GCP.",0,2,1737129244.0,dataengineering
1i3ikds,File intake - any service out there?,"So we take in a LOT of CSV files - thousands - all of different formats and structures. Already right there need to start lining things up. Most of them drop to s3 via SFTP and then get processed via something like dbt into our lake. 

Are there any tools out there though to simplify the ingestion process (i.e. setup an API or SFTP upload endpoint for files to send them to) and then providing a specified format only allow files that follow that format (i.e. 10 columns with first being text, second being a number, etc)

Is there any service or combo of services that might provide this?",1,2,1737126932.0,dataengineering
1i32b1y,Data Quality (PySpark) with Databricks Labs' new DQX tool.,,15,4,1737069457.0,dataengineering
1i3lpiw,Need some guidance ,"""Hey everyone, I’m thrilled to share that I’ll be starting as a Data Engineer Intern soon, and I’ve got just a week left to prepare! 😄

As someone stepping into the field, I’m eager to make the most of this time. Could you guide me on what to focus on before joining? Maybe specific skills, projects, or tools that would make an impact?

I’m open to suggestions, whether it’s brushing up on SQL, learning about data pipelines, or even building a mini-project in Python or Spark. Your insights or experiences would mean the world to me. Let’s make this first step a strong one! 🚀

Thanks in advance for your advice!""

",0,7,1737134959.0,dataengineering
1i2u8zf,"Do you use DBT Cloud? If yes, how much do you pay approximately? ",I'm trying to evaluate pros and cons of having DBT core vs cloud vs use another tool for transformation altogether. Any help would be appreciated. ,32,37,1737048570.0,dataengineering
1i2vkhe,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",27,3,1737051886.0,dataengineering
1i3a8fw,Bytebase 3.3.0 released -- Database DevSecOps for MySQL/PG/MSSQL/Oracle/Snowflake/Clickhouse,,4,0,1737095017.0,dataengineering
1i2l41z,A single course/playlist to learn Data Modeling and Data Architecture?,"I recently failed to land a job because I didn't know almost nothing about data modeling/data Architecture (Kimball, OBT...) and I want to fullfill my gap, any advice?",126,32,1737019235.0,dataengineering
1i3l9jf,Am I qualified enough to ask for a Full time?,"I’m currently interning for a company that had laid off the entire data engineering team in the US. I’m a data engineer intern have been here for over 6 months. 

I have build around 10 end to end data pipelines on AWS using glue, s3 and other services as part of the internship. I have a strong data experience and prior to this I have 1 year of full time DE experience.

Given the situation in my company, should I ask for a full time offer as I’m set to graduate from my graduate program this May?",0,4,1737133838.0,dataengineering
1i2tfmh,Blending DuckDB and Apache Iceberg for Optimal OLAP,"[https://www.bauplanlabs.com/blog/blending-duckdb-and-iceberg-for-optimal-olap](https://www.bauplanlabs.com/blog/blending-duckdb-and-iceberg-for-optimal-olap) 

  
I wrote a blog post about how we at Bauplan Labs leverage the strength of both to deliver a versioned, fast SQL and Python system. Check it out!",26,0,1737046520.0,dataengineering
1i3gtfe,What do u think the future tech roles look like?,"With the burst of AI and its rapid adoption across various industries, I see there is a rapid growth in data related jobs! Example : AI engineer etc… I also see a decrease in swe roles. 

With tools like cursor and many other AI powered tools the requirement for additional swe’s is decreasing.

What kind of roles do you think we will see more in the near future?

Prompt engineer, AI engineer, data engineer, etc…?",0,24,1737121996.0,dataengineering
1i2qj1c,"Seeking Advice as a Junior Data Engineer hired to build an entire Project for a big company ,colleagues only use Excel.","Hi, I am very overwhelmed, I need to build an entire end-to-end Project for the company i was hired in 7 months ago. They want me to build multiple data pipelines from Azure data that another department created. 

they want me to create a system that takes that data and shows it on Power BI dashboards. i am the fraud data analyst is what they think. I have a data science background. My colleagues only use/know Excel. a huge amount of data with a complex system is in place.

",33,29,1737038911.0,dataengineering
1i3d5c0,Need Input for Planning a Tech/Engineering Conference – Quick Survey!,"Hi everyone,

I'm an event management student and have been tasked with planning a 4-day conference for professionals in the tech or engineering fields. To make it engaging and valuable, I’m doing some market research on what activities and experiences people in these fields would enjoy at such an event.

If you have 2 minutes to spare, I’d be super grateful if you could fill out this short survey: https://forms.office.com/r/iextU9sQD7

Thanks so much in advance for your help!",0,0,1737108386.0,dataengineering
1i35gx3,"How Much DSA Knowledge is Needed for a Data Engineering Role, and Do All Companies Have a DSA Round?","Hi everyone,  

I’m new to the data field and planning to transition into a data engineering role. I’ve been learning about the skills required, and I’m a bit confused about the importance of Data Structures and Algorithms (DSA) for this role.  

1. How much DSA knowledge is typically expected for a data engineering position?
 
2. Do all companies include a DSA round in their selection process for data engineers, or is it more focused on SQL, data modeling, ETL pipelines, and tools like Spark, Hadoop, etc.?

3. If DSA is important, which specific topics should I prioritize learning?

I’d really appreciate insights from those who have gone through the selection process or are working in the field. Thanks in advance.",4,8,1737078695.0,dataengineering
1i3p2ie,I’m looking to change my life around. Is there anyone here that purely self taught coding and did a couple of courses and then got an entry into software dev/coding jobs? Even data analyst jobs? ,"HI’m looking to change my life around. Is there anyone here that purely self taught coding and did a couple of courses and then got an entry into software dev/coding jobs? Even data analyst jobs? 

Right now I got 3. options because of financial constraints. 

1. Do a 9 month software dev bootcamp at a university and come out with some connections and a good portfolio and then apply from there 

2. Simply learn from Udemy and coursera and use my certificates and a good portfolio to apply 

3. Maybe (MAYBE) I do 3 jobs this year so I can afford a masters in data science and then apply for job. 

I don’t have a degree in anything and I can’t afford a full 4 year degree, I was thinking of cyber security, but have heard this is even harder to get into as real experience is required INSIDE the companies, and you can’t learn all the confidential stuff until your hired… so essentially you start as IT support. Am I wrong in this? ",0,10,1737143545.0,dataengineering
1i34c6p,Orchestration tool for windows server,"Hi folks, I need to build a data pipeline to ingest company data in MSSQL to a new data warehouse (currently using postgres as the volume is not that huge), but the only resource that can connect to that database is a windows server due to network limitations.

For orchestration, which orchestration tool that works well in windows server? Airflow definitely out of question, right now I am splitted between Prefect, Dagster, or good ol windows scheduler to run the ingestion script, and probably also dbt in the future if possible.

Currently trying out Dagster, which works in windows for developmenr but not sure whether it is production-ready for windows environment.",5,17,1737075308.0,dataengineering
1i2wmbz,Guys I have a big data degree and I am overwhelmed with how much tools that I have or should Learn to be a data engineer,I know hadoop hive pyspark kafka java and python and some Bi tools like tableau on what should I focus to complete the data engineer profil and to be out of this damn loop of mental overwhelming ?,11,11,1737054543.0,dataengineering
1i36y5w,Should I pursue a master's degree or focus on building a portfolio to become a Data Engineer?,"Hi, I have a question. I'm currently a chemical engineer with 4 years of experience. In my work, I've used Power BI and Excel, and I really enjoy working with data. I’d like to transition into a data engineer role. Right now, I’m taking online courses to get certified as a Data Engineer with AWS, as well as learning Spark and databases in parallel. Do you think I should pursue a master’s degree to improve my chances of landing a data engineering job, or would it be better to focus on building a portfolio and start applying for different roles, given my engineering background?",4,22,1737083349.0,dataengineering
1i3414i,Dealing with data mapping from business?,"Hey guys, I've been in my first DE job for about half a year now. My team maintains pipelines to process bi-annual survey data for analysts to do modelling / analysis on. Each year the schema of the raw data tends to change a little, with new questions added, old questions removed, and some fields modified. Some of the fields also have specific logic or calculations involved that need to be replicated in code.

Twice a year, business (who write the surveys) provides new mappings of raw variables to silver fields, and then we spend time integrating the new survey into our pipelines.

The problem is that these mappings are provided in spreadsheets that are edited by business analysts. These spreadsheets sometimes have errors and are updated after the source it represents has been integrated, so we need to continually update older pipelines in line with updated spreadsheets, all the while integrating new data sources when new surveys are completed.

My question is... is there another way to handle this business workflow? I expect in 5 years time it's going to be a total mess, with at least 10+ spreadsheets capturing mappings of various sources being maintained manually by analysts, and the DE team just playing catch up. I want to move to a more programmatic workflow but I have no idea what to propose (I am the newest on the team, and aside from my boss, other team members don't really care that much and are happy with the status quo). Asking analysts to maintain a simple yaml or json per data source would be ideal, but then there are calculated fields and they can only qualitatively explain how they should be created (analysts in question don't know very much code).",3,0,1737074386.0,dataengineering
1i2wg2g,Palantir,"Any users here have experience using Palantir’s product ? 

Is it worth the investment ?

Would love to hear feedback! ",9,13,1737054102.0,dataengineering
1i2lc70,"Enhanced PySpark UDF Support in Sail 0.2.1 Release - Sail Is Built in Rust, 4x Faster Than Spark, and Has 94% Lower Costs",,44,16,1737020280.0,dataengineering
1i2ea4z,In over my head at work… I know nothing about data engineering,"Joined a shit show company run by a bunch of MBAs who are former bankers and consultants. I’m the only person coming in with practical experience and it’s on the more analytical side. Because of this, the company thinks I should build out the data warehouse. 

We run retail companies and it’s two Shopify stores. We need the basics like GA4, Shopify, klaviyo, and meta. What’s most cost effective way for me to do this with someone who has almost no programming experience? We need this data to feed reports. The company is interested in a tool that will let us query data into our spreadsheets and also write back to the warehouse. 

Please help I’m overwhelmed and don’t know what to do. I was without a job for for over six months and worried I’ll be laid off again because now I’m expected to be a data engineer when I’m a retail supply chain guy. ",140,76,1736993321.0,dataengineering
1i2t9c6,Best data warehousing options for a small company heavily using Jira ?,"I seek advice on a data warehousing solution that is not very complex to set or manage

Our IT department has a list of possible options :

* **PostgreSQL** 
* **Oracle**
* **SQL server instance**

other suggestions are welcome as well 

***Context:***

Our company uses Jira to: 

1-  Store and Manage Operational data and Business Data ( Metrics , KPIs , performance) 

2- Create visualizations and reports ( not as customizable as QLik or powerBI reports ) 

As data exponentially increased in the last 2 years Jira is not doing well in  RLS and valuable reports that contains data from other sources as well . 

We are planning to use a Datawarehouse to store data from Jira and other sources in the same layer and make reporting easier ( Qlik as Front End tool)",10,33,1737046069.0,dataengineering
1i35ii6,Main DE tool or plataform in 2025,"What is the main DE tool or platform you currently use to manage data? Feel free to justify, suggest others, or share which one you'd like to migrate to.

[View Poll](https://www.reddit.com/poll/1i35ii6)",1,3,1737078831.0,dataengineering
1i2uxvr,"My sample project to scrape simple craigslist data
",My sample project to scrape simple craigslist data - [https://www.youtube.com/watch?v=iGJoTAMNZpg](https://www.youtube.com/watch?v=iGJoTAMNZpg),6,1,1737050311.0,dataengineering
1i2lq6m,Which is the best data engineering course online from scratch ?,Which is the best data engineering course online from scratch ?,17,7,1737022035.0,dataengineering
1i3cw6z,NoSQL,Please share with me a real world scenario where NoSQL is better than SQL.,0,7,1737107219.0,dataengineering
1i3b6dn,AI Data Modeling Launched!🚀🚀 You can use it for elegant data modeling,"👋Hi there, I'm a Java programmer who has been working for many years, and I'm using a MacBookPro.

🚀I'm so happy and excited, this is my first launch as an indie developer!Let me tell you why I made this [product](https://ai-data-modeling.com)!

😞I'm very frustrated that MacOS doesn't have a good database modeling tool, which leads to me having to use a virtual machine to install Windows.

Whenever I get a new project and start working on the database design, I have a very hard time opening a virtual machine, opening PowerDesigner, and creating the physical data model. You know, working with Windows on a MacBook is torture, and PowerDesigner is a very old application with an ugly interface that is very awkward to use😫.

After years of working this way, I thought I should develop a good web program to solve my ordeal, why didn't I think of that years ago? At the same time as this, ChatGPT came out of nowhere, and I wondered if I could integrate AI into data modeling to speed it up, since the company often gets outsourced projects, and this would make developing outsourced projects a snap.

I started to conceptualize the idea from the end of 23, at first I was going to make a MacOS native application, then I considered that the web version is more practical and convenient, and changed to develop the web version, I am now using AI Data Modeling every day to help me to generate the database architecture, and one-click to export the SQL, which is really convenient, I recommend it to the developers, I hope to get your likes, and I hope that you will put forward your precious suggestions!

So, here's my product: [AI Data Modeling](https://ai-data-modeling.com)

🙏😄

I hope you will like it and give me your valuable suggestions!",0,0,1737099164.0,dataengineering
1i2okir,Understand a full ETL/ELTworkflow in real job,"Hi everyone,

I'm new here and just starting out in the world of data engineering, snowflake and ETL tools. I have basic knowledge of Snowflake and technical ETL concepts, but since I have limited experience in the field, I struggle to understand how a real-world workflow would look when working with tools like Snowflake, Airflow, Python, SQL, Spark, Alteryx, etc.

For example, in Snowflake, are Python scripts written within the platform that point to APIs and load data into Snowflake tables? Or are these external Python scripts executed on servers using Airflow periodically, which then send data to Snowflake for transformation? Why aren't these transformations done directly in the scripts using tools like Spark or Pandas?

I'm a bit confused about where the ETL (or ELT) process typically happens and what the most commonly used steps are in the industry today. What are the best practices? Many people recommend SQL and Python, but is this combination enough to handle all the necessary transformations?

Apologies for my ignorance, and thank you so much in advance for your insights!",7,1,1737033091.0,dataengineering
1i2lsj4,Accelerating Iceberg Analytics: How Apache Arrow Can Help get the best out of SIMD processing,,8,5,1737022328.0,dataengineering
1i2s98y,AWS Glue update?,"Hello All, We have a aws glue infrastructure set up in our company which stores schema definitions only in aws catalog tables and actual data gets stored in oracle database. 
Using aws glue we can perform read/write operations but still unable to perform update operations so far. Any tips or tricks to help achieve update functionality using glue is appreciated.",3,5,1737043528.0,dataengineering
1i2yo9u,Exploring Database Isolation Levels,,0,0,1737059786.0,dataengineering
1i2t4ty,Best option for streaming json,"Background: I have existing data pipeline to create a data lake from Kafka events which is fed out of CRUDs from Mongo. 

What is the best option to build a near real time data set with CDC, parse and flatten complex JSON into star schema.?

I have access to Databricks, Spark and data stored in S3. I have been doing custom ETLs which don’t scale well, hard to manage source contract changes.

Need recommendation for open source tools ",2,5,1737045748.0,dataengineering
1i2szie,Technical part - Bio Tech ,"Hello Guys!

 I just wanted to ask a few questions about Technical part for Data Engineer positions. I've done a few but I've never been able to find a pattern to 'study' IT. I did one that required me to take a homework assignment and others that were just occasional questions.

Can anyone who has also gone through these topics tell me what's normal for this type of position, more specifically in the health and bio-tech sectors? ",2,0,1737045370.0,dataengineering
1i2x71m,Requesting some guidance on which Cloud Service certification ,"I've been working as data analyst from past 7 months and gained some experience in dashboardinf and Scripting languages,I've been working with Google sheets/Excel,SQL for 4.5 years now . (I switched from QA to data analyst role) 

I want to move towards data engineering role. 
I'm hoping a certification in a cloud services would help in achieving this.

Could someone suggest which one would be a better choice? 
I'm confused between AWS and Azure.
I know AWS is the market lead, but few people suggested that Azure will take over as OpenAI is involved.

Please let me know if this path is the right way ahead or if I need a  different approach. 

I don't have any proper guidance and any suggestion would help a lot",0,2,1737056016.0,dataengineering
1i2wjci,Adding an AI agent to your data infrastructure in 2025,,0,1,1737054330.0,dataengineering
1i39iqr,Job Stacking,We see ALOT of engineers in tech / data starting to job stackHow do you think this is going to go?,0,4,1737092180.0,dataengineering
1i2k7rc,"Stakeholders question my teams estimate for delivery timelines, and demand we follow theirs. How do you deal with this?","Our business side stakeholders are questioning the delivery timelines we estimated based on the requirements given. 

More over the requirements are pretty complicated, stuff like:
- autoamtically  pull data from various upstream sources based on when the data becomes available in the upstream (we also have to coordinate with the system owner to get permissions to access all this data)
- feed them to some prediction models, and generate resulted
- write back this outputs to the upstream system 
- have an user interface for the users to edits, save and use inputs for this models, for them to do ad hoc prediction runs 
- the models are in sas and some half configured python code, which we need to standarise and make functional, to be able to take those user input

These are just the high-level requirements, the details keeps getting revield as we progress in implementation. 

But they demand that it all has to be done in 3 months. And refuse to accept it's going to take longer. 

I am one of the senior guys in the team, so the PM keeps asking me how we can accommodate these requests. I have already told him and the stakeholders what is possible in 3 months, but they keep fighting back.

Sorry just wanted a place to vent, but also to ask, how would you deal with this kind of situation?",6,6,1737015011.0,dataengineering
1i245j0,What's the worst thing about being a data engineer? ,Title,73,119,1736965817.0,dataengineering
1i2oifa,Need help on access mode for clusters,"Currently I am creating a cluster using api call at the very start of the pipeline execution. 

When I create a cluster using Access Mode: SHARED, I am not able to unzip the zip file due to some limitations on the storage explorers.

When I create a cluster using no specified Access Mode, the default is CUSTOM through which I’m able to unzip the file but not able to access Unity Catalog🥲.

Can you please suggest what can be fixed so that I can access both, i.e The File Storage(ADLS) where I can have access to the DBFS root and can unzip my files. Simultaneously, I can use Unity Catalog as well.


Thanks in advance.",2,0,1737032910.0,dataengineering
1i2d1ny,Data career path dilemma: tech skill cert vs CDMP cert,"Hi friends, i am at a crossroad here.

I work at the ‘client’ side, means that it is non consulting and I am building domain knowledge of the industry I am working on.

On one end I am tempted to go with tech cert like databricks/snowflake/fabric/redshift as seems like job postings are looking for them

The cons is that I always felt like this becomes a never-ending race as the next new shiny tool being introduced to the market

On the other hand I am also considering CDMP path (Certified Data Management Professional) where it focuses more on the data management and governance side of things (DMBOK)

A quick check on job postings in my area and only 2 jobs (a government and a very large company) lookign for CDMP.

Any thoughts on which path I should be taking?

CDMP that covers DMBOK:



",18,14,1736989578.0,dataengineering
1i300ic,Curious about how do you even find unpaid Data engineer internship,can't find any online but need experience desperately ,0,3,1737063275.0,dataengineering
1i2aa4v,Tobiko Acquires Quary,,19,9,1736981698.0,dataengineering
1i2fsfw,Seeking a Mentor,"I have 9 YOE, 3 of those in data with my current company. A little over 2 years of that was as a senior data analyst. I started out building reports in Power BI. Towards the end of my first year, I became more interested in SQL and taught myself how to write queries to optimize how we transformed the data before ingesting it into reports. I created new tables, views, stored procedures, etc. After some time, I worked myself into being the admin for our team's SQL server and focused some time learning more about managing the server's performance. As our team's reporting umbrella grew, we needed more data so I taught myself Python to create basic ETL pipelines of data from internal systems to our SQL server.

Several months ago, our team began a project to migrate the SQL server to Snowflake and use Informatica for ETL. I was able to attend the demos for different systems and provide feedback, but Snowflake and Informatica were ultimately chosen by our org's leadership. I immediately pivoted to learning both new systems from scratch. I've enjoyed Snowflake. Informatica has made some processes easier, but has its pain points. During the early stage of the project, I earned a promotion to Data Engineer with the intent that I would be the lead between myself and another engineer to manage Snowflake and Informatica. Due to the quick timeline of the project, a consulting team was brought on to manage the bulk of the transition and provide some hands-on training to myself and the other engineer.

I feel very thankful for the opportunity I've had to grow in my roles and work for a great manager that supports me and my growth. I'm concerned though that I'm stuck only having experience working on certain systems. I've seen posts where people say to focus on having transferable skills, like coding in Python versus knowledge on certain systems. I'm struggling with figuring out what to focus on next. Anyone willing to mentor or provide advice is greatly appreciated.",7,12,1736997963.0,dataengineering
1i2vl4w,Are there any PhDs in Data Engineering space?,I have seen some professors and universities have been doing research in data space specifically driven towards some data science stuff. I’m just curious if there are any professors or universities doing data engineering research? ,0,12,1737051932.0,dataengineering
1i21ouy, Genuinely curious regarding atlassian,Been seeing their job postings for the longest time. Why do they keep hiring DEs? Is it because they cant find suitable candidates or their attrition is high? Or maybe they just really have a lot of demands that involves data engineering. I’m just curious lol. ,16,6,1736959607.0,dataengineering
1i1ymyy,Data Lakehouse: What Really Happens in the Silver Zone? Survey!,"**Survey:**

[https://forms.gle/2QfYWHQ9imsd5KYu9](https://forms.gle/2QfYWHQ9imsd5KYu9)

After reading countless whitepapers, blogs, and comparing notes from providers, one thing is certain: it’s not clear what should really happen in the Silver Zone. I’m running a survey to uncover how teams really design and implement this zone. Results will be shared in a post to shed light on real-world practices—and maybe clear up some confusion.

The **Silver Layer** in a Datalakehouse architecture is often described at a high level as the place where data undergoes **“just enough” cleaning and transformation** to create a **unified, enterprise-wide view of core business entities**. However, in practice, its design and implementation can vary significantly across organizations.

This raises critical technical questions:

* **What does “just enough” cleaning and transformation actually mean in practice?**
* **What data operations are being performed in this layer (e.g., merging, appending)?**
* **How are schema changes handled, such as standardizing data types and naming conventions?**",20,17,1736951511.0,dataengineering
1i1sivd,Quick Tip for System Design in Data Engineering,"https://preview.redd.it/fq88lj1744de1.png?width=912&format=png&auto=webp&s=bddc346a32af30e2b7a8ea2cfdb13a5b88943555

Above Architecture will cover almost 75% of the ELT pipeline requirements.

  
So next time when you get a question to design a pipeline to design a real time pipeline then you can suggest above architecture.

PS: There are pros and cons of this system too.",59,41,1736927312.0,dataengineering
1i292yy,[Project] Tracking Orcas — Harnessing the Power of LLMs and Data Engineering,"Worked on a small project over the weekend. 

Orcas are one of my favorite animals, and there isn't much whale sighting information available online, except from dedicated whale sighting enthusiasts who report them. This reported data is unstructured and it's challenging to structure for further analysis. I tried implementing a mechanism using LLMs to process this unstructured data, which I have integrated into a data pipeline.

[Architecture](https://preview.redd.it/sg0zmndac8de1.png?width=3232&format=png&auto=webp&s=61f26b2356d4786551e6b5e9c5b0c431c574c36e)



Read more: [Medium article](https://solo11.medium.com/tracking-orcas-harnessing-the-power-of-llms-and-data-engineering-bcf0132bacc6)

Github: [https://github.com/solo11/Orca-Tracking](https://github.com/solo11/Orca-Tracking)

Tableau: [Dashboard](https://public.tableau.com/app/profile/solomon8607/viz/OrcaSightings/Dashboard1)

Any suggestions/questions let me know!",4,1,1736978524.0,dataengineering
1i25a5d,"Best resources (ideally videos, but books are fine) to teach an infant DE team how to make more robust pipelines and understand software engineering principles?","I've been on a DS/DE team for several months now. We are more so analytics engineers than true data engineers, however I'd like to help the team upskill and I know many would be interested in doing so. Most people on the team come from analytics rather than SWE and so our pipelines get the job done but are not super robust. They fail more often than they should. What resources would be best to teach people how to: 

* Make more robust pipelines (scalability, fault tolerance, quality checks, etc.)
* Learn software engineering principles (Modularity, Maintainability, Performance Optimization, etc.)

Most courses I've seen online seem to focus on just understanding the tools and building pipelines rather than making them robust. Any suggested resources to upskill?",6,2,1736968641.0,dataengineering
1i2h34u,Are Surface Pro Books/Laptops good for Data Engineering?,"Due for a new laptop at work (wont' have to pay anything but have to get it approved). I really want something more portable that can still handle my workload. Most of the data isn't too bad but still a good amount of work that is resource intensive. Mainly using SSMS, Tableau, Azure DS, Microsoft Suite, VS Code. 

Any suggestions? Lenovo Thinkpads T/P Series and MacBooks are also recommended a lot, so looking into those as well!

Bonus question: 

If Thinkpad, which version? 

And if MacBook Pro, which year/model? I think I want 14"" for the portability. M3 Pro or Max would be great but might be a tough sell. Would an older version like M1 or M2 be sufficient? 

Thanks appreciate any advice, meeting with my team at work soon so trying to research as much as I can!",0,15,1737002290.0,dataengineering
1i2fuf5,"Job Search - Confused About ""Data Engineer"" Jobs","Hi everyone,  
I have about 3 years of experience at my current company as a ""data engineer.""  I build streaming and batch spark jobs end to end using pyspark and scala and use airflow to orchestrate them.  Recently I've been getting the itch to move to a different company because things have somewhat stagnated here.  No new products to build, just monitoring existing jobs.

So I started looking for new jobs and have been applying to some ""data engineer"" positions.  What I've quickly found out after speaking to hiring managers is.... Am I not a data engineer? They seem to only want people with dashboards and SQL exp. They tell me there is no code, it's all just templates and queries.

So what am I.... A software engineer or a data engineer? I want to continue working with large sets of data. Preferably in spark but am fine using things like dataproc, etc. Is there a role I should be looking for? I look at key skills and they all look like things I'm interested in working on: python, java, scala, spark, kafka, airflow.... Then I speak to the hiring manager and they tell me they don't actually code anymore they just write queries and maybe change some DAGs. Which doesn't seem like what I want.

Edit: I rarely just use just sql in my job. most of my work is dataframe operations with some spark sql thrown in. Should I just change my title to ""Software Engineer - Data Platform"" or would that get me in trouble? Some of these ""Data Engineer"" roles pay significantly less than their software engineer roles at the same companies.",0,8,1736998136.0,dataengineering
1i2dkgc,Lambda architecture ,"So i read multiple blogs on lambda architecture, i want to know if its used frequently in entreprises, if yes why?
",0,2,1736991152.0,dataengineering
1i27oer,Multiple cloud providers,"What are some companies that use multiple cloud providers? Specifically, that has a big split (aws, gcp, azure 33% each for ex) and not just one cloud provider for 3% of the things they do",2,1,1736974871.0,dataengineering
1i1t2a7,Best Courses for AI Engineering and DevOps to survive?,"Hi guys,

Existential dread is setting in as we march toward a future where many IT jobs might be taken over by some AI guy named Bob. (I hate you, Bob.) But instead of sulking, I want to upskill and stay ahead of the curve.

I’m looking for high-value courses in **AI Engineering** and **AI DevOps**—preferably something with awesome content.

Specifically, I want to focus on skills that’ll stay relevant for a while:

* **RAG (Retrieval-Augmented Generation) Engineering**
* **Agentic AI and Vertical AI development**
* **Fine-tuning LLMs (e.g., using QLoRA)**
* **DevOps and Integration with Legacy Systems**

If you’ve taken a course or know of any gems that hit these areas, I’d love to hear your recommendations! ",17,11,1736929865.0,dataengineering
1i2062i,"Biotech pros, dive into our Apache NiFi demo for big-scale data automation.","We created a demo video in how Apache NiFi can be used. The video doesn't explicitly show data or workflows specifically pertaining to biotech, but it does show NiFi functionality. 



Reason for this post, is I'm looking to see if other biotech business are running into data ingestion limitations and need solutions at scale for ingestion.

Sharing below is our case studies, and the video link to the demo. I would love to get feedback as to the effectivness this solution is for biotech businesses.

Case Studies: [https://dasnuve.com/case-studies](https://dasnuve.com/case-studies)

NiFi Workflows Demo:  [https://videoshare.dasnuve.com/video/nifi-workflows-demo](https://videoshare.dasnuve.com/video/nifi-workflows-demo)",6,2,1736955638.0,dataengineering
1i21uxq,Mysql to Kafka via Debezium,"I want to extract cdc data from MySQL to Clickhouse, below is the process:

* docker compose deployment, single server
* mysql -> debezium container (kafka connect) (mysql-connector)
* debezium container -> kafka container
* kafka container -> clickhouse sink container
* clickhouse sink container -> clickhouse

If I had the following:

* 5 large tables (1 terabyte)
* 20 medium tables (50gb)
* 25 small tables (1gb)
* 300gb available RAM to use



Right now I'm debating between inital and incremental snapshot modes. 

What other settings can I tune to effectively use as much of 300gb RAM as possible, to reduce sync time?

",3,4,1736960045.0,dataengineering
1i20eg9,Data synchronization with no CDC or tx logs access.,"Hi all,

I will soon be working on a consultancy where I am tasked with syncing data from a database (sql server) into a data lake (big query). There are some pretty tight restrictions on the database  where I will only have read / select \* access, and there is no ability to enable features like CDC or transaction logs.  I would ideally like to do an initial full sync of data, and then only incremental syncs on new records / updated records without having to truncate and reimport entire tables as some of the tables we need to sync could be quite large.

I know there is fivetran's 'teleport sync' which offers the ability to sync data under these limitations, but ideally we want something which we can manage / host ourselves to limit costs, as fivetran can become quite costly.

I am currently considering something like airbyte with a custom source connector where we can use computed hashes on record batches stored in the destination to determine if we need to sync a batch of records after the initial load. But hoping to hear from others if there is a better way which might be more maintainable by the customer in the long run, or if there are any out of the box solutions which are OSS / low cost.

Thanks in advance!",3,14,1736956248.0,dataengineering
1i22drb,Database DevOps survey (<10min): Five chances to win $100 for submitting your responses!,"Hello to our friends in r/dataengineering – the database DevOps community eagerly seeks your input on the state, needs, and opportunities of database change management workflows in 2025. 

If you’re on a developer, database, DevOps, platform, or data team, we want to hear from you! Your participation helps make modern pipelines faster, easier, safer, and better integrated.

We’re also giving away five, $100 gift cards (or charitable donations) to survey respondents. Plus, you’ll get early access to the report containing the survey’s findings and perspectives from industry experts. 

Submit your responses by February 7, 2025, and help shape database workflows that support modern opportunities and challenges like:

* Cloud ecosystems
* Platform engineering
* AI/ML workloads
* Security and compliance

Take the 2025 Database DevOps Adoption & Innovation Survey: [https://hubs.li/Q0324Mk40](https://hubs.li/Q0324Mk40) 

https://preview.redd.it/5iqu7tmqx6de1.png?width=1080&format=png&auto=webp&s=7cc647e1dd7f18f5b0a88ffe0aa3d24fd48f6d36

",3,1,1736961339.0,dataengineering
1i1z09f,COveR - Clustering with Overlap in R,This is a R library work on in the past that include a set of clustering algorithm with overlapping class and intervals data. Hope it can helps some people,3,1,1736952570.0,dataengineering
1i1whcw,Evolving Data Models: Backbone of Rich User Experiences (UX) for Data Citizens,,4,0,1736944744.0,dataengineering
1i1vipi,Go to full text search from a data lake,"Hey folks, 
I've been asked to provide search capabilities over our data lake house. We currently store data in iceberg tables and have obt type tables in the gold layer that I've been requested to add search capabilities for...

My thoughts are just pipe the obt into elastic and index it and that's search done but was wondering if theres a better way to go about it?",6,4,1736941039.0,dataengineering
1i17bqf,Python for Data Engineers: Key topics & techniques 👇,,184,41,1736865119.0,dataengineering
1i22xzd,"""Let’s Talk About Data Observability and Rakuten SixthSense: Thoughts?","
Curious to know if anyone here has experience with Rakuten SixthSense or similar Data Observability platforms?

For context: Data observability seems to be one of the most critical yet under-discussed aspects of modern data infrastructure. With companies handling millions of events per second, ensuring data integrity, real-time anomaly detection, and automated alerts is essential.

I came across Rakuten SixthSense, which focuses on end-to-end observability across data pipelines, APIs, and infrastructure. It positions itself as a platform that helps businesses identify, debug, and resolve data quality issues in real time.

Some questions I’m exploring:

How crucial is data observability in your current organization?

Are platforms like Rakuten SixthSense overkill for smaller companies?

How does it compare to other solutions like Monte Carlo, Databand, or Bigeye?


Would love to hear insights from people who’ve implemented data observability frameworks. What challenges did you face, and how did you measure success?

Let’s spark a discussion around data observability, its importance, and tools like Rakuten SixthSense!


",0,2,1736962767.0,dataengineering
1i17fyc,dbt Labs acquires SDF Labs,What are your thoughts on this acquisition?,140,27,1736865450.0,dataengineering
1i1j7ow,Fact table with 2 levels of grain,"I have a fact table called fact_bills that stores bill details of items purchased. Each row is an item for a specific bill. This works well for my current use case. 

I was tasked with adding a department dim to the fact table but it messes with the grain. Items can be billed to multiple departments. For example, a company buys 10 laptops but 5 are for engineering and 5 are for finance. There would be 1 row in fact_bill for the 10 laptops, and 2 rows in a different table-one for engineering and one for finance. If I add the department dim, then each bill item’s attributes are repeated for N departments. 

Some use cases include counting number of billed items. Some include department specific filtering. Obviously adding department dim complicates this. We could use count distinct, but I’m wondering if there is a better approach here?",22,23,1736895932.0,dataengineering
1i1ubv7,DBA upskill to Data Engineer?,"I am a MySQL DBA with almost 15yrs experience.I also have experience in AWS with SAA certification.
Nowadays specialized DBA jobs are getting rare and whatever DBA jobs there are, they are MSSQL, Oracle or PostreSQL.
So I am trying to evaluate whether it would be beneficial to learn other RDBMS or learn Data Engineering?",4,2,1736935838.0,dataengineering
1i1oa6j,Setting up a simple CDC pipeline using open source tooling,"Hello!

For a personal project, I am trying to set up a CDC (change data capture) pipeline to pull data from a MariaDB server located in one VPC to a PostgreSQL server in another VPC. My database is only 2 GB in size and a few minutes of latency is acceptable during replication. 

What open source tools/solutions do you recommend I use to set up such a pipeline that's resource light, maintainable, debuggable and will not result in data loss? 

I am not a data engineer, rather an analyst who can manage low code/config tools. Through Google search and Claude prompting, I have come across the following options and I need help evaluating these or finding out about other possible options:

1. Airbyte - Possibly the easiest but seems very resource intensive. Also needs docker desktop so I don't think I can run it in a small VM.

2. Debezium - Seems overly complicated for my use case, no?

3. Meltano - CLI/conflig based, seems lightweight but I couldn't find detailed instructions for setting up CDC.

  
Are there other options that I have not found and may be better suited to my use case?",11,12,1736911023.0,dataengineering
1i1vcyw,Suggestions on best tools for data,"I am building pipelines and I often need various kinds of data in huge volume to validate the performance of the pipeline. I see there is one tool called mockaroo but it needs paid subscription if needed beyond 1000 rows. Any other suggestions that can give huge data in json and csv format. 
",2,6,1736940367.0,dataengineering
1i1t9hs,"MSc Data Science here, working as Project Manager and having huge career crisis. Should i switch to DE?","I know there are many posts about these, but i'd like to put my own perspective into this.

I've a little more than 1.5 years of experience, really early in my career but i've been struggling a lot. 

I love working with data but i absolutely SUCK at the delivery part. I have this job where i present project status to CEO every monday and i rush all these plots, graphs and w/e they want, then i struggle in choosing the right way to present the data; my storytelling is terrible. I'm also very insecure so even if i've done the work to be confident in my process, i've been unable to deliver that confidence in my presentations. ADHD won't help in the project managing part too. 

My keypoints are that, in this precise moment, i lack the soft skills to handle business presentations. What troubles me is that (having no real-world experience) i think that presenting data is absolutely crucial for success as a Data Scientist (which is where i'm aiming my career for) even if it's a little more technical and a little less ""business oriented"" than Data Analysts. 

So, the self-sabotage part of me tells me that i just suck working with data, however, maybe there is an opportunity for me in other roles. I'd like to use my MSc Data Science somehow but maybe if i land the DS job, i'll struggle with the same thing i'm having right now.

Could DE be the career for me? Maybe ML Engineer could be an option too?

  
",4,9,1736930841.0,dataengineering
1i27taf,Struggling with Keeping Database Environments in Sync? Here’s My Proven Fix,,0,13,1736975219.0,dataengineering
1i1u04t,Passing column data with regular ingest,"Hi everyone,

Just opening up a post for a discussion on what your opinions might be

\-- Situation:

Let's say that we are simply ingesting flat files from a source, TSVs

And let's say that those files typically are facts, and the TSV file names reflect when the TSV was generated (\`15012025\_sales.tsv') for example

This works fine and well, ingesting it is no problem--I feel like this is a very easy situation

  
What I was wondering would be, what might the practices be to take into account the data types of the columns though, pre-ingestion?

I was simply thinking to run a \`DESCRIBE\` on each of the tables we take facts from, output them as their own TSVs, that would then be used to help build out downstream logic like explicit casting, maybe even tracking the schema drift

  
So now, instead of just \`15012025\_sales.tsv\`, it is partnered with an accompanying \`15012025\_sales\_metadata.tsv\` (perhaps there might be a better name) that includes the column names and their data types

\-- My question is:

1. Why dont we see this practice more often? Sitting down on it, it makes quite a lot of sense for me, but maybe I'm missing something (or maybe it is common, just not in the places I've worked at)

I, for sure, didn't invent this but maybe people can help me with some further reading or some opinions that you might have implementing something similar before implementing this myself?",1,1,1736934360.0,dataengineering
1i187ob,Who should own the data transformation/modeling layer (data platform team or the analytics team)?,"I am a part of the data platform team and the team is responsible for creating and maintaining data pipelines (CDC pipelines + APIs which other applications use to publish events + some ad hoc data pipelines). The data (from all sources) is dumped into tables on data platform with almost no transformations i.e. there is a 1-1 mapping between the tables in the source database (in case of CDC pipelines) and the table on the data platform (e.g., a table on the data platform replicated from a MongoDB collection has the exact same schema plus some additional metadata columns injected by the data pipeline). The same goes for data coming from the event tables (there is minimal/no transformations). We make sure that the data is partitioned correctly, regular compaction is run on the data to avoid the small file problem etc. We don't deal with any business logic or analytical modeling and focus solely on the engineering. In an ideal scenario, the application teams should collaborate with the analysts (and the data platform probably) and think of metrics instrumentation in advance but most of the times, they do not have the bandwidth and the end result is that we have 100-200 tables created from tables on application DBs (MongoDB, MySQL etc). I have observed that in my company, analytics is an afterthought.

There is a team of analysts in my company who are responsible for producing dashboards, metrics on top of the data on the data platform and reporting numbers to stakeholders. The analysts are the end consumers of data platform, they use it the most and they write the queries mostly. For ad-hoc queries, there are a few BI tools that they use. To enable the analysts to schedule queries, the data platform has provided them with a platform where they can submit the queries, specify a schedule and their queries will be materialized to tables. As of now, they are responsible for gathering context on the business logic from the application teams and writing the queries.

A few problems arise from the above setup: -

1. Duplicate queries. Different analysts sometimes end up writing multiple queries to answer the same business questions. Although we have provided them with a tool to document the queries and their columns, it doesn't seem to be helping. There are about 300-400 scheduled queries running everyday on our system and the analysts don't take ownership of the web of inter-dependant tables they have created, hence there is no cleanup initiative from their end (which is understandable, as they don't always have the bandwidth to care about it, they are just focussed on getting the numbers).
2. Inefficient queries. Analysts (at least in our company) are unaware/have limited knowledge of how to write efficient queries (e.g., use partitions in queries), add filter in subqueries early on so that less data is scanned etc. As a result, they end up scanning 100s of GBs of data sometimes. Limited knowledge about how the underlying query engine works is one of the reasons for bad queries. Another reason is that we have not given them the option to create incremental tables, as a result, they end up scanning the same data (with an additional time delta) almost everyday (we are working on providing that option to the analysts). In my opinion, the problem of writing inefficient querying is solvable to some extent with good documentation on the best practices to query and sessions to educate them on how to write queries.

Our company is planning to improve the platform so that the analysts can get better feedback on the quality of their queries (data scanned, detecting early on if the query is bad), introducing true incremental queries etc so that the queries are more efficient. This solves 2 but doesn't solve for 1 above. We are also thinking about analyzing the scheduled queries on a regular basis to identify query patterns but we are a long way from actually implementing the solution.

In the midst of all this, I came across this article, [Databases in 2024: A Year in Review](https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html), which mentions the following: -

\> Most OLAP queries do not access that much data. Fivetran analyzed traces from Snowflake and Redshift and showed that the [median amount of data scanned by queries is only 100 MB](https://www.fivetran.com/blog/how-do-people-use-snowflake-and-redshift). Such a small amount of data means a single DuckDB instance is enough for most to handle most queries.

This made me realize that we are doing something wrong and led me to the conclusion that a transformation layer is missing from our company. To create that modeling layer, we need people who can efficiently populate the data models and also have business context. This will serve as a single source of truth plus the data models should (ideally) be easily queryable by the analysts (ideally less data scan, faster queries). But then the responsibility of data modeling will shift to the data platform and I think my company is not aligned with this idea (I am trying to bring up this point with the upper management and get their opinion on this).

So my question is who should shoulder the responsibility of data modeling? Should it be the data platform team or the analysts? How is it done in your company? Is the conclusion that we are missing a transformation layer correct or is there any other reason I am missing?

TL;DR: There are two teams in my company that interact with the data platform. The data platform team which brings data to the data platform and the analysts who form the majority of consumers of data platform. Currently, the analysts are responsible for data modeling (they schedule the queries whose results they require daily to produce reports/dashboards/metrics) but the queries are not efficient and there are duplicate data models. Should data platform own the transformation/modeling layer?

Thanks for reading.",30,25,1736867596.0,dataengineering
1i1i453,Completed DP-203,"I just passed the DP-203 exam for Microsoft certified Associate Data Engineer . I have studied for it for a month. I was absolutely new to cloud when I started it, watched videos of Tybul who explained all the services and concepts very clearly however later I found out those 54 1 hour long videos were not enough to pass the test. So I solved a lots of practice questions available online. I was easily able to pass the test with all the material
",5,4,1736893049.0,dataengineering
1i1iogg,"Just finished building a job scraper using Selenium and mongoDB. It automatically scrapes job listings from Indeed at regular intervals and sends reports (e.g., how many new jobs are found) directly to Telegram.",,5,2,1736894506.0,dataengineering
1i1hygv,Is anyone using pyAirbyte?,PyAirbyte seems like the most useful tool to come out of the whole airbyte endeavor.  Unfortunately there's virtually no documentation and i can't figure out how to configure my sources and destinations.  Does anyone have experience with using PyAirbyte?  How did you figure out how to setup your connections/destinations?,5,3,1736892637.0,dataengineering
1i0yvo3,Rise of Data Engineers,"Is it just me or everyone is seeing the trend that companies have been asking data engineering+ml engineering for the position of data scientist?

So my last client was hiring 3 ml engineer+data engineer combination and then hired 1 lead data scientist. His idea was that Lead DS guy will be designing and provide the solutioning. While as DE+ML team will be implementing the entire project(coding part). He did it because this helped him implement projects faster as DE knew the data and they already knew ML part too.

So are you guys seeing that trend too?",123,32,1736831927.0,dataengineering
1i19t2u,Do you guys think cloud networking should be part of a data engineers skill set?,"There's usually been a separate team handling cloud infrastructure where I've worked, but it's always caused problems with connectivity in my pipelines involving white/black listing, firewalls, subnets, VPNs, etc. Do you all feel like networking is something data engineers need to have a strong knowledge of, or is it too much additional complexity or too much overhead and requires a dedicated position or team?",13,23,1736871812.0,dataengineering
1i1eche,Is freeCodeCamp good for learning Data Science/Engineering? Looking for additional free resources,"I'm planning to start my data science journey through freeCodeCamp, specifically focusing on Data Analysis and Data Engineering. Would love to hear from people who've used their curriculum:

1. How was your experience with freeCodeCamp's data science track?
2. Did you land a job after completing it?
3. What supplementary resources did you use alongside i",8,7,1736883219.0,dataengineering
1i1i8b7,Certification to make up for no internships,"Hi everyone, I am a recent graduate. I completed my bachelors from a mid tier university this semester. Regrettably, I do not have any internships. I am trying to get some relevant certifications to make myself more marketable. I am also making projects after finishing certification for a certain tech stack. What can you recommend me to work on to land a data engineer job ? ",5,3,1736893347.0,dataengineering
1i19kqh,Fuzz Matching Bulk Wine Data,"Hi All,

My background is in Computer Science with a short stint as a data engineer, but I currently work at a wine storage provider/retailer. I am currently working on getting the inventory management on the retail side into the 21st century so that I can tie it into the warehouse management system on the storage side and start to generate better analytics for customer preferences. 

My current predicament is that our item naming and classification system is both inconsistent and also not inline with industry standard. The best system as of now is called LWIN which can be found here: https://www.liv-ex.com/wwd/lwin/. The LWIN ""database"" is open source and downloadable as a gigantic Excel file. My current plan is to pull LWIN into an actual database and to automatically pull and update the LWIN table nightly when we are closed.

This leads me into my main question, which is how can I best match up our existing item data to the items in the LWIN database so that I can then append things like inventory levels and internal pricing data to the items? The goal is to add functionality to our Shopify store to automatically pull the naming conventions from LWIN on item creation and automatically populate most of the data automatically with the only remaining fields being things that are unique to us, i.e. pricing, vintage, size. 

An example of an item in LWIN:


LWIN | DISPLAY_NAME | PRODUCER_TITLE | PRODUCER_NAME | WINE | COUNTRY | REGION | SUB_REGION | SITE | PARCEL | COLOUR | TYPE | SUB_TYPE | DESIGNATION | CLASSIFICATION
---|---|----|----|----|----|----|----|----|----|----|----|----|----|----|----
1077904 | Jean-Marc Pillot, Chassagne-Montrachet Premier Cru, Les Macherelles Blanc | NA | Jean-Marc Pillot | Blanc | France | Burgundy | Chassagne-Montrachet | Les Macherelles | NA | White | Wine | Still | AOP | Premier Cru


And then the same wine as we currently have it store in our inventory system:


id | descr | group | department | sub_department 
---|---|----|----|----
17113 | Chassagne-Montrachet, 1er Cru Macherelles, Jean-Marc Pillot | White Wine | France White Wine | Burgundy Saint-Aubin

As you can see we have a close enough match, but like the example I gave, many of the items have incorrect or missing sub_regions. 

How would I best go about finding a match where at least most of the process is automated? I am happy to manually confirm the match from a short list of items from LWIN, but I would love to not have to manually match 17,000+ items in their entirety.

Thanks in advance for any help, this is going to be an interesting project, I just don't want to waste a million years on it.",13,7,1736871217.0,dataengineering
1i1cbwp,How is the job market for data engineering?,"So I have about 1 - 2 yoe of experience in software engineering. However, I haven't done anything at all computer science related and looking for a new field to go into. I have a good amount as in months that i can devote to learn something. The thing is there's no point in working hard for the market to be bad. Also, know that you can't predict the market so I'm just for your thoughts of how the market for data engineering is and what you think it be like in months/years.",3,31,1736878169.0,dataengineering
1i0v8c1,Would you guys quit over a full time RTO call?,"I started working for a new place recently. The agreement, which conveniently wasn’t in my offer letter, was that I’d get a schedule of 3days/2days in/out of office. Pending two months, I’d get upgraded to a 2/3 in/out schedule.

We also just recently migrated from CRM ABC to CRM XYZ, and it’s caused a lot of trouble. The dev team has been working long hours around the clock to put out those fires. The fires have yet to be extinguished after a few weeks. Not that there hasn’t been progress, just that there’s been a lot of fires. A fire gets put out, a new one pops up.

More recently, a nontechnical middle manager advised a director that the issue belongs with poor communication. Since then, the director called a full time RTO. He wants everyone in house to solve this lack-of-communication, “until further notice.”

Now, maybe some of you are wondering why this affects the data engineer? After all, I am not developing their products… I am doing BI related stuff to help the analysts work effectively with data. So why am I here? It’s because they want my help putting out the fires.

Part of me thinks that this could be a temporary, circumstantial issue—I shouldn’t let it get to me.

But there’s another part of me that thinks this is complete bullshit. There isn’t a project manager / scrum master with technical knowledge anywhere in the organization. Our products are manifestations of ideas passed onto developers and developers getting to work. No thorough planning, nobody connecting all the dots first, none of that. So, how the fuck is sticking your little fingers into my daily regime—saying I need to come in daily—supposed to solve that problem?

Communication issues don’t get solved by brute forcing a product managers limited ability to manage a project like a scrum master. Communication issues are solved by hiring someone who speaks the right language. I think it’s royally fucked up that the business fundamentally decided that rather than pay for a proper catalyst of business to technical communication, they’ll instead let their developers pay that cost with their livelihood. 

I know that, in business, you ought to best separate your emotional and logical responses. For example, if I don’t like this change, I’d best just find a new job and try hard not to burn any bridges on my way out. It’s just frustrating, and I guess I’m just venting. These guys are going to loose talent and it’s going to be a pain in the ass getting talent back, all because of the inability of upper management to adequately prepare a team with the resources it needs and instead allowing their shortsightedness to be compensated with my regime. Fuck that.

My wife carpools with colleagues whenever I need to go into the office. My kids stay longer at after school. I loose nearly two hours in commute. Nobody gives a shit about my wife, my kids, nor myself though. I guess it’s only my problem until I decide it isn’t anymore, and find a new job.",79,70,1736820193.0,dataengineering
1i1141c,"Python vs Pyspark vs Snowpark
","Hello Experts,

Want to understand, how different are these technologies from each other? What basic level certifications should be targeted to get started?

Actually recently many team members moved to modern data engineering role where our organization uses snowflake and pyspark as key technology. Not having background of python but many of the folks have extensive coding skills in sql and plsql programming. Currently our organization wants to get certified in pyspark and snowflake. So want to understand which certification in pyspark should be attempted? 

Any documentation or books which will help to get started in quick time? Or it would be difficult for the folks to switch to these techstacks from pure sql/plsql background?

",26,6,1736841204.0,dataengineering
1i1fo6n,Struggling to Connect Trino to ADLS Gen2 — Need Guidance!,"Hi everyone,

I’m trying to set up Trino to connect with Azure Data Lake Storage Gen2 (ADLS Gen2) for querying Parquet files. While I’ve made some progress, I’m stuck on a few key points and would appreciate any advice from the community.

What I’ve Done So Far:

	1.	Installed Trino: Running Trino via Docker.

	2.	Configured Catalog: Created hive.properties for the Hive connector with the following config:

connector.name=hive
hive.metastore.uri=thrift://localhost:9083  # Using or trying to avoid Hive Metastore?

ADLS Gen2 Config
fs.native-azure.enabled=true
azure.auth-type=ACCESS_KEY
azure.access-key=sas token
azure.endpoint=core.windows.net


	3.	ADLS Path: Trying to query directly like:

SELECT * FROM hive.default.""abfs://my-container@storage-account.dfs.core.windows.net/path/to/parquet-file"";



Key Questions:
	1.	Do I Need a Hive Metastore?
	•	Can Trino query ADLS Gen2 files directly without a Hive Metastore?
	•	If a Hive Metastore is mandatory, is there a lightweight alternative, or do I need the full setup?
	2.	Connection Issues:
	•	With the above setup, I’m getting errors like “Could not read table schema” or connection failures to hive meta store
	•	Is my fs.native-azure.enabled=true config correct, or do I need additional settings?
	3.	Best Practices for ADLS Gen2:
	•	Should I consider using Iceberg or Delta Lake instead of Hive for managing metadata?
	•	If anyone has successfully queried ADLS Gen2 from Trino, could you share your configuration?",2,1,1736886531.0,dataengineering
1i1fgnu,Kafka Transactions Explained (Twice!),,2,0,1736886001.0,dataengineering
1i0uaou,FAANG Job Opportunity - Feels Weird?,"Need some opinions on a situation I find myself in...

I'm a DE with about 3-4 years experience, mostly at a start-up where I was more of an ""analytics engineer"" by function, but held a Senior DE title. Back in September, I had started a new job as a DE at a different startup, much more technical place where I'd be doing true DE work. At that same time...I was offered an IC4 role at Meta. I was pretty shocked honestly, even more so when they pushed so aggressively to bring me onboard, as I don't think I'm all that well-versed in the DE space. I ended up turning them down, as the role I had just started was remote and moving to NYC was too daunting.

Last week, I was laid off from my job at the new start-up -- they said it came down to ""fit"". I had been trying so hard, but was struggling without any guidance, support, or standards. I was learning, but was not nearly as technical as they had thought I was, or I needed to be. 

I reached back out to Meta and, just 3 days later, they put that original offer *back on the table*, with their NYC, Menlo Park, and Seattle offices all possibilities. 

I want to accept *so badly*, even more so now that I am out of a job. But two things worry me:

* My last job made me feel so incompetent, despite having been very successful at previous stops before. Will Meta's culture crush me? I'm willing to do whatever it takes to learn, just need an environment where I can do so.
* I am a little concerned by how hard they pushed for me originally **and** how quickly they made that offer available again. I am worried that it speaks to making me expendable if they had to cut people. Moving to a big city only to feel vulnerable to a layoff...that's not a good feeling!

Am I overthinking this? Should I just simply trust that my experience and performance in the interviews/tests was good enough for them to want me? HELP!",54,38,1736817443.0,dataengineering
1i16ptk,Using Node.js + Apryse to Convert DOCX to Web-Ready PDFs on the Server ,,3,0,1736863349.0,dataengineering
1i15aov,Trino - How to work with arrays of structs without unnesting?,"Hi everyone, I work with Trino on a daily basis and I keep coming against a problem that results in what I consider to be less than optimal solutions: 

Tables containing arrays of structs, where I want to do some basic aggregation of specific members. 

As an example, a row contains a column with an array of payment line items for an order. each entry in the array is a struct of uniform format, and I want to aggregate the values from a particular member of the struct across the whole array.

A possible solution is of course to unnest the array and then I can handle the struct members as individual columns, but is there no way to directly address the array directly? 

Is there no way to do something like 

Select SUM(arrayCol.localPriceAmount) 

which outputs the total value of every array elements localPriceAmount member?",3,6,1736858893.0,dataengineering
1i1bb5k,Data engineering dev tool SaaS companies : Boom or Bust?,"With acquisitions of Datavolo, SDF labs & Upsolver does it mean that data engineering dev tool SaaS companies are falling apart and rest are doomed or the expected boom time for data is here?",0,12,1736875560.0,dataengineering
1i1b60g,Need help on a weird thing,"My background: currently working as a DE in a good project and in a good team. Here good team means really good, colleagues are nice to me, care about me, shield me in case I make mistakes and encourage me as well. Even clients are good towards me.


So the problem? All these people are far away from me and I always meet them in a virtual calls. And due to this I am having a hard time socialising as I am always stuck on screen with almost no time to interact with people beside me and if I am working from home.
I am now a days unable to sleep properly as it is just my work in my mind. I love my job but not to the point of breakdown. No proper sleep, no exercise, no socialising, unable to take leave due to fear that people will not work properly in my absence.


What should I do now? I want to switch but it will take time but until then what I should do?
Guide me please on this as I am kind of on verge of a breakdown.",0,8,1736875197.0,dataengineering
1i19lz5,GCP bucket to s3,"Hi all, 

I would need advice about transferring around 8TB of files from GCP to s3 bucket (potential ly I would need to change the format of the file) . The GCP is not under our ""control"" which means it is not ours. Is there some inexpensive solution or generally how to approach to this? Any information which could point me in the right direction would be great. Also any personal experiences i.e. what not to do would be welcomed! Thanks! ",1,6,1736871300.0,dataengineering
1i19emm,Documenting the data world,"So I searched and there is no real answer in the subreddit, that I can see at least. If you know a thread please send it :)

We are mostly doing analytics with our data but there are also some data processes that are not related to any analytics.

I want to document everything we have very well but i am struggling to find some resources for data world/platform/model documentation. Bits and paces here and there but nothing of real substance.

What I want to do is a top bottom level approach so that people who want to know what data is available to use and thats it. Then a bit more in depth of what the data is and who uses it, who is responsible, what values do this data bring to the organization, what are the cost of this pipeline(actual and man hours to maintain develop). Then a layer that describes the process more in detail such as a DFD (data flow diagram) that I found recently no idea if you have a better solution.

If anyone has any good resources or has been working in the data world for a while i would appreciate any tips. I am not that experienced so please be kind if some of my points do not make sense or whatever sounds obvious to you does not to me because of my experience. Thanks : )",1,4,1736870768.0,dataengineering
1i0hj62,Database from scratch,"Currently I am tasked with building a database for our company from scratch. 
Our data sources are different files (Excel,csv,excel binary) collect from different sources, so they in 100 different formats. Very unstructured. 

 1. Is there a way to automate this data cleaning? Python/data prep softwares failed me, because one of the columns (and very important one) is “Company Name”. Our very beautiful sources, aka, our sales team has 12 different versions of the same company, like ABC Company, A.B.C Company and ABCComp etc. How do I clean such a data? 

 2. After cleaning, what would be a good storage and format for storing database? Leaning towards no code options. Is red shift/snowflake good for a growing business. There will be a good flow of data, needed to be retrieved at least weekly for insights. 

 3. Is it better to Maintain as excel/csv in google drive? Management wants this, thought as a data scientist this is my last option. What are the pros and cons of this

",70,59,1736784899.0,dataengineering
1i12rlw,How to configure Snowflake MySQL native connector,"Hello everyone

I’m excited to share my first article on Medium! In this article, I walk through the steps to configure the Snowflake MySQL native connector, including key setup details and best practices. I wanted to share these insights that might help others in similar roles.

Would love to hear your thoughts and feedback!

https://medium.com/@khaled.khouli175/how-to-configure-the-snowflake-mysql-connector-1b096f009c19",2,0,1736848770.0,dataengineering
1i14xiw,Build a Database for Semi-Structured Data with Cloud Services,"Hello, data engineers! After introducing the rise of Shared Disk Architecture and Semi-structured Data Processing, we release the third blog to introduce the challenge we encountered in designing a database optimized for handling semi-structured data and our solution, [ScopeDB](https://www.scopedb.io/)

[https://flex-ninja.medium.com/build-a-database-for-semi-structured-data-with-cloud-services-4bce1b8f15f3](https://flex-ninja.medium.com/build-a-database-for-semi-structured-data-with-cloud-services-4bce1b8f15f3)

Takeaways:

* By leveraging object storage services (OSS) for user data and relational database services (RDS) for metadata, we can handle data in petabytes properly and achieve approximately *30 times* cost reduction compared to Shared Nothing Architecture solutions on the cloud.
* We define a dedicated internal format for variant data and optimize data analysis with a specific columnar data file format. As a result, searching among hundreds of millions of log records can be done in *one minute*, while if a time range filter in hours is specified, the query can be done in *seconds*.
* Our database can reflect the object schema and retrieve data from a variant value with its object path, allowing users to play with semi-structured data smoothly.

There are many more details and numbers we omit due to the space. Welcome to comment or drop an email to discuss :D",1,0,1736857611.0,dataengineering
1i0qifq,I'm not sure if I'm allowed to web scrape,"Quick question!   
I'm working on a project that involves NBA2k attributes and tendencies.  
I was able to find CSV file of all current player attributes BUT couldn't find player tendencies.

ChatGPT gave me few options but I decided to go with web scraping this info from [https://2kdb.net/](https://2kdb.net/)  
This was my first time web scraping. I used Jupyter Lab. Took me about 2 hours but I got it done while learning in the process.

Am I allowed to extract information like this from this or any other website? I have no idea what are legal and ethical guidelines for web scraping. ",10,9,1736807117.0,dataengineering
1i0t15s,Old person but early in DE career. Stick with comfortable but low growth job or take what is likely a huge stretch but ambition-aligned new role?,"What would you advise someone who has the opportunity to take a huge leap forward in terms of career development (first DE hire for a lead data engineer role responsible for migrating and modeling data, gradually building a team, and doing ML Ops for data science) but it would be a stretch and require a ton of learning while simultaneously having to deliver? When do you know it’s going to be too demanding and you risk performing poorly versus when you’re ok to take the leap since this isn’t a spectator sport and we often learn best on the job with all the trappings and complexity of a business, its people, and their data? 

My current gig is a dream, especially for someone new to data engineering. Low stress. Lots of encouragement. Learning a lot. Great team. No micromanaging. Work-life balance. Great benefits. Good comp. However, advancement seems rare. This new gig pays a good deal more, I’d advance (eventually leading a team and continuing to move up), and I get to satisfy my ambition of building something from the ground up that will scale and grow. Invariably, the pressure will be high and I’ll be working a good bit harder since, well, I’m pretty new at this. Oh, and I’m old, like, probably 15-20 years older than I imagine most folks on this sub. No kids. Just me, spouse, and I’d argue an enviable amount of free time. What would you do?",7,15,1736813875.0,dataengineering
1i0qtzc,Data Management / Governance / Quality - Where do you go to stay up to date?,"Hey all,
Essentially the name of the topic.

I follow Dataversity, but I'd like to hear more from you.

People you like or websites / newsletters Essentially focused mainly in Data Management / Governance / Quality

Thanks!",11,11,1736807945.0,dataengineering
1i1e0eh,Your production code contains ?,"Most of your production code contains the following library? Just a fun poll to understand the usage.

[View Poll](https://www.reddit.com/poll/1i1e0eh)",0,12,1736882377.0,dataengineering
1i12i6w,"Integrating Medallion Architecture (yes, that) in a Data Lakehouse with a Semantic Layer Tool: where does the GOLD sit?","At my company, we're implementing the Medallion Architecture (Bronze, Silver, Gold) within a Data Lakehouse framework and are looking to integrate a Semantic Layer tool (to support BI Team).

I believe that to fully leverage the Medallion Architecture, semantic modeling could begin even in the Silver Zone (IMHO) and definitely in the Gold Zone. In this scenario, should the Gold Zone be managed entirely within the Semantic Layer tool?

Looking for your **insights** or **experiences**: **approaches to utilize both the Medallion Architecture and a Semantic Layer?**

**P.S.**

I’ve noticed that the term ""Medallion Architecture"" is often met with skepticism. To clarify our approach: the Bronze layer handles raw data ingestion, the Silver layer focuses on key business data entities, and the Gold layer consists of datasets tailored for KPIs, which are joined, aggregated, and filtered. The Semantic Layer is centered on business KPIs, where it overlaps with the Gold Zone, but in addition manages derived metrics such as counts, averages, and sums, time context.",1,8,1736847591.0,dataengineering
1i0p0yh,Would you stage report-ready data directly into the gold lakehouse?,"I have a system that we're bringing into our lakehouse architecture that is already in a format that we will report on directly. I'm stuck on whether to bring that data into the bronze lakehouse or the gold lakehouse based on standard best practice. Technically, this is gold-level data, but at the same time I would think it would be best to land all external data into the bronze lakehouse, then report off of that data. For the end user, I could see this as confusing since they're used to gold being where to pull clean data from. I'm curious, how would you go about this?",9,20,1736803327.0,dataengineering
1i0r2pj,Blog post: Data Masking with Azure Databricks,,5,0,1736808575.0,dataengineering
1i0qrmz,Video walkthrough for setting up a GCP VM with the updated UI,,4,0,1736807782.0,dataengineering
1i0qpxg,Am I Being Underpaid? C2H Data Warehouse Developer Position via Staffing Agency,"Hello everyone, hope you're all having a great day! 

I'm a new grad and recently received a contract-to-hire (C2H) offer through a staffing agency for a Data Warehouse Developer position at a company. 

I don't have previous work experience for this position but I am proficient in the programming languages they require so I got the offer. The agency initially offered me $30/hour and then called me and said they increased it to $35/hour the next day, explaining that they realized the initial offer was low. This includes standard holidays and 10 days of PTO. 

However, I found that this rate is still on the lower end of the market average for my role in Rochester, NY, which ranges from $34 to over $60 per hour (not sure if it's reliable). Given that this is a C2H position, shouldn't the rate be even higher than the average on the market? The job description indicates that the company is looking for someone with 3-5 years of experience, which I don't have. 

I'm concerned about how this might affect my perceived value. I'd love to become a direct hire with the company, but I'm uncertain how the staffing agency's interests might influence this transition, especially if they prefer to keep me as a contractor for revenue reasons... 

I'm also concerned about whether the company is fully aware of my actual pay rate, as I want to avoid any misunderstandings about my efforts and their value. (Imagine you are working with them as a $35/hour developer, but they believe they are paying for a $55/hour developer. How am I ever going to reach their expectations.)

I appreciate any advice on handling and understanding this situation. Thanks!",6,6,1736807658.0,dataengineering
1i0kb85,Daily copy to Azure SQL,"Hi, I have a SQL Server database containing 15 tables that need to be copied daily to an Azure SQL database (for various business reasons); this will be a mixture of incremental and full copy jobs.
I have established I can do this either using SSIS (Hosted on the Azure runtime of the Azure SQL instance, which I believe ADF needs to be enabled for) or using ADF and a copy data pipeline.
Can anyone comment on the price comparison for each of these transfer methods? It'll be about 100k new records every day.

Full disclosure, this is for work. But also for learning!",8,9,1736791752.0,dataengineering
1i0z9y5,The Future of Unified Observability: Integrating Data Observability with OpenTelemetry and eBPF,,1,0,1736833462.0,dataengineering
1i0z0bv,Getting the opportunity to eventually promote to Data Engineer,"Hey everyone,

I currently work as a Data Analyst / BI Analyst for a large non tech company. We recently got a new CIO and she wants to build a much more comprehensive data structure (which is much needed)

Long story short, she wants me to pursue some online certifications to become well versed in data engineering. She’s giving me a timeline of 6 months to a year. My homework assignment is to come up with a plan with courses and projects to gain these skills. Not too concerned with specific technologies, but rather learning the concepts and skills in some form. Obviously I would be in a more “junior” role, so I don’t need to necessarily come out guns blazing, but rather just have a good grasp on the fundamentals and be able to have something to show for it. 

All that to say is that I came seeking advice for someone in my position, good courses, certifications, etc.

Any help or advice would be great, thanks!",1,4,1736832421.0,dataengineering
1i0pd6j,Tips for junior engineer,"Hi I hope everyone had a good start on the year. Some information about me, I graduated last year and have been working for a little over a year as a data platform engineer in aws. We have an ETL platform and are building it using IaC, I went from not knowing what aws was to be involved in some big projects which sometimes can feel overwhelming and a lot of pressure, but the upside is the learning curve. 

But I want to know if you guys have some tips that I can take with me in my career  ",6,5,1736804196.0,dataengineering
1i0tvnm,"Is databricks workspace running on K8s? If yes, what kind of PV they use?","Hi folks,

I'm trying to create a pipeline mimicking Databricks workspace. Basically just a simple Jupyterhub + Spark on K8s.

By documentation, Databricks use cloud object storage, when deploy a workspace. But Jupyterhub needs a file system as PV, not object storage.

How the hell did they do that? Did they convert the object storage to file system? Hope some experts can shed some light on this. Thanks in advance.",2,1,1736816251.0,dataengineering
1i0ya7d,"Looking for the next step, B.S. in Data Sciences with 3+ years work experience, what now?","hey y'all! I just recently hit a career milestone and have achieved 3 years work experience, I accepted a job right out of college and ended up landing a position in philly as a junior data engineer. now with the experience collected, I wanted to explore opportunties to work elsewhere. im from the great lakes area and have been meaning to return, but struggling to find what/where to apply for in those areas or find stuff remotely.

What has been booming recently in the industry? Anything new or up and coming I should be out on the look for? I'd be happy working for any of these positions:

* Data Engineer
* Data Architect
* Data Integratior
* Data Automation Specialist
* Data Scientist
* SQL Developer
* Business Intelligence Developer
* Database Analyst
* Database Specialist
* DBA
* Data Warehouse administrator 
* IT Architect

& any other appliable position within the field of data science!

Thank you. I appreciate you for reading my post :)",1,5,1736829791.0,dataengineering
1i0poso,Tools for CDC integrity checks,"Hi everyone!

I have implemented a CDC system which replicates changes from a SQL server to Big Query. Now that I have the data flowing I want to implement some integrity checks.

I am think of checks like(matching values from source and destination):

1. Same row count

2. Similar average/sum for numerical columns

3. Same count of distinct values

So far, I have written a python script which does this and it works well. I also tried to use great expectations and it was a headache. 

Is there any other tool I should try?",3,0,1736805020.0,dataengineering
1i0cpuy,Data Warehousing Architecture for a Bank’s Data Infrastructure,"I'm working on a data warehousing project for a bank, and I need advice on the best architecture for their needs. The bank handles large volumes of transactional and customer data, and we aim to support real-time analytics, improve reporting, and integrate multiple systems.

Specifically, I’m looking for input on:

1. Should we use a traditional star or snowflake schema, or is a modern cloud-native solution like Snowflake or BigQuery better?
2. Best practices for secure ETL processes in the banking sector?
3. Tools/architectures for handling real-time data, especially for fraud detection and customer behavior?
4. Should we use open-source tools like Kafka, PySpark, Postgres, etc., or stick with enterprise solutions?
5. How to ensure compliance and data security (GDPR, PCI DSS)?
6. Strategies for scaling the warehouse as data grows?

Any recommendations based on experience with financial institutions would be greatly appreciated.",19,42,1736770865.0,dataengineering
1i0mkm3,What are my options once my dbt project grow beyond a couple hundred models,"So here is my situation. My project grew to the point (about 500 models) where the compile operation is taking a long time significantly impacting the development experience.

Is there anything I can do besides breaking up the project into smaller projects?

If so, is there anything I can do to make the process less painfull?",3,11,1736797257.0,dataengineering
1i0mklp,What are my options once my dbt project grow beyond a couple hundred models,"So here is my situation. My project grew to the point (about 500 models) where the compile operation is taking a long time significantly impacting the development experience.

Is there anything I can do besides breaking up the project into smaller projects?

If so, is there anything I can do to make the process less painfull?",3,4,1736797257.0,dataengineering
1i0j47l, Databricks or Informatica for AI pipelines?,"Hey folks, I’m currently trying to get a better understanding of AI architecture and researching how to build efficient data pipelines for machine learning. While digging into different tools, I keep coming across Databricks and Informatica, but honestly, I’m struggling to fully grasp the differences and scope of each platform.

From what I’ve gathered so far:

\- Databricks: Seems great for big data and ML workflows, with unified analytics and strong support for data lakes.

\- Informatica: Appears to focus more on data integration, governance, and traditional ETL processes.

But here’s where I’m stuck:

When would you pick one over the other? 

And why is Informatica gaining more traction lately in the AI/ML space? Is it just marketing, or does it have an edge for things like compliance and data governance?

Would love to hear from anyone who’s worked with either (or both). ",5,33,1736788820.0,dataengineering
1i0978r,Current data analyst- should my next step be data engineering or data science?,"Hi everyone!

I’m currently working as a data analyst in the product development space but feel like I’ve hit a plateau in my career growth. I’ve been considering transitioning into either data engineering or data science, but I’m a bit torn and would love some advice!

A bit about me for context:

I have an engineering degree in computer science.
My first job was as a frontend developer, but I hated it- partly due to imposter syndrome but also because I didn’t enjoy just coding all day. I wanted to be closer to the decision-making process.

I shifted into data analysis, which I enjoy for the storytelling and exploratory analysis aspects. However, I dislike building dashboards and tbh I'm mainly working with descriptive stats with nothing too advanced. My personal development has stalled as a result.

Where I’m struggling:
Data Science feels like the natural next step, but I struggled with statistics and machine learning courses in university. I’m worried I’ll never be “great” at the statistical side of things, and that’s holding me back.

Data Engineering seems more aligned with my strengths, but I’m concerned it might feel like my old developer role—just coding for 8 hours a day and building things based on other people’s requirements.

I know I sound a bit lost, but I’m really trying to figure out the best direction for me. Have any of you faced a similar situation, or do you have advice on how to decide between these two paths?

Thanks in advance for your help!",22,24,1736755359.0,dataengineering
1i15f8k,Introducing Raden: The World's First AI Data Engineer. Looking forward to hearing your thoughts!,,0,5,1736859318.0,dataengineering
1i0ow0x,Data Virtualization in Data Lakehouse,"I work in data operations team. We have an on premise data Lakehouse and on top of it there is a data virtualization layer using Denodo. Which means actually Denodo is connected to only one source which is the data Lakehouse. All BI tools are connected to Denodo, and all data APIs are implemented in Denodo. The issue we are facing is that almost all BI jobs are calling views in Denodo that are actually deriving the data from the staging zone directly, And the curated zone is almost having  few tables and the dependency is on the raw data. As operations we are suffering from this design due to multiple reasons:
* complex queries are implemented in Denodo not in Data Pipelines
* historical data are always being re-retrieved every weekend although it should be cold data, and sometimes due to jobs failure this data could change which shouldn’t happen
* jobs failure rate is high due to massive queries pushed down to the data lake impacting SLA
I want to suggest enhancing this current model, what do you suggest I should propose. ",1,1,1736802982.0,dataengineering
1i0o9t2,Data Engineering to Data science using azure ,"Hi 

I am looking for a simple path to learn data science on medium level expertise 

Currently I am a data engineer with good Knowledge of spark pyspark pandas

Also let me know which one is easier to learn from my situation data analysis or data science.

Also looking forward to learn data visualization 

I currently have Azure MSDN subscription so i can explore more on it.


Looking forward to short and simple courses. Ms learn I am not getting hooked into to learn",1,2,1736801463.0,dataengineering
1i0hngi,Rant about dealing with boring and hacky work,"How do you deal with when you are on a project where you are doing incredibly mundane work.

I work for a consulting industry and atm I have to deal with incredibly poorly written code, implemented by data scientist who developed incredibly complicated modules for data ingestion. Everyone knows this framework is bad and the data scientist who authored it (another external consultant) is the only one who understands it as he developed it.

Nothing is documented, the ingestion is over engineered and is an abstraction hell. The entire data processing pipeline works where you need to provide bunch of glom json data transformations where a custom transformation abstractions are inplemented and are tightly coupled with the source json structure. 


My job is to understand what is happening and implementing it on a new dataset.

The problem is, I don’t think I can do it. For two weeks i have tried working with it but I am unable to. I get too frustrated and get nothing done. I have asked for help from the author but he is not inclined to help me.

I feel like I cannot do what I am paid to do and really struggle mentally to focus as this is frustrating me too much. I work as a consultant where I was sold as an expert in this sort of stuff.

Allegedly for the author of that monstrosity, implementing mew data pipeline for a new data source probably would take a fraction of a time as they know what needs to be done and how that module works. 

Part of me makes me think that this is not the poor code that I essentially inherited, but it is also my problem. Most software engineers encounter this type of situation where they need ti understand an utterly shitty code and get up to speed with it. But I really struggle with this activity.

After working in this industry for 4 years, am not sure if this career is right for me.",3,3,1736785195.0,dataengineering
1i0fwv7,Data governance practices in 2025,"I just saw a question here asking what tools and strategies people on this sub are using for data governance. At a recent event we held about data governance, we polled attendees about their data governance practices, we cover topics like the tools people use, cultural practices, how to balance speed and control - we aimed to get a snapshot of what data governance looks like for enterprise organizations.

If you are interested - it's free to download [here](https://www.secoda.co/state-of-data-governance?utm_campaign=sdg&utm_source=reddit&utm_medium=social&utm_term=red) (we do need your work email though, if this is an issue just send me a message and I'll send you the report)

Edit: Here's a [page to download the report directly](https://www.secoda.co/rdt-download-sdg-report-2025), no email required",2,0,1736780610.0,dataengineering
1i0fkxn,Data consumers accessing data from bronze layer,Do you allow your data to consumers to ingest data from your bronze layer or do you require that data assets are moved from bronze to silver in order to be consumable?,2,3,1736779711.0,dataengineering
1i0khuz,DataExpert free course - is it still available?,"Hi, I just found Zach kicked of free DE course in Q4 last year and it should be available till end of January. It took me a while to find it because sign up page redirects to pricing and here it is - [link](https://github.com/DataExpert-io/data-engineer-handbook/blob/main/bootcamp/introduction.md) to repo with links and materials. 

Around 1/3 of materials (mostly labs I believe) is not available because of purchase required page. Is this how it should have worked from the beginning?",0,1,1736792195.0,dataengineering
1i002zv,4.5 years at the same company time to switch?,Hi I have been working as a Data scientist at the same company - my last pay rise was in 2023 and does not look like will change this year also the work i am doing is simple data engineering - in terms of promotion nothing. is it better to start looking for something else ? ,37,31,1736724159.0,dataengineering
1i0jrvz,Parent child modeling ,"I'm getting into a project that requires me to make a parent child relationship between items. For example if a electrical box contains a fuse we would want to trace back to the department that made the fuse and all other components that have same fuse.
It's for recalls.

It almost feels like a gedcom db as a component(sku A) can have en electrical box(sku B) with a circuit SKU C.

I've never had to deal with this. I've heard of graph db's being used for this but it looks complicated ",1,6,1736790440.0,dataengineering
1i0i94t,"How to Process and Access Attachment Files (PDFs, Images, etc.) in JSONL Data to prepare data for LLM Fine-Tuning?","Hi, I'm working on a project where I ingested data from a ticketing system using the Airbyte open-source tool. Ingested data includes folders and each folder contains JSONL format files.  In the conversation folder under`_airbyte_data` structure, there is a  specific column, `attachments`, includes file paths or data for various formats like PDFs, and images exchanged between agents and clients during ticketing conversations.

# Here's what I've done so far:

1. **Ingestion**: Successfully ingested data into the JSONL files.
2. **Initial Exploration**: Observed that the `attachments` column contains metadata or references to these files in different formats.

# Problem:

I aim to clean and preprocess the `attachments` column for downstream tasks, specifically to prepare a dataset for fine-tuning a Large Language Model (LLM). However:

* The attachments are in multiple formats (PDFs, images).
* I'm unsure of the best approach to extract, clean, and organize this data for LLM fine-tuning.

# My Questions:

1. How can I systematically access and process the `attachments` column in the JSONL data?
2. What are the best practices for handling and cleaning multi-format files (PDFs, images, videos) for LLM fine-tuning?
3. Are there any libraries or tools you'd recommend for handling such mixed data formats?

# Additional Context:

* My dataset includes textual data from the ticketing system conversations, and I plan to use this for training a retrieval-augmented generation (RAG) model.
* The eventual target is to build a chatbot.
* I'm proficient in Python and open to using tools or libraries that can simplify this process.

Any insights would be greatly appreciated. Thanks!",1,1,1736786730.0,dataengineering
1i0i8t4,DE to DE would you ise this tool?,"Based on my experience with trying to backfill historical GA4 data into Big Query, I decided to build a tool to help non technical users backfill their historical GA4 data. Could you give it a try and tell me your critiques?

Www.databackfill.com ",0,4,1736786706.0,dataengineering
1hzx8jd,"Storing large quantity of events, fast reads required, slow writes acceptable.","I am trying to store audit events for a lot of users. Think a 12 million events a day. The records itself are very concise, but there are many of them. In the past I used to use dynamodb but it was too expensive, now I switched to s3 bucket with athena, split the events per day and query the folders using SQL queries.

Dynamodb used to work much faster but the cost was high considering we would almost never query the data.

The problem is that the s3 solution is just too slow, querying can take 60+ seconds which breaks our UI-s where we want to occasionally use it. Is there a better solution?

What are the best practices?

Edit:

Sorry I double checked my numbers, for december the scan took: 22 seconds and resulted in 360m records, the same query would take 5+ minutes when I pick a date which is not a full month. 1. dec - 15 dec took over 5 minutes+ and still keeps churning even tho it only analysed 41gb, while the full month was 143gb.

Since the data is partitioned by year/month/date folders in the bucket and I use GlueTables.

The data is stored as JSON chunks, each JSON contains about 1mb worth of records. Example record being

    {""id"":""e56eb5c3-365a-4a18-81ea-228aa90d6749"",""actor"":""30 character string"",""owner"":""30 character string"",""target"":""xxxxx"",""action"":""100 character string"",""at"":1735689601,""topic"":""7 character string"",""status_code"":200}

1 month example query result:

Input rows 357.65 M

Input bytes 143.59 GB

22 seconds

  
Where it really falls apart is the non full month query, half the data, about 20x the time

    SELECT id, owner, actor, target, action, at, topic, status_code
    FROM ""my_bucket""
    WHERE (year = '2024' AND month = '11' AND date >= '15')
    OR (year = '2024' AND month = '12' AND date <= '15')
    AND actor='9325148841';

Run time: 7 min 2.267 sec 

Data scanned:151.04 GB",34,30,1736716653.0,dataengineering
1i02ws9,Need advice on simple data pipeline architecture for personal project (Python/AWS),"Hey folks 👋

I'm working on a personal project where I need to build a data pipeline that can:

* Fetch data from multiple sources
* Transform/clean the data into a common format
* Load it into DynamoDB
* Handle errors, retries, and basic monitoring
* Scale easily when adding new data sources
* Run on AWS (where my current infra is)
* Be cost-effective (ideally free/cheap for personal use)

I looked into Apache Airflow but it feels like overkill for my use case. I mainly write in Python and want something lightweight that won't require complex setup or maintenance.

What would you recommend for this kind of setup? Any suggestions for tools/frameworks or general architecture approaches? Bonus points if it's open source!

Thanks in advance!

Edit: Budget is basically ""as cheap as possible"" since this is just a personal project to learn and experiment with.",15,18,1736732507.0,dataengineering
1i04it6,Compression under the hood,"Hey all,

As data engineers, we work with compressed data all the time. It saves storage space and speeds up transmission.

But behind names such as GZIP, Snappy and LZ4 lie some pretty interesting techniques. I took a closer look at how they work and shared my findings in this [blog post](https://cefboud.github.io/posts/compression/). 

Would love to hear your thoughts!",12,0,1736737641.0,dataengineering
1i07n0u,Looking for a low level and advanced Data Engineering online training,"Hello, I am a data engineer with an experience of 3 years in python, go, Big Data and AWS. My job requires me to get an online training, and this year I want to take a training related to low level data engineering, maybe data processing with gpu, nlp data engineering, data engineering with low level languages, in memory data formats, distributed systems and something like that. I want your suggestions about which course I can take. Thanks!

P.S. I do not want to take courses like ""Data Engineering on AWS""...",5,8,1736748593.0,dataengineering
1i0n7kd,Should I go down the route of Engineering? Pros and Cons?,"I'm currently a Data Analyst and I have a some interviews coming up. 

Some of the titles are:

* Senior Data Engineer
* Senior Commercial Analyst (job spec is like Senior data analyst)
* Senior Digital Analyst (lots of product analytics and standard data analyst stuff)

The Senior Engineer role seems like it could be a great one if I'm thinking long term however I'm quite keen on the Senior Digital Analyst position because the company is one of the largest Supermarkets in the country (UK), heard lots of good things about them, team members I could learn from and they are local to me so the commute is ideal. 

  
I would learn new skills at the supermarket: Hadoop, Tableau, product analytics

Engineering role: DBT, Snaplogic, Looker, Domo/Superset

Commercial Analyst: Has the most scope for progression and responsibility as there isnt much of a data function there currently, but it seems like it would be very similar to what I've done at my most recent position as a data analyst (automating, SQL , Power BI)

  
I love learning new skills and tools and want to be as well-rounded as i can be.

  
I'm thinking long term for all of these roles (and hypothetically if I were to have the choice of them all), they all pay similar amounts, some are easier in terms of commutes than others but the big thing i guess is; will data engineering be more in demand down the line (thus also more job security) as we continue to shift to everything being data-driven? ",0,7,1736798805.0,dataengineering
1hzqw8i,Data Engineering and Data Science Networking,"Hi Data Engineers,

I'm looking to compile a list of conferences and meetup groups that one could go to, to network with other data engineers and data scientists. Could you help me create an exhaustive complete list.",29,11,1736700599.0,dataengineering
1i0b0q0,"MS SQL Date Time as a long string of numbers, not epoch","Hi guys,

I cant seem to find this in the documentation (sorry) but I receive the following (sample) data for a timestamp column that I got from an MS SQL db

\`635243198284680023\`

how am I supposed to read this? It doesnt seem to be epoch--dates are supposed to be around 2015 or later

appreciate the help!",1,2,1736763791.0,dataengineering
1i0e8xn,Wallace & Gromit's Wake Up Machine is a metaphor,"Enjoyed watching Vengeance Most Fowl this weekend and saw a lot of DE parallels in how Gromit manages his stakeholder's semi-automated pipeline.

  
[https://www.netflix.com/watch/81351936?t=190](https://www.netflix.com/watch/81351936?t=190)",0,0,1736775880.0,dataengineering
1i05cel,Tool to map out a database migration?,"We're looking to move data from an Oracle DB to our Azure space. I was going to use Data Factory to push everything to Azure SQL. However, the tables in the Oracle DB are very flat - a single table will contain policy data, building data, customer information, etc. So I want to break it up into multiple tables, sort of like this:

https://preview.redd.it/gy88i6kenoce1.png?width=803&format=png&auto=webp&s=4101b8b21bda1e88b50bb7def909a0520c65bf59

That's a simplified version, but you get the idea. So far I've been using Vertabelo to map everything but the trial ran out and now it's basically crippled. My manager doesn't mind forking over for a data modelling application (like probably $1000/year max), but I was wondering if there are any alternatives? Any recommendations? I wanted to see if there was a Microsoft solution, but it's not clear if Data Studio does what I'm looking for. Any advice would be appreciated.",3,4,1736740344.0,dataengineering
1i05bmg,Good Bluesky Feeds for DE?,"I’m new to Bluesky and trying to figure out how to curate a feed focused on data engineering. I’m looking for ways to stay updated on:

- Data engineering trends
- Useful tools and new libraries (e.g., Python)
- Insightful analysis and discussions
- Prominent data engineers to follow
- Cool datasets or open-source projects

When I search for “Data Engineering,” I only find a single feed with about 26 followers, and it doesn’t have much of what I’m looking for.

Has anyone successfully pieced together a good DE-focused experience on Bluesky? If so, how did you go about finding the right people, hashtags, or feeds to follow? Any tips would be greatly appreciated!",5,10,1736740268.0,dataengineering
1i048n3,Ape-DTS: Share an open-source data migration tool,"[https://github.com/apecloud/ape-dts](https://github.com/apecloud/ape-dts)



\# Introduction

Ape Data Transfer Suite, written in Rust. Provides ultra-fast data replication between MySQL, PostgreSQL, Redis, MongoDB, Kafka and ClickHouse, ideal for disaster recovery (DR) and migration scenarios.



\# Key features

\* Supports data migration between various databases, both homogeneous and heterogeneous.

\* Supports snapshot and cdc tasks with resuming from breakpoint.

\* Supports checking and revising data.

\* Supports filtering and routing at the database, table, and column levels.

\* Implements different parallel algorithms for different sources, targets, and task types to improve performance.

\* Allows loading user-defined Lua scripts to modify the data.



https://preview.redd.it/c4hzr3fsdoce1.png?width=2054&format=png&auto=webp&s=8dd09908037289b9a02116b7e5be792ed27ea49a",3,0,1736736746.0,dataengineering
1hzgnmv,FAANG data engineering job board ,"Hi everyone,

I created a job board and decided to share here, as I think it can useful. The job board consists of job offers from FAANG companies (Google, Meta, Apple, Amazon, Nvidia, Netflix, Uber, Microsoft, etc.) and allows you to filter job offers by location, years of experience, seniority level, category, etc.

You can check out the ""Data Engineering"" positions here:

https://faang.watch/?categories=Data+Engineering

Let me know what you think - feel free to ask questions and request features :)

",128,18,1736663279.0,dataengineering
1i04gw0,Analyst masquerading as a DE seeking advice on what to learn next,"Hi, all! 

I have been working as a marketing data analyst for a few years now at a big tech company and during that time I have been primarily doing what I think would be considered jr. data engineering work. I really like the work I am doing but I am stuck on trying to figure out what I should be doing to upskill. Ideally I’d like to go for a role change to officially become a DE and continue to grow my career in this space. There’s a million articles and videos out there on what a data engineer needs to know, but I’m not sure what actually pertains to my situation and I don’t want to waste my time learning something that turns out to be a fringe use case. I would greatly appreciate feedback on where to focus my time and energy from here! 

Some info about what I do, tools I use, and perceived proficiency:

Maintaining data pipelines in Airflow (Intermediate) - I am responsible for ensuring our pipelines run as expected and if they fail for some reason I look to the underlying source code to debug and rectify any issues. I’ve not setup a pipeline from scratch yet.

Contribute to Git by updating our Python (Intermediate) notebooks and SQL (Advanced intermediate) scripts when changes are made upstream, or if a new feature is requested by an internal customer - The global data team is notoriously always making making changes like switching to a new source of truth table or deprecating fields with little to no notice. I’m the guy whose job it is to fix any issues this might cause for our immediate team of marketers. Also, if the marketers come to our BI Analyst requesting a new viz or an update to an existing one, I am responsible for figuring out what data is required to make that happen, where it currently lives (RedShift, Starburst, Google Sheet), and how to get that data into a table in our prod RS schema for use in Tableau.

Snowflake migration - This is the big project for me this year. We are switching to a data mesh model, and part of that switch means we will be moving all of our data from RS and Starburst to Snowflake. We will also be utilizing dbt for transforming data (although at this point it’s unclear if that will fall to me or if the transformations requiring dbt will be occurring prior to the data entering the central warehouse). I have no experience with Snowflake or dbt.

I do some ad-hoc data pulls as well but those utilize the tools listed above.

Any advice would be greatly appreciated! In my search for resources I’ve realized I don’t know what I don’t know. ",2,2,1736737467.0,dataengineering
1i07q2u,SnowPro Core Certification ,"Hi, my organisation is pushing me to complete SnowPro Core certification. I started working with Snowflake in Nov'24 and previously worked with Azure for 2.5 years. Now from my research I understood that this certification is mostly theoretical so right now I'm brushing all core theory concepts. But I don't have a lot of ot time, they were pushing me to give the exam by 31st Jan, but somehow I extended the deadline to middle of Feb. I know ExamTopics is reliable for exam questions but they have only 250 questions for free, and the pro membership is around 70$ which is a little high for me, with my other costs. Does anyone who previously prepared for the same have any suggestions? If you have the pdf with all the questions would you be kind enough to share? Or maybe you have a fleeting ExamTopics membership which you'll be willing to share? I understand this is a big ask so no worries if you don't want to! Chris Snow had a good course on Udemy which got taken down, does anyone have the PDFs from that?

Anyways thanks in advance to everyone who replies and have a great year ahead everyone 💯",0,5,1736748946.0,dataengineering
1hzqlu3,Building Dashboards ,Is it common for data engineers in small size companies to take the ownership of building and managing dashboards like powerbi dashboards?,10,15,1736699835.0,dataengineering
1hz5ytw,Become a Data Engineer in 2025 (Based on 100 jobs data!),"Happy New Year, everyone! Reposting a combination of 3 of my most upvoted posts last year at the start of the year for those looking to set ambitious career goals in 2025 assuming lot of new people are looking for this info now. After all, there’s no better time to plan your next big leap into Data Engineering!

**1. Top skills in demand -**

I analyzed 100 data engineering job descriptions from Fortune 500 companies to find the most frequently mentioned skills. Here are the top skills in demand:

|**Skill Group**|**Frequency**|**Constituents with Frequency**|
|:-|:-|:-|
|Programming Languages|196|SQL (85), Python (76), Scala (21), Java (14)|
|ETL and Data Pipeline|136|ETL (65), Pipeline (46), Integration (25)|
|Cloud Platforms|85|AWS (45), Azure (26), GCP (14)|
|Data Modeling and Warehousing|83|Data Modeling (40), Warehousing (22), Architecture (21)|
|Big Data Tools|67|Spark (40), Big Data Tools (19), Hadoop (8)|
|DevOps, Version Control and CI/CD|52|Git (14), CI/CD (13), Jenkins (7), Version Control (7), Terraform (6)|
|Data Quality and Governance|42|Data Quality (20), Data Governance (13), Data Validation (9)|
|Data Visualization|23|Data Visualization (11), Tableau (6), Power BI (6)|
|Collaboration and Communication|18|Communication (10), Collaboration (8)|
|API and Microservices|11|API (8), Microservices (3)|
|Machine Learning|10|Machine Learning (7), MLOps (2), AI/ML Model Development (1)|

**2. 4 Month Study Plan -**

**Month 1: Foundations**

* DBMS & SQL: Basics of database concepts, querying, and design.
* Python: Focus on Python essentials, including libraries like Pandas and NumPy.
* Linux: Basic commands and navigation.
* DSA: Data structures and algorithms, especially for big tech roles.

**Month 2: Key Concepts & Tools**

* Data Concepts: Topics such as Data Lake, Data Mart, Fabric, and Mesh.
* Data Governance: Management, security, and ethics in data.
* Spark: Introductory concepts with Apache Spark.
* Distributed Systems: Overview of Hadoop, Hive, and MPP systems.
* Cloud Services: Options such as AWS, GCP, or Azure.

**Month 3: Advanced Topics**

* Orchestration: Basics of workflow orchestration with tools like Apache Airflow.
* Compute: Databricks, Snowflake, or equivalents like AWS EMR.
* Containers: Introduction to Docker and Kubernetes.
* CI/CD: Tools such as Jenkins and SonarQube.
* Streaming: Fundamentals of Kafka.
* ETL/ELT: Tools like dbt and Talend, along with architecture basics.
* Terraform: Code-based infrastructure setup.

**Month 4: Projects & Portfolio**

* Build a project portfolio to showcase skills. Examples include:
* Bank Data Warehouse
* Fraud Detection ETL
* Reddit Review Tracker
* Retail Analytics
* Trip Data Transformation
* YouTube Clone

**3. Certifications**

Note - You don't have do all of these, do 1/2 of AWS or Azure, 1 of Datarbricks or Snowflake, and 1/2 of optional certifications based on your interests. Also I have mentioned resources only for the ones I know - for the ones I haven't attempted/know have left it empty - please add the same in the comments.

|**Certification**|**Coverage**|**Cost (USD)**|**Resource**|
|:-|:-|:-|:-|
|AWS Certified Cloud Practitioner|Basics of AWS Cloud concepts, services, and support.|$100|Stephane Maarek's Udemy courses|
|**AWS Certified Solutions Architect – Associate ⭐**|Designing and deploying scalable systems on AWS.|$150|Stephane Maarek's Udemy courses|
|**AWS Certified Data Engineer – Associate ⭐**|Managing data pipelines, analytics, and ETL workflows on AWS.|$150|Stephane Maarek's Udemy courses, AWS Builder Labs|
|Microsoft Azure Data Fundamentals (DP-900)|Core data concepts and implementation using Azure.|$99|Eshant Garg/Scott Duffy Udemy courses, Coursera prep courses|
|**Microsoft Azure Data Engineer Associate (DP-203) ⭐**|Integrating and transforming data for analytics on Azure.|$165|Eshant Garg/Scott Duffy Udemy courses, Coursera prep courses|
|Databricks Lakehouse Fundamentals|Basics of Databricks Lakehouse architecture and workflows.|Free||
|**Databricks Certified Data Engineer Associate ⭐**|Building ETL pipelines and managing data workflows.|$200|Ankit Mistry's Udemy courses|
|Databricks Certified Data Engineer Professional|Advanced data engineering skills on Databricks platform.|$200||
|**SnowPro Core Certification ⭐**|Foundational knowledge of Snowflake architecture and operations.|$175||
|SnowPro Advanced Certification|Advanced expertise in complex Snowflake solutions and optimizations.|$375||
|SnowPro Advanced: Data Engineer|Data modeling, ETL, and tuning on Snowflake.|$375||
|Astronomer Certification for Apache Airflow Fundamentals|Core Apache Airflow concepts, including DAG authoring and scheduling.|$150|Mark Lamberti's Udemy course|
|Confluent Certified Developer for Apache Kafka|Developing applications with Kafka, architecture, and APIs.|$150||
|dbt Analytics Engineering Certification|Building and maintaining data workflows with dbt.|$200||
|HashiCorp Certified: Terraform Associate|Managing cloud resources using Terraform.|$70||
|Data Management Fundamentals Exam|Core principles: data architecture, governance, and quality.|$311||
|Data Governance Specialty|Best practices for governance, compliance, and data quality.|$311||

Tips to save money on these:

* AWS offers 50% discount on next exam: So after you give your first certification you can use a coupon code for the next ones.
* Azure - Coursera prep courses for Azure certifications offer 50% exam discount upon completion.
* For Airflow Fundamentals - Astronomer sometimes runs a promotion to get the certification for free. Follow them/Marc on LinkedIn to know the dates - I got mine in Jan last year.

**➡️Dive deeper! - Checkout my playlist ""Data Engineering Career"" with details of all of the above -** [https://www.youtube.com/watch?v=5b4CIon\_1pY&list=PLYAUClNVzmDN5D9IW-COX0xy\_8fz8r51k&ab\_channel=AnalyticsVector](https://www.youtube.com/watch?v=5b4CIon_1pY&list=PLYAUClNVzmDN5D9IW-COX0xy_8fz8r51k&ab_channel=AnalyticsVector)

Thanks, hope it added some value! All the best!",282,59,1736629839.0,dataengineering
1hzrzrw,Need Advise: Data Engineer Company Switch (DEA CO1 exam),"Hi Everyone, 

I am a data engineer in an MnC for 2 years. My work revolves mostly around internal tools of my org. I feel like my career is stuck and I am neither growing skill wise nor compensation wise. I am looking to switch company but I have experience in Python Sql, some devops tools, some Genai concepts, API building. I feel like my work is mixture of multiple domains with limited knowledge, I applied for Data Engineer role in companies but got feedback that I dont have much experience that a core Data Engineer should have (like nosql, pyspark, big data, apache, hadoop, etc.). I am looking to prepare to appear for DEA CO1 certification exam to put it in my CV. Can some one help me clarify whether it will be helpful/worth it. Please give suggestions.  
thanks in advance",4,3,1736703384.0,dataengineering
1i0bli0,Enterprise Data Architecture and Modeling: Key Practices and Trends,"**Enterprise Data Architecture and Modeling: Key Practices and Trends**

This article dives into the essentials of modern enterprise data architecture, focusing on **system architecture**, **data modeling**, and **emerging trends** like **Data Mesh** and **Federated Multi-Cloud Architecture**. Key highlights include:

1. **System Architecture**

• Explains data storage systems (OLTP, ODS, Data Lakes, and Delta Lakes) and flow optimization strategies like the Medallion Architecture.

• Covers storage optimization techniques and the importance of scalable designs.

2. **Data Modeling**

• Discusses dimensional modeling, including **Star** and **Snowflake schemas**, and advanced concepts like incremental ETL and slowly changing dimensions.

3. **Emerging Trends**

• **Data Mesh**: Decentralized data management with domain ownership.

• **Federated Multi-Cloud Architecture**: Integration across multiple cloud platforms for scalability and governance.

4. **Advanced Practices**

• Insights into security, compliance, and monitoring in data systems.

• Use cases across industries like retail, finance, and healthcare.

🔗 **Read the full article here**: [Enterprise Data Architecture and Modeling: Key Practices and Trends](https://medium.com/towardsdev/enterprise-data-architecture-and-modeling-key-practices-and-trends-7a40d4fe8746)",0,2,1736766358.0,dataengineering
1i03x9o,Career Advice ,"Hi everyone! I wanted to get some feedback on whether I’m on the right path to becoming a data engineer, especially since I don’t have a formal CS degree. My background: I transitioned from being a data analyst (6 months) to a data manager, and I’ve been in this role for about a year now. It’s at a non-profit, so while the pay isn’t great, I do have a lot of flexibility. I work with AWS daily and help manage their cloud infrastructure. My only issue I can see is that most of their data is rather small fitting on csv's. So I am not working with ""big"" data. 

I’m considering starting a part-time job as a web developer to gain more experience, particularly with backend development—things like creating tables, organizing data, and ensuring everything runs smoothly. I also earned two AWS certificates (Data Engineering and Foundations).

I’ve been applying for data engineering roles for a couple of months, but I haven’t gotten any responses yet. I understand it might have been slow due to Q4, so I’m hoping Q1 will bring better results.

Are there any other steps I should take to improve my chances or anything specific I should focus on? Any advice would be greatly appreciated!",0,3,1736735720.0,dataengineering
1hzyuq4,Need recommendations for books,"I need recommendations for any and all IT books. I'm currently working on the Google IT Certificate, but they ""WGU""  needs it so I can get in.  need books so I can go indepth to fully understand additional information to completely grasp it. I'm also hoping that reading all of those books will help me decide which IT field I enjoy the most. I have no background in IT. 
",1,1,1736720835.0,dataengineering
1hzt7de,ADF Handling of CSV Files with Commas in Data,"Hi I'm a beginner who recently started working on  Azure Data Factory (ADF) . I'm facing an issue while trying to load data from a CSV file into my SQL Server table.

The CSV file contains data with commas within the records, which is causing problems. Here’s what’s happening:

1. **CSV File Example:** Column1,Column2,Column3 ABC,XYZ,""A,QWERTY""
2. **SQL Server Table:** My table has exactly three columns:
   * Column1
   * Column2
   * Column3
3. **Issue:** When using the Copy Activity in ADF, I’ve set the column delimiter to ,. However, ADF is splitting the data based on all commas, including those within the records (e.g., ""A,QWERTY""). This results in more columns being detected in the source than in the target SQL Server table, causing the pipeline to fail.  The  d**esired output should be** 
   * Column1: ABC
   * Column2: XYZ
   * Column3: A,QWERTY  

**Question:**  
How can I handle this situation in ADF? Is there a way to configure ADF to properly parse the CSV file without splitting data within quotes? Any guidance or suggestions would be greatly appreciated!

",2,4,1736706428.0,dataengineering
1hzcsdd,My Experience as a Candidate for a Data Engineering Role at Airwallex 🚀,"I recently went through the process of being considered for a [Data Engineer](https://premvishnoi.medium.com) role at **Airwallex**, one of the most exciting fintech companies. Here’s my experience and tips for anyone aiming to join a similar environment. 🎯

  
**About Airwallex:**

• A global payments and financial platform supporting 100,000+ businesses, including SHEIN and Brex.

• Valued at **$5.6B**, backed by investors like Sequoia and Mastercard. 💳

• Solves complex financial challenges using cutting-edge technologies.



**Role Overview:**

• Build scalable ETL pipelines and data models.

• Collaborate across teams (Finance, Marketing, Product).

• Work on real-time systems like treasury and ledger operations.



**Tech Stack:**

• **Data Warehousing**: BigQuery, Snowflake, Databricks.

• **ETL Tools**: Apache Airflow, Kafka, Nifi.

• **Big Data**: Spark, Flink, Hadoop.

• **Languages**: Python, SQL, Java.

• **Cloud Platforms**: GCP, AWS.



**Steps in the Process:**

1. **Initial Discussion**:

• Focused on my background and experience with ETL and real-time pipelines.

• **Tip**: Highlight projects where you’ve used relevant tools like Airflow or BigQuery.

2. **Technical Evaluation**:

• Covered topics like SQL queries, data structures, and algorithms.

• **Tip**: Practice solving tasks like finding duplicates in databases or optimizing queries.

3. **Take-Home Task**:

• Designed a data pipeline and a schema for a financial ledger.

• **Tip**: Spend time writing clean, well-documented code.

4. **Architecture Discussion**:

• Explored scalable, fault-tolerant system designs.

• **Tip**: Use diagrams to showcase your solution and clarify requirements.

5. **Final Conversation with Leadership**:

• Talked about the company’s vision, team culture, and future opportunities.

• **Tip**: Ask meaningful questions about long-term challenges and goals.

[https://premvishnoi.medium.com](https://premvishnoi.medium.com)  
",35,6,1736649500.0,dataengineering
1hzou5y,ADF related Assistance needed ,"Hi all,


I have multiple notebook activities in my adf pipeline and I am using new job cluster in the databricks linked service. For every notebook activity it creates a new job cluster and the creation of job cluster takes time, can I use a single new job cluster to run the whole pipeline. Also, I want different notebook activities in order to display the run details on my application. The concurrency of my pipeline runs is high (15-20 runs in parallel). How can I implement this?

Thanks in advance",3,8,1736695071.0,dataengineering
1hze67u,Help Needed: Synapse vs. Snowflake for Warehousing,"Hi everyone,

I’m currently working on proposing a data warehouse solution to my manager, and I need your help deciding between **Azure Synapse Analytics** and **Snowflake**. Here's the context:

1. **Current Setup:**
   * All data is stored in **Azure SQL Database**.
   * We use **Azure Data Factory** daily to extract data and load it as **Parquet files in Azure Data Lake Storage**.
   * ADLS has **hierarchical namespace and tiered storage**, but there’s no specific partitioning strategy.
2. **Future Plans:**
   * We plan to integrate **Power BI** for dashboards and reporting.
   * Data marts will be built, but aggregations are expected to be moderately complex.
   * Current data volume is small but could grow to **GBs per month** for six tables.
3. **Workload Details:**
   * We process data in **batches**, not real-time.
   * OLAP workloads are the primary focus, and we don’t plan to add data sources outside Azure SQL Database.
4. **Considerations:**
   * How well does each platform scale for small-to-medium workloads, given that we may grow in the future?
   * Which platform performs better for moderately complex queries, especially when dealing with Parquet files in ADLS?
   * What are the pros and cons of each platform’s pricing model? How can we manage costs effectively in either solution?
   * While we don’t have a strict preference, using Synapse would minimize onboarding and operational overhead because we’re already familiar with Azure services.

I’ll be presenting this proposal to my manager, so I’d appreciate detailed feedback on which platform would be better suited for this setup. Any insights into technical recommendations, cost management strategies, or potential risks would be greatly appreciated!

Thank you in advance for your help!",6,19,1736654105.0,dataengineering
1hzgkee,Title: On-Prem vs. Cloud Data Engineers – Which is Preferred for FAANG?,"Hi Reddit,

I’m a data engineer with 2.5 years of IT experience, currently diving into the world of big data. I’ve been reflecting on whether FAANG-level companies lean more towards **on-premise data engineering expertise** or **cloud-based data engineering skills**, and what depth of knowledge is truly required for each.

Here are my questions and thoughts:

1. **On-Premise Focus**:

• When working with on-prem solutions, how deep do engineers need to go? For example, do FAANGs expect us to customize Spark at the **source code level**, optimizing for their unique infrastructure? Or is it more about managing and orchestrating existing frameworks effectively?

• What skills or knowledge would help me transition from basic usage to that expert level in on-premise systems?

2. **Cloud-Based Focus**:

• With cloud solutions dominating the industry, how deep is the backend knowledge expected of a data engineer? Do companies expect us to understand the internals of AWS, GCP, or Azure, or is it more about leveraging services like Glue, BigQuery, or Redshift efficiently?

• Would gaining knowledge about cloud infrastructure (e.g., Kubernetes, serverless computing, or networking) boost my profile as a data engineer for cloud-heavy organizations?

3. **General FAANG Expectations**:

• Are data engineers at FAANG expected to know both on-prem and cloud solutions deeply, or is specialization more valuable?

• For someone like me with 2.5 years of experience, focusing on foundational big data tools like Spark, Kafka, and Airflow, how should I prioritize learning on-prem vs. cloud in preparation for FAANG interviews?

I’d love to hear thoughts from experienced engineers, especially those working at FAANG or similar companies. Any advice or resource recommendations would be great! Punch the poll what kind of engineer are you

Thanks in advance!

\#FAANG #DataEngineer #IT #Software #MAANG

[View Poll](https://www.reddit.com/poll/1hzgkee)",5,11,1736662904.0,dataengineering
1hzghp1,flink streaming with failure recovery,"Hi everyone, i have a project for streaming process data by flink job from kafkasource to kafkasink. I have a case with handling duplicating and losing data - kafkamessage. WHen job fail or restarting, i use checkpointing to recovery task but lead to duplicate message. In some ways else, i use savepoint to save job state after sinking message, it could handle duplicate but waste time and resources. Any one who has experiences in this streaming data, could you give me some advices. Merci beaucoup and Have a good day!!!!!!!",2,1,1736662601.0,dataengineering
1i00a8n,LLMs can do way more for analytics than just converting english to SQL,"Everyone's obsessing over text2sql since chat gpt.. Converting English to SQL is just scratching the surface of what LLMs can do for analytics. SQL generation is a narrow use case that often creates more problems than it solves. Instead we should be

\- Automatically documenting data pipelines

\- Generating test cases for data quality

\- Creating natural language interfaces for entire apps

\- Building context-aware data exploration tools

Stop limiting yourself to text2sql. Start thinking about how LLMs can help your whole analytics workflow. The technology is ready - we just need to think bigger as a discipline

What creative ways are you using chatgpt/llms in your analytics work?",0,5,1736724722.0,dataengineering
1hz3ylt,how to answer this data engineering question ?,"i was asked what kind of transformations you did in experience. My answer to this was not technical enough for them. What kind of transformations will be considered technical. please someone give example. I am new in the data engineering world, wanna make sure for my next inter?

my answer ""I performed data cleaning by handling nulls, removing duplicates, standardizing formats, and encoding categorical variables into numerical values. I conducted aggregations and built data models with fact tables (e.g., purchase orders, sales invoices) where I calculated metrics like profit %, previous year profit, etc. The dimension tables included customer, vendor, item lists, and a date dimension table.""",13,22,1736624463.0,dataengineering
1hz0vsx,Mastering Common Data Engineering techniques with Apache Spark in Databricks,"Apache Spark is one of the most popular Big Data technologies nowadays. In this end-to-end tutorial, I explain the fundamentals of PySpark- data frame read/write, SQL integration, column and table level transformations, like joins and aggregates and demonstrate the usage of Python & Pandas UDFs. I also demonstrate the usage of these techniques to address common data engineering challenges like data cleansing, enrichment and schema normalization. Check out here:https://youtu.be/eOwsOO\_nRLk",17,2,1736616386.0,dataengineering
1hz2wwk,Financial Data engineering career entry - tips and suggestions,"Hey everyone, I worked as data engineer for over 4 years in various industries like online gaming, real estate and marketing and i am yet not satisfied with the data. I am looking to move into the field of finance and become a financial data engineer. The problem is that every job opportunity is asking for previous experience in financial industry. So i decided to learn some finance data structures and concepts of finance. Please give some suggestions and any help would be appreciated. My tech stack is snowflake, Aws, Airflow, DBT.",12,7,1736621691.0,dataengineering
1hyuj6c,Getting Started with the Data Engineer Handbook,,43,5,1736597210.0,dataengineering
1hyxphy,The State of Cloud Data Warehouses – 2024 Edition,"Hi, I've been writing these ""end-of-the-year"" blogs where I try to gather all the most important features released per data platform in 2024. I'll focus on the bigger names (Redshift, Google BiqQuery, Databricks and Snowflake), which I face on my job. This means that even though we use PostgreSQL+DuckDB, those are not included because those are not used as data warehouses.

Hopefully, this gives some value to someone.

[https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2024-edition](https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2024-edition)",18,1,1736607757.0,dataengineering
1hyoi77,How to visualise complex joins in your mind,"I've been working on an ETL project for the past six months, where we use PySpark SQL to write complex transformations.

I have a good understanding of SQL concepts and can easily visualize joins between two tables in my head. However, when it comes to joining more than two tables, I find it very challenging to conceptualize how the data flows and how everything connects.

Our project uses multiple CSV files as data sources, and we often need to join them in various ways. Unlike a relational database, there is no ER diagrams, which makes it harder to understand the relationships between them.

My colleague seems to handle this effortlessly. He always knows the correct join conditions, which columns to select, and how everything fits together. I can’t seem to do the same, and I’m starting to wonder if there’s an issue with how I approach this.

I’m looking for advice on how to better visualize and manage these complex joins, especially in an unstructured environment like this. Are there tools, techniques, or best practices that can help me.",106,66,1736571436.0,dataengineering
1hz232c,any insights about the market in europe in data engineering ?,"i have covered a data engineering related project in my uni , last semester 

and i liked it tbh 

i thought about exploring this road but i wanted to know its prospects ( market wise ) in Europe 

from my small research , jobs in data engineering offers  require skills in cloud computing as well . 

any advice or insights about the demand for the specialty  would be highly appreciated and thank you ",6,8,1736619503.0,dataengineering
1hy6if0,Small Data: Returning to Our Roots,,207,13,1736521851.0,dataengineering
1hytseo,I have a data lake setup with incremental daily uploads but I am so lost on how to do backfill and what would be the best way forward,"My pipeline is in AWS and involves S3 and Glue. I have two glue crawlers. One that takes my JSON string data and turns it into a table and another that does ETL to make it database ready like turning the date strings in the JSON data to instead be a timestamp. I can probably improve this in the future because some aspects seem redundant.



I also had asked a similarly worded question in the past but I am just as lost now as I was then so I am hoping to get lucky with a satisfying answer that helps to avoid in me a sense of dread that I am doing something wrong

1.  I have a helper app on my phone that automates sending a post request every day. I can get get a manual export that dumps earlier data to my computer. From here I was thinking one thing I can do is set up a python server and push all these payloads as POST request with a loop and delay so I do not overwhelm my basic API configuration
2. Similar to 1. and something that I really want to know about because I am drawn to simplistic solutions. I also feel like I can skip the API part and create a routine to flatten and filter my historical download into JSON files separated by day. With one big local directory containing many folders I can then upload it in one go into my s3 bucket because folder structure automatically gets converted to prefixes. This seems so obvious to me but I am worried that no one seems to do this
3. Another thing is to do backfill with a one time glue job in pyspark. There are advantages I foresee to doing it in a glue job. I am dealing with several GB and that seems to place huge stress on my computer with limited ram. However one trade off I foresee is that it is harder to debug the scripts and there is no variable window
4. My last resort and something I would choose reluctantly is to suck it up and learn apache airflow. I would be keen if it would be better off in terms of career growth",3,6,1736594057.0,dataengineering
1hyrkif,Guidance on using Dagster and hiring DE’s on an as-needed / project basis,"We're a new startup, and we've been building our data pipelines using Duckdb (with geospatial) and [https://hamilton.dagworks.io/](https://hamilton.dagworks.io/) DAG engine. 

We've been using local-dev container setup with all the tools installed in the docker container. This decision was made because the amount of data is not much, only a few gigabytes. 

But our pipelines are fairly complex -- external services are used for matching because there is no foreign key between data (Typesense is used for in-memory search), external algorithm services, H3 geospatial calculations, etc.  Unit testing is also done.

Since our pipelines are so complex, we are hitting the bottleneck of our mini DAG system. I'm not a data engineer and looking for guidance. 

1. Is Dagster the right solution for us? 

2. How to find someone that can help out on a project basis. 

3. It feels like I need someone who has previously built complex Dagster pipelines because this project already has all the code, we need someone with expertise in architecture, code structuring and software engineering practices to help us cleanup our pipelines? Am I thinking in the right direction?",6,9,1736584058.0,dataengineering
1hydbqq,Whats new in dbt 1.9?,"**Hey dbt users! 👋**

The latest release of **dbt 1.9** is here, and it’s packed with exciting updates that can make your data workflows more efficient and powerful.

To keep you ahead of the curve, we combed through the release notes and docs to pull out the highlights, key features, and compatibility considerations—so you don’t have to. 

👉 [Read the Full Article Here](https://datacoves.com/post/dbt-1-9).

Have you started exploring dbt 1.9? Which features are you most excited about? If there’s something we didn’t cover or a feature in this article you’re eager to take advantage of?. We’d love to hear your thoughts!",39,7,1736539035.0,dataengineering
1hyd0pm,PostgreSQL to Snowflake - Change Data Capture (CDC) Streaming,"Hey folks, I've been seeing a lot of questions floating around about what's the most optimal way of moving data from PostgreSQL to Snowflake so I thought I write up a hands-on tutorial using [Estuary Flow](https://estuary.dev/) and along the way talk about some best practices. By the end, you’ll have learned everything you need to know about building a pipeline on your own.

You'll use Flow's PostgreSQL capture connector and Snowflake materialization connector to set up an end-to-end CDC pipeline in three steps:

* First, you’ll ingest change event data from a PostgreSQL database, using a table filled with generated realistic product data.
* Then, you’ll learn how to configure Flow to persist data as collections while maintaining data integrity.
* And finally, you will see how you can materialize these collections in Snowflake to make them ready for analytics!

Before you get started, make sure you do two things.

1. Sign up for Estuary Flow [here](https://dashboard.estuary.dev/register). It’s free!
2. Make sure you also join the Estuary [Slack Community](https://go.estuary.dev/slack).

# What is CDC?

CDC, or Change Data Capture, is a method used to track and capture changes made to data in a database. It enables the real-time capture of insertions, updates, and deletions, providing a continuous stream of changes.

This stream of change data is invaluable for keeping downstream systems synchronized and up-to-date with the source database, facilitating real-time analytics, replication, and data integration. In essence, CDC allows organizations to capture and react to data changes as they occur, ensuring data accuracy and timeliness across their systems. CDC provides a lower-latency, lower-load way to extract data. It’s also often the only way to capture every change as well as deletes, which are harder to track with batch-based extraction.

If you are interested in the intricacies of change data capture, head over to [this](https://estuary.dev/cdc-done-correctly/) article, where we explain the theory behind it - this is not a requirement for this tutorial, so if you want to dive in head first, keep on reading!

# Prerequisites

This tutorial will assume you have access to the following things:

* Docker: for convenience, we are providing a docker compose definition which will allow you to spin up a database and a fake data generator service in about 5 seconds!
* ngrok: Flow is a fully managed service. Because the database used in this tutorial will be running on your machine, you’ll need something to expose it to the internet. ngrok is a lightweight tool that does just that.
* Snowflake account: The target data warehouse for our flow is Snowflake. In order to follow along with the tutorial, a trial account is perfectly fine.

# Step 1. Set Up The Source Database

# PostgreSQL setup

As this tutorial is focused on CDC replication from PostgreSQL, we’ll need a database. We recommend you create this database first, so you can learn Flow more easily. Then try these steps on your own database. Let’s take a look at what we are working with!

Save the below yaml snippet as a file called docker-compose.yml. This docker-compose.yml file contains the service definitions for the PostgreSQL database and the mock data generator service.

>Since V2, compose is integrated into your base Docker package, there’s no need to download any separate tooling!

    docker-compose.yml
    services:
      postgres:
        image: postgres:latest
        container_name: postgres_cdc
        hostname: postgres_cdc
        restart: unless-stopped
        user: postgres
        environment:
          POSTGRES_USER: postgres
          POSTGRES_DB: postgres
          POSTGRES_PASSWORD: postgres
        command:
          - ""postgres""
          - ""-c""
          - ""wal_level=logical""
        healthcheck:
          test: [""CMD-SHELL"", ""sh -c 'pg_isready -U flow_capture -d postgres'""]
          interval: 5s
          timeout: 10s
          retries: 120   
        volumes:
          - ./init.sql:/docker-entrypoint-initdb.d/init.sql 
        ports:
          - ""5432:5432""
    
      datagen:
        image: materialize/datagen
        container_name: datagen
        restart: unless-stopped
        environment:
          POSTGRES_HOST: postgres_cdc
          POSTGRES_PORT: 5432
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        entrypoint:
          ""datagen -s /app/schemas/products.sql -n 10000 -f postgres -w 1000""
        depends_on:
          postgres:
            condition: service_healthy
    
        volumes:
              - ./schemas/products.sql:/app/schemas/products.sql

Don’t be alarmed by all these Docker configurations, they are made to be reproducible on any machine, so you don’t have to worry about modifying anything in them! Before you spin up the database, let’s take a quick look at what exactly you can expect to happen.

Next up, create a folder called schemas and paste the below SQL DDL into a file called products.sql. This file contains the schema of the demo data.

>This file defines the schema via a create table statement, but the actual table creation happens in the init.sql file, this is just a quirk of the Datagen data generator tool.

    products.sql
    CREATE TABLE ""public"".""products"" (
      ""id"" int PRIMARY KEY,
      ""name"" varchar COMMENT 'faker.internet.userName()',
      ""merchant_id"" int NOT NULL COMMENT 'faker.datatype.number()',
      ""price"" int COMMENT 'faker.datatype.number()',
      ""status"" varchar COMMENT 'faker.datatype.boolean()',
      ""created_at"" timestamp DEFAULT (now())
    );

If you take a closer look at the schema definition, you can see that in the `COMMENT` attribute we define Python snippets which actually tell Datagen how to generate fake data for those fields!

Finally, create the `init.sql` file, which contains the database-level requirements to enable Flow to stream CDC data.

    CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;
    
    GRANT pg_read_all_data TO flow_capture;
    
    CREATE TABLE products (
      ""id"" int PRIMARY KEY,
      ""name"" varchar COMMENT 'faker.internet.userName()',
      ""merchant_id"" int NOT NULL COMMENT 'faker.datatype.number()',
      ""price"" int COMMENT 'faker.datatype.number()',
      ""status"" varchar COMMENT 'faker.datatype.boolean()',
      ""created_at"" timestamp DEFAULT (now())
    );
    
    CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT);
    GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture;
    CREATE PUBLICATION flow_publication;
    ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true);
    ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;

In the `init.sql` file, you create the products table and all the database objects Flow requires for real-time CDC streaming.

Configuring PostgreSQL for CDC To enable CDC replication in PostgreSQL, several database objects need to be created and configured. These objects facilitate the capture and propagation of data changes to downstream systems. Let's examine each object and its significance in the context of CDC replication:

    CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;

This user is dedicated to the CDC replication process. It is granted the necessary permissions to read all data from the database, allowing it to capture changes across tables efficiently. In a production environment, make sure you use a more secure password than what is in the example.

    GRANT pg_read_all_data TO flow_capture;

Granting the pg\_read\_all\_data privilege to the flow\_capture user ensures that it can access and read data from all tables in the database, essential for capturing changes.

>pg\_read\_all\_data is used for convenience, but is not a hard requirement, since it is possible to grant a more granular set of permissions. For more details check out the connector docs.

    CREATE TABLE products (...)

The source tables, such as the products table in this example, contain the data whose changes we want to capture and replicate. It is recommended for tables to have a primary key defined, although not a hard requirement for CDC.

    CREATE TABLE IF NOT EXISTS public.flow_watermarks (...)

The `flow_watermarks` table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents.

    GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture;

The flow\_capture user needs full privileges on the flow\_watermarks table to insert, update, and query metadata related to the replication process.

    CREATE PUBLICATION flow_publication;
    ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true);
    ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;

A publication defines a set of tables whose changes will be replicated. In this case, the flow\_publication publication includes the public.flow\_watermarks and public.products tables.

These commands configure the flow\_publication publication to publish changes via partition root and add the specified tables to the publication. By setting publish\_via\_partition\_root to true, the publication ensures that updates to partitioned tables are correctly captured and replicated.

>The table in this tutorial is not partitioned, but we recommend always setting publish\_via\_partition\_root when creating a publication.

These objects form the backbone of a robust CDC replication setup, ensuring data consistency and integrity across systems. After the initial setup, you will not have to touch these objects in the future, unless you wish to start ingesting change events from a new table.

With that out of the way, you’re ready to start the source database. In order to initialize Postgres and the fake data generator service, all you have to do is execute the following (to free up your current terminal, use the -d flag so the containers run in a daemonized background process):

    docker compose up

After a few seconds, you should see that both services are up and running. The postgres\_cdc service should print the following on the terminal:

    postgres_cdc  | LOG:  database system is ready to accept connections

While the `datagen` service will be a little bit more spammy, as it prints every record it generates, but don’t be alarmed, this is enough for us to verify that both are up and running. Let’s see how we can expose the database so Flow can connect to it.

Expose the database to the internet via ngrok As mentioned above, the next step is to make the database available for other services. To do this in one quick command, we can use ngrok, a free CLI tool that enables tunneling of services. In our case we only want to expose the port 5432 and only the tcp protocol.

    ngrok tcp 5432

[ngrok](https://preview.redd.it/ejta9eu2w7ce1.png?width=1136&format=png&auto=webp&s=015e834cd08595141ceec1362c14cb3226b282a0)

You should immediately be greeted with a screen that contains the public URL for the tunnel we just started! In the example above, the public URL [5.tcp.eu.ngrok.io:14407](http://5.tcp.eu.ngrok.io:14407) is mapped to localhost:5432, which is the address of the Postgres database.

\> Don’t close this window while working on the tutorial as this is required to keep the connections between Flow and the database alive.

Before we jump into setting up the replication, you can quickly verify the data being properly generated by connecting to the database and peeking into the products table, as shown below:

    ~ psql -h 5.tcp.eu.ngrok.io -p 14407 -U postgres -d postgres Password for user postgres: psql (16.2) Type ""help"" for help.
    postgres=# \d List of relations Schema |      Name       | Type  |  Owner --------+-----------------+-------+---------- public | flow_watermarks | table | postgres public | products        | table | postgres (2 rows)
    postgres=# select count(*) from products; count
    2637 (1 row)
    postgres=# select count(*) from products; count
    2642 (1 row)

By executing a count(\*) statement a few seconds apart you are able to verify that data is in fact being written into the table.

# Step 2. Set Up a Capture

Good news, the hard part is over! Smooth sailing from here on out. Head over to your Flow dashboard (if you haven’t registered yet, you can do so here.) and create a new Capture. A capture is how Flow ingests data from an external source. Every Data Flow starts with a Capture.

Go to the sources page by clicking on the Sources on the left hand side of your screen, then click on + New Capture

[Add new Capture](https://preview.redd.it/fmv9m9tcw7ce1.png?width=1600&format=png&auto=webp&s=fedfa77aa80d3a309bfa8c6b376fc4a2b21fa2cf)

Configure the connection to the database based on the information we gathered in the previous step and press Next.

Configure Capture

[Configure Capture](https://preview.redd.it/efk389tcw7ce1.png?width=1600&format=png&auto=webp&s=2169a773a3665424904f665d3083d6f431ea09df)

On the following page, we can configure how our incoming data should be represented in Flow as collections. As a quick refresher, let’s recap how Flow represents data on a high level.

**Documents**

The documents of your flows are stored in collections: real-time data lakes of JSON documents in cloud storage. Documents being backed by an object storage mean that once you start capturing data, you won’t have to worry about it not being available to replay – object stores such as S3 can be configured to cheaply store data forever. See docs page for more information.

**Schemas**

Flow documents and collections always have an associated schema that defines the structure, representation, and constraints of your documents. In most cases, Flow generates a functioning schema on your behalf during the discovery phase of capture, which has already automatically happened - that’s why you’re able to take a peek into the structure of the incoming data!

To see how Flow parsed the incoming records, click on the Collection tab and verify the inferred schema looks correct.

[Configure Collections](https://preview.redd.it/eb17f8tcw7ce1.png?width=1600&format=png&auto=webp&s=2466327f79f560ebd1f2096c7beba3288d99da2f)

Before you advance to the next step, let’s take a look at the other configuration options we have here. You’ll see three toggles, all turned on by default:

* Automatically keep schemas up to date
* Automatically add new source collections
* Breaking changes re-version collections

All of these settings relate to how Flow handles schema evolution, so let’s take a quick detour to explain them from a high-level perspective.

Estuary Flow's schema evolution feature seamlessly handles updates to dataset structures within a Data Flow, ensuring uninterrupted operation. Collection specifications define each dataset, including key, schema, and partitions. When specs change, schema evolution automatically updates associated components to maintain compatibility.

It addresses breaking changes by updating materializations or recreating collections with new names, preventing disruptions. Common causes of breaking changes include modifications to collection schemas, which require updates to materializations.

Overall, schema evolution streamlines adaptation to structural changes, maintaining smooth data flow within the system.

For more information, check out the dedicated documentation page for schema evolution.

For the sake of this tutorial, feel free to leave everything at its default setting and press Next again, then Save and Publish to deploy the connector and kick off a backfill.

# Step 3. Set up a Materialization

Similarly to the source side, we’ll need to set up some initial configuration in Snowflake to allow Flow to materialize collections into a table.

Preparing Snowflake for use with Estuary Flow involves the following steps:

1. Keep the Flow web app open and open a new tab or window to access your Snowflake console.
2. Create a new SQL worksheet. This provides a platform to execute queries.
3. Paste the provided script into the SQL console, adjusting the value for estuary\_password to a strong password.

&#8203;

    set database_name = 'ESTUARY_DB';
    set warehouse_name = 'ESTUARY_WH';
    set estuary_role = 'ESTUARY_ROLE';
    set estuary_user = 'ESTUARY_USER';
    set estuary_password = 'secret';
    set estuary_schema = 'ESTUARY_SCHEMA';
    -- create role and schema for Estuary
    create role if not exists identifier($estuary_role);
    grant role identifier($estuary_role) to role SYSADMIN;
    -- Create snowflake DB
    create database if not exists identifier($database_name);
    use database identifier($database_name);
    create schema if not exists identifier($estuary_schema);
    -- create a user for Estuary
    create user if not exists identifier($estuary_user)
    password = $estuary_password
    default_role = $estuary_role
    default_warehouse = $warehouse_name;
    grant role identifier($estuary_role) to user identifier($estuary_user);
    grant all on schema identifier($estuary_schema) to identifier($estuary_role);
    -- create a warehouse for estuary
    create warehouse if not exists identifier($warehouse_name)
    warehouse_size = xsmall
    warehouse_type = standard
    auto_suspend = 60
    auto_resxme = true
    initially_suspended = true;
    -- grant Estuary role access to warehouse
    grant USAGE
    on warehouse identifier($warehouse_name)
    to role identifier($estuary_role);
    -- grant Estuary access to database
    grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role);
    -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP)
    use role ACCOUNTADMIN;
    grant CREATE INTEGRATION on account to role identifier($estuary_role);
    use role sysadmin;
    COMMIT;

5. Execute all the queries by clicking the drop-down arrow next to the Run button and selecting ""Run All.""

6. Snowflake will process the queries, setting up the necessary roles, databases, schemas, users, and warehouses for Estuary Flow.

7. Once the setup is complete, return to the Flow web application to continue with the integration process.

Back in Flow, head over to the Destinations page, where you can create a new Materialization.

[Add New Materialization](https://preview.redd.it/mpnsrnucw7ce1.png?width=1600&format=png&auto=webp&s=6aa6abf1f8f318407dcb23b6629447314dc3187a)

Choose Snowflake and start filling out the connection details based on the values inside the script you executed in the previous step. If you haven’t changed anything, this is how the connector configuration should look like:

[Configure Materialization Endpoint](https://preview.redd.it/so7i19tcw7ce1.png?width=1600&format=png&auto=webp&s=ce0fd48d4c078c84ad88c75eae736b36d1e0adb7)

You can grab your Snowflake host URL and account identifier by navigating to these two little buttons on the Snowflake UI.

[Grab Snowflake account ID](https://preview.redd.it/kqzc19tcw7ce1.png?width=1600&format=png&auto=webp&s=dd43088c01e2641e6525dcf17dbaee1fba58ff82)

If you scroll down to the Advanced Options section, you will be able to configure the ""Sync Frequency"" parameter. If you leave this parameter unset, the default value of 30 minutes will be used

The Sync Frequency parameter in Estuary materializations offers a flexible approach to data ingestion scheduling. It represents the amount of time the system will wait before it begins materializing the latest data.

For example, if a Sync Frequency is set to 2 hours, the materialization task will pause for 2 hours before processing the latest available data. This delay ensures that data is not pulled in immediately after it becomes available, allowing your Snowflake warehouse to go idle and be suspended in between updates, which can significantly reduce the number of credits consumed.

After the connection details are in place, the next step is to link the capture we just created to Flow is able to see collections we are loading data into from Postgres.

You can achieve this by clicking on the “Source from Capture” button, and selecting the name of the capture from the table.

[Link Capture](https://preview.redd.it/27oq5atcw7ce1.png?width=1600&format=png&auto=webp&s=508a02e8057322a401e981a73e3a66b0121c4d3d)

After pressing continue, you are met with a few configuration options, but for now, feel free to press Next, then Save and Publish in the top right corner, the defaults will work perfectly fine for this tutorial.

A successful deployment will look something like this:

[Successful Deployment Logs](https://preview.redd.it/9c6nu8tcw7ce1.png?width=1600&format=png&auto=webp&s=6caa9a09f8400e0f7a4c95fdefc178f32eb55e84)

And that’s pretty much it, you’ve successfully published a real-time CDC pipeline. Let’s check out Snowflake to see how the data looks.

[Results in Snowflake](https://preview.redd.it/svyto9tcw7ce1.png?width=1600&format=png&auto=webp&s=c24b346dc1bf9c645be9a5e96c7b0748f9f533d7)

Looks like the data is arriving as expected, and the schema of the table is properly configured by the connector based on the types of the original table in Postgres.

To get a feel for how the data flow works, head over to the collection details page on the Flow web UI to see your changes immediately. On the Snowflake end, they will be materialized after the next update.

\> Based on your configuration of the ""Sync Frequency"" parameter when setting up the Snowflake Materialization, you might have to wait until the configured amount of time passes for your changes to make it to the destination.

Party time! Congratulations! 🎉 You've successfully set up a CDC pipeline from PostgreSQL to Snowflake using Estuary Flow. In just a few minutes, you've learned how to configure log-based CDC replication, handle schema evolution, and deploy a robust data integration solution.

Take a moment to celebrate your achievement! You've not only gained valuable technical knowledge but also demonstrated the agility and efficiency of modern data engineering practices. With your newfound skills, you're well-equipped to tackle complex data challenges and drive innovation in your organization.

# Clean up

After every party, it’s customary to clean up after ourselves. After you are done exploring the flow, make sure to remove any resources which you won’t use anymore!

**Postgres**

Simply stop the Docker containers & terminate the ngrok process. This will terminate both the database process and the tunnel which exposed it to the internet.

**Snowflake**

To clean up resources in Snowflake use the following SQL script.

    -- Revoke all privileges from Estuary role
    revoke all privileges on schema ESTUARY_SCHEMA from role ESTUARY_ROLE;
    revoke all privileges on database ESTUARY_DB from role ESTUARY_ROLE;
    -- Drop the warehouse
    drop warehouse if exists ESTUARY_WH;
    -- Drop the role and user
    drop user if exists ESTUARY_USER; drop role if exists ESTUARY_ROLE;
    -- Drop the schema and database drop schema if exists ESTUARY_SCHEMA;
    drop database if exists ESTUARY_DB;

**Flow**

In the UI, disable or delete any resources you don’t wish to keep.

# Next Steps

That’s it! You should have everything you need to know to create your own data pipeline for loading data into Snowflake!

Now try it out on your own PostgreSQL database or other sources.

If you want to learn more, make sure you read through the Estuary documentation. Also, don’t forget to join the Estuary [Slack Community](https://go.estuary.dev/slack)! Let me know if I should post more long-format tutorials like this.",34,5,1736538246.0,dataengineering
1hygt3p,New to data: Curious about real-time pipelines!,"Hey everyone,  
I’m totally new to the data field and just started learning Python. One topic I already find super fascinating is ""real-time data."" It seems to be getting more and more important, right?

Has your company started implementing real-time data? What challenges have you faced with real-time data pipelines?

I’d love to hear your experiences!",20,41,1736547971.0,dataengineering
1hymovu,What do you expect from vendors?,"Genuine question guys

As a Vendor what’s the best way to approach Data Engineering teams? 

- Reach out to managers/directors? 
- Email, phone, LinkedIn?
- Send specific product details?
- just don’t and crawl back into my dark little sales cave ",9,7,1736565243.0,dataengineering
1hybfly,What data governance tools are you using in 2025?,"The last time this question was asked on this sub, it was 2 years ago. I've been seeing a lot of data governance tools cropping up like Collibra, Atlan, Monte Carlo, Secoda. Does anyone use these? And if not, what do you use?

I feel as if data governance is more of a cultural practice, but I am seeing more tools to help facilitate governance practices. Wdty?",23,39,1736534302.0,dataengineering
1hyc7oi,To Bundle or Not to Bundle: How the Data Ecosystem Stands Still,,12,1,1736536224.0,dataengineering
1hygffx,Where and how to learn Purview,"Hey everyone!

Finally joining next coming week on my first ever Data Governance project (bank industry in this case).

I'd like to explore more about MS Pureview. What is the best way to get some ideas about it? Besides the documentation...  
Any YT channel explaining this tool on a good way? Any crashcourse or something?   
All DG tools have this downside: or you're working with them purely in DG, or it seems impossible to have contact with them (for obvious reasons).

Thanks!",4,8,1736546967.0,dataengineering
1hxdbn8,End to End Data Engineering,,1352,65,1736431007.0,dataengineering
1hy8gqq,Testing approach for Databricks especially Spark,"I'm used to using PyTest, Behave and Soda in my data engineering. I've just started learning DataBricks and, while a lot of it is very familiar, I could do with a leg up with regard to best approaches for testing Databricks jobs and DataBricks code.
",10,5,1736526931.0,dataengineering
1hy7pfx,Storing and organizing ETL queries for enterprise level projects,"Hey folks, just for some context I’m a data engineer with about 6 months of experience, and all the members of my team only have experience working with Informatica to build ETL pipelines. We’re currently working on modernizing our tech stack, and making the move to Python. Since I’m the only one with any meaningful Python experience, my team has essentially left it up to me to design our entire ETL infrastructure. One of the problems I’m facing is in storing and accessing SQL queries for the pipelines. The approach we’re using right now is to just store the queries in a Python file as variables and import them where needed. While this solution is simple and does work for the time being, I know it’s not going to be feasible as time goes on. I was interested in knowing what the industry standard is for storing and accessing queries for enterprise ETL systems. ",10,20,1736524961.0,dataengineering
1hy1x8m,Should I learn all of Python or just focus on Airflow/PySpark?,"Hello! I'm a junior data engineer, and most of my work revolves around PySpark and Airflow. I’m trying to figure out how to grow further in my career but feel a bit lost about where to focus.

Should I spend time reading huge Python books, building complex pet projects, and diving into advanced stuff like design patterns? Honestly, it feels like overkill since my day-to-day work mostly involves writing small scripts. At the same time, there are books like *The Data Warehouse Toolkit*, *Data Internals*, and *Spark: The Definitive Guide* that seem super relevant, and I’d like to focus on them instead.

What do you think? How did you decide where to focus early in your data eng career?",16,12,1736506693.0,dataengineering
1hy4xno,S3 or Redshift for storing Geolocation Data in AWS,"We just took a project to build an analytics store by merging different proprietary geolocation data vendors. 

The client wants final processed data to be stored in their AWS and wants us to figure out best analytics store in terms of query performance and cost.

The final result from that processed data will be visualized in QGIS which will also be hosted on their AWS. 

We are merging 3 data sources and each data source is around 90-100 GB in size (so total ~300 GB).

Can you recommend a best storage layer whether we should go with Redshift or S3/Parquet for GIS data? 

Other recommendations on AWS are welcome too - as long as it interfaces with QGIS, is cost effective and query performance is optimal.",8,14,1736517408.0,dataengineering
1hxmoqe,Is it just me or has DE become unnecessarily complicated?,When I started 15 years ago my company had the vast majority of its data in a big MS SQL Server Data Warehouse. My current company has about 10-15 data silos in different platforms and languages. Sales data in one. OPS data in another. Product A in one. Product B in another. This means that doing anything at all becomes super complicated.,150,85,1736455290.0,dataengineering
1hyd4i4,Need some help with DW SQL,"Hey all.

Recently I´ve been having some issues in my Azure SQL DW in test environment with a query taking several hours (stored procedure).

I've since then tried some stuff like change TEST to same DWU as PROD, still same issue and looking around on the interwebs i found out that it might be due to RoundRobinMoveOperation/ShuffleMoveOperation which can take over 120+ minutes to read 200k rows which is weird?

Do you guys have any idea how I can fix this? It's the same distribution on PROD as TEST, but i still get this issue, even updated statistics for the tables affected. It's a stored procedure with several joins but, if there is no issue in prod, why is there an issue in test while it is the same code and same performance plan/DWU etc. everything is identical between test and prod lol

hope y´all have a nice day regardless if u can help me or not =)



thanks

joe",2,1,1736538525.0,dataengineering
1hxso2g,Do I have to go back to basics of programming?,"Here I am, a career-shifter with over 2 years experience. A door opened up to me and introduced me to Data Engineering. I think I have learned a lot and I would say that I can perform my job well.

However, staying on this sub has made me realize I knew only what I had to learn in my current job. If I go to next job I might have a problem. Sometimes I feel like you guys are talking Chinese here: lot's of terms and ideas I do not understand.

Is it necessary for me to learn the basics of programming to have a deeper understanding of DE?

Say for instance, if I learn CS50, would that be beneficial to me?",40,30,1736471544.0,dataengineering
1hy8i0s,"Big Data engineer with 1.5 yoe, need to know what are the next steps to advance in this field. ","Hello everyone,

I joined this company fresh from campus and was fortunate enough to be given this role as it seems interesting and quite different from what I was expecting my job as a sde to be. 

As I said in the title, I have 1.5 years of experience majorly on Apache spark (Scala), Hive, Oracle SQL, Hadoop, Databricks and Airflow. (70% of my work involves spark, hive and Airflow).

I want to enhance my skills further in this field as I genuinely find this field interesting. I would be quite grateful if y'all could answer a series of my questions or atleast some of them.
 
1. Should I give preference on getting certifications or focus on having a personal project portfolio (Right now I haven't done any). Most of my projects as a student involved web development and ML (python).
 
2. Does it matter which language I choose to work on spark with? If yes, I see most of the courses are done in pyspark over scala. Should I switch to using pyspark when working on personal projects or Scala is fine?

3. What other technologies should I focus on learning and make projects using? 

4. Please provide a list of courses/ youtube playlists/certifications/ ideas for personal projects. 

5. Should I also spend some time in learning cloud computing?

Thank you so much in advance for taking time to reply to my doubts. (I know I am asking for a lot). 

Hope to return the favour someday :).  ",2,6,1736527024.0,dataengineering
1hyd49j,Took me a week to get a ml.g6.xlarge for SageMaker?,Took a whole week for AWS to grant my service quota limit increase to get 1 ml.g6.xlarge for endpoint usage for SageMaker. By far the longest time it has ever taken me to get access to anything in AWS. Was curious if this has happened to anyone else?,0,2,1736538506.0,dataengineering
1hy4ezh,Need Help with Fabric for a large scale migration ,"We are exploring the **Database Mirroring** feature for an enterprise client to mirror their **7,200+ Azure SQL Managed Instance databases** to **Fabric** for data warehousing workloads. While this feature aligns well with most requirements, certain critical limitations pose challenges. As per the official Fabric documentation, the following database-level limitations exist:  


* *Fabric Mirroring is supported only on a writable primary database.*
* *Databases with Change Data Capture (CDC), Transactional Replication, or existing mirroring in another Fabric workspace cannot be mirrored.*

We aim to combine **Database Mirroring** with **Event Streaming** to leverage both batch and real-time analytics. However, event streaming typically relies on **CDC**, which is not permitted for mirrored databases.  
Challenges  


* The limitations exclude solutions like CDC or transactional replication, which are essential for our use case.
* We are struggling to identify the right combination of services to integrate Database Mirroring and Event Streaming effectively.
* Due to the complexities on the source side, capturing events directly from the sources is not feasible, necessitating the use of a database as an intermediary layer.

We need eventstreaming for downstream applications and move data to Onelake for datawarehousing.

Each database has identical schemas   
Realtime event streaming for 15 tables out of 900

What would be the best way to approach this?",3,0,1736515798.0,dataengineering
1hxnysl,Amazon Data Engineering Interviews prep call - why no dimensional modeling?,"I am less than a week away from my virtual on-site Amazon Data EngineerInterview and some of the things prep-call recruiter suggested for me to focus for my technical rounds were - unit and integration testing, designing ETL workflows and performance tuning (normalization etc), big data processing and data architecture design (speed and memory tradeoffs). No mention of Dimensional Modeling (he said we don't focus on system design for Data Engineering interviews) which is weird as thats what I hear everyone talk about when it comes to these rounds.

But didn't emphasize on SQL and Python based questions at all and said they weren't important for these rounds, I am confused as that is what I was mainly focussing on.

What resources do you suggest for reading and practicing unit and integration testing? For the other parts I will talk about my experience with Azure Data Engineering ecosystem (my background)",40,37,1736458587.0,dataengineering
1hy7q2b,Polars: interprets dates as str,Any solution?,1,2,1736525008.0,dataengineering
1hy10yv,Stream Processing Resources,I want to learn stream processing preferably a video course that teaches the concepts from the first principles. I don't want a surface level youtuber course. can someone help me here.,3,2,1736502752.0,dataengineering
1hxly2b,Is Big Data Significantly Harder Than Small Data?,"My career has only been at small companies where the datasets are small. My definition of small is something like 10's of thousands to 10's of millions of rows. So whenever I've run Spark jobs, I have never encountered the limits of scalability. Throughout some job interviews, the interviewer has expressed hesitation in hiring me. I'm guessing I can pick up big data just fine, but it has been a hindrance to getting offers. So is it that much harder?",28,22,1736453390.0,dataengineering
1hxrsmr,How Do You Navigate Changing Jobs in Data Engineering?,"Hey everyone,

I recently started thinking about how tricky it can be to change jobs in data engineering. Each company seems to have its own unique tech stack and tools—stuff you might not have used before. Plus, the first few weeks are hectic: learning the business domain, understanding existing pipelines, and getting up to speed with the team’s workflow.

Balancing all that while also picking up new tools or technologies can feel overwhelming. So, how do you guys manage this?
	•	Do you focus on business knowledge first and pick up tools as you go?
	•	Or do you prioritize getting comfortable with the tech first?
	•	Any tips on making the transition smoother?

Would love to hear how you’ve handled this kind of challenge!",11,10,1736468959.0,dataengineering
1hyfmub,Databricks is data engineering on easy mode ,This is a change my mind post: I’ve recently started working with delta live tables and am astounded at how easy it is to build a pipeline that works with such little tweaking. Is this the future of data engineering? ,0,28,1736544890.0,dataengineering
1hy8fgo,Can anyone suggest a good article for incremental data load into a data warehouse? I’m exploring possible opportunity for the same.,Can anyone suggest a good article for incremental data load into a data warehouse? I’m exploring possible opportunity for the same.,0,9,1736526839.0,dataengineering
1hxlaj0,TIL Slack bot sdk is super easy to use,"I wrote my first slack bot alert using their sdk and set it up on a cron job...it was surprisingly easy, I was done in < 1 hour. I just saved one of the finance guys 60 hours a year since he doesn't have to post a daily alert that took him \~15-20 minutes per day...and he would get chewed out if it ""wasn't on time"".

It's surprising in the age of AI and automation, that managers still want people to make manual posts when the company has resources that can automate them. Anyone experience this?",22,19,1736451763.0,dataengineering
1hycijt,Could My Skills Help Me Transition Into Data Engineering? Advice Needed!,"I’m currently a fellow with over 3 years of experience in data-focused roles, and I’m considering transitioning into data engineering. Here’s a quick rundown of what I’ve been working on:

* **Data Collection and Analysis**: Using Python, R, and tools like QGIS/ArcGIS to handle large datasets, apply statistical techniques, and generate insights.
* **Data Integration and Automation**: Building algorithms, automating data pipelines, and integrating APIs for real-time data exchange.
* **Visualization and Reporting**: Creating visual tools and reports to help stakeholders make data-driven decisions.
* **Project Oversight**: Collaborating with teams and managing resources on large public health projects.

I also hold an MPH and certifications in Python Data Wrangling and GIS for Global Health. While I’ve mostly been in data science-adjacent roles, I’m wondering if my current skills (and a bit of upskilling) would make it possible to break into data engineering.

I’d love to hear from folks currently in the field:

* Based on my experience, do you think transitioning into data engineering is realistic?
* What key skills, tools, or certifications should I focus on to make the switch (e.g., SQL, cloud platforms, big data tools)?
* Any advice on how to position myself for this shift or what kinds of projects to build for my portfolio?",0,4,1736536959.0,dataengineering
1hy2bdn,Moving Google Sheet Data to Graph Databases,"Hi,

I work with a small company which uses Google sheets for its daily operation and they want a kind of a system that could be used to see the relations among different entities and then later on build recommendation systems on top of that data. The database that make sense is graph database, so I am starting with writing ETLs jobs using python and pandas and then figure out different relations among data in sheets and insert in GRAPH DB ( knowledge graphs ). Due to the nature of the business the new sheets and even forms gets created very often and information stored in different places all the time and it seems like a lot of ETLs to be written and then keep track of changes in the sheets to be reflected in graph db is a must and looks like a nightmare.

Can anyone suggest the best method or process or have something similar ? Any techstack that could be more handy as this work is to be done by 1 person only in a short span of time?

Thanks",0,1,1736508253.0,dataengineering
1hxv8d8,Any Udemy Course/YouTube Playlists/Blogs for Databricks admin stuffs?,Please suggest,4,4,1736479493.0,dataengineering
1hy0jc2,Data modelling,"https://medium.com/@renan.de.moraes777/mastering-data-modelling-a-comprehensive-guide-to-relational-foundations-in-sql-7fd1a65ea03f

Glad to help ",0,3,1736500528.0,dataengineering
1hxugz7,"First Internship, how to make the most out of it?","Hi all, I'm a Second Year CS student that was fortunate enough to land a data engineering internship in one of my country's statutory boards.

The job scope will mainly be building ETL and CICD pipelines for LLM products and RAG optimisation on Azure, with some MLOps down the line.

This is my first proper internship and I want to make the most out of it, any suggestions would be appreciated. I'm currently combing through their documentation and repos to try to understand how their codebase works.",4,6,1736477107.0,dataengineering
1hy4z7k,Are the DE jobs disappearing?,"Recently, I have been casually looking for DE jobs. Still, everything that I find is DevOps jobs or something more hybrid, involving a software development background and technologies like React or Typescript. What are your overall impressions of DE in 2025?

*Edit: thak you guys, the replies were very helpful. ",0,14,1736517541.0,dataengineering
1hxnmbq,Is Kafka high in demand?,"Recently I started implementing Kafka in clients codebases writing consumers and producers and I figured that is somewhat a pita to learn and implement, at least during the first week, but now we are starting to get along and I see myself diving deeper into it.

Is Kafka high in demand right now? Is it considered a big thing to have as a DE or is it more like a nice to have, but not a huge deal?

",4,6,1736457679.0,dataengineering
1hxr84g,Approaches for structuring the AWS Glue Catalog?,"I'm reviewing means of shoehorning a 4 tier structure (branch, tier, domain, table) into the two tiers of namespace provided by the Glue Catalog (database, table).

For example, lets say I have the following configuration:

- **Branches**: dev, staging, prod  
- **Tiers**: raw, refined, curated, presentation  
- **Domains**: Salesforce, NetSuite, DocuSign  
- **Tables**: ...varies by domain...

Given the fact that AWS only allows one Glue Catalog per region, it's a bit troublesome figuring out how to organize this into their catalog.

It may or may not be relevant to know that the plan is, we will utilize dbt (through the Athena connector) to connect to the catalog and transform the data, promoting it upward between tiers. There will be separate places to store dev work, staging, and production data--which will get merged with CI/CD pipelines.

I want to prioritize having a setup where devs dont need to worry about making configuration changes as their work gets promoted to different environments. Ideally, their work should be ""*prod compatible""* immediately--even though they're working in an isolated development area.

Ive figured these are probably in the right direction:

**Catalog A**:  
- Database Convention: `{branch}`  
- Table Convention: `{tier}_{domain}_{table}`

**Catalog B**:  
- Database Convention: `{branch}_{tier}`  
- Table Convention: `{domain}\_{table}`

**Catalog C**:  
- Database Convention: `{branch}_{tier}_{domain}`  
- Table Convention: `{table}`

With exception of the raw data, which is shared between dev, staging, and prod (and thus, isn't branched).

Which setup would be considered best and why? Or rather, would you recommend a different structure? I appreciate any insights on the matter! Please feel free to let me know any of your experiences or past determinations.",3,3,1736467314.0,dataengineering
1hxd4oe,Welcome to the Data Engineering Dream Factory,,19,1,1736430405.0,dataengineering
1hxx6ab,Slow Airbyte Sync and Rate Limit Issues with Facebook Marketing Data,"I am currently learning data engineering and working on a project to move all Facebook marketing data into BigQuery. For this, we are using Airbyte.

The issue is that the first sync was very slow and didn’t finish for all streams. For the second attempt, I reduced the number of streams to just two, but the sync no longer works and shows that the rate limit was hit. How is this possible for a small business with minimal ad spend?

Also, why is the sync so slow? The first time, it ran for over 40 minutes before I had to stop it.

I would appreciate any help. I’ve attached screenshots for reference.

https://preview.redd.it/g6y21873p3ce1.png?width=2590&format=png&auto=webp&s=e5212fa4e7380041f322b2034841d48411dc6cc3

https://preview.redd.it/ffmng873p3ce1.png?width=2566&format=png&auto=webp&s=d439c1a0e22ffda5ccb99ab849bb0d925acf8ff7

",1,5,1736486237.0,dataengineering
1hxhwir,Need Advice: How to Find a Data Engineering Job now I have added AWS Data Engineer Certification to my data engineer experience?,"Hi everyone,

I’m reaching out for advice on landing a data engineering job. I recently earned my AWS Data Engineer certification (yesterday 🎉), and now I’m eager to put my skills to use. However, I’ve been struggling to make progress in the job search:

1. Recruiters don’t seem to reply to my applications or messages.
2. Applying through company career pages hasn’t yielded any responses even though I tailor my application.
3. I have 1-2 years of Data Engineering Experience

I’d really appreciate guidance on effective strategies to get noticed in the job market. Should I focus on improving my LinkedIn profile, cold-emailing, networking, or something else? Are there any specific communities, events, or platforms for AWS-certified professionals that could help?

If anyone has gone through a similar journey or has tips to share, I’d be incredibly grateful!

Thank you in advance for your support and insights.",7,8,1736443228.0,dataengineering
1hy0hih,Is programming must in data engineering ,I am pretty weak at programming. But have proficiency in SQL and PL/SQL. Can i pursue DE as a career?,0,44,1736500290.0,dataengineering
1hxcl8b,Need advice an setting up ETL with python.,"Hello everyone,  I had a requirement at work to build an application that can run ELT pipelines as we are moving away from paid solutions. As far as I had read , FastAPI + Redis + Celery seems to like a good option. But it would be really helpful if you had any advice for any other approaches. I have a couple of EC2 instances to play around with and the average size of the data I would be pulling and pushing to oracle databases would be around 10gb. Thanks in advance.",13,17,1736428682.0,dataengineering
1hxh7rf,Bronze Layer - Is staging the data part of this layer?,"Hey everyone,

I have some confusion about the bronze layer in medallion architecture. I've seen some conflicting guidance on whether the bronze layer is strictly an ""as-is"" data repository, or whether it also includes some very preliminary file type conversions.

  
For example, I receive client data in different, non-traditional data formats (ie. not CSV, JSON, or parquet). They need to be converted first before they are of any use to me. 

  
Is it appropriate to do this conversion in the bronze layer? Would the bronze layer store both the original and the converted (e.g. parquet) copies of the files? Then, the data would be staged and transformed in Silver?

  
Any help is appreciated, thank you!

\- Steve",5,10,1736441492.0,dataengineering
1hxtcmf,Mock interviews for a DE role,I am working as a DE for the past 5+ years and planning to switch my job. I have been prepping for the past month and would really appreciate it if anyone is willing to navigate me to a good mock interviews platform or help me with the mock interviews. Thanks!,1,5,1736473664.0,dataengineering
1hxn2r5,Power BI for ADF run history reporting ,"Any pointers to a power bi template to get the ADF monitoring done?

We have a complex ADF and leadership would like look at the trend. I have all the connections setup but need some template to quickly put something in a day or 2. Thank you",2,0,1736456287.0,dataengineering
1hxlrqj,Snowflake Pruning not Working with Subquery,"THIS IS KILLING ME!!!

I have a target table with about 10 sources. In this target table, I am storing the update\_ts of each source table in every row. So for example, if two of the tables are 'orders' and 'open\_dates', I am storing an update\_ts of each of these in my target table named 'orders\_update\_ts' and 'open\_dates\_update\_ts'. My largest source table contains 8 billion records in it. I am trying to select the delta of these source tables so that I can ultimately get a list of primary keys in my target table that need to be updated. Below is an example of how one of these is accomplished:

\--Get max  update\_ts from target table for orders:  
with orders\_max\_timestamp as (  
select orders\_update\_ts from target order by orders\_update\_ts desc limit 1 --performed better than max()  
)

, orders\_delta as (  
select \* from orders where update\_ts > (select orders\_update\_ts from orders\_max\_timestamp)  
)

The orders\_delta cte is resulting in a scan of all the partitions for the table. When I take the 'orders\_update\_ts' value generated and statically place it in the query, only 2 of the partitions are scanned. Something about this subquery is throwing off the optimizer.

I have tried to make sure Snowflake understands only one row is returned by adding a redundant aggregate function: (select max(orders\_update\_ts) from orders\_max\_timestamp))

But this had no impact on the performance.

I also tried replacing the subquery with a join but this actually performed even worse:

, orders\_delta as (  
  select orders.\* from orders  
  join orders\_max\_timestamp  
on orders.update\_ts > orders\_max\_timestamp.order\_update\_ts  
)

Any thoughts? I really want the logic to exist in a view. I am using dbt. If this was in a table then I could just set the variables up front in dbt and reference the variables instead of the subquery but since this is in a view that wouldn't be an option. I thought about using a UDF and seeing if that is treated differently but you can not pass in column names or table names into UDFs so I would need a UDF for every source of every target table which seems insane. You can pass them in for Procedures but I don't see how that helps my case.

THANK YOU EVERYONE FOR YOUR HELP!",2,2,1736452948.0,dataengineering
1hxlo7n,Switching from ML Research to Data Engineering: Will My Experience Count?,"Hello everyone! I’m currently working as a research assistant in ML. My job is to implement certain models in software, somewhat like an ML engineer. I don’t usually train the models (though sometimes I have had to), but I mainly focus on building the app alongside other people who also work on it, and then someone else takes care of the deployment.

I’d like to know if it would be difficult for someone with my background to move into a Data Engineering position and whether my experience would be counted, since I’ve noticed they usually ask for 1-2 years of experience. I have 2 years of experience doing this, plus another 2 years working as a python backend developer.",1,4,1736452718.0,dataengineering
1hxigmr,Project ideas for portfolio ,"Hi guys, I’m about to start looking around for Data Engineering internships. What project ideas do you guys recommend for me to showcase my skills? I’m familiar with Python (still learning), SQL (basics) and recently did an ETL using SSIS…",3,7,1736444653.0,dataengineering
1hxqfvt,How to make Data AI ready ? ,"Hi Everyone, I started a startup in 2023 to bring LLM capabilities to BI and analytic workflows. However, what I realized is that the enterprise world is not ready to deploy AI. The main reason is that our data systems are not ready for AI. I know this is a bold statement, but that is the lesson I learned by working with many customers.

As you already know, LLMs are very good with text data. Also starting from GPT-4 and later models, they have some capabilities like reasoning, planning, and coding. Those capabilities can be used to build agentic workflows and automate most tasks.

One key method of how you connect your data with LLMs is RAG. Data can be structured or unstructured. But the problem is when you apply RAG to a messy data system, it fails miserably.  
Just vectorizing everything with embeddings and searching on vector databases does not help if you are expecting to hit 90% and up accuracy.

What do I mean by data systems not being AI-ready?

* Lack of metadata structures
* No proper data models in warehouses
* Disconnected data silos and lineage issues

I would like to hear your thoughts on this. If you are experiencing the same issues, please comment, or if you have a different opinion or success stories, please show us the way.

I’m also hosting a virtual event to discuss this problem. If you are interested, please join and share your knowledge: [https://lu.ma/1ak4bvdk](https://lu.ma/1ak4bvdk).",1,7,1736465123.0,dataengineering
1hxc31d,Why Apache Doris is a Better Alternative to Elasticsearch for Real-Time Analytics,,7,0,1736427045.0,dataengineering
1hxcwar,Is there a tool to sync portion of data from two different Database while anonymizing data to the target?,"Imagine data from production then 'copying' an anonymized slice of data to your local dev environment. Could be useful for developing feature or debugging with realistic data. 

It's not just rows from a single table, but also rows on other tables that depend on it / it depends on ",6,6,1736429650.0,dataengineering
1hwt0al,Really hate those tech influencers who only know how to spread bs like “three reasons you should not become a data engineer”. ,Those mfs need to stop spreading anxiety and fake info. I used to be anxious when I was a student and watched all these types of videos like AI will replace us blah blah blah. Bruh just pick what you want to be and go for it. ,134,39,1736365523.0,dataengineering
1hxoueq,Optimizing Partitioning by Date and State,"I am currently using pyspark to process some data and I eventually write it out to s3. Without using df.write.**partitionBy**, I noticed that the time it takes to write a year's worth of data is 5 minutes.

  
However when I partition by both **dt** and **state** this pushes the write out time to close to 2 hours. While I accept that I may incur longer time to partition the data since I am dividing the data into alot of different subfolders (365 days \* 50 states = 18,250 unique combinations), I was wondering if there are things I can do to help optimize the writing time.

  
Currently I do a repartition(num\_partitions, ""dt"", ""state"") before I write it out but are there any configurations or tuning I can do on my end?

  
Thank you!",1,14,1736460886.0,dataengineering
1hxcdsj,A Snap Package for DuckDB,"Hi,

I made a Snap package to help install DuckDB's stable releases and keep it up-to-date on different machines.

The source code for the package is available here: [duckdb-snap](https://github.com/habedi/duckdb-snap)

The snap files are available from Canonical's Snap Store here: [duckdb](https://snapcraft.io/duckdb)

I hope it can be of use to some of the people here.",7,1,1736428005.0,dataengineering
1hxmaog,"Data Analyst - Found this comprehensive DE bootcamp, anyone have experience with DataEngineerCamp?
","Hey folks! 👋

I'm currently working as a data analyst and looking to level up my career into data engineering. After endless Udemy courses and YouTube tutorials, I stumbled upon [dataengineercamp.com](http://dataengineercamp.com) and their 16-week bootcamp. I'm someone who is an analyst at the moment but would love to work in Dataops in the future. Im also working towards my ASW ML Engineer/DE Engineer Associates, booked for Jan.  
  
The curriculum looks incredibly comprehensive and modern, covering everything from ETL fundamentals to streaming analytics.

What caught my eye is how they structure the learning path:

* Start with Python ETL fundamentals
* Move to modern ELT with dbt and Snowflake
* Cover the data lakehouse concept with Databricks
* Deep dive into streaming with Kafka and ClickHouse
* End with practical CI/CD implementation

The tech stack seems really current - they're teaching Dagster for orchestration (not just Airflow), using dbt for transformations, and even covering Delta Lake format. Plus there's a focus on real production deployment using Docker and AWS.

I'm particularly intrigued by their approach to data integration using Airbyte. As an analyst, I've always been frustrated by building one-off scripts for different data sources, so learning about automated integration sounds promising.

Has anyone here gone through this bootcamp? I'd love to hear about:

* What the hands-on labs are like
* Project difficulty level
* Job support/placement (if any)
* Whether it's worth the investment for someone coming from an analytics background

Also, for those who made the analyst-to-engineer-dataops transition, does this curriculum align with what you needed to learn? I'd love to in the future work in a DataOps role.

Would really appreciate any insights or alternatives you'd recommend! Been lurking here for a while and always impressed by how helpful this community is.

Thank you! 🙏

Edit: Their website is [dataengineercamp.com](http://dataengineercamp.com) for those interested in checking out the full curriculum.  
P.s [u/j\_\_neo](https://www.reddit.com/user/j__neo/) \- I think you are the founder - would love any insights :)  
",0,5,1736454290.0,dataengineering
1hxm8wt,Is it a good idea to switch from react native development to data engineering?,"I am a react native developer (YOE: 2) and i have recently got an opportunity to work as a data engineer in my organisation, I have a knowledge of SQL, python. I am having a hard time in deciding whether or not i should take this opportunity or not. I need to decide ASAP any advice, suggestions, pros/cons are welcomed at this point. ",0,3,1736454166.0,dataengineering
1hxg85w,Looking for a Cloud-Hosted (Wikipedia?) Dataset for a Cloud Computing Class,"I’m putting together a distributed computing class for an applied data science course, and one of the projects involves processing large files in a distributed way. We’re starting with some naïve approaches and progressing toward parallelized solutions and more ""real"" ways of doing it.

The idea is to use Wikipedia data (or something similar) as a dataset that students can chunk, process, and analyze. I know Wikimedia provides dumps, and there are options on Kaggle, but I’m wondering if  anyone knows of a pre-mounted blob or some hosted solution where the dataset is already accessible in the cloud?

It would save my students (and me) from having to deal with downloading and uploading multi-GB datasets, and it’d fit nicely into our cloud-based workflows. Ideally, I’d like a dataset hosted in some Blob or S3 type environment.

",2,10,1736439030.0,dataengineering
1hxld10,Grow beyond Tech Skills through Effective Collaboration,,0,0,1736451934.0,dataengineering
1hx9tzv,Tips and Best-Practices for Clickhouse,"We are currently evaluating Clickhouse as a data warehouse solution. I suggested it as an alternative to the current state (S3 Datalake + Postgres for serving Dashboards). After rewriting a lot of things from Pandas to Polars, the main bottleneck in our pipelines is bulk inserts into Postgres (by a huge margin). Since we use Parquet, I was looking for a DB that natively supports direct Parquet inserts. Our read-patterns look mainly transactional-like, but from the experiments I've done that doesn't seem to be an performance issue. 

Obviously Clickhouse would offer other ways to transform and transport data than the way we currently do it, but it seems like it provides a good onramp for us without having to change much about our current architecture.

If there are some Clickhouse users among you, I'd like to hear some first hand experiences, tips and things to look out for. ",6,10,1736418111.0,dataengineering
1hxjmzp,Voicing concerns to the founder of Great Expectations,"I work for Sparkfish, a data and analytics consultancy serving private equity. Like many folks here, we've struggled with the difficult learning curve of the GX library and the deprioritization of the open source library vs the paid SaaS product, and we griped about it publicly in various forums and on GH issues. Mainly we've struggled with the steep learning curve, obtuse APIs, lack of sensible defaults, and dated documentation.

It's a great library and surprisingly one of few truly focused on data quality in the Python data ecosystem, but it's a but like matplotlib -- most people who are using it are using it because it's the largest among a handful of options, not because we love it

A few months ago after reading our complaints, Abe Gong, the original developer of the library and founder of the company around it, reached out to us to schedule a call. He was generous with his time and resources and very graciously listened to our feedback -- this is that conversation. 

Hopefully we captured broader concerns -- let us know if we missed anything important that you would have mentioned",0,2,1736447602.0,dataengineering
1hx37ce,[Personal Project] Built an end-to-end data pipeline that extracts insight from AI subreddits,"Hey everyone,

I’ve been working on a personal project—a fully automated system designed to efficiently collect, process, and analyze AI subreddits to extract meaningful insights. Check out the [GitHub Repo](https://github.com/SulmanK/reddit_ai_pulse_local_public), [Website](https://reddit-text-insight-and-sentiment-website-local.vercel.app/), and [Blog](https://sulmank.github.io/Blog/writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/)!

Here’s what the project does:

* **Data Collection**: Gathers posts and comments using the Reddit API.
* **Data Processing**: Utilizes Apache Spark for data processing and transformation.
* **Text Summarization and Sentiment Analysis:** Hugging Face models
* **LLM insights**: Leverages Google's Gemini for insights
* **Monitoring**: Implements Prometheus and Grafana for real-time performance tracking.
* **Orchestration**: Coordinates workflows and tasks using Apache Airflow.
* **Visualization**: Includes a web application.

Soon, I’m planning to expand this pipeline to analyze data from other platforms, like Twitter and Discord. I’m currently working on deploying this project to the cloud, so stay tuned for updates!

I want to express my gratitude to this community for providing resources and inspiration throughout building this project. It has been an enriching experience, and I’ve enjoyed every moment.

I hope this project can be helpful to others, and I’m excited to keep building more innovative applications in the future (currently, upscaling my portfolio)

Thank you for your support, and I’d love to hear your thoughts!

PS: The OpenAI post is gone (gemini blocked explicit content, I am going to use a better content filter!)",17,6,1736392293.0,dataengineering
1hximzd,Data fabric layer for data mesh engineering,"What do people think about data fabric layer or shall I say data streaming platform for a data mesh architecture? Has anyone actually implemented this architecture in their company? We now have platform engineering in SWE side, but I don't think it covers the data layer.

I only know the basics such as Kafka, Flink, Iceberg and stuff, but never really has any first-hand experience with them.

And if yes, what tools are out there regarding observability and just monitoring in general that companies use? open source tools would be preferable, but don't mind any saas for now.

Thank you in advance.",0,0,1736445088.0,dataengineering
1hxdcpp,Aws RDS database connection,Does anyone know of a free tool or preferring a vs code extension that allows for connection via aws secrets? I have toolkit setup but it’s not helpful in that I can’t really query my databases or see my table structures. Other teammates use pycharm but have to pay for premium. How is this not a very common pattern that it’s not in SQLTools or similar? SOS ,2,5,1736431099.0,dataengineering
1hxhfkt,Platform or Tool For Presenting Large Sums of Un-summarized Data,"Hi all, 

I have a need to provide access to un summarized data with massive row counts (600 mil records +) to users, many of which are non-technical, so platforms that require use of small SQL statements is not an option.

The idea is to give the users access to the entire dataset and allow them to filter it for their needs with some GUI.

Is there any such platform or product on the market for this use-case?",1,3,1736442040.0,dataengineering
1hxa7ai,Need advice: modernizing our data ingestion pipeline with S3 - full load vs incremental approaches,"Hey fellow DEs! Looking for some architecture advice. Here's our current setup:

We have a webservice that receives data (CSV/JSON/XML) from multiple customers and dumps everything into a single column in SQL Server. A second SQL Server then transforms this into a relational model using stored procedures. Currently doing full loads for everything.

We're planning to modernize and want to incorporate S3. Two main questions:

1. What are the compelling reasons to include S3 in this new architecture?
2. We also need to handle API data sources (both full load and incremental). Should we store full loads in S3? Is there a better approach for managing this mix of full and incremental data? Most of the data are consumers (of the clients, clients would be for example restaurants and clients would be the guests which have data such as age, subscription etc...). There are also data from some sources which we have to join ourselves and are more complex.
3. We would want to eventually start using AWS Glue, hence the S3, but we are not sure on how store our data right now (either full load, which would be way easier or incremental). How should we store the data (full load or incremental), assume we are doing a daily extract

Some context: We work with many clients, and full loads have been our go-to since they're simpler to manage. But I'm wondering if we're missing out on better practices.

Would love to hear your experiences and recommendations, especially if you've done similar modernization projects!",4,7,1736419721.0,dataengineering
1hxgja7,Help please with setting up CI/CD with Snowflake and GitHub using GitHub Actions,"I’ve followed this QuickStart by Snowflake but I keep getting an error on how argument -a/—snowflake-account expects one argument. Does it want the account name only or account name formatted with the URL or account name with the org name? Also, I’m not even sure if this is exactly what I need. What I’m trying to do is automate building objects in Snowflake when we make changes to our sql scripts in GitHub. I would appreciate any help or if anyone could point me to a really well written article about this. Thanks!",0,2,1736439781.0,dataengineering
1hwo1zp,Built an open-source dbt log visualizer because digging through CLI output sucks,"DISCLAIMER: I’m an engineer at a company, but worked on this standalone open-source tool that I wanted to share.  
—

I got tired of squinting at CLI output trying to figure out why dbt tests were failing and built a simple visualization tool that just shows you what's happening in your runs.

It's completely free, no signup or anything—just drag your manifest.json and run\_results.json files into the web UI and you'll see:

* The actual reason your tests failed (not just that they failed)
* Where your performance bottlenecks are and how thread utilization impacts runtime
* Model dependencies and docs in an interactive interface

We built this because we needed it ourselves for development. Works with both dbt Core and Cloud.

You can use it via cli in your own workflow, or just try it here: [https://dbt-inspector.metaplane.dev](https://dbt-inspector.metaplane.dev) GitHub: [https://github.com/metaplane/cli](https://github.com/metaplane/cli)

[quick overview: why a run failed and inspecting performance](https://reddit.com/link/1hwo1zp/video/rzbxscpo5sbe1/player)

  
",72,4,1736353418.0,dataengineering
1hxlmlz,Design an ETL Workflow Using ADF and Databricks with Templates and Reusability,"I need help with designing a  comprehensive ETL workflow leveraging Azure Data Factory (ADF) for orchestration and Databricks for transformations. The design should prioritize **reusability** and **template-driven workflows**. Key considerations include:

1. **Reusable Templates**: The ETL process should use a modular approach with reusable components/templates for common tasks such as:
   * Extracting data from various RDBMS sources.
   * Loading data into different layers (Landing, Bronze, Silver, Gold).
   * Configuring transformation logic dynamically.
2. **Metadata-Driven Configuration**:
   * Include Delta tables in Databricks to store connection details, source and target configurations, and table-specific transformations.
   * Use these configurations to dynamically parameterize workflows for each table or domain.
3. **Workflow Example**:
   * **Source to Landing Zone**:
      * Extract data from an RDBMS (e.g., SQL Server or Oracle).
      * Use metadata tables to define source connection details and landing zone schema.
   * **Landing to Bronze**:
      * Clean and standardize the raw data.
      * Ensure transformations are generic across multiple tables using a single reusable template.
   * **Bronze to Silver and Gold**:
      * Handle domain-specific transformations dynamically using metadata.
      * For instance, a firm with Vendor A and Vendor B data might have distinct business logic or schema for Silver and Gold layers.
      * Ensure the workflow is flexible enough to accommodate these differences using configuration-driven logic.
4. **Key Challenge**:
   * Silver and Gold layers will vary significantly between domains (e.g., Vendor A and Vendor B may have unique transformations or schema requirements). Provide a strategy to maintain reusability while allowing for these domain-specific customizations.",0,2,1736452608.0,dataengineering
1hwhi80,"I recently passed the SnowPro Core exam, here are my notes to prepare","**My Stats:**

* Snowflake Experience: 1.5 years on and off
* Studied: 60 hours over 6 weeks.
* Scored: 860/1000

**Resources I paid for:**

Nikolai Schuler – Udemy - The Complete Masterclass - 16 hours - Updated recently. Gave it 4 stars, a little repetitive, but overall good.

Tom Bailey – Udemy - Ultimate Snowflake SnowPro Core Certification Course - 7 hours - Very good, gave it 5 stars.

I found my own Test Prep questions, you can download these in the link below.

Real exam uses a pool of questions, but for some reason I got many questions on -

Snowflake Editions, How to calculate credit usage, Roles, Privileges, Pre-signed URLs.

**Final Tips:**

* Aim for 100% on practice tests: Don't take the real exam until you're scoring highly.
* Use Snowflake (30 days free) while practicing: Best way to remember.
* Reschedule if you're not ready. rescheduling is free and can be done online.
* I didn't tell anyone I was doing it, I didn't need the pressure.
* Plan your time: Based on your current skill level, anywhere from 2 weeks to several months prep.

Here are some free resources.

**Free Test Prep Questions I used:**

[https://www.analystlaunch.com/c/testprep-snowprocore-landing](https://www.analystlaunch.com/c/testprep-snowprocore-landing)

Video on passing the exam:

[https://www.youtube.com/watch?v=RU\_\_xSc6TFM](https://www.youtube.com/watch?v=RU__xSc6TFM)

Good luck.",126,20,1736333710.0,dataengineering
1hwt5a1,Is anyone using Polars in Prod? ,"Hi, basically the title, if you are using Polars in Prod, can you describe your use case, challenges and any other interesting facts? 

And, if you tried to use Polars in Prod but ended up not doing so, can you share why?

Thank you! ",25,58,1736365870.0,dataengineering
1hx8six,Anyone knows Honeybook? How to pull the scheduled outstanding payments of all clients?,"Hi! I just need to pull the data on scheduled payments of all the clients in Honeybook to calculate the outstanding balance per month.

Thanks in advance!",2,2,1736413318.0,dataengineering
1hx2zh5,What Data Stack on Azure to use?,"Hey everyone, in the scenario below, what would estou use as data stack to build a self service analytics platform for my company?

Data will never be enormous, but there may be hundreds of thousands of time series.

1) I am limited to Azure
2) Company is very strict about data compliance.


I am currently using:
- Postgres (relational data provided by sales application)
- Azure Data Lake (Blobs)

And I am starting to implement some data pipelines, so I am considering using a combination of:
- Airflow (preferrably managed by astronomer)
- Data Warehouse: Clickhouse - Open Source  (as Azure synapse analytics documentation is terrible)
- Metabase Open Source (really loving it, and has built in connector to clickhouse, and not to synapse analytics)
- Azure functions serving as step functions for airflow;
- dbt to have version control on sql transformations.

Any recommendations especially on data warehouse side? Is it too crazy to deploy clickhouse on kubernetes? Will I lose too much time on maintenance?

They currently do not offer a “bring your own cloud” to azure, unfortunately



",7,15,1736391680.0,dataengineering
1hxbnjk,[MEETUP] Secure Development: How Docker & Kubernetes can enhance Cybersecurity | Functional World #14,"The first Functional World meetup of this year will take place on January 21st at 6 pm CET. The topic might not sound like classic ""functional programming,"" but we believe secure software is a must-have in every developer's toolkit :) Check out more info on our Meetup group & watch us live on YouTube: [https://www.meetup.com/functionalworld/events/305477041/?eventOrigin=group\_events\_list](https://www.meetup.com/functionalworld/events/305477041/?eventOrigin=group_events_list)",1,1,1736425504.0,dataengineering
1hx1j8s,"best way to access facebook, instagram and google text and image data for social science research ","I need to access text and image data from meta platforms and google to conduct research on political communication. What is the best and convenient way to access their data? Help a beginner. 

Thanks! ",6,4,1736387684.0,dataengineering
1hxab2h,Federated Modeling: When and Why to Adopt,,1,0,1736420164.0,dataengineering
1hx9kek,Which certification to go for?,"As part of this year's goals, I have to complete one of the following certification 1. Snowflake's SnowPro Core 2. Confluet Kafka CCDAK.

Which one should I select based on below criteria
1. Which one hold higher value in data circle?
2. Which one would be easier to pass? Both are quite costly and I couldn't find any discounts or vouchers. ",0,1,1736416918.0,dataengineering
1hx1fvl,How good is StarRocks?,"Has anyone evaluated StarRocks against Druid or Pinot?

Druid and Pinot are widely used, wondering how good StarRocks is.

Any high level points that come to mind, I would like to learn from experience if you use, good and bad.

Some points to consider:
- maintenance overhead
- features
- scale
- ?",4,2,1736387424.0,dataengineering
1hwlu0p,Will Data Mesh still be a thing in 2025?,"Hello,

I did not see yet a company really moving forward with Data Mesh so far in my career (10+ xp).

Obviously I don't know all companies , so looking for people to share their observations.

It has seemed to me they often struggle to do so because they don't have yet proper data management standards in place. Also, the business units often misses DE knowledge and start doing some mess without proper SWE practices (CI/CD, versioning,..).

Have you got maybe some good Data Mesh success stories to share? Is it a dying trend in your opinion? Or rather a lower priority as companies often need first to mature their data management practices?",25,24,1736347615.0,dataengineering
1hx2my6,Ensuring datasets are in sync between two systems,"Hello

I have a work project where I am fetching delta changes from Snowflake and persisting that in Databricks delta tables.

Due to regulatory/audit requirements, we have to prove that Databricks and Snowflake are in sync (say some 20-30 tables).

One naive approach might be to grab the tables from both sides and run a comparison in Databricks. But, the volume of data is expected to grow over time for these datasets, so I am sceptical of this approach.

Any thoughts on achieving this in a performant way ?",2,14,1736390686.0,dataengineering
1hwa9ql,I just passed AWS Data Engineer Associate !! With a couple of tips and resources to share,"This is the first achievement of 2025, a great way to start this year :)

Background:

I worked as a data engineer that implemented data pipeline solutions using AWS services for almost 2 years until I lost this job. While unemployed, I was preparing a related certification that would help boost my profile for the future job.

Resources:

* Cheat sheet : [TutorialDojo’s AWS cheat sheet](https://tutorialsdojo.com/aws-cheat-sheets/) \-  I referred to this resource as a quick review for each service covered in the below course.
* Course : [Udemy’s AWS Certified Data Engineer Associate Hands-on!](https://www.udemy.com/course/aws-data-engineer/)

What I like about this course is the hands-on videos that exemplify some key services to help me understand more about configurations.

* Practice Exams : [Practice Exam | AWS certified data engineer associate](https://www.udemy.com/course/practice-exams-aws-certified-data-engineer-associate-r/?srsltid=AfmBOopYJKtbGNAUCSGySgUc01o7X7HSFwQ0vMCdeLNEt-TNp5eR59y6&couponCode=NEWYEARCAREER)

The practice exam pack that bundles 4 practice exams that are closely related to the real exam    that I took.

* Random youtube videos for exam question explanations
* Real use-cases: With AWS account, I followed along with these videos for real-life pipelines to hone my comprehension on data engineering skills learned from the above courses.",149,21,1736306814.0,dataengineering
1hxfh6y,"Is relational database modeling coming back into fashion, or is it just more fashionable?","I'm interested how recently I am seeing more emphasis in organisations on data modeling / data quality and consistency. When I started in data analytics, the data warehousing team was always very far removed from me, I didn't even know who they were (even though I used the data they transformed daily) and it felt like data science was the very cool job to have. Now as things evolve, I feel like it's becoming more fun to be a data engineer - or even cooler an analytics engineer.   
  
In my mind I'm thinking of it as it becoming fashionable to work in the field of transforming data, especially with tools like dbt making it so easy to do so. 

What are your thoughts? Have I just made a link to fashion for no reason or do you feel anything similar?

Btw, I now work in data engineering education, so I'm interested from a motivational stand point of people wanting to learn these kinds of skills. ",0,17,1736437085.0,dataengineering
1hwebl4,I built a data warehouse in Postgres and I want to convince my boss that we should use it. Looking for a reality check.,"Get your bingo cards ready, r/dataengineering. I'm about to confess to every data engineering sin and maybe invent a couple new ones. I'm a complete noob with no formal training, but I have enough dev knowledge to be a threat to myself and others around me. Let's jump into it.

**I rolled my own data warehouse in a Postgres database. Why?**

I was tasked with migrating our business to a new CRM and Accounting software. For privacy, I'll avoid naming them, but they are well-known and cloud-based. Long story short, I successfully migrated us from the old system that peaked in the late 90's and was on its last leg. Not because it was inherently bad. It just had to endure 3 generations of ad-hoc management and accrued major technical debt. So 3 years ago, this is where I came in. I learned how to hit the SQL back-end raw and quickly became the go-to guy for the whole company for anything data related.

Now these new systems don't have an endpoint for raw SQL. They have ""reports"". But they are awful. Any time you need to report on a complex relationship, you have to go through point-and-click hell. So I'm sitting here like wow. One of the biggest CRMs in the world can't even design a reporting system that lets you do what a handful of lines of sql can do. Meanwhile management is like ""you're the data guy & there's no way this expensive software **can't** do this!"" And I'm like ""YEAH I THOUGHT THE SAME THING"" I am baffled at the arbitrary limitations of the reporting in these systems and the rediculous learning curve.

To recap: We need complex joins, pivots and aggregations, but the cloud systems can't transform the data like that. I needed a real solution. Something that can make me efficient again. **I need my SQL back.**

So I built a Linux server and spun up Postgres. The plan was to find an automated way to load our data onto it. Luckily, working with APIs is not a tall order, so I wrote a small python script for each system that effectively mirrors all of the objects & fields in their raw form, then upserts the data to the database. It was working, but needed some refinement.

After some experimenting, I settled on a dumbed-down lake+warehouse model. I refined my code to only fetch newly created and modified data from the systems to respect API limits, and all of the raw data goes into the ""data lake"" db. The lake has a schema for each system to keep the raw data siloed. This alone is able to power some groundbreaking reports... or at least reports comparable to the good old days.

The data warehouse is structured to accommodate the various different reporting requirements from each department in our business. So I made each department their own schema. I then began to write a little library of python scripts that transforms and normalizes the data so that it is primed for quick and efficient reports to meet each department's needs. (I'm not done with them all, but I have good momentum, and it's proving to be really pleasant to work with. Especially with the PostgreSQL data connector from Excel PowerQuery.)

Now the trick is adoption. Reactions to this system were first met rather indifferently by my boss. But it seemed to have finally dawned on him (and he is 100% correct) that a homebrew database on the network LAN just *feels* kind of sketchy. But our LAN is secure. We're an IT company after all. And my PSQL DB has all the basic opsec locked down. I also store virtually nothing locally on my machine.

Another contention he raised was that just because I think it's a good solution, that doesn't mean my future replacement is going to think the same thing (early retirement?? 😁 (Anyone hiring??)). He's not telling me to tear it down per-se, but he wants me to move away from this ""middleware"".

His argument to me is that my ""single source of truth"" is a vulnerability and a major time sink that I have not convinced him of any future value.
He suggested that for any custom or complex reports, I write a script that queries within the scope of that specific request. No database. Just a file that, idk, I guess I run it as needed or something.

I know this post is trailing off a bit. It's getting late.

---

**My question to you all are as follows.**

Is my approach worth continuing? My boss isn't the type to ""forbid"" things if it works for the human, but he will eventually choke out the initiative if I can't strongly justify what I'm doing.

What is your opinion of my implementation. What could I do to make it better?

There's a concern about company adoption. I've been trying to boil my system's architecture and process design down to a simple README so that anybody with a basic knowledge in data analytics and intermediate programming skills could pick this system right up and maintain it with no problems.
-> Are there any ""gold standard"" templates for writing this kind of documentation?

I am of the opinion that we *need* a Warehouse because the reporting on the cloud systems are not built for intense data manipulation. Why the hell shouldn't I be able to use this tool? It saves me time and is easier to build automations on. If I'm not rocking in SQL, I'm gonna be rocking in PowerQuery so all this sensitive data ends up on a 2nd party system regardless!

What do you think?

Any advice is greatly appreciated! (Especially ideas on how to prove that a data warehouse system can absolutely be a sustainable option for the comoany.)",57,37,1736321029.0,dataengineering
1hwxuir,Show /r/DataEngineering: Using Bacalhau & DuckDB for processing remote data,"FULL DISCLOSURE: I co-founded [Bacalhau](http://bacalhau.org)  
We've been playing around with combining DuckDB and Bacalhau for distributed query processing, and I wanted to share our experience and get your feedback on what we could improve.

What we were trying to solve: We often deal with large datasets (in this case, the not so large, but meaningful NYC Taxi data) where downloading the entire dataset locally isn't ideal. We wanted to find a way to run SQL queries directly where the data lives, without setting up complex infrastructure.

Our approach: We experimented with using Bacalhau as a distributed compute layer and DuckDB for the actual query processing. The basic idea is:

1. Define queries in SQL files (kept them simple to start - just counting rides and doing some time-window aggregations)
2. Use Bacalhau to execute these queries on remote nodes where the data already exists
3. Get results back without having to move the raw data around

For example, we were able to run a complex query remotely (on shared servers), using DuckDB & Bacalhau, rather than having to download all the data first:

    WITH intervals AS (
        SELECT
            DATE_TRUNC('hour', tpep_pickup_datetime) AS pickup_hour,
            FLOOR(EXTRACT(MINUTE FROM tpep_pickup_datetime) / 5) * 5 AS pickup_minute
        FROM
            your_table_name
    )
    SELECT
        pickup_hour + INTERVAL (pickup_minute) MINUTE AS interval_start,
        AVG(ride_count) AS avg_rides_per_5min
    FROM (
        SELECT
            pickup_hour,
            pickup_minute,
            COUNT(*) AS ride_count
        FROM
            intervals
        GROUP BY
            pickup_hour,
            pickup_minute
    ) AS ride_counts
    GROUP BY
        interval_start
    ORDER BY
        interval_start;

Then to execute it, you simply type:

    bacalhau job run template_job.yaml \
    --template-vars=""query=$(cat window_query_complex.sql)"" \
    --template-vars=""filename=/bacalhau_data/yellow_tripdata_2020-02.parquet""

What's working well:

* No need to download datasets locally
* SQL interface feels natural for data analysis
* Pretty lightweight setup compared to spinning up a full warehouse

Where we're struggling / would love feedback:

1. Are there more features we could build into Bacalhau natively to enable this? (Yes, i'm aware having a more native way to identify the files would be nice)
2. Is this interesting - do you have large datasets you'd like to query before you move them?
3. Would love to hear if anyone has done something similar and what pitfalls we should watch out for
4. Anything else?

I've put our full write-up with more details here: [https://docs.bacalhau.org/examples/data-engineering/using-bacalhau-with-duckdb](https://docs.bacalhau.org/examples/data-engineering/using-bacalhau-with-duckdb)

Really curious to hear your thoughts and experiences! We're still learning and would appreciate any feedback on how we could make this better.",5,3,1736377666.0,dataengineering
1hx0fzt,Seeking Advice on Building a Database-Driven Comparison Tool for Asset Finance,"Hello Engineers,

I’m an asset broker working on a project to streamline how I compare lending options for my clients. My goal is to build a database-driven tool that filters and compares lending criteria from multiple financial institutions. Think of it as an **asset finance version of Quickli** (a tool used in mortgage brokering) but tailored for consumer and commercial loans, personal loans, and unsecured business loans.

# What I Want to Accomplish:

* **Centralised Database**: Store lending criteria such as loan amounts, interest rates, credit score requirements, repayment terms, and asset age limits.
* **Dynamic Filtering**: Allow filtering by multiple criteria (e.g., loan amount range, credit score thresholds, asset type).
* **Comparison Engine**: Generate side-by-side comparisons of lenders based on client needs, showing key differences like interest rates, repayment terms, and eligibility conditions.
* **Automations**:
   * Notifications when a client’s criteria match a lender.
   * Automated updates of lender data (via APIs, scraping, or PDF parsing).
   * Loan repayment calculations and summaries.
* **Reporting**: Create client-facing reports with tailored loan options, repayment estimates, and comparisons.
* **Scalable and Usable**: Design a system that can grow as more lenders and products are added while remaining user-friendly.

# What I’ve Explored So Far:

* **Airtable**: Seems promising for a no-code/low-code solution but may require external integrations for advanced functionality.
* **Zapier/Make.com**: Useful for automation and connecting data sources.
* **Custom Development**: Considering Python and SQL for a more robust, scalable solution, but I lack coding expertise and would likely need to hire someone.

# What I Need Advice On:

1. **Platform Suggestions**:
   * Are there platforms/tools that can create such a system efficiently?
   * Should I stick with Airtable or consider something more specialised?
2. **Database and Comparisons**:
   * For building a comparison engine, what’s the best approach?
3. **APIs and Automation**:
   * How feasible is it to integrate data from lenders (e.g., via APIs or scraping)? Is AI an option?
4. **Custom Development**:
   * If I were to hire a freelancer, what should I look for in terms of skills?
   * What’s a realistic timeline and budget for a project like this?
5. **Scalability and Maintenance**:
   * How can I ensure the system remains scalable as data grows?

I appreciate any advice, insights, or examples you can share! Whether it’s on choosing the right tools, structuring the database, or managing such a project, your expertise would be incredibly valuable.

**My initial plan:**  
\- Obtain product guides  
\- Highlight all potential data points/criteria  
\- Use AI to seek data points of each product guide  
\- Create a database  
\- Use database to create comparison tool  
\- Use comparison tool to provide a quicker and better result for my clients  
\- Update the comparison tool with new features overtime such as Approval Rate (based on it's suggestions)  
\- Eventually plug into a website to have available on PC or Mobile

Thanks in advance!",2,3,1736384447.0,dataengineering
1hwnslh,Advice on how to allow customers bulk access to their SaaS web application data,"The title pretty much says it all. I am working for a small SaaS company. We have several customers who would like to access their data in bulk to load into their data warehouses. For some reason the product manager and developers don't want to use REST APIs to grant access to the data.

The data currently resides in SQL Server databases hosted in Azure Windows Server virtual machines.

The simplest approach I can think of is to create data replication pipelines to read-only databases, and then use VPN or SSH and grant the customers direct access to the copies of their respective databases.

Has anyone experienced a requirement like this?",10,10,1736352766.0,dataengineering
1hwpxts,Merge vs Rebase in Git: Data Engineer’s Guide to Choosing the Right Workflow,,5,3,1736357965.0,dataengineering
1hwfyh2,The Rise of Single-Node Processing: Challenging the Distributed-First Mindset,,29,0,1736327588.0,dataengineering
1hwpqet,DE90.com - job board for data engineering jobs,,5,1,1736357462.0,dataengineering
1hwuwui,Advice for an intern ,"Hey everyone,

I recently started as a data engineering intern at a large organization.  I’m excited but also feeling nervous and overwhelmed by the amount of information and terminology I need to absorb. The organization is migrating from legacy software(s) to quite a few cloud based platforms like Microsoft fabric. 

My background is primarily in software development, particularly with Python but I’m still learning. I was assigned to the data engineering team after interviews with the hiring manager. However, it seems I may not receive formal training beyond the Microsoft Learn modules, Snowflake hands-on badges, and Microsoft Fabric resources I’ve been provided.

I want to make the most of this opportunity and make a lasting impact that will translate to a full time position with this organization. Apart from the resources mentioned, do you have any recommendations for tools, tutorials, or general advice to ease my anxiety and help me perform in this role?

Thanks in advance for any guidance!",3,3,1736370259.0,dataengineering
1hw8uqm,What skills are most in demand in 2025?,"What are the most in-demand skills for data engineers in 2025? Besides the necessary fundamentals such as SQL, Python, and cloud experience. Keeping it brief to allow everyone to give there take.",91,77,1736302417.0,dataengineering
1hww30u,Help me decide between two job offers: GCP consulting vs. freelance contract,"Hi everyone,

I’m from France and currently working in Belgium, but I’ve decided to quit my consulting job to move back to Paris for personal reasons. After applying to many positions, I now have two offers, and I could really use your advice on which one to choose.

Offer 1: GCP Consulting Company

	•	Type: Permanent employment contract (CDI).
	•	Specialization: The company focuses exclusively on GCP (Google Cloud Platform) projects.
	•	Salary: Around €2,700/month after taxes.
	•	Pros: Stability and potential for career growth in a high-demand specialization (GCP).

Offer 2: Freelance Contract (Public Sector)

	•	Type: Freelance contract (2 years).
	•	Project: Migration from Teradata to Cloudera using HDFS and Spark.
	•	Rate: Average Daily Rate of €400, which translates to roughly €5,500/month after taxes based on my current situation.
	•	Pros: Much higher short-term income and valuable experience with data engineering tools like Spark and Cloudera.

My Dilemma

At first glance, the freelance option seems better financially, but I’m worried about the long-term impact on my career. What happens after the 2-year contract? Is this freelance mission a potential trap?

On the other hand, the GCP consulting role offers stability and the opportunity to specialize in a growing field, but the salary is significantly lower.

Another thought I’ve had is to accept the GCP job now, gain experience, and transition into GCP freelancing later for a better daily rate.

What do you think?

Has anyone faced a similar choice? What would you do in my position? Any advice would be greatly appreciated!

This version is more structured, clearer, and formatted for easy reading, which is ideal for Reddit. It also presents the key details and concerns more succinctly.
",2,4,1736373231.0,dataengineering
1hwhhjl,[Apache Airflow] Does this setup look fine to you? (Our Costs Are $1500/Month),"Hey Engineers,  
We’re thinking of running Airflow with this setup:

* **Executor:** Celery
* **Workers:** 2 (GCP VMs)
* **Schedulers:** 2 (GCP VMs)
* **Web Servers:** 2 (GCP VMs)
* **RabbitMQ:** 2 (GCP VMs)

That’s 8 nodes in total, costing us around **$1500/month** (excluding shared storage which we are not sure about). Does this sound reasonable? Are we overdoing it for a medium workload (ie, 100 to 200 dags with few tasks per dag running every hour)?

**Would love to hear how your Airflow is set up and any tips to keep it efficient without overspending.**

Specifically:

1. What does your setup look like?
2. What shared storage solution are you using?
3. How are you building images (especially if using Kubernetes executor)?
4. Does your Airflow scale manually or automatically?
5. How do you ensure recovery for workers, schedulers, or web servers when they go down? Is it manual?

**Thanks for any insights!** ",12,31,1736333633.0,dataengineering
1hwnezt,A little bit of complexity won't hurt DEs,"Hello !   
  
In my experience I have met teams that abstracted away really basic and foundational key concepts, to allow other more junior people to deliver quickly or to be on-boarded quickly, , I've seen it create dangerous knowledge gaps in data teams. Been thinking about this lately and wrote a blog post about it. Anyone else deal with this? How do you handle it in your team?

[https://uncultureddata.substack.com/p/the-hidden-cost-of-over-abstraction](https://uncultureddata.substack.com/p/the-hidden-cost-of-over-abstraction)",6,4,1736351857.0,dataengineering
1hwikxo,Need Advice: Best Analytics Tool for BigData with Slice & Dice Capabilities and Metrics Tracking,"Hey folks,

We’re managing 1TB–10TB of data daily, and while we’re using Looker for some reporting, it’s not cutting it for slice-and-dice analysis that data analysts or scientists can use flexibly.

***Here’s the challenge:***

\- We need a platform that can handle large-scale datasets with options for sampling/filtering for better performance.

\- It should support advanced analysis for metrics like CTR and CVR, letting us dig into product-level insights (e.g., spotting trends or issues).

\- Bonus if it provides charts, graphs, and easy visualization tools!

***What We Have Now:***

\- **Looker**: Great for BI and dashboards but feels limited for ad-hoc exploration or deeper analytics.

\- **Druid** (considering it): From what I know, Apache Druid is awesome for real-time data ingestion and fast OLAP queries, especially for time-series and aggregation-heavy use cases. It sounds perfect for slicing and dicing data with speed and also supports visualizations with tools like Superset.

***What I’m Asking:***

1. What tools or platforms are you using for this kind of data size and analytics?
2. If you’re using Druid, what’s your experience—does it meet these needs?
3. How does your tech stack enable slicing, dicing, and visualization at this scale?
4. Are there alternatives that work better for real-time and historical analysis?

***Would love to hear what’s working for you and any suggestions you have. Thanks!***",6,5,1736337618.0,dataengineering
1hwn3i7,Assessment tool like prowler but for Database?,"Hi,

  
Prowler is an open source tool which assesses your infra in AWS as per AWS published best practices.

  
Now; you could very easily do such a thing for a database. so has anyone already built that? I had a search but didnt really know what terms to use.  There's plenty of performance analyzers, but thats not what i'm after here, and it somewhat muddys the search.

I'm thinking stuff like key balance, foreign key definitions, nulls, constraints, naming conventions, data type usage, etc.  Some of these would be generic and run against any jdbc database, others may be e.g. redshift (or otherwise) specific.",3,0,1736351016.0,dataengineering
1hwjtuj,Best option for reading data from other DBMS systems in Azure,"Hi,

I am new into Data engineering. I have a task of  loading the data into Azure synapse from other DB’s like oracle,db2 and may be snowflake as well for reporting purposes. We will have to process this data from the source system and deliver it for reporting. These are big data. As far as I researched I got to know using either pyspark or synapse pipeline to directly read them from source DB and process them seems to be good option. But how do one decide which would be better option and which of this to go for? ",4,3,1736341621.0,dataengineering
1hwo9hm,"Ranger, trino, dbeaver","I could use some help setting up some boilerplate policies

I have a Trino service connected to several sources. 

I want to allow group a to only see source 1.  If I only allow catalog1, schema * and column *, it seems to work with the Trino cli. 

BUT when I connect using dbeaver, I get errors when expanding the tree. I get that dbeaver requires addl access to draw the tree, but I can pinpoint these settings

Does anyone have any insight into what is needed to secure access to trino sources but make dbeaver happy?  

Most of my users will NOT be using Trino cli. 😂 ",2,3,1736353929.0,dataengineering
1hwc7q9,Big Data ,"What does everybody think “big data” is?  I don’t mean structured vs unstructured, since all orgs have both.  I’m talking about size.  I know it’s subjective, but for my experience, anything over 50PB is big data. Under that is mid.  

I’ve consulted for orgs that think 1TB is big data and others that ingest 1 TB per second.  ",16,17,1736313269.0,dataengineering
1hws9kj,"Oracle Exadata X11M Delivers Extreme Performance, Increased Efficiency, and Improved Energy Savings for Data and AI Workloads",https://finance.yahoo.com/news/oracle-exadata-x11m-delivers-extreme-160000695.html,0,0,1736363659.0,dataengineering
1hwireo,DE use case by industry focused,"Is there any DE use case that will be used or like only can implemented in some industry focus field? Cause my company is focusing on Multifinance industry and hope the engineer can build use case related to the industry.

But I think the use case specifically to DE is will still same, different only the data that will be used, all the source is mostly same like RDBMS, API, scraping and etc. Store in Data Warehouse or Data Lake, and process it followed by the goal or target that will make it difference.

If there's any case or any advice, I really appreciate it.",2,1,1736338210.0,dataengineering
1hw9d5d,Difference between Data Engineer and Analytics Engineer,"This is a topic that started intriguing me because I see that in many roles it isn't clear enough what the responsibilities of a Data Engineer are compared to those of an Analytics Engineer. Guys, how do you evaluate the main differences between these two roles, and where does one role end and the other begin?",16,18,1736303987.0,dataengineering
1hwiar3,How and where to learn system design principles for Data Engineers?,I was going through article and saw that system design question may be asked in Data Engineering assessment. What is system design mean in Data Engineering and what are the best resources to learn it?,4,2,1736336599.0,dataengineering
1hwvcw4,Need advice,"I am software engineer working as business analyst remotely.(less than a year). Currently I had the opportunity to enroll in physical course for DE which is costing me $233 right now. They are offering these skill set in 3.5months weekend course. 
Now the issue is I am kinda confused, I am not good at programming nor work in fully dedicated development so don't have strong foundation in it.
Should I go for this course or not? Should I explore the self paced learning path with free resources on Internet. ",0,9,1736371398.0,dataengineering
1hwhhyx,Data Contracts,"I'm writing an article about a concept I call **Data Contracts**. Before I officially post it, I would really love a reality check and feedback.

Any feedback is appreciated!

Especially:

* Have you tried this before, and what were your experiences and learnings?
* Can I structure the article better?
* What additional information could make this article more practical or applicable?
* Have you come across similar concepts under different names?
* **Most importantly:** am I giving advice that could harm some organizations?

[**Link to article**](https://starlightinsights.com/starlight-data-framework/data-contracts).

>[*To adhere to Community Rules 4 and 5:*](https://www.reddit.com/mod/dataengineering/rules/) *I am the owner of the website Starlight Insights, and the website offers a product that can be bought. However, this post is only related to the concepts of Data Contracts.*",3,1,1736333680.0,dataengineering
1hwoo1g,Best ETL & Warehousing Tools in Finance Industry,"What are the best/most used ETL and Warehousing tools for the finance industry? (Banks, Investment Firms etc)

And what are pros and cons of each?

P.S Talking about modern tools like Informatica here",2,17,1736354881.0,dataengineering
1hwgtdr,What should I do if I need to change the database for the reports? Always having to change SQL is tedious and prone to errors. Is there a permanent solution?,"Migrating reports between different databases requires modifying the SQL statements inside each time. The SQL statements in the reports are often lengthy, making the migration time-consuming and prone to errors.

Is there any good way to make SQL statements cross-database compatible, or to implement automated conversion through some tool or framework?

For example, are there any good SQL abstraction layers or ORM tools recommended? But it should be able to be integrated with reporting tools. Or is there a reporting solution that supports multiple databases and can address dialect differences between databases.",4,6,1736331011.0,dataengineering
1hvne3s,Data Engineering Zoomcamp starts next week - learn DE for free!,"The DE zoomcamp starts next week on Monday.

They are covering:

* Module 1: Containerization and Infrastructure as Code
* Module 2: Workflow Orchestration
* Workshop 1: Data Ingestion
* Module 3: Data Warehouse
* Module 4: Analytics Engineering
* Module 5: Batch processing
* Module 6: Streaming

[https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

  
See you on the course!",286,37,1736240377.0,dataengineering
1hvz9ye,"Foolproof csv reading into pandas?  I spend way too much time on ""_clean_csv"" functions it seems.","I swear I spend more time dealing with csv files that have oddities like unescaped escape characters, contain html in a handful of hidden rows, or other little issues that bork the entire pipeline.  Every time I think I have a foolproof ""clean\_csv"" function, it fails on the next project.

What do people tend to do with csv files?  If I can get the damned things into a pandas dataframe I'm golden, but more often than not the reading/parsing fails silently, and I end up with a row that has an offset, then I get data typing errors when actually pushing to the database. 

I have implemented strict schema validation in some scripts, which can at least help ensure you can abort on a bad line/row, but does anybody have an approach that makes reading the things in, and cleaning up some of the typical difficult issues we find with csv files a bit easier to handle?

Does everybody else also spend more time on csv cleaning than seems reasonable?  And it's just part of the job b/c every source system makes its own flavor of csv with their own sets of issues?",32,38,1736277168.0,dataengineering
1hw9c0o,How should we organize our dbt projects with dbt-core?,"Edit: I think I decided I’m going to restructure things so that S3 is set up more like:

```

# raw - shared
S3://raw/{tier}/{table}/…

# private to each dev
S3://dev/{dev-name}/{tier}/{table}/…
# tiers: raw, processed, curated,
# presentation

# staging - shared
S3://staging/{tier}/{table}/…

# prod
S3://{tier}/{table}/…
```

and have staging be a copy of prod. the dev ones, not sure how exactly just yet… maybe views? Or maybe they build only the tables they need as needed? I’d want their dbt scripts to match what’s expected in prod, no matter the change for the most part. I’m not sure how much views would impede that.

This should make everything else much simpler, I imagine.
___

**ORIGINAL:**

I’m building a data lakehouse on the AWS platform. The plan is to have 4 cataloged tiers: Raw, Staging, Curated, and Presentation. All of which implemented as S3 buckets storing Iceberg format tables.

To move data downstream between the tiers, we’re going to use dbt. But the lakehouse is going to be replicated so that there exists two: a production and a sandbox lakehouse. The idea is that analysts can modify the sandbox environment directly, whereas `git push` triggers a CI/CD pipeline to build a docker image for their dbt project and run it in Fargate as needed (scheduled using EventBridge and Step Functions) to affect production.

There is going to be hundreds of dbt projects more than likely. I don’t want hundreds of git repos though… so I’m imagining that we would just house dbt projects together if they move data between the same lakehouse tiers. Anything moving raw-> staging gets stored in the same repository, anything moving staging -> curated gets stored in the same repository, and so on…

But, to prevent large and wasteful CI/CD pipelines, it now becomes necessary to detect which dbt projects have actually changed on any `git push`, within the CI/CD pipeline, and build / replace only those necessary containers within ECR. This is getting a bit convoluted, right?

I’d like some feedback from engineers experienced down this path. How should I be simplifying the design? In what ways can I elegantly address the sudden need for really dynamic CI/CD pipelines—or is it not worth addressing (such as, if GitHub Actions handle this workload quite well). What are your thoughts?

Appreciate any feedback!",8,14,1736303889.0,dataengineering
1hw6tan,Walkthrough - Adding and Removing Roles from an Existing GCP Service Account,"Quick Walkthrough I made to help people add and remove Roles to an Existing Service Account in GCP. And honestly I also made this for my own reference as I always forget where to do this. 

[https://www.youtube.com/watch?v=nRb4CeCdlek](https://www.youtube.com/watch?v=nRb4CeCdlek)",8,0,1736296525.0,dataengineering
1hw1htd,Best Practices for Partitioning Master / Dimension Data,"Example: Employee, Customer (high volumes)

Most transactional data can be easily partitioned by extract/update date. This will often match how the data is accessed (e.g., purchase orders from December)

Master data, particularly high volume data like customer, can be partitioned by extract date (e.g., day the customer was updated) but partitioning in this way has no relationship with how the data will be queried (e.g., by some key, particularly attributes (""active"" flag, rating). It seems like if you're storing it somewhere after the initial extract, you'd want to partition that data by something that matches the likely queries (e.g., last activity date).

What do you think are the best practices for managing higher volume master data.

(assume for the purposes of this discussion that you're not necessarily focused on Object Storage or normal Database partitions, but thinking about partitioning in general).

Thanks!",4,5,1736282666.0,dataengineering
1hvp3tg,Automate the boring stuff with python,"I want to learn Python for data engineering, would you recommend this book for DE? or is there any better Python book/course for Data Engineers? thank you",29,12,1736248140.0,dataengineering
1hvyevq,How to Secure the Public Schema in PostgreSQL – Key Insights,"If you're working with PostgreSQL, you’ve probably encountered the **public schema**. But do you really understand its role and the potential security implications?

With PostgreSQL, the **behavior of the public schema differs significantly depending on the version** you're using:

* **Versions <15:** The public schema allows all users to create objects, making it a potential security risk in multi-user environments.
* **Versions >=15:** Default permissions have been tightened. CREATE permissions are revoked for all users, and the schema is owned by the database owner.

I’ve written a detailed guide that covers:

* What the public schema is and how it works in different PostgreSQL versions.
* Common risks associated with the default setup in older versions.
* Best practices to secure and manage it effectively, including steps for migrations to avoid carrying over outdated settings.

Whether you're a database administrator or just diving into PostgreSQL, this article provides actionable insights to improve your setup.

Check it out here: [The Public Schema in PostgreSQL](https://datenakrobat.blogspot.com/2025/01/the-public-schema-in-postgresql.html)

I’d love to hear your thoughts or any additional tips you use to handle the public schema! Let’s discuss below! 👇",5,1,1736275036.0,dataengineering
1hvofdz,Data engineer to AI engineer,"hello r/dataengineering 

is it worth it? 

Any tips on how do i effectively move to this role? 



background : I was offered an AI engineering role and I am a data engineer since pretty much our data platform is already stable and mostly i am doing EDA with data scientists.",16,10,1736245177.0,dataengineering
1hw7yam,Salary opinions in France.,"Hi, I have around 3.5 years of post internship experience in Paris in data engineering. Total comp 54k without any performance based bonus. Am I underpaid? 
(Don't tell me how much better the pay is in the US, not everyone can move there 🥲 )",2,10,1736299843.0,dataengineering
1hw256z,Different values in group by columns before and after group by in pandas dataframe,"I had following code:

    sum_columns = ['p', 'q', 'r', 'Ax', 'Ay', 'Az']
    avg_columns = ['Bx', 'By', 'Bz', 'G2 C03']
    agg_map = {col: 'sum' for col in sum_columns}
    agg_map.update({col: 'mean' for col in avg_columns})
    
    df = df.groupby(['MAC C01', 'MAC C02', 'MAC C03'], as_index=False).agg(agg_map)

Dataframe before group by:

```
         p    q    r  Ax  Ay   Az  Bx  By  Bz  G2 C03   MAC C01   MAC C02  MAC C03
0      0.0  0.0  0.1   0   0    0   0   0   0     0.0   0.00000   0.00000      0.0
1      0.0  0.1  0.1   0   0    0   0   0   0     0.0   0.00000   0.00000      0.0
2      0.0  0.1  0.0   0   0    0   0   0   0     0.0   0.00000   0.00000      0.0
3      0.0  0.0  0.1   0   0    0   0   0   0     0.0   0.00000   0.00000      0.0
4      0.0  0.0  0.0 -34   2  251 -23  78  45     0.0  19.12377  73.07937      0.0
...    ...  ...  ...  ..  ..  ...  ..  ..  ..     ...       ...       ...      ...
14676  0.3 -0.3 -1.0 -39  -9  250  66 -23  49     0.0  19.12376  73.07938      0.0
14677  0.9  0.3 -1.0 -39  -9  250  66 -23  49     0.0  19.12376  73.07938      0.0
14678  0.5  0.1  2.5 -39  -9  250  66 -23  49     0.0  19.12376  73.07938      0.0
14679 -0.2 -0.1  1.8 -39  -9  250  66 -23  49     0.0  19.12376  73.07938      0.0
14680 -0.9  0.2 -2.3 -39  -9  250  66 -23  49     0.0  19.12376  73.07938      0.0

[14681 rows x 13 columns]
```

Data frame after group by:

```
       MAC C01   MAC C02  MAC C03             p     q     r   Ax  Ay    Az    Bx    By    Bz  G2 C03
0      0.00000   0.00000      0.0  0.000000e+00   0.2   0.3    0   0     0   0.0   0.0   0.0     0.0
1     19.12135  73.07947     75.0  1.330000e+01   6.4 -31.9 -140   0  1230  -4.0 -79.0  26.0    75.0
2     19.12135  73.07959     75.8  1.160000e+01   8.3 -47.0 -248   8  2080   1.0 -78.0  28.0    75.0
3     19.12135  73.07968     76.6  1.030000e+01   5.1 -32.9 -174   6  1560   6.0 -77.0  29.0    76.0
4     19.12136  73.07938     74.6  2.260000e+01  14.9 -56.7 -224   0  2144 -10.0 -78.0  23.0    74.0
...        ...       ...      ...           ...   ...   ...  ...  ..   ...   ...   ...   ...     ...
1452  19.12607  73.07969    184.8  8.881784e-16 -18.9 -27.7  216 -24  2240   8.0  74.0  47.0   184.0
1453  19.12608  73.07933    178.4 -1.600000e+00  15.2 -17.8  408 -16  2080   1.0  75.0  47.0   178.0
1454  19.12608  73.07941    180.1 -4.100000e+00   8.6 -19.0  328   8  1656   3.0  76.0  46.0   180.0
1455  19.12608  73.07949    181.6 -3.500000e+00 -10.8 -22.5  312 -32  1768   5.0  75.0  46.0   181.0
1456  19.12608  73.07960    183.2  2.600000e+00 -17.1 -23.9  296  -8  2296   6.0  75.0  47.0   183.0

[1457 rows x 13 columns]
```


How ""first non zero"" MAC C01 (row at index 4) `19.12377` before group-by, but the same after group-by is `19.12135` (row at index 2)? Shouldnt both be same? Similarly, how last MAC C01 before group by is `19.12376`, but that after group by is `19.12608`? 

I guess what group by does is it keeps unique values for a group-by columns and aggregates other columns using function specified. Since it ""keeps unique values"" for group-by-columns, I feel the ""first non-zero"" group-by-column values should be same and similarly last group-by-column values should also be same. But that does not seem to be case here.

What am missing?",2,3,1736284276.0,dataengineering
1hvnzp2,Kimball Modeling Question,"I'm asking what a Kimball purist would do in the following situation:

You have event data for school students competing in the 100 yard dash across the whole country. The raw data you have is:

* student\_id <-- FK
* date <-- FK
* school\_represented\_id <-- FK
* time to complete race <-- measure

(You may have other data to, but it is irrelevant for my question.)

Your business is going to want reports based on the *grade* level of the student. However, the grade level of the student should not show up in the fact table because it is a dimensional attribute that depends on existing dimensional FKs (student\_id, date, school\_represented\_id), breaking normalization. \[Note that a student can enrolled and represent multiple schools on the same date, so all 3 are needed.\]

Generally speaking, Kimball argues strongly in favor of 3FN fact tables.

From a Kimball standpoint, the solution can not include a Bridge table, for [bridge tables are only used to address multi-valued dimensions or ragged hierarchical data](https://www.kimballgroup.com/2008/09/design-tip-105-snowflakes-outriggers-and-bridges/), neither of which apply to this situation. (And even if I did use a bridge table, I'd end up with same issue I mention below.)

It seems the very natural option is to create a combined dimension table for the correlated dimensions of student\_id, date, and school\_represented\_id. This combined dimension table would include the grade level as well as all the dimensional data from **dim\_student** and **dim\_school**.

On its face this makes a lot of sense because we do expect these to be correlated. Unfortunately, due to the date element, this combined dimension model grows much faster than either of its components.

Just wondering if I'm missing anything.  


",16,25,1736243154.0,dataengineering
1hwde8e,Data engg,"Anyone want to study sql(leetcode ) together or any other DE topic?

I am in USA.
Looking for someone who can collaborate in my learning journey ",0,4,1736317453.0,dataengineering
1hvqyut,To what point do you use IaC to deploy resources? Do you sometimes use the console in production?,"(I'll be using GCP services as examples)

&nbsp;

I understand the need of IaC if you want to deploy buckets, databases, etc.

&nbsp;

Do you use it as well for other resources such as Datastream (continous CDC pipelines), Database migration stuff? Or do you just create the template with it and then trigger the job using CI/CD?

&nbsp;

What I'm implying is that, in order to use any cloud provider's services, you are now depending on another technology, in this case Terraform.
Wouldn't that require a big devops team?

&nbsp;

If a new feature of an existing service is released, or if a new service is created, do you wait for Terraform connectors to be updated before using this service or do you just deploy using the console or gcloud commands?

&nbsp;

To what extent is it still good practice to use IaC?",8,12,1736255029.0,dataengineering
1hvxocz,Schema handling and validation in PySpark,"With this project I scratching my own itch:

I was not satisfied with schema handling for PySpark dataframes, so I created a small Python package called [**typedschema**](https://jwbargsten.github.io/typedschema/) ([github](https://github.com/jwbargsten/typedschema)). Especially in larger PySpark projects it helps with building quick sanity checks (does the data frame I have here match what I expect?) and gives you type safety via Python classes.  
  
 typedschema allows you to  

* define schemas for PySpark dataframes
* compare/diff your schema with other schemas
* generate a schema definition from existing dataframes

The nice thing is that schema definitions are normal Python classes, so editor autocompletion works out of the box.",5,3,1736273203.0,dataengineering
1hvboip,Feeling So Stuck in My Remote DE Job – Need Advice,"Hey everyone,

I could really use some advice. I’ve been working as a data engineer for two years now, but I’m starting to feel like I made a big mistake transitioning into this role.

A little background: I joined my current company five years ago as a business analyst right after graduating. Those first few years were great—I was part of an amazing team, worked on interesting projects, and learned so much. Then, an opportunity came up to move into a newly formed data engineering team, and since I’ve always enjoyed more technical work, I decided to go for it.

The team is relatively new and fully remote. I’m the only member in my country, while everyone else is spread across other locations. The idea was to bring someone in with a business background, which made sense. But looking back, I’ve realized this move hasn’t been what I hoped for.

Since transitioning, my workload has dropped drastically—I work maybe 30 minutes to an hour a day, tops. On top of that, I’m not doing much actual DE work. Most of my tasks are still what I did as a business analyst: writing SQL queries, creating data models, and building dashboards.

The team itself lacks structure and proper leadership. Everyone is pretty new to the data field, including our manager, so there’s no focus on industry standards like version control, code reviews, documentation, or DevOps practices. To make things worse, our tech stack is outdated—no cloud solutions, and we’re still running on MSSQL Server.

I’m worried because I know the DE field is advancing rapidly, and my current experience isn’t helping me stay competitive. I’ve been teaching myself modern tools and concepts since last year, but every time I intervw  for a new role, I get stuck around the second round. Feedback is usually that my technical skills aren’t strong enough yet.

I really don’t want to stay stuck in this role. My plan is to work on some side projects to build up my technical skills, but I’d really appreciate any guidance:

* What kind of projects should I focus on to demonstrate relevant DE skills?
* Any recommendations for resources (courses, tutorials, etc.) to help me level up?

Thank you so much for taking the time to read this. I’d be super grateful for any advice or tips you can share! 🙏",64,43,1736202078.0,dataengineering
1hvqhn9,How can I optimally run my python program using more compute resources?,"I am working on a text analysis project at work, where I have a Python program in a modular structure. It looks something like this:

    project_name/
    ├── configs/..
    ├── data/...
    ├── src/...
    ├── notebooks/...                 
    ├── .env                       
    ├── .gitignore                 
    ├── main.py                    
    ├── requirements.txt           
    └── README.md                  

  
Until now I have been running my python programs (through main.py) on my local machine. In short, the program downloads data via an API, preprocesses the data and inserts it into a sqlite database, and I am using *VS Code* and *anaconda* for this.

Due to the large amounts of data, I've had to load chunks of data in at a time, but it takes an endless amount of time to do. Eg. it takes 3.5 seconds per file for the download part of my program and 2 minutes per file the processing part.

  
With +100 000 files to handle, I can easily estimate that this would take weeks (more like months) to run on my local machine.

  
I am a data scientist and still relatively new in the role. I have some knowledge of Azure/cloud computing and the services available. However, I am still a little on engineering part of this project what is the best option (so far I have looked into Azure VM, Azure Functions and Azure ML). I am looking for a way to run my code more efficiently, and if it requires more compute resources, I would need to present the options to my boss and/or IT department.

  
I'd highly appreciate some help with my question:  
**What options can you suggest to run my python program with more compute resources (also taking costs into consideration)?**",4,17,1736253406.0,dataengineering
1hvu8d4,ETL to Data Engineering??,"I’m a traditional tool based ETL developer with Informatica and Informatica cloud and experience around 15+ years. Have been struggling to convert myself into new age data engineering roles. Are there anyone like me who converted into new age data engineering roles, that could earn big paycheck and long term career. What methods to follow? And any genuine review about data engineering academy, who benefited from them ? TIA. ",4,7,1736264625.0,dataengineering
1hv4pd9,"Is it just me who finds it mildly infuriating, when you say you have 4 years of experience in Scala, Spark and the recruiter goes ""Oh but do you have experience in pyspark?""",.,109,50,1736185080.0,dataengineering
1hvtogp,Stuck in QA: Should I Stick It Out or Seek New DE Opportunities as a Student?,"Hey fellow (I guess) DEs,

I'm a biomedical engineer who’s always been into computer science. After graduating, I decided to go for a computer science engineering master’s, and I’ll be graduating in June. I landed an internship as a Data Engineer Intern, and after completing it, they offered me a part-time job (which might turn into a full-time role after graduation).

Here’s the catch: Three months into the job, and I haven’t done any DE-related tasks. I'm stuck doing QA instead. When I complained, my team leader said, “We’re short-handed, and you’re more appreciated there. Plus, it’s just one project, and you’ll be done with it soon.” Fast forward, the project finished, they liked my performance, and now they’ve assigned me to another QA project.

What should I do here? Should I start looking for another DE internship and avoid wasting my time gaining zero DE experience? Given that I’m a student and can’t take full-time jobs (I guess), or should I stick with this job and wait until graduation to look for a better chance?

My only concern is the short time I’ll have to land a new job once I graduate. Being a citizen of a third-world country, I’ll need a visa to look for a job.

Any advice?",2,9,1736263117.0,dataengineering
1hvgyv8,How do you deal with being the single point of failure on a Data Team?,"I'm a Data Engineer who is currently contracting full-time. My team comprises of other contractors, too. Our team's objective is to build a new Data Platform for the company. Essentially automating data extraction from disparate sources (PostgreSQL, Oracle, SQL Server DBs, etc.), designing a new data model to centralize everything (star schema chosen, not by me), transforming data (AWS Glue) and realizing the models in a data warehouse (Redshift). After all of this, data is pushed to a BI tool (Tableau), where it is visualized. 

  
It was nice to have what felt like a true Data Team finally and to be working on a big project from the ground up. I felt I needed this as it gave me a chance to better my skillset and address weaknesses I have been ignoring or never had the chance professionally to take on. 

  
But, I feel as things have progressed we've not set standards as a team on how we're working or structuring things. It feels like we're snowballing in the wrong direction, constantly trying to meet deadlines instead of taking a breath and getting organized. 

I'm responsible for building the Glue jobs/Workflows, which I've done fine. But, the issue is that I'm the only one. I haven't had time to set up things like alerting for failures or even research how we could version control (not with AWS CodeCommit, it's being deprecated). We have a tight permissions issue where I work if you catch my drift. So, getting access to anything takes a second. 

Anyway, not having versioning in place stresses me out because this is one of the points of weakness I was referring to. I've never consistently had to version control except for one job, so it's been a minute. I used Azure Data Factory or DataBricks in the past, and at my other jobs, I'd just saved my SQL queries, which I know, looking back, is not great. This is more of a personal insecurity thing that I cannot blame on the team. I just wish we'd gotten started with it earlier. I'm going to be proactive about really practicing this each day. I have and want to.

I've also been asked to build Tableau dashboards and act as a ""Team Lead"", which I'm still weighing the benefits of. There's no extra pay, and I'm a point of contact for my Business Analyst & QA Engineer teammates, of whom I seem to have more experience. So, it doesn't help that I have to help them design their dashboards/reporting and build deliverables to plan test strategies. The other half of the team has a team lead as well and it includes the Cloud/DB Architects.

**We're just now, after 1 year, establishing SOPs.  Is this normal for a newer team?** I'm responsible for developing the SOPs for ETL implementation, Data backup & recovery. **What SOPs would you include to reassure the organization of a new Data Team?**

All I want is to get better organized and support the team better. I just don't enjoy being the person people depend on most when I'm the only one. It feels like everyone else has time to focus on their jobs and I kinda have to multi-task, help them with theirs, and do mine.

Sorry for being scatterbrained in this response. I'm just trying to calm down and find a way to better organize myself.

  
",23,19,1736216595.0,dataengineering
1hvc81f,I created a ML project to predict success for potential Texas Roadhouse locations. ,"Hello. This is my first end-to-end data project for my portfolio.

It started with the US Census and Google Places APIs to build the datasets. Then I did some exploratory data analysis before engineering features such as success probabilities, penalties for low population and low distance to other Texas Roadhouse locations. I used hyperparameter tuning and cross validation. I used the model to make predictions, SHAP to explain those predictions to technical stakeholders and Tableau to build an interactive dashboard to relay the results to non-technical stakeholders.

I haven't had anyone to collaborate with or bounce ideas off of, and as a result I’ve received no constructive criticism. It's now live in my GitHub portfolio and I'm wondering how I did. Could you provide feedback? The project is located [here](https://github.com/emilyschnepp/PredictingNewLocationsForTexasRoadhouse/tree/main).

I look forward to hearing from you. Thank you in advance :)",36,18,1736203467.0,dataengineering
1hvl29r,Questions on ELT pipeline,"I have built a ELT pipeline using Apache Airflow, DBT and snowflake. However, my lecturer wants me to prove it is ELT if the DBT only generate SQL queries to be run in Snowflake. May I seek for any suggestions on how to prove it?",8,11,1736230179.0,dataengineering
1hvwn3z,Higher education or continue job,"Hello everyone,

I am currently working as a Data Engineer for a European company in India, with 5 years of experience specializing in Snowflake. However, I feel the need for a change in my life to advance personally.

I am considering pursuing higher studies in AI/ML in Europe. Do you think this would be a worthwhile move, or should I continue progressing in my current career?

I would greatly appreciate any advice or insights you can share. Thank you!
",0,6,1736270700.0,dataengineering
1hvr3en,Language in a job posting,"What do you think about the language in this part of the job listing? Fine, kind of awkward, inappropriate?

https://preview.redd.it/mahht14smkbe1.png?width=868&format=png&auto=webp&s=78fcdb522163a7caa7bc7837a6b2b84b05c218b1

",2,8,1736255455.0,dataengineering
1hvm6xn,Is the AWS Certified Solutions Architect a good way to understand AWS and also the 'cloud' as a DE?  ,"I'm a junior DE who is about 18 months into my first gig. My team (1 senior, 2 juniors) uses S3 a lot and the senior guy handles most of the AWS infrastructure besides our S3 buckets. Recently, I was asked to familiarise myself with Terraform to create some resources code to go into the existing codebase. 

It got me thinking that I have a fuzzy understanding of scattered AWS/Cloud resources but I should probably invest time to properly understand the AWS landscape on the ""Associate"" level at least. 

Interested to hear perspectives from the community here. Thanks!",6,21,1736234853.0,dataengineering
1hvmc5q,Datahub with Kafka,"Hello everyone. I want to integrate Kafka with Datahub. My expectation is that it can recognize the data but everything i see is a blank. Does Datahub requires data (mine is JSON) which sent from Kafka to be in a specific format (like dataset specification, schema, etc)??

Pls help as i don't see much docs about this @@.

My outcome:

https://preview.redd.it/q7uy43ztwibe1.png?width=1902&format=png&auto=webp&s=a3e1ac1b20d1c493d907506f9ce37dfc7e3723c2

Expectation:

https://preview.redd.it/sr6zvpz3zibe1.png?width=1901&format=png&auto=webp&s=aea75c2c364cc4d36b28f833bec8d2e97bb588c1

",3,4,1736235514.0,dataengineering
1hvh2xp,What tool(s) are you using to visualize data pipelines in Azure DataBricks?,"We're exploring Azure DataBricks as the platform for our organization's core data engineering work and are currently struggling to figure out the potential options and best practices for visualizing pipelines in this product.

When I say pipeline, I mean a series of Python functions that ultimately read and write to tables. In our org we have hundreds of pipelines across various various business units, where each pipeline commonly has dozens of transforms averaging 100-200 lines of code. Pipelines often reference other pipelines, so it's effectively a giant DAG.

I've browsed the Azure & DataBricks documentation, thinking it would be easy to find examples, but I'm getting totally inundated by all the buzzwords. Anyone have working experiences visualizing pipelines made with Azure DataBricks?

EDIT: Appreciate everyone's responses, I've learned a lot. I'll be looking first at DataBricks Workflows / Azure Data Factory then expanding out to Unity Catalog, Asset Bundles, Airflow and also Orchestra.",11,5,1736216943.0,dataengineering
1hvglqo,Using DuckDB to read JSON files in S3,,5,5,1736215496.0,dataengineering
1hv4v8j,Career vent,"I just want to vent about my career for a bit. I’m very frustrated with a raise I just got and am stressed out and hope that writing things down will make me feel a bit better.

I’m someone with no degree who managed to go from retail -> analyst -> BI developer -> Analytics Engineer (without the title)

I started a job as an analyst just over 2 years ago. The job was incredibly basic, but even then was a bit underpaid. I was preparing reports, working only with Excel and proprietary systems. The industry was pretty complex though, which made things a bit harder. I got hired at 44k. (In the US)

I did incredibly well as an analyst. Within a few months I was being assigned more advanced work. I ended up building a macro that automated a lot of work. It impressed some higher ups, but one executive didn’t like the fact that some analyst with no degree was making macros. It ended up with us not using the macro because that executive wanted us to use the macro our tech team already built, which turned our reports from good looking easy to navigate reports to incredibly ugly reports.

However, they were impressed with everything I had done so they moved me to BI developer work. I was kind of happy about this but kind of worried, they had me using a program that was low-code. This meant no SQL or Python, which was incredibly important to me. The tool we were using had no career opportunities, but I thought that it might lead to a better salary and better opportunities at my current job.

After some time, the executive team decided they would not pay for the licenses, it was just too expensive and the program didn’t meet our needs. I said it would not meet our needs about a year before this point, but nobody listened to me.

They still had me keep building these reports. They said we are going to be going a totally new route but want me to prove I can build these reports and then I’ll get a promotion and a huge raise, especially if I can get things done in time. I ended up finishing every report before the deadline by quite a bit.

Then we got directive for how we would build the next system. We would take a much more modern approach, using Fivetran (maybe) for EL from source systems into Snowflake, while using dbt for transformations. Then we would have a user portal using PowerPages.

I learned a bunch about all these tools while things were being set in place. I was the person on the team helping guide us in the right direction and figure out how our projects will be structured, coming up with software requirements spec documents, deciding naming conventions, etc.

Then **Promotions** came around. I was excited, ready for a promotion. They told me that it will have to be in 2025, I WAS on the business team and now I’ll be on the software team, which has much bigger salaries. SO, in order to get the budget for my salary increase cleared, I would have to wait until 2025 instead of getting a promotion with anyone else.

Between this, and the fact that they expect me to have a skillset of SQL, dbt, Snowflake, Python, Fivetran, Power BI, Fabric in general, Power Pages, Power Apps, and Power Automate, I assumed I was about to get a pretty good raise.

After all, I had worked for 2 years, delivered everything better than expected and faster than expected. I constantly worked overtime and sacrificed time that should have gone to personal projects instead of low-code tools that were useless for my career. I didn’t expect my raise to be CRAZY, but I thought that maybe I could at least break out of the bottom 10% of BI developers. I was still making 44k and doing work that was significantly more complex than many others in the company making twice my salary.

Then I got my raise. A huge, giant raise to 52k.

I am expected to be able to build data pipelines that are business critical. To the point that if I delivered the wrong numbers it could destroy our reputation and cost us hundreds of thousands on the low end. I will be expected to make sure that the reports being supported by these pipelines are up to date within a few minutes, fully accurate, and have an interactive user portal, managing complex security configurations for analysts and clients to access their data.

All this and so much more, and they think that I am worth 52k.

Now I have no degree, I have spent a long time thinking my career would pay off, that if I worked with that crappy low-code tools it would lead to a much better salary and better opportunities.

It has led me to the point where now I am going to gain very valuable career experience, but my pay is incredibly frustrating. The market sucks, I have no degree, and my low-code tool experience is virtually useless. I worked my ass off for this company when I should have left after 6 months so that I could find a job where they would actually use decent tools and maybe even pay me fairly.

I feel burnt out, frustrated, used, and just upset overall. When they told me what my raise would be it felt like I was broken up with it hit so hard. I know people working in call centers with no degree making more than me. My previous job in retail was selling mattresses and I still make less hourly now than I did selling mattresses, which I was pretty awful at.

Thanks if you read this, I know it was incredibly long. Just needed to vent, because this situation has absolutely sucked. Hopefully in 2025 the market is decent and I will be able to take the skills I learn and switch to a new job ASAP.



",24,23,1736185479.0,dataengineering
1hvcb1n,Would transitioning from Data Migration to Data Analytics be lateral or vertical?,"I am in data right now, underpaid and looking for a new role but one still in data.

Would going from the following to a data analyst role be a pro or con?  
Role: Data Migration Specialist  
Tech: Excel, Pandas, SQL  
YOE: Nearing 4th year at this company(first data/engineering role)  
Salary: 62k  
Location: Role is Remote but i am in a VHCOL area  
Edit: Client facing as well and have developed my soft skills quite a bit. That said, not sure if it unusual to be client facing as a few DE Managers I spoke with told me that was an oddity. 

My company is not great in terms of tech and the rest of my team is pretty much 95+ percent excel only. I am the only one automating and consistently using SQL/Python. 0 access to anything more industry standard in terms of tools.

Currently learning Looker and applying to more roles. I was hoping to go into data engineering but everyone I've spoken with in the field mentioned my experience would grant me a jr DE role at best and even then, it's been very hard finding anything.

Would broadening my scope to DA roles but frowned upon in terms of work history? My long term goal is DE.

If there is any information that I may have left out needed for feedback, please let me know.",2,5,1736203687.0,dataengineering
1hv4h6k,How do you do EDA,Curious how y'all do EDA (exploratory data analysis) on new tables.  I find writing tons of SQL to be a bit repetitive and time consuming.,12,19,1736184539.0,dataengineering
1hutkwf,Effecicient way to prepare for Jr Data Engineer,"I recently got an Amazon opportunity for Jr. Data Engineer role, but I only have about 2 weeks to prepare. And I really value this opportunity given the climate of the market and my non-traditional background.

The difficulty is with my current job, I really don't have hands on data engineering experiences: ETL Designing, Data Modeling, Data Pipeline, and designing database/schemas. 

What I do know is knowledge in programming languages (proficient in Python and semi-proficient in SQL). My day-to-day job is extracting data with client and company API and organize data with Python scripting. I also write SQL queries to interact with database. However, I have never worked on the designing part.

The instruction specifically mentioned I will be tested on **""create data model"", ""design/optimize data pipeline/platform""** and **""provide two examples of past projects to demonstrate these skills""**

I really want to do my best to prepare and study for it. I have done over 700 Leetcode questions so that part I am confident. But I need guidance on how to effectively study for the relevant data engineering skills. What resource is great to brush up and learn data modeling/designing and would also give you hands on project experience which I can discuss during the call?

Thank you very much for help in advance.",36,21,1736148141.0,dataengineering
1hvczpl,Data modeling vs different update frequencies,"Hi, how do you approach data modeling in situations, when the majority of the use cases for a data warehouse only need daily updates, but there are a few that require more frequent updates to the data (as close to real-time as possible)?

Assume that you have hundreds of tables in your DWH, and it's not feasible from both a cost and duration perspective to update them all ""real-time."" The ""real-time"" use cases only need a small subset of your data. That said, both your daily and ""real-time"" data need to have the same complex business logic applied.

I assume that this is quite common and would love to hear about the best practices others have established and that work well from them from maintainability, cost and usability perspectives. Thank you!

**Example**  
Here is a very simplified example: the Customer table contains 2 fields that are needed by the ""real-time"" use cases (violet colored) and is created by transforming data from several different source systems, with only two of the source systems contributing to the creation of the required fields.

https://preview.redd.it/6x9cx4dihgbe1.png?width=992&format=png&auto=webp&s=e992fde5bccd5347eb65b6b3fd756eb19711099e

Note that the Customer table is a part of a central DWH model, which is meant to consolidate and conform data from many data sources to create a single analyst-facing model, that provides data for all kinds of analyses and dashboards. The Customer table can reference and be referenced by other tables that are part of this central model. If these tables are updated with different frequencies, it will lead to referential integrity and consistency issues.",2,3,1736205480.0,dataengineering
1hvgnte,Relational database question,"Im kinda new to the DE role, but i do have some question when working with mobile/web applications:

When working with data in a relational database (RDBMS), how do we handle the normalization process for application transaction data? Do backend engineers typically normalize the data before writing it to the database via an API, or is the data stored first in a raw format and then normalized afterward? In other words, what is the usual flow of transforming raw transactional data into a normalized schema in a typical data engineering or backend engineering setup?",1,2,1736215665.0,dataengineering
1hv3l0t,Understanding Apache Hive Metastore: The Key to Metadata Management in Modern Data Lake,"Apache Hive Metastore is the backbone of structured data and metadata management in modern data lakes. From its role in lakehouse architectures to integration with tools like Spark and Presto, this article covers it all. 



Curious about how it enables efficient querying and metadata management in distributed systems? Check out the article here:

\[Read the full article\](https://towardsdev.com/apache-hive-metastore-the-heart-of-metadata-management-and-structured-data-in-modern-data-lakes-d97260adb4f0)



I'd love to hear your thoughts or experiences with Hive Metastore! 💬",2,0,1736182386.0,dataengineering
1hv579w,Database DevOps Question ,"What is global trend for database(relational, nosql) deployment devops? 

What companies prefer? Inhouse developed  tools?

Environment wise deployment,  rollback feature, database monitoring post deployment etc.  List is long and I don't know if simple deployment using tools like flyway serve the purpose or a new breed of tool which has lots of bells and whistles.",3,2,1736186300.0,dataengineering
1huu4lg,Help on Upskilling myself,"I'm a data engineer with 8 yoe and woking for big tech from India.  
What I learned and worked through out my career is mostly on SQL, data warehousing, designing data warehouses and datamarts, Azure data platform tools like ADF, Datalake, blob storage, building small pyspark pipeline and maintaining large scaled pyspark pipelines. I won't call myself an programmer, but i can understand and manage python codes which is used for data engineering. And most of the time solving data related problems like bug fixing on code and preparing data on the SQL for the reports.

In recent times, I'm giving interviews but i'm not able to land in a job due to lack of programming knowledge. I'm looking for advice in how and where can i start learning python. And what else should I focus like streaming pipelines, etc apart from my skills to step up my game,",20,9,1736150531.0,dataengineering
1huuaxr,Real talk: Self-managed Airflow on k8s?,"Hi there fellow DEs,

so over the holidays this year it finally happened: I dipped my fingers into k8s, rather vanilla Airflow setup locally with kind, a custom Docker image, custom persistent volumes, secrets etc. 

That went reasonably well, but for all the apparent benefits, I am wary of how well that translates into a production cloud setting. 

I am very familiar with cloud infrastructure, Terraform, CI/CD, monitoring and automation, currently on GCP, although that can be changed.

My expected production workloads would be in the range of ca. 30 GiBs per day, way more if we're talking about historical backfills. 

I'm currently in a limbo between some contacts telling me ""Airflow on k8s is great, 15 bucks per month for us"" (no details on their workloads), switching to AWS to be able to use Astronomer, going beefy single node with Airflow or making the jump and really going self-managed GKE/EKS although I suspect a ton of foundational code and gotchas for the last option. 

I'd like to avoid MWAA/CC at the moment due to cost and slow deployments at the moment. Familiar with both.

What are your experiences with any of the above options?",12,15,1736151338.0,dataengineering
1hvbzyd,"Reasonable Junior/midlevel Expectations, aka what am I","I've been in my position as a junior data engineer for 2 years (after 2 years SWE and 2 in IT), and I'm currently the only data engineer (originally a small team). There are a couple of seniors heading up departments (including mine) and doing occasional PM duties for my work which is largely DE for BI, but I'm the only person that does my work. Small shop, I know. Work exclusively in cloud remotely.

I do a lot of what I think of traditionally as software architecting (which makes me uncomfortable)- translating business requirements into modules, classes, microservices, workflows and entire projects. I've migrated some of our ETLs across platforms, and translated low-code dashboarding into SQL. I do so while researching new tools and the new platforms, writing the documentation on how to use those tools/platforms, and answering data questions from stakeholders about keys, schema fields, and logic for revenue calculations and such. 

I get a half-hour meeting with a senior once per week which is also my management check-in, and other than that feedback comes in at PR. Since nobody else touches my work, I don't feel like those are getting looked at very close, it's PMs reviewing.  I somehow feel both senior and incredibly incompetent most of the time. Complaints about my lack of DE skills (lambdas, recursion, reusable code, etc) in my solutions are brought up when I've asked about a promotion despite being the person that everybody relies on for all this migration and maintenance. I don't get to write much Python because of that work, and when I do I'm doing it solo. 

My technical skills with both Python and SQL are decent, but the data engineering implementation to solve problems is where I keep getting stuck. It's been suggested that perhaps a data infrastructure engineering or cloud infrastructure position is a better fit for me within the company, because I'm great at fixes, bug hunting, provisioning and monitoring but struggle at finding technical solutions to new problems. If I know how to do something or I'm maintaining existing code, it's not a problem. Whenever I try to do something new, the project stalls because I don't have the experience to know how to solve a complex problem and nobody is there to share experience. It's one reason I've been looking into AI tools, but they usually don't help much. 

Am I a functionally a junior, midlevel, or what? Does any of this seem abnormal? I'm kind of at a crisis point, I don't want to quit this job but I also don't feel supported or that I'm going to get promoted anytime soon. Promise I am a human being, and I would love any advice I could get from seniors in the field right now. ",1,4,1736202889.0,dataengineering
1hv68fq,Data Leader's Roles and Responsibilities,"I am evaluating a couple of opportunities in the leadership side of data engineering. I was a data architect and am currently a senior manager. What are some of the roles and responsibilities of a data leader of a larger organization? Maybe include some of the positive traits and how a data leader can impact the organization's overall growth.

Few skills I think:

\- Strong understanding of data engineering, analytics, and emerging technologies like AI and ML.

\- Oversee the design and implementation of scalable, robust data platforms. 

\-  Future Readiness.",2,2,1736188832.0,dataengineering
1hv52jx,Need Help Redesigning Airflow Setup for Hundreds of DAGs,"https://preview.redd.it/2i1h8ba9webe1.png?width=494&format=png&auto=webp&s=fa751bbdb75590c313c8e40cfd5d13fa0509bc1a

Hey everyone,

I’m dealing with an old Airflow setup (local executor, older version, SQL metadata DB) running on a GCP instance with DB setup on a cloud sql instance of GCP. We have 100+ DAGs (hourly, daily, weekly), with 5-10 tasks per DAG. Most of the tasks submit jobs to Dataproc clusters.

The current setup is clunky and hard to scale, so I’m starting fresh. I need an Airflow setup that can handle hundreds of DAGs with better scalability and maintainability because as you can see, the existing setup is already bloated and we don't want to look at cloud composer or similar cloud managed airflow as well!(because of some limitations that we have)

How do you manage your Airflow setups?

What’s worked for you?

If you’ve been in a similar situation, I’d love to hear how you approached it, along with any resources or advice. Thanks! 🙏",2,7,1736185973.0,dataengineering
1huzjak,Bronze Layer of a medallion Architecture VS DataLake?,"Hi all, 

I am currently trying to implement a data Lakehouse. Hence, I've come across the Medallion architecture, and I am wondering : what differenciates a Data Lake and the Bronze Layer of a Lakehouse ? ",4,18,1736171682.0,dataengineering
1hv4phj,Meet ups in January in Dallas/Fort Worth ,Hey guys ! Hope this is allowed here. Made a post with some meetups around data I'll be visiting this month if anyone is interested in some good events for networking. You can check it out [here](https://www.linkedin.com/posts/sergio-ramos-analyst_dfw-user-group-snowflake-genai-ml-toolbox-activity-7282083790993141761-ryZO?utm_source=share&utm_medium=member_android),2,0,1736185088.0,dataengineering
1hurrg4,Data Governance - What do you do on your day to day?,"My company is creating a new dept for data governance, but I don't know anyone that has experience in it, so does anyone here have some exp regarding this topic? How's your day to day? What type of projects you have? 

Thanks!",19,13,1736141013.0,dataengineering
1hv82p5,Career Evolution Advice: SQL Server DBA Exploring PostgreSQL and Data Engineering Paths,"Hello DE Community, and Happy New Year!

Long-time lurker here seeking career advancement advice. I know this topic has been discussed multiple times, and I’m actively researching it, but I’d greatly appreciate your patience and thoughts from your personal experiences. Please bear with me as English is my second language.

I currently have a great job, but in recent years, I’ve noticed significant shifts in the data and database management landscape. These trends make me slightly concerned about my career security as a SQL Server Admin/Engineer. At the same time, I’m eager to learn new concepts, approaches, and technologies related to data and databases to expand my skill set.

I’ve identified two major directions I’m considering for my career growth, and I’d like to get practical insights into each:

**PostgreSQL Adoption**

Many companies, including mine, are moving towards PostgreSQL as the RDBMS of choice. We’ve already migrated several systems from SQL Server to PostgreSQL, particularly on AWS Aurora and RDS.

**Data Engineering Transition**

The shift towards using Snowflake and Databricks for managing, analyzing, and transforming data also interests me. These platforms seem pivotal in modern data workflows, but I don’t fully understand their specific use cases or the problems they solve.

# Here’s what I’m looking for:

Insights into the career potential of these two paths (PostgreSQL vs. Data Engineering).

Recommendations on which path offers more job flexibility, remote opportunities, and strong compensation prospects.

Advice on developing practical experience and understanding real-world problems solved in these areas.

# Concerns About Each Path:

**PostgreSQL Focus**

While I am know a lot are considering PostgreSQL as fantastic RDBMS, I’m concerned that focusing  on it I will limit career perspectives and lock myself to two RDBMS platforms (SQL Server and PostgreSQL).

**Data Engineering**

Data Engineering seems exciting but also complex, with undefined responsibilities and many required skills. I lack a clear understanding of the problems Snowflake and Databricks solve and the complementary technologies I’d need to potentially  master.

# My Current Role and Resources:

At my current job, I have the option to look into all these technologies—PostgreSQL (Aurora, RDS), Snowflake, and Databricks—but only in a DEV environment(which is used by different than mine team so I dont have any use cases to look into and not involved in any projects). I also have access to a Pluralsight account for training.

# About Me:

\-15 years working on SQL Server- on-prem as well as few years on Azure(mostly Azure SQL server VMs and currently on AWS(mostly EC2 but some SQLRDS as well). So I have quite good fundamental knowledge about both cloud providers in respect of how to provision and manage SQL Server.

\-Very good at HA\\DR - a lot of managing WSFC , Alwayson AGs, Mirroring, Log shipping(crafted my own implementation using Azure blob as backup share)

\-Quite good at performance tuning and troubleshooting using various of available tools(query store for sure) and self-crafted scripts, traces, extended events, etc.

\-Was involved in quite a few different infrastructure\\Devops projects related to SQL Servers provisioning\\management with usage of terraform, ansible, Jenkins, so have some practical experience there as well.

\-Sufficiently good at Powershell scripting and using it daily(also crafted few Python codes for automation but not much practice here)

I sincerely appreciate any insights from those who have made similar transitions or work in these areas. Thanks in advance for your guidance- any advice, resources, or insights would be greatly appreciated!",1,4,1736193290.0,dataengineering
1hv1fz8,question about a hybrid approach to relational and document db,"hey guys,

**i have a problem:**

    people need both high-level aggregated views, as well as visibility to customer-specific granular and transactional data.

essentially, a customer 360.

i currently load data into parquet files on s3.  any aggregations go into redshift.  from there, someone connects powerbi, and shows metrics.

what i dont want to do is give them the ability to load all events/transactions.

hell, i don't even want them to be able to do it in athena.  they will just join everything, and load it all in anyway.

i also don't want to have to create a bunch of materialized views, and have to work with analysts on which ones they can use for a specific dashboard.  there will be no end to this - and they will end up wanting to combine anyway.  they are also not good analysts, so i would be doing too much work.

  
**possible solution:**

    create documents in dynamodb (one for each customer/prospect) and run processes that update each document with data in parquet.  including aggregations done outside of dynamodb (redshift, etc.)

this way, users have redshift for their powerbi dashboards.  but they can also query dynamodb (possibly via an api endpoint) for a specific customer when they want to know all about them.  only part im not sure about is if you can have some kind of trigger in a dashboard to call that.

  
anyone have thoughts here?  would be really helpful.

",2,1,1736176954.0,dataengineering
1hux2i1,Processing Multiple json files with different schema into a dataframe using pyspark.,"How to load multiple json files with different schema in to a dataframe. I am trying to use pyspark (local) and it's exiting with memory issues after processing around 30 files. I try to optimize the script but it's not providing good results. Below is the script. any suggestions?. My machine has 64g RAM. What is the best way to processes 1000's of files?.

    from pyspark.sql import SparkSession
    from pyspark.sql.functions import lit
    from pyspark.sql.types import StructType, StructField, StringType
    import os
    import psutil
    
    # Function to get memory usage
    def
     get_memory_usage():
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            ""rss"": memory_info.rss / (1024 * 1024),  # Resident Set Size in MB
            ""vms"": memory_info.vms / (1024 * 1024),  # Virtual Memory Size in MB
            ""percent"": psutil.virtual_memory().percent  # Percentage of memory used
        }
    
    # Initialize SparkSession with increased memory settings
    spark = SparkSession.builder \
        .appName(""Load JSON Files from Local Directory"") \
        .config(""spark.driver.memory"", ""32g"") \
        .config(""spark.executor.memory"", ""32g"") \
        .getOrCreate()
    
    # Path to the local directory containing JSON files
    json_dir = ""jsondir/flatten_json/""
    
    # Initialize an empty DataFrame with a defined schema
    schema = StructType([
        StructField(""filename"", StringType(), 
    True
    ),  # This will hold the filename
        # Add other expected fields here as needed
    ])
    final_df = spark.createDataFrame([], schema)
    
    # Counter for processed files
    file_count = 0
    
    # Read all JSON files from the local directory
    for file_name in os.listdir(json_dir):
        if file_name.endswith("".json""):
            file_count += 1
            
    print
    (
    f
    ""Processing file: {file_name} | Total files processed: {file_count}"")
    
            # Construct full path to the JSON file
            file_path = os.path.join(json_dir, file_name)
    
            # Read the JSON file into a DataFrame
            current_df = spark.read.json(file_path)
    
            # Add a column with the filename only if it doesn't exist already
            if 'filename' not in current_df.columns:
                current_df = current_df.withColumn(""filename"", lit(file_name))
    
            # Ensure both DataFrames have consistent columns before unioning
            all_columns = set(final_df.columns).union(set(current_df.columns))
    
            for col_name in all_columns:
                if col_name not in final_df.columns:
                    final_df = final_df.withColumn(col_name, lit(
    None
    ).cast(StringType()))
                if col_name not in current_df.columns:
                    current_df = current_df.withColumn(col_name, lit(
    None
    ).cast(StringType()))
    
            # Union the DataFrames with null filling for missing columns
            final_df = final_df.unionByName(current_df, allowMissingColumns=
    True
    )
    
            # Monitor memory usage after processing each file
            memory_usage = get_memory_usage()
            
    print
    (
    f
    ""Memory Usage after processing {file_name}: {memory_usage}"")
    
    # Remove prefixes from column names after combining all DataFrames
    for col_name in final_df.columns:
        new_col_name = col_name.replace(""file_info_"", """").replace(""result_"", """")
        final_df = final_df.withColumnRenamed(col_name, new_col_name)
    
    # Coalesce to a single partition for output (if necessary)
    final_df = final_df.coalesce(1)
    
    # Save the DataFrame to a CSV file locally
    output_path = ""outputdir/flatten_json_output.csv""
    final_df.write.csv(output_path, header=
    True
    , mode=""overwrite"")
    
    # Show the DataFrame (optional)
    final_df.show(truncate=
    False
    )
    
    # Stop the SparkSession
    spark.stop()
    
    ",3,6,1736163437.0,dataengineering
1huumss,How data engineers can prepare for AI era?,I am currently a data engineer. I am wondering if it’s worth to study some AI knowledge in depth or just keep updating myself with latest data engineering tools? ,4,38,1736152871.0,dataengineering
1hucak8,Which Pipeline Design Patterns and Storage Formats Do You Use The Most?,"Hey, r/dataengineering!

We’ve been doing some research on pipeline design patterns and data storage formats that are commonly used by data engineers. We identified 10 Pipeline Design Patterns and 11 Storage Formats and published them in our substack newsletter where we write as 2 Data Engineers.

As a Data Professional, how often do you actually use these patterns and formats in your job? Are there specific ones you rely on, or perhaps variations that you’ve found particularly effective?

**10 Pipeline Design Patterns**

1. Raw Data Load
2. ETL (Extract, Transform, Load)
3. ELT (Extract, Load, Transform)
4. EtLT (Extract, transform, Load, Transform)
5. Data Virtualisation
6. Streaming Pipelines
7. Lambda Architecture
8. Kappa Architecture
9. Data Mesh (Decoupled Architecture)
10. Data Lakehouse Pattern

Post Link: https://open.substack.com/pub/pipeline2insights/p/10-pipeline-design-patterns-for-data

**11 Storage Formats**

1. Row-Based
2. Columnar
3. Key-Value
4. Document-Oriented
5. Graph
6. Time-Series
7. Object
8. Distributed File System
9. In-Memory
10. Wide-Column
11. Hybrid

Post Link: https://open.substack.com/pub/pipeline2insights/p/11-storage-formats-for-data-engineers",31,17,1736098525.0,dataengineering
1humkeg,Regarding pipeline for streaming data,"Hi All,

I am quite new to Data engineering and want to understand what is the workflow/orchestration tools do you use for in-house deployment for streaming data. 

I have this pipeline with 3 stages namely dataGather, dataPrep and featureGenerate. Unlike batch processes where each stages runs in a sequence one after another like in DAG, I am setting it for realtime data received as a stream. I understand there are tools like Kafka Streaming Pipeline to setup a pipeline for streaming data but I am looking for something more pythonic in nature.

Does Airflow do the same? As per my understanding it is DAG in nature.",5,5,1736124940.0,dataengineering
1hu9opp,Breaking up a monolithic dbt repo with dbt-loom: Seeking lessons learned,"Hey folks, I’m leading an effort to split our single dbt-core monolith (thousands of models on snowflake) into a multi-project, multi-repo setup leveraging both dbt-core and dbt-loom. We’re currently in the proof-of-concept stage, we’d love to hear from anyone who has done something similar.

1. **If you decided** ***not*** **to move forward with splitting your repo, what led to that decision?**

2. **What were the biggest challenges you faced during the migration (technical, organizational, or otherwise)?**

3. **For those who have implemented this in production, how do you handle multi-repo package upgrades?** 

4. **If you used** dbt-loom**, what was your experience like? Any caveats or must-knows? issues with dbt-power-user vs code package ?** 

5. Any CI insights will be welcomed 

Whether you’ve completed this journey or are still in the thick of it, your insights would be super valuable. Please feel free to share any lessons learned, resources, or best practices that made your transition smoother. Thanks in advance!

*#dbt #dbt-loom #datamodeling #migration*",28,19,1736091656.0,dataengineering
1hucims,DB creation use case ( i am newbee) ,"Hey guys
I have been a data engineer now for a few months.
There is a usecase in the team where we want to create database using all of our image/video metadata so that users can easily query.
Below are some use cases
1. There are multiple different datasets with different attributes like image dataset , video dataset. In images also we can have multiple datasets.
2. The database will only be for metadata i.e. we wont be saving image/videos . Just their metadata - for example image width etc
3. The datasize can increase as new data gets added.
4. Right now metadata is saved as parquet files


My consideration 
At first i thought of using duckdb , since it loads data from parquet easily. but other people are not liking the idea since they feel its still new and can constraint us in future if we want more features

I am thinking of using postgresql, since it is open source, more stable.
I want help in thinking how to think more broader.
My usecase is very simple now - i want free/cheap database, it should support size till TBS , the response time should be less. Update or write time is not much of a issue since its wont be frequent. Even database is down sometime its not much of issue since its internal to the team and we can survive that. 

Are there any other things i should consider, is there any other database i should consider? ",10,15,1736099103.0,dataengineering
1hu2ysg,Data engineering with cybersecurity ,"Hi All,

I’m curious about the potential of expanding my knowledge in cybersecurity as a data engineer.

I’m interested in understanding the benefits of this combination and the job market for individuals with a background in data engineering and cybersecurity.

Additionally, I’d like to know which courses would be beneficial in this field.

I’ve seen a Google certificate (https://grow.google/certificates/cybersecurity/) that seems relevant. Is it worth pursuing?",31,14,1736069479.0,dataengineering
1hup27a,Guide/suggestion on taking zach wilson's Dataexpert.io bootcamp.,"Hey guys, I'm (21m)  a final year cs under grad but interested in finance and data science,AI. To build an automated trading bot or a stand alone system , I think zach wilson's bootcamp will help me provide all the tech requirements apart from decision making on which tickets to select. Is it a good thing to pursue or am I too early? Also, having a bootcamp on my cv can help me  kickstart my journey i think. Share your views suggestions, experiences.",1,3,1736132265.0,dataengineering
1hu40lw,1 Million needles in a Billions haystack,"Hi All,

we are looking for some advice regarding available engines for the relatively easy, but practically hard problem:  
suppose we have long(few years) history of entities life events, and we want each time to query this history(or data lake if you'd like) by some very small subset of entity ids(up to single digit Millions)

We looked at BQ(since we have it) and Iceberg(following Netflix case why Iceberg was create at the first place, however there is subtle difference that Iceberg supports select by specific user id or very few of them very well)  
However, all of them seem to fail to do this ""search"" by 1Million entities efficiently and dropping to sort of ~~full table scan~~ ""too much data scan""(what is too much? suppose each history entry is few Kbs and from BQ query stats we scan almost 30MB per entity id) (e.g. for query select h.\* from history h join referenced\_entities re on h.entity\_id = [re.id](http://re.id) and h.ts between X and Y; i.e. 1Mil entity ids sit at some table referenced\_entities and we want to filter by joining with this reference table)

history table is partitioned by hour(ts), and clustered/bucketed by entity\_id

Another option would be to create some custom format for index and data and manage it manually, creating api on top etc, but this would be less maintainable

Would like to hear ideas what solutions/engines permit such queries today in efficient way ?

update: this history of events contains rather nested structure, i.e. each event is less suited to be stored as flat table (think about highly nested entity)

thanks in advance,

Igor

update: added that join query has condition by ts, added mention that history table partitioned & clustered

update2: full table scan I've mentioned is probably wrong term. I think I created a lot of confusion here. what I meant is that after pruning partitions by time(obvious pruning that works) we still need to open a lot of files(iceberg) or read a lot of data(BQ)  
",22,59,1736072757.0,dataengineering
1htz17n,Udacity vs DataCamp: Which Data Engineering Course Should I Choose?,"Hi

I'm deciding between these two courses: 

1. Udacity's Data Engineering with AWS 

2. DataCamp's Data Engineering in Python

Which one offers better hands-on projects and practical skills? Any recommendations or experiences with these courses (or alternatives) are appreciated!
",45,37,1736056417.0,dataengineering
1husn2c,Help me transition into a data engineer,"Hi all,

Wanted some help transitioning into a data engineer from a data analyst

My current tech stack is :
Gcp (Worked on / have exp in Bigquery, Cloud functions, scheduler, IAM, pub/sub, VM instances, Billing, cloud storage, datastream)
Python
Sql
Looker
Have experience in web scraping (selenium)
Api integration 
ETL

Currently learning :
Pyspark
Hadoop (HDFS)

What more should i learn ?",0,3,1736144221.0,dataengineering
1hu1s9i,Is there a free tool which generates around 1 million records by providing a sample excel file with columns and few rows of sample data?,I wanted to prepare some mock data for further use. Is there a tool which can help do that. I would provide an excel with sample records and column names. ,18,42,1736065847.0,dataengineering
1huaa13,Access Databricks Unity Catalog through Apache Doris,Any idea on how to access Databricks Unity Catalog from big data platforms like Apache Doris?,5,0,1736093252.0,dataengineering
1huq9sj,🔍 Searching for the latest AI breakthroughs in BI and Big data?,"Check out our in-depth video exploring how AI is transforming automation and analytics. From analyzing real-time social media trends to executing tasks dynamically, discover how Large Language Models (LLMs) are making traditional methods obsolete.

💡 Perfect for anyone working on a new AI project or curious about reimagining automation workflows. Watch the full video here: [https://youtu.be/fkFopFgA0ec](https://youtu.be/fkFopFgA0ec)

Let’s discuss:

* What’s your favorite AI application in real-world scenarios?
* Have you tried replacing SQL with NLP-based queries?

\#AI #ReimagineAI #TechInnovation #BigData",0,0,1736136019.0,dataengineering
1hu99hg,SnowPro Core Certification,"Hi all.
I am currently working as a Data Analyst in a startup..i currently make dashboard and maintaining Database like updating it with new data, cleaning and makeing change as per organization requirement.

I came across Snow Pro Core certification.
I want to know how and well it would help me in my career growth..Will it help me ?

Any advice and suggestions...?
Thankyou 🙏",2,8,1736090488.0,dataengineering
1hugtxu,Advice Needed: current challenges and future growth with databricks skill,"Hi Reddit Community,

I am involved in data migration activities. Skilled in python, sql, apache spark, databricks,github. I have few queries wrt databricks. 

1. How valuable is Databricks expertise in the current job market?

2. Apart from PySpark and SQL, what additional skills should I focus on to grow my career with Databricks?

3. Is it worth pursuing Databricks certifications, and which ones should I target?

Any advice, resources and personal experiences you can share would be very helpful. 

THANK YOU!!!
Looking forward to your suggestions!

",1,3,1736109999.0,dataengineering
1hu4uah,What are the essential skills needed for Enterprise Data Architecture?,"  
**Are you curious about what it takes to master Enterprise Data Architecture?**



Data architecture is not just about storing information; it’s about designing a robust framework that transforms raw data into actionable insights for scalability, innovation, and consistency. 🌐💡



In my latest Medium article, I dive deep into:

1. **System Architecture** \- Explore key concepts like OLTP, ODS, Data Lakes, and the Medallion Architecture.

2. **Data Modeling** \- Understand dimensional modeling, Star vs. Snowflake schemas, and the Galaxy schema.

3. **Emerging Trends** \- Discover cutting-edge approaches like Data Mesh and Federated Multi-Cloud Architectures.



**Here’s a quick highlight from the article:**

• Why Medallion Architecture enhances data governance.

• The difference between Data Mesh and Federated Multi-Cloud Architecture.

• How to build secure, scalable ETL pipelines.



👉 Read the full article here: [Enterprise Data Architecture and Modeling: Key Practices and Trends](https://medium.com/@premvishnoi/enterprise-data-architecture-and-modeling-key-practices-and-trends-7a40d4fe8746)



**I’d love to hear from the community:**

• What skills do you think are crucial for excelling in Enterprise Data Architecture?

• Are there specific tools or approaches you swear by?

\#DataArchitecture, #DataEngineering, #CloudComputing, #DataMesh, #BigData

Let’s discuss and grow together! 🚀

",2,0,1736075650.0,dataengineering
1huo7j7,Anyone else think its crazy that frontend devs get all these cool AI tools while there's no equivalent interface for data apps? ,"I've been seeing some insanely cool frontend ai tools popping up lately.  that make it stupidly easy to prompt, run, edit, and deploy full-stack web apps. But they’re all p much entirely focused on frontend apps.

I feel like there hasn't been a good analogous experience for data apps that's sufficiently easy to use that's gone viral since ai.

Does anyone know a tool out there doing for data what Vercel’s v0 did for web apps?",0,15,1736129721.0,dataengineering
1htlxu4,"You programming RLHF, RLHF programming you...","The more I think about this, the more I realize the meme undersells how deep this goes.

RLHF isn't just developers training AI - it's a two-way mirror where users unknowingly shape AI behavior while being shaped in return. Every interaction, every thumbs-up, becomes part of a feedback loop where the AI optimizes not for truth, but for reward.

And here's the kicker: users end up reward-seeking too, subtly adapting to elicit the most engaging (or emotionally validating) responses from the AI.

We’re not just programming AI to be helpful—sometimes we’re training it to be entertaining, bias-confirming, or manipulative. It’s like Goodhart’s Law but with human cognition in the loop. When the measure (user feedback) becomes the target, both the AI and the user drift toward reinforcing patterns that aren't aligned with reality.

The really concerning part?

This loop accelerates.

As models get better at predicting preferences, users become more reliant on AI-generated content that matches their expectations. The AI becomes a cognitive mirror that subtly warps both reflections over time, bending toward what gets rewarded rather than what's true.",41,9,1736018286.0,dataengineering
1htjbdp,First time extracting data from an API,"For most of my career, I’ve dealt with source data coming from primarily OLTP databases and files in object storage. 

Soon, I will have to start getting data from an IoT device through its API. The device has an API guide but it’s not specific to any language. From my understanding the API returns the data in XML format. 

I need to:

1. Get the XML data from the API

2. Parse the XML data to get as many “rows” of data as I can for only the “columns” I need and then write that data to a Pandas dataframe.

3. Write that pandas dataframe to a CSV file and store each file to S3.

4. I need to make sure not to extract the same data from the API twice to prevent duplicate files. 

What are some good resources to learn how to do this? 

I understand how to use Pandas but I need to learn how to deal with the API and its XML data. 

Any recommendations for guides, videos, etc. for dealing with API’s in python would be appreciated. 

From my research so far, it seems that I need the Python requests and XML libraries but since this is my first time doing this I don’t know what I don’t know, am I missing any libraries? 
",47,31,1736011430.0,dataengineering
1htm40x,Is it worth it.,"Working as a Full time Data Engineer in a US based project.

I joined this project back in July 2024. I was told back then them then it'll be a project for snowflake data engineer lots of etl migration etc.

But since past 5 months i am just writing SQL queries in snowflake to convert existing jet reports to powerbi,they won't let me touch other data related stuff.

Please guide me whether its part of life of DE that sometimes you get awesome project and sometime boring.",16,40,1736018734.0,dataengineering
1htoha6,What skills should I work on to improve marketability,"Title says it all, I am currently a data warehouse developer, where I am getting exposed to BI software (Microstrategy) and ETL processing. My company I am currently at does not use any cloud platforms, so this is an area that I was planning on familiarizing myself with, most likely AWS. 

What other skills should I work on? Python and Pyspark seem like the other low hanging fruit, but was curious if anyone else has some other things that I could benefit from learning more about. I am currently have a little over a year of experience and want to move jobs soon. ",11,7,1736024949.0,dataengineering
1ht5swe,hot take: most analytics projects fail bc they start w/ solutions not problems,"Most analytics projects fail because teams start with ""we need a data warehouse"" or ""let's use tool X"" instead of ""what problem are we actually solving?""

I see this all the time - teams spending months setting up complex data stacks before they even know what questions they're trying to answer. Then they wonder why adoption is low and ROI is unclear.

Here's what actually works:

1. Start with a specific business problem

2. Build the minimal solution that solves it

3. Iterate based on real usage

Example: One of our customers needed conversion funnel analysis. Instead of jumping straight to Amplitude ($$$), they started with basic SQL queries on their existing Postgres DB. Took 2 days to build, gave them 80% of what they needed, and cost basically nothing.

The modern data stack is powerful but it's also a trap. You don't need 15 different tools to get value from your data. Sometimes a simple SQL query is worth more than a fancy BI tool.

Hot take: If you can't solve your analytics problem with SQL and a basic visualization layer, adding more tools probably won't help.",262,60,1735963051.0,dataengineering
1hu2biw,"Seeking Remote Job Opportunities in Australia, Europe, and New Zealand: Tips and Visa Sponsorship Insights Needed!",,0,16,1736067600.0,dataengineering
1htdvcl,How/where do I find experts to talk to about data engineering challenges my company is facing?,"I started a SaaS company 6 years ago that accounts microtransactions for our customers and uses a multi-tenant architecture with a single Postgres DB. We're a small self-funded company, 12 people total with 2 engineers including me. At this point, our DB is 1.8TB with \~750 million rows in our largest table. Our largest customers have \~50 million rows in that table.

When we first started running into performance issues I built a service that listens to Postgres CDC via Kafka and caches the results of the most critical and expensive queries we use. Generally, it has worked out ok-ish, as our usage pattern involves fewer writes than reads. There have been a few drawbacks:

* Increased complexity of the application code (cache invalidation is hard), and as a result slower velocity when building new features
* Poor performance on real-time analytics as we can't anticipate and optimize for every kind of query our customers may make
* Poor performance during peak usage. Our usage pattern is very similar to something like TurboTaxes, where a majority of our customers are doing their accounting at the same time. At those times our cache recalculation service falls behind resulting in unacceptably long wait times for our customers.

I've been looking into potential solutions, and while my data engineering skills have certainly grown over the last few years, I have little experience with some of the options I'm considering:

* Vertical scaling (ie throw money/hardware at our single DB)
* Git Gud (better queries, better indices, better db server tuning)
* Horizontal scaling using something like Citus
* Leveraging a DB optimized for OLAP

I would love to talk to a person with more knowledge that has navigated similar challenges before, but I'm unsure of how/where to look. More than happy to pay for that time, but I am a bit wary of the costs associated with hiring a full on consulting firm. Any recommendations would be greatly appreciated.",30,37,1735995557.0,dataengineering
1ht8832,I made it! Finally titled properly!,"After a long stint doing the job duties of a Data Engineer without the title. Having had ""Data Engineer"" as my email signature as an aspiration. I got a new DE Job!

I transitioned from a BI Analyst (python, SQL, R) position

Professional Career:
4yrs as a generalist Analyst
3yrs as a specialized BI Analyst/DE (heavy coding)

My experience domain is insurance. Primarily life insurance. 

I'm a pretty big neophyte when it comes to provisioning databases or SQL optimization but I'm fantastic with domain specific knowledge.

If you get a chance to be an expert in operational eccentricities and goals take copious notes they will come in handy.
",67,7,1735971524.0,dataengineering
1hto8v5,Any recommendations for an MBA/Masters program for management?,"So, I'm currently doing some research into MBA programs since I'm wanting to learn more about the business side of things/build more of a network/my ex-boss mentioned it might be a good fit for me.

Bit about me: Worked as a junior DE for about 3 years before I got laid off recently. I really enjoyed the technical work, but I also got a chance to liaison between some client and development teams, which was great as well. In one of my last 1-on-1's with my boss, he mentioned the MBA thing since he said I seemed pretty good with aspect. Personally, I still want to stay in the DE/Analytics sphere, but I'm interested in moving up to product/engineering management or something like that eventually and wanted to get more opinions from more people. Anybody know any good programs or people who went through something similar? I've read a lot of people saying experience always comes first, so I'm thinking of just doing a part time program and continuing to work alongside it (throwing out a lot of job apps atm).

  
TL;DR: Wondering if people have suggestions for MBA or similar programs that'll give me an edge in tech or analytics/what people's opinions or experiences are with them.",0,8,1736024349.0,dataengineering
1htcxy3,sqlmesh open source for production pipelines in enterprise,"Hello,

  
I am working at an enterprise and evaluating simplified way to build automated pipelines for data transformation. Have came across dbt and sqlmesh and wondering if anyone is using sqlmesh for production workloads in enterprise. 

  
I know dbt-core is definelty being used but wondering the same sqlmesh. 

",10,4,1735991939.0,dataengineering
1ht5vo2,What is a cost cutting method that surprised you when in use?,This post is part of my learning process with respect to data engineering. ,44,39,1735963297.0,dataengineering
1hteopp,Which sector is better? ,"Hello I have been working for 4 years now since graduation in the data management function specially Data quality and Governance.

All my experience in FMCG , should I continue in this field or should I move to the banking sector ? 

Which is better? Career wise , learning wise and money?",2,13,1735998386.0,dataengineering
1hsnqzv,Databricks Certified Data Engineer Associate - I PASSED!!!,"Hi everyone! I got my first Databricks certification last week! It wouldn’t have been possible if it hadn’t been for Reddit and a couple of bucks. At first, I was so lost about how to approach studying for this exam, but then I found a few useful resources that helped me score above 90%. As a thank you (and also because I didn’t see many up-to-date posts on this topic), I’m sharing all the resources I used.

Disclaimers:

* The voucher was paid for by the company I work for.
* The only thing I paid for was a 1-month Udemy Personal Plan subscription (the Personal Plan allows you to explore numerous courses without having to make individual payments).

**Resources**:

1. **Mock Tests** These were the most useful. You’re studying for an exam rather than directly for Databricks, so emphasize the questions (and the way they’re presented) that appear on the exam. My personal preference order: [Practice Exams | Databricks Certified Data Engineer Associate (Udemy)](https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/learn/quiz/5731990/) It contains most of the questions you’ll find in the exam. If I had to guess, around 70% of them appeared in the real exam. [Databricks Certified Data Engineer Associate | Practice Sets (Udemy)](https://www.udemy.com/course/databricks-certified-data-engineer-associate-practice-sets/learn/quiz/6227872) Some reviews mention incorrect answers, spelling mistakes, and difficult questions, but it’s still worth doing. The mock tests are divided into six sets, three of which focus on two topics at a time, like a revision set. This approach helps you concentrate on specific areas, such as “Production Pipelines,” because you’ll get 20+ questions per topic. [Databricks Certified Data Engineer Associate Practice Tests (Udemy)](https://www.udemy.com/course/databricks-certified-data-engineer-associate-practice-tests/learn/quiz/5596958/) This one is quite challenging without prior experience in Databricks. Skip it if you’re already comfortable with the first two, but it’s there if you want extra practice.
2. **Courses** I know it’s odd to put mock tests first and then courses, but trust me, if you already have Databricks experience, courses might not be strictly necessary because they tend to cover basics like %magic commands or attaching a cluster to a notebook. However, if you need a complete and useful course to sharpen your knowledge, here’s the one my colleagues and I used: [Databricks Certified Data Engineer Associate (Udemy)](https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664610) It’s simple, complete, and gets straight to the point without extra fluff.
3. **ChatGPT** Despite what some might think, ChatGPT is invaluable. Not sure what LIVE() is? Ask ChatGPT. Want to convert something into Spark SQL? Ask ChatGPT. Need to ingest an incremental CSV from AWS S3? Ask ChatGPT. If the documentation isn’t clear or you’re struggling to understand, copy and paste it into ChatGPT and ask whatever you want.
4. **Reddit User:** [**Background\_Debate\_94**](https://www.reddit.com/user/Background_Debate_94/) Not much to add other than: thank you, Background!

P.S.: Spanish is my mother tongue, and I work as a Lead Data Engineer. I have some Spanish texts I’ve written that go into detail on many topics. If anyone is interested, feel free to DM me (I won’t translate 100 pages, sorry xd).",180,18,1735914784.0,dataengineering
1hsnbca,"The job market in Data Engineering is tough at the moment, applied for 40 jobs as a current Senior Data Engineer and had 3 get back and then ghost. Before last year I had loads lined up but decided to stay.","Not sure what’s going on at the moment, seems to be that companies are just putting feelers out there to test the market. 

I’m a Python/Azure specialist and have been working with both for 8/5 years retrospectively. Track record of success and rearchitecting data platforms. Certifications in Databricks as well as 3 years experience. 

Hell i even blog to 1K followers on how to learn Python and Azure. 

Anyone else having the same issue in the UK?",181,82,1735913523.0,dataengineering
1htojya,Live Webinar - Building Reliable Generative AI,"**AI Observability with Databricks Lakehouse Monitoring: Ensuring Generative AI Reliability.**

Join us for an in-depth exploration of how Pythia, an advanced AI observability platform, integrates seamlessly with Databricks Lakehouse to elevate the reliability of your generative AI applications. This webinar will cover the full lifecycle of monitoring and managing AI outputs, ensuring they are accurate, fair, and trustworthy.

**We'll dive into:**

* **Real-Time Monitoring:** Learn how Pythia detects issues such as hallucinations, bias, and security vulnerabilities in large language model outputs.
* **Step-by-Step Implementation:** Explore the process of setting up monitoring and alerting pipelines within Databricks, from creating inference tables to generating actionable insights.
* **Advanced Validators for AI Outputs:** Discover how Pythia's tools, such as prompt injection detection and factual consistency validation, ensure secure and relevant AI performance.
* **Dashboards and Reporting:** Understand how to build comprehensive dashboards for continuous monitoring and compliance tracking, leveraging the power of Databricks Data Warehouse.

Whether you're an AI practitioner, data scientist, or compliance officer, this session provides actionable insights into building resilient and transparent AI systems. Don't miss this opportunity to future-proof your AI solutions!

🗓️ Date: January 29, 2025 | 🕐 Time: 1 PM EST

➡️ [Register here for free!](https://www.linkedin.com/events/7280657672591355904/comments/)

https://preview.redd.it/v91z8eral1be1.png?width=1920&format=png&auto=webp&s=590b943ee04e9e73a7fa7cc31848521e877a9b4f

  
",0,0,1736025137.0,dataengineering
1hthxt7,"50+ Incredible Big Data Statistics for 2025: Facts, Market Size & Industry Growth",,1,0,1736007819.0,dataengineering
1hta1ms,Realistic and Challenging Practice Queries for SQL Server,"Hey SQL enthusiasts -

Want some great challenges to improve your T-SQL? Check out my book [Real SQL Queries: 50 Challenges](https://www.amazon.com/dp/B0DQCDQBHN).  
These are all very realistic business questions. For example, consider Question #12:

# ""The 2/22 Promotion""

A marketing manager devised the “2/22” promotion, in which orders subtotaling at least $2,000 ship for $0.22. The strategy assumes that gains from higher-value orders will offset freight losses.

According to the marketing manager, orders between $1,700 and $2,000 will likely boost to $2,000 as customers feel compelled to take advantage of bargain freight pricing.

You are asked to test the 2/22 promotion for hypothetical profitability based on the marketing manager’s assumption about customer behavior.

Analyze orders shipped to California during the fiscal year 2014 to determine net gains or losses, assuming the promotion was in effect.... 

*(the question continues on with many more instructions).*

All problems are based on the AdventureWorks2022 database, which is free and easy to install.

If you're not from the US, visit [https://RSQ50.com](https://rsq50.com/) and scroll to the bottom to get the link for your country.

If you do buy a copy, please review it (good or bad) - it helps.

Please let me know if you have any questions. I'm very proud of this book; I hope you'll check it out if you are thinking about sharpening up your T-SQL ",5,2,1735979202.0,dataengineering
1hsiyq3,Your executives want dashboards but cant explain what they want?,"Ever notice how execs ask for dashboards but can't tell you what they actually want? 

After building 100+ dashboards at various companies, here's what actually works:

1. Don't ask what metrics they want. Ask what decisions they need to make. This completely changes the conversation.

2. Build a quick prototype (literally 30 mins max) and get it wrong on purpose. They'll immediately tell you what they really need. (This is exactly why we built Preswald - to make it dead simple to iterate on dashboards without infrastructure headaches. Write Python/SQL, deploy instantly, get feedback, repeat)

3. Keep it stupidly simple. Fancy visualizations look cool but basic charts get used more.

What's your experience with this? How do you handle the ""just build me a dashboard"" requests? 🤔",249,58,1735897649.0,dataengineering
1htetyb,What is the right way to ask for a referral ,"As a grad student soon to be graduate I’m really interested to know what is the right way to ask someone for a referral? (cold-reach out)


I have been aiming for DE role post graduation and I have 1 year of full time DE experience and little over 1+ year of DS and DE experience.


If you are a DE Manager, DE 2, DE1, Director DE or anyone what is the right way I need to approach you for a referral?

Consider this situation I came across a job post for DE1 on your career website and I’m really interested and I feel I’m qualified.",0,6,1735998859.0,dataengineering
1hsv0v9,How to market yourself and standout as a Jr Data Engineer?,"Hey, I hope the new year started gently for you.

The trick is to build trust so any corporation would hire me, by spotlighting my strengths and experiences, especially the ones that align with what they are looking for.

**My methodology** (aside from doing quality 2–3 applications daily):

1. I would share my best projects on LinkedIn, Reddit, and even some Discord communities to get feedback and showcase my skills. This would supposedly help build trust.
2. I am thinking of making a public blog to document my projects.
3. I would contact recruiters for jobs posted by corporations I genuinely want to work for, being selective because there is a DM limit on LinkedIn. I don’t think spamming tech leads or recruiters' DMs would result in anything good, especially if there are no postings at their companies (waste of my time).
4. I would target local or foreign startups and apply there if there are any openings. If not, I will contact someone there to show my interest in their work.
5. Nothing beats 1:1 connections, so I plan to target some companies, especially startups, by going there in person and talking to someone. I’ll try the old-fashioned way with local companies next week after researching them for a while.

What are your thoughts?",26,22,1735933214.0,dataengineering
1ht4p2e,"Date engineers, do you ofter give presentations in your job settings? ",title,7,11,1735959503.0,dataengineering
1hsoi1n,Anyone really like the domain/business they're in? What does your company do? Did you aim for that industry?,"For ~6 years I've done well as a DE by learning the business side of things and working in engineering. Being that bridge is a pretty profitable role.

But it's starting to become a grind. I would rather do straight engineering. But this is tough to do at a start up in a data role since it's so central to very loosely defined business operations, which are necessary for me to know. It's been like this at the few companies where I've worked.

Or if I can't spend more time strictly in engineering then I'd like to enjoy the domain more. I've worked in mostly in marketing and I simply don't care about marketing. 

Any anecdotes about how you all have found your way into a DE role in a cool domain?",28,28,1735916832.0,dataengineering
1hsytve,Trino query plan analysis focus areas & interest,"I'm pulling together a info session or three focused on Trino query plan analysis and wondering how useful folks think this might be and/or to hear about topics folks think ought to be present in them. Disclaimer: Starburst DevRel here, but all sessions will be publicly available once created.",7,2,1735942902.0,dataengineering
1hsk6g9,Building a LeetCode-like Platform for PySpark Prep,"Hi everyone,
I'm a Data Engineer with around 3 years of experience worked on Azure ,Databricks and GCP, and recently I started learning TypeScript (still a beginner). As part of my learning journey, I decided to build a website similar to LeetCode but focused on PySpark problems.

The motivation behind this project came from noticing that many people struggle with PySpark-related problems during interv. They often flunk due to a lack of practice or not having encountered these problems before. I wanted to create a platform where people could practice solving real-world PySpark challenges and get better prepared for interv.

Currently, I have provided solutions for each problem. Please note that when you visit the site for the first time, it may take a little longer to load since it spins up AWS Lambda functions. But once it’s up and running, everything should work smoothly!

I also don't have the option for you to try your own code just yet (due to financial constraints), but this is something I plan to add in the future as I continue to develop the platform.
I am also planning add one section for commonly asked interviw questions in Data Enginnering Interviws.

I would love to get your honest feedback on it. Here are a few things I’d really appreciate feedback on:

    Content: Are the problems useful, and do they cover a good range of difficulty levels?
   
    Suggestions: Any ideas on how to improve the  platform?

Thanks for your time, and I look forward to hearing your thoughts! 🙏

Link : https://pysparkify.com/


",57,15,1735902668.0,dataengineering
1hsu786,Worth it to do AWS Certified Data Engineer?,"Hey there,

Seeking some advice as I'm a Senior data analyst with experience in both DE and DevOps. Been thinking to move into DE and doing a Udemy course currently to get AWS Certified Data Engineer Associate. 

My stack is mostly Python/SQL but I have done tasks set by Principal DevOps (Kubernetes, Docker, Bash scripting, Terraform, AWS Lambda etc) in my current company, as I'd basically been begging for something technical due to company not really needing a data analyst, and repurposing me as a business analyst mostly.

My previous company was where the cool stuff happened as I did a lot of ETL with Python/SQL for on-premises database systems, and getting rid of Excel as their point of data storage (greenfield project mostly). Did build dashboards with Power BI too.   
  
I mostly got cloud experience in my current company. I do enjoy the DevOps tasks like using Terraform to automate AWS S3 infrastructure and Lambda scripts where needed. Not really much Python as it's mostly used for alerting and no real big push there.

I've got almost 6 years experience as data analyst with ventures into DE and DevOps. My head is thinking DevOps career but heart is set on data engineering. I am in the UK market and have struggled to find a DE role where my skills can be applied and hence doing this AWS course.

What should I do? DE or DevOps?",12,7,1735931170.0,dataengineering
1hsgnob,Dev: No Time for STAGING. It was URGENT!,,102,8,1735887771.0,dataengineering
1hszvc3,Data Engineering Design.,"I am starting to apply for jobs now and would like to know how does a typical DE design round goes? Been a while since I looked outside my current job. Targeting some FAANG or same level jobs. 

Also, what resources do you recommend? I have gone through some system design materials and but they somewhat relate to data overall but want some DE specific. Thank you!",3,9,1735945644.0,dataengineering
1hske2h,Project Ideas for a Final Year Student Interested in Data Engineering,"Hey everyone! I’m currently in my final year of university and I’m really passionate about Data Engineering. As I work on my final projects, I’d love to dive deeper into real-world applications of data engineering, including any hardware-related aspects.

I’m looking for ideas on what kinds of projects I can pursue that are related to data engineering, with a particular interest in how hardware can be integrated into these projects. Some areas I’m interested in are:

* Data pipelines
* Data warehousing
* ETL (Extract, Transform, Load) processes
* Big Data technologies (like Hadoop, Spark)
* Cloud platforms (AWS, GCP, Azure)
* Data modeling
* Real-time data processing   

If anyone has suggestions for projects or challenges that would be a good fit for a student with an interest in Data Engineering and potentially integrating hardware components, I’d greatly appreciate your input!

Thanks in advance!",19,20,1735903495.0,dataengineering
1hsl86f,"GitHub - chonalchendo/football-data-warehouse: Repository for parsing, cleaning and producing football datasets from public sources.","Hey r/dataengineering,

Over the past couple months, I’ve been developing a data engineering project that scrapes, cleans, and publishes football (soccer) data to Kaggle. My main objective was to get exposure to new tools and fundamental software functions such as CI/CD.

**Background**:

I initially scraped data from transfermarkt and Fbref a year ago as I was interested in conducting some exploratory analysis on football player market valuations, wages, and performance statistics.

However, I recently discovered the **transfermarkt-datasets** GitHub repo which essentially scrapes various datasets from transfermarkt using scrapy, cleans the data using dbt and DuckDB, and loads to an S3 before publishing to Kaggle. The whole process is automated with GitHub Actions.

This got me thinking about how I can do something similar based on the data I’d scraped.

**Project Highlights:**

\- **Web crawler** (**Scrapy**) -> For web scraping I’ve done before, I always used httpx and Beautiful Soup, but this time I decided to give scrapy a go.  Scrapy was used to create the Transfermarkt web crawler; however, for fbref data, the pandas read\_html() method was used as it easily parses tables from html content into a pandas dataframe.

\- **Orchestration** (**Dagster**) -> First time using Dagster and I loved its focus on defining data assets. This provides great visibility over data lineage, and flexibility to create and schedule jobs with different data asset combinations.

\- **Data processing** (**dbt** & **DuckDB**) -> One of the reasons I went for Dagster was its integration with dbt and DuckDB. DuckDB is amazing as local data warehouse and provides various ways to interact with your data including SQL, pandas, and polars. dbt simplified data processing by utilising the common table expression (CTE) code design pattern to modularise cleaning steps, and by splitting cleaning stages into staging, intermediate, and curated.

\- **Storage** (**AWS S3**) -> I have previously used Google Cloud Storage, but decided try out AWS S3 this time. I think I’ll be going with AWS for future projects, I generally found AWS to be a bit more intuitive and user friendly than GCP.

\- **CI/CD** (**GitHub Actions**) -> Wrote a basic workflow to build and push my project docker image to DockerHub.

\- **Infrastructure as Code** (**Terraform**) -> Defined and created AWS S3 bucket using Terraform.

\- **Package management** (**uv**) -> Migrated from Poetry to uv (package manager written in Rust). I’ll be using uv on all projects going forward purely based on its amazing performance.

\- **Image registry** (**DockerHub**) -> Stores the latest project image. Had intended to use the image in some GitHub actions workflows like scheduling the pipeline, but just used Dagster’s built-in scheduler instead.

I’m currently writing a blog that’ll go into more detail about what I’ve learned, but I’m eager to hear people’s thoughts on how I can improve this project or any mistakes I’ve made (there’s definitely a few!)

Source code: [https://github.com/chonalchendo/football-data-warehouse](https://github.com/chonalchendo/football-data-warehouse)

Scraper code: [https://github.com/chonalchendo/football-data-extractor](https://github.com/chonalchendo/football-data-extractor)

Kaggle datasets: [https://www.kaggle.com/datasets/conalhenderson/football-data-warehouse](https://www.kaggle.com/datasets/conalhenderson/football-data-warehouse)

transfermarkt-datasets code: [https://github.com/dcaribou/transfermarkt-datasets](https://github.com/dcaribou/transfermarkt-datasets)

How to structure dbt project: [https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview)",11,3,1735906667.0,dataengineering
1hsp6g1,Real time CDC from Postgres with DBT,"I have few questions for any one doing realtime or near real time replication from Postgres to BigQuery or any other downstream system using DBT:

1. What’s the lag time between a change made in PG to it being available in downstream system?
2. Let’s say there are 10 tables and a change involved couple of tables. Do you run all the transformations or just those which are affected by those changes?
3. If all the transformations, how do you ensure that compute intensive transformations don’t impact overall lag?
4. How do you maintain transactional integrity if the downstream system doesn’t support transactions? For example, a transaction in PG might affect two tables and the goal is to make changes to both tables visible in downstream system at the same time .

We are currently using Airbyte and it can be pretty slow (minimum of about 2 minutes to up to 10 minutes based on tables involved) and I am looking to reduce lag to less than a minute. Is that possible for simple changes?",7,7,1735918637.0,dataengineering
1hsv5n3,How to Retrieve Data from AWS SageMaker Feature Store using PySpark?,"Hi,

I was going through this [article](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html) and understand that we can ingest data into SageMaker Feature Store using PySpark. However, there is no mention in the whole documentation for retrieving data from Feature Store's offline store (S3) using PySpark.

I am new to Glue and SageMaker Feature Store so wanted to confirm my understanding. If we choose Iceberg format to store data in offline store then I know SageMaker Feature Store will create a AWS Glue Catalog on top our parquet files. So should we use this Glue Catalog to query the Feature Groups using PySpark on EMR? And are there any complications to this process that I might not be aware of?

Also, is it possible to test this using a local Python Environment by just installing the relevant libraries? Or do I need to setup some kind of Glue notebooks to test this out?

Thank you.",2,0,1735933541.0,dataengineering
1hsg422,Is airflow or prefect cheaper? ,"My team is doing POC for ETL with Python and we are currently using Informatica for all the ETL process. We might migrate and our considerations on the table now are Airflow and Prefect, and my team lead says that we definitely need to subscribe to their support package, but my senior is saying that Airflow is more expensive than Prefect. Is this true? For all of u guys that are currently using Airflow, do you get their support, and how much is it?",23,40,1735885660.0,dataengineering
1hsho79,What to use for ingest before databricks?,"Hi. I'm an infrastructure engineer working on a data platform and currently we're using Databricks for almost everything. We use data factory for some of the simpler ingest jobs.  I want to explore not using databricks for ingest and rather using something that's more cost effective as well as making it easier to secure the network on databricks side. I would like to make it simple for the data engineers to use since they don't know docker/kubernetes. I'm thinking some sort of serverless framework that I can abstract away and they just write python. But there is many challenges to solve. Orchestration between ingest and databricks, development workflow, monitoring, troubleshooting, restarting etc. 


I'm wondering what you guys are using for this and if there is something out of the box or standard components we can use?",8,9,1735892110.0,dataengineering
1hs4uk0,"Just Launched: dbt™ Data Modeling Challenge - Fantasy Football Edition ($3,000 Prize Pool)","Hey data engineers! I just launched my a new hackathon that combines NFL fantasy football data with modern data stack tools.

What you'll work with:

* Raw NFL & fantasy football data
* Paradime for dbt™ development
* Snowflake for compute & storage
* Lightdash for visualization
* GitHub for version control

Prizes:

* 1st: $1,500 Amazon Gift Card
* 2nd: $1,000 Amazon Gift Card
* 3rd: $500 Amazon Gift Card

You'll have until February 4th to work on your project (winners announced right before the Super Bowl). Judges will evaluate based on insight value, complexity, material quality, and data integration.

This is a great opportunity to enhance your portfolio, work with real-world data, and win some cool prizes.

Interested? Check out the full details and register here: [https://www.paradime.io/dbt-data-modeling-challenge](https://www.paradime.io/dbt-data-modeling-challenge)",58,12,1735852942.0,dataengineering
1hsk3qc,CS50's Introduction to Databases with SQL,"Hi, do you guys recommend this for people who want to become data engineers? Thank you for your insights.",3,6,1735902357.0,dataengineering
1hssv2t,Are a good grasp of k8 give some advantage,"Hi folks, do you think that if you have knowledge about k8 is a huge advantage in the market right now?

Something that if you know what is a svc,ns,ingress. How to write a deployment with affinity to schedule the pods and how to mount volumes and make PVC or consume configmaps or something like RBAC in the cluster. Give an advantage with the HR people?

Obviously there are some devops job like if you want a CRD or HA, deployment custom controllers that are out of the scope, but for a DE do you think is solid for a job?

",1,7,1735927894.0,dataengineering
1hswlo6,Advice for starting a data platform in the cloud,"I’m a Data Engineer with experience in the Azure ecosystem, and I’m currently working in a company that has little to no experience with the cloud.

They want me to initialize a Data Lake/Warehouse/Lakehouse and start migrating the data into this platform, they hired me as a Data Engineer partly because of this. We’re considering both Azure and AWS for this platform.

The company currently uses Sharepoint for storing the data in Excel files, those files are used by Power BI reports and some predictive analysis as well. There are some ETL-like python codes running in the cloud through Azure DevOps pipelines.

What I have in mind is to start first with a data lake service (S3 or ADLS) and copy the data from each source (SAP, API, and other SaaS) with pipelines built on Airflow, by using an instance running with docker compose on a VM (EC2 or Azure VM), and use that data for the reports and analysis. The code that is running in DevOps can be migrated to Airflow too. We’ll start with a couple of projects for the data migration so, there won’t be much data initially.

It might seem overkill to use a Data Lake service and Airflow for such a low volume of data, but, what we’re considering is that the rest of the business units will want to move their data into this platform at some point, so in that case, we’ll need to have a scalable solution.

I don’t want to use Data Factory or another tool like that because I want to take this opportunity to get more experience with Airflow and Docker (possibly Kubernetes too).

I think we can eventually consider adding a Warehouse/Lakehouse service like Snowflake, Databricks or Redshift, and even other tools like DBT or Airbyte if we need a more robust solution, but I want to keep things simple in the beginning.

So what do you guys think about this approach? Does Azure make more sense considering this context? Or should I consider other options?",0,6,1735937247.0,dataengineering
1hsi7mh,Data preprocessing ,"Hello everyone,

I am working on a dataset , Need an advice or best approach 

1) Should I split the dataset to train and test then do the preprocessing techniques separately on both?

2)Should I do the preprocessing techniques on the whole dataset then split?

3)To imbalance the dataset it should be done only on the train and never touch the test?

Thanks in advance",3,3,1735894415.0,dataengineering
1hs94yk,How to get business metadata (data management),"Am I stupid or does it seem like every Data Management platform primarily focuses on functionality around technical metadata (data about tables, columns, etc). We are currently looking at options to buy a data cataloguing tool, but the way I see it, once we ingest all the technical metadata, we need to enrich it with business metadata (context) for the business side.

Our current situation is our business metadata is scattered across many places (excel sheets, pdf files, data models in visual diagrams). It seems like someone will have to go through all the technical metadata and manually add business context to it.

Is there a better way? Any SaaS recommendations?

Industry: Healthcare, medium size business ",19,15,1735863918.0,dataengineering
1hshsuu,Bytebase 3.2.0 released -- Database DevSecOps for MySQL/PG/MSSQL/Oracle/Snowflake/Clickhouse,,3,0,1735892677.0,dataengineering
1hshn2j,"Building a SQL Bot with LangChain, Azure OpenAI, and Microsoft Fabric",,3,5,1735891968.0,dataengineering
1hssfdc,Columbia CVN (MS CS) vs. UChicago (MS Applied Data Science) – Which is Better for Data/ML Engineering?,"
I’m deciding between Columbia’s MS in CS (CVN) and UChicago’s MS in Applied Data Science. Both programs are online and part-time. My goal is to break into data engineering or ML engineering while working full-time.

Which program will better prepare me for an MLE role? Would love insights from those in the field!",0,9,1735926844.0,dataengineering
1hsk683,Building DataOps for AI Agents - Looking for others facing similar challenges,"I'm developing a DataOps solution for AI agents and wanted to connect with others who might be experiencing similar challenges.

Two main pain points I'm trying to solve:

1. Development Phase: There's often a significant gap between the input data we have and what we actually need to get the expected outputs from our AI agents. Transforming and preparing this data is time-consuming and complex.
2. Maintenance Phase: As production data evolves over time, maintaining AI agent performance becomes challenging. The data drift requires constant attention and updates to our training/fine-tuning datasets.

Has anyone else encountered these issues?  
I'm particularly interested in hearing about your experiences with:  
\- Data preparation pipelines for AI agents  
\- Handling data drift in production  
\- Tools you're using to manage these challenges  
Would love to discuss potential solutions and share experiences!",1,0,1735902643.0,dataengineering
1hru0zu,Excessive denormalization in production DB's,"I'll preface this by saying I am still a junior more or less. I'm starting a mid level job shortly and have been brushing up on my back to basics stuff as a pallete cleanse from the current orgs weird but not so wonderful data management practices.

When I learned how to design databases, first theoretically and then as a pet project (which paved the way for me pivot into this field), the concept that duplication of data is bad was repeatedly hammered into my brain compartment. 

But at my last 2 companies, there was a lot of duplication of data. 

As a general example, imagine a products table, where you have PK, productID, ProductName, ProductCost etc.

Then you have an OrderItem table, which has the ProductFK, but also then contains ProductName, or ProductID etc. 

You then get analysts doing joins on none indexed columns like summing total sales of a current product by joining on the productID in the OrderItem table rather than a join on the indexed Sales-OrderItem relationship and a further join on the OrderItem-Product relationship. 

This is a simple example, the data is a lot more complex, but the principle remains, there is shit loads of unnecessary denormalisation in company systems in my very limited experience.  It's called a RDBMS for a reason right!

So what I wanted to ask is Why? 

Why design a table like this? Am I just working for Mickey Mouse companies so far and this is not how it will be in any company worth its salt. Or is there another reason why, in reality, duplication happens and I should learn to live with it. 

I have read that it is used to reduce joins in read-heavy use cases, but I'm yet to find a query that uses this data that wouldn't be about 5 times more efficient had they used the correct index relationships. 

",44,29,1735825582.0,dataengineering
1hs0s61,DuckDB + Webassembly = WhatTheDuck,,16,1,1735843010.0,dataengineering
1hsinuc,Doubt regarding Azure Data engineering,"Hello people, I would be graduating(MS in DS) in May'25. I am aiming for a D.E./Jr. D.S./D.A/ ML roles. I have azure DP 203, DP 100, PL 300 certifications. 

I am learning spark/pyspark. I also want to learn computer vision. I want to make two different projects(mining using pyspark and a small object detection proj.) by the time I graduate. 

But my classmates are saying getting Microsoft Fabrics certification would be helpful. But I feel it's time for hands on project experience. 

I solved all hackerrank and leet code questions till intermediate level in SQL and Python.

I read as many reasearch papers as I can and am confident with Stat. I have 2 yrs work experience in India as dot net web dev ( I don't think that'll help my DS career)

Please suggest how I have to plan my next few months to get a Data related job in US.

Also pls tell if  I have to learn Data structures and algorithms?

I feel that I am hustling but I need guidance. 

Thank you in advance.
",1,11,1735896371.0,dataengineering
