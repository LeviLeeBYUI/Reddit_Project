id,title,selftext,score,num_comments,created_utc,subreddit
1i5inrb,"Weekly Entering & Transitioning - Thread 20 Jan, 2025 - 27 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,23,1737349304.0,datascience
1i7bnpd,Meta: Career Advice vs Data Science,"I joined the thread to learn about Data Science.  Something like 75 percent of the posts are peoples resumes and requests for career advice. I thought these were supposed to go into a weekly thread or something - I'm getting a warning about the weekly thread even as I'm posting this comment. 

Can anyone suggest alternative subs with more educational content?",112,33,1737554248.0,datascience
1i7b8id,"Graduated september 2024 and i am now looking for an entry level data engineering position , what do you think about my cv ?",,81,43,1737552944.0,datascience
1i7a8e0,DS interested in Lower level languages,"Hi community,

I’m primarily DS with quite a number of years in DS and DE. I’ve mostly worked with on-site infrastructure. 

My stack is currently Python, Julia, R… and my field of interest is numerical computing, OpenMP, MPI and GPU parallel computing (down the line)

I’m curious as to how best to align my current work with high level languages with my interest in lower level languages. 

If I were deciding based on work alone, Fortran will be the best language for me to learn as there’s a lot of legacy code we’d have to port in the next years. 

However, I’d like to develop in a language that’ll complement the skill set of a DS.

My current view is Julia, C and Fortran. However, I’m not completely sure of how useful these are outside of my very-specific field. 

Are there any other DS that have gone through this? How did you decide? What would you recommend? What factors did you consider.",5,11,1737549714.0,datascience
1i7eaj9,Scrapy MRO error without any references to conflicting packages,"Hi all,

I'm working on a little personal project, quantifying what technologies are most asked for in Data Science JDs. Really I'm more using it to work on my Python chops. I'm hitting a slightly perplexing error and I think ChatGPT has taken me as far as it possibly can on this one. 

When I attempt to crawl my spider I get this error:   
TypeError: Cannot create a consistent method resolution order (MRO) for bases Injectable, Generic

Previously the code was attempting to import Injectable from scrap\_poet until I eventually inspected the package and saw that Injectable doesn't exist. So I attempted to avoid using that entirely and omitted all references to Injectable in my code. Yet I'm still getting this error. Any thoughts?

Here's what the spider looks like: 

    import scrapy
    import csv
    from scrapy_autoextract import request_raw
    
    class JobSpider(scrapy.Spider):
        name = ""job_spider""
        custom_settings = {
            ""DOWNLOADER_MIDDLEWARES"": {
                ""scrapy_autoextract.AutoExtractMiddleware"": 543,
            },
        }
    
        # Read URLs from links.csv and start requests
        def start_requests(self):
            with open(""/adzuna_links.csv"", ""r"") as file:
                reader = csv.reader(file)
                for row in reader:
                    url = row[0] 
                    yield request_raw(url=url, page_type=""jobposting"", callback=self.parse)
    
        def parse(self, response):
            try:
                # Extract job details directly from the response JSON data returned by AutoExtract
                job_data = response.json().get(""job_posting"", {})
    
                if job_data:
                    yield {
                        ""title"": job_data.get(""title""),
                        ""description"": job_data.get(""description""),
                        ""company"": job_data.get(""hiringOrganization"", {}).get(""name""),
                        ""location"": job_data.get(""jobLocation"", {}).get(""address""),
                        ""datePosted"": job_data.get(""datePosted""),
                    }
                else:
                    self.logger.error(f""No job data extracted from {response.url}"")
    
            except Exception as e:
                self.logger.error(f""Error parsing job data from {response.url}: {e}"")",0,1,1737561336.0,datascience
1i6qcch,Is this a normal data analyst experience? Expectations for new data analysts in the field,"I am a data analyst for a corporate company, this is my first year in a role like this and it has been a year. My manager is concerned that I have holes in my understanding about the company, but I feel like it is the lack of training and resources. I've never struggled so much in a role before, I previously worked in sales/sales admin for 5 years at a scientific company.

When I was interviewed, I explained that I had no experience with pivot tables or vlookup. It was my understanding from the interview that they were looking for someone to mentor, and I was hired on for having a great attitude. During onboarding, I was given pretty surface level material to review and met maybe a handful of times with others on the teams on building basic reports. I've had to do a lot of studying on my own time. During the year though, I have continued to struggle on the reporting aspect of my job and feel the relationship strains at work because of it. I am proud to say that I have been practicing excel files online with sample data at home for months and can successfully create files on my own. I've asked to shadow and practice files at home, but I was told to just learn more about the company and ask more questions. This is the kind of scenario I keep running into at my current job:

Ex: A few weeks ago, I was tasked to create a report. I was told to look at a few automated reports and essentially play around/figure it out. I was trained on two automated reports, but had not been trained on the others. My team was a bit annoyed with my confusion on which report I should use and that I should know based on the data. They gave me a suggestion on what report to try. I played around with the data on my own and got like 70% with the data I had. I was told yesterday that they decided to pull data elsewhere (because it would cover everything they wanted on the report more easily)  from a space I don't have access to and haven't been trained on. 



",43,38,1737486274.0,datascience
1i6qa6u,Analyzing changes to gravel height along a road,"I’m working with a dataset that measures the height of gravel along a 50 km stretch of road at 10-meter intervals. I have two measurements:

Baseline height: The original height of the gravel.

New height: A more recent measurement showing how the gravel has decreased over time.

This gives me the difference in height at various points along the road. I’d like to model this data to understand and predict gravel depletion. 

Here’s what I’m considering:Identifying trends or patterns in gravel loss (e.g., areas with more significant depletion).

Using interpolation to estimate gravel heights at points where measurements are missing.

Exploring possible environmental factors that could influence depletion (e.g., road curvature, slope, or proximity to towns).

However, I’m not entirely sure how to approach this analysis. Some questions I have:

What are the best methods to visualize and analyze this type of spatial data?

Are there statistical or machine learning models particularly suited for this?

If I want to predict future gravel heights based on the current trend, what techniques should I look into? Any advice, suggestions, or resources would be greatly appreciated!",4,4,1737486130.0,datascience
1i6tsvc,Syracuse online MSDS,"5 YoE DS here. Looking to get that next level piece of paper. Looking for something where I can complete a degree while doing full time job.

Anybody have any experience? Cash grab program or similar to Georgia tech?

Thanks in advance! ",3,16,1737494772.0,datascience
1i658fp,What should I do to build a strong foundation in developing?,"I’m interested in becoming a developer. I’m currently proficient in Tableau, Alteryx, Power BI etc. 

I feel like there’s 1 million different avenues. I’m not sure which route to take. 

I want to get around a community, where I can connect and get exposed to more. I’m in the Miami area. 

I’ve checked out YouTube videos on Java script

What do you all recommend? ",9,15,1737417755.0,datascience
1i5d77u,Anyone ever feel like working as a data scientist at hinge?,"Need to figure out what that damn algorithm is doing to keep me from getting matches lol. On a serious note I have read about some interesting algorithmic work at dating app companies. Any data scientists here ever worked for a dating app company? 

Edit: gale-shapely algorithm

https://reservations.substack.com/p/hinge-review-how-does-it-work#:~:text=It%20turns%20out%20that%20the,among%20those%20who%20prefer%20them.",430,143,1737331692.0,datascience
1i6pu2t,How to get individual restaurant review data?,,0,7,1737485051.0,datascience
1i60m31,Question about Using Geographic Data for Soil Analysis and Erosion Studies,"I’m working on a project involving a dataset of latitude and longitude points, and I’m curious about how these can be used to index or connect to meaningful data for soil analysis and erosion studies. Are there specific datasets, tools, or techniques that can help link these geographic coordinates to soil quality, erosion risk, or other environmental factors?

I’m interested in learning about how farmers or agricultural researchers typically approach soil analysis and erosion management. Are there common practices, technologies, or methodologies they rely on that could provide insights into working with geographic data like this?

If anyone has experience in this field or recommendations on where to start, I’d appreciate your advice!",9,15,1737406032.0,datascience
1i5576y,Should I Try to postpone my FAANG Interview?,"So I got contacted by a FAANG Recruiter for a Data Scientist Role I applied for a month and a half ago. But as I have started to prep, I realize I am not ready and need 1 to 2 months before I would be able to do well on all the technical interviews (there are 4 of them). My SQL is rusty because I have been using Pyspark so much that I didn't really need to do medium to hard SQL queries at work (We're also not allowed in most cases since SQL is slower). So I would just do everything in Pyspark. But now, as I start practicing my SQL I realize it's very basic, and it's going to take some time before I can get it on the level my pyspark is at.

I've noticed that I feel like there is no chance of me performing well enough on this interview, and it sucks because the recruiter said that the hiring manager was looking at my resume and really wants to interview me as soon as possible since he thinks I have strong experience for the role (They made me bypass the phone screens because of it). I have no doubt I would be able to do the role, but interviews are another beast. According to the prep guide, my Stats, ML Theory, SQL, and Python all have to be perfect. Since I joined my current company as an intern, I didn't have to do as many in-depth technicals as I have to do here. I've interviewed at a couple other big companies last year and didn't make it to the final round for one simply because I needed more time to prepare. The FAANG recruiter wants me to do the first 2 interviews within the next two weeks, and I'm worried about what it would do to my confidence if I failed this interview since this is pretty much my dream Data Scientist role. My mind is already telling me just to make the best of this and use it as a learning experience, but another part of me is wondering if I should just cancel it altogether or try to delay it as much as possible. I have a mock interview with a Company Data Scientist they set up for me in a few days, but part of me feels defeated already and it sucks...

I honestly am not sure what to do as I need a lot more time. I've heard others say it took them as long as 2-6 months before they were ready to crush their FAANG interview and I know I am not there yet...",200,81,1737311263.0,datascience
1i57vx1,Where to Start when Data is Limited: A Guide,"
Hey, I’ve put together an article on my thoughts and some research around how to get the most out of small datasets when performance requirements mean conventional analysis isn’t enough. 

It’s aimed at helping people get started with new projects who have already started with the more traditional statistical methods.

Would love to hear some feedback and thoughts.",67,6,1737317875.0,datascience
1i4yyoe,Influential Time-Series Forecasting Papers of 2023-2024: Part 1,"This article explores some of the latest advancements in time-series forecasting.

You can find the article [here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting).

Edit: If you know of any other interesting papers, please share them in the comments.",177,28,1737294977.0,datascience
1i5ok61,There can be no reasoning without inference-time weight updates,"Reasoning is learning from synthesis. Frozen model weights can not reason. Find a way for the model to update its weights during inference based on its findings and watch AGI emerge. 

This is my hypothesis. A quick google search returned nothing relevant. If you know of such experiments, please link them here!",0,6,1737374168.0,datascience
1i4f1go,AI is difficult to get right: Apple Intelligence rolled back(Mostly the summary feature),"Source: https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html#:\~:text=Apple%20is%20temporarily%20pulling%20its,organization%20and%20press%20freedom%20groups.

Seems like even Apple is struggling to deploy AI and deliver real-world value.  
Yes, companies can make mistakes, but Apple rarely does, and even so, it seems like most of Apple Intelligence is not very popular with IOS users and has led to the creation of r/AppleIntelligenceFail.

It's difficult to get right in contrast to application development which was the era before the ai boom. ",310,41,1737227935.0,datascience
1i3y1qs,What salary range should I expect as a fresh college grad with a BS in Statistics and Data Science?,"For context, I’m a student at UCLA, and am applying to jobs within California. But I’m interested in people’s past jobs fresh out of college, where in the country, and what the salary was. 

Tentatively, I’m expecting a salary of anywhere between $70k and $80k, but I’ve been told I should be expecting closer to $100k, which just seems ludicrous. ",122,114,1737169081.0,datascience
1i3x2cf,Are there any ways to earn a little extra money on the side as a data scientist?,"Using data science skills (otherwise I'm sure there are plenty). 

I know there is data annotation, but I'm not sure that qualifies as data science.",98,37,1737165883.0,datascience
1i40izz,Do these recruiters sound like a scam?,"Hi all, unsure of where else to ask this so asking here. 

I had a recruiter (heavy Indian accent) call/email me with an interesting proposition. They work for the candidate rather than the company. If they place you in a job within 45 days they ask for 9% of your first year's salary.

They claim their value add is in a couple of things. First they promise that they have advanced ATS software that will help tweak professional qualifications. Second, they say they will apply to approximately 50 JDs per day (I am skeptical this many relevant jobs are even being posted).

I have never had luck with Indian recruiters before but I have had good experiences professionally in offshoring some repetitive tasks for cheap. This process sounds like it fits the bill. The part where it gets sketchy is they want either access to my LinkedIn/Gmail or they want me to create second LinkedIn/Gmail accounts that they would have control over. Access to my gmail is a nonstarter obviously. But creating spoof LinkedIn/Gmails feels a little sketchy. 

If we're living in a universe where these guys are simply trying to provide the service they've described, I'm all in. I just don't want to get soft-rolled into some sort of scam.",14,36,1737177777.0,datascience
1i3zajz,Huggingface smolagents : Code centric Agent framework. Is it the best AI Agent framework? I don't think so,,1,2,1737173285.0,datascience
1i33mt0,I've been given the choice between being a Data Scientist or an Analytics Manager. Which would you choose and why?,"I'm coming from a Data Analyst position, and I've essentially been given the choice between being a Data Scientist and or an Analytics Manager. I thought Data Scientist was my dream job, but the Manager position would pay more, and I've been dreaming about working my way up to Director or CDO... Does Analytics Manager make the most sense in this case?

Update for context: I'm 25, have a master's in data analytics, and have been working in the same industry for 7 years but in different roles. I've been an Analyst for 1.5+ years, and previously was a Data Manager, and a Researcher.",196,77,1737073226.0,datascience
1i3cgo0,guys is web crawling and scraping +1 for data science or it doesn't matter. ,"by web crawling and scraping i mean advanced scraping with multiple websites for prices and products then building further things around it like strategic planning and buisness analytics. 

edit: is it a necessary skill or not. +1 it means its a great add on to ur skill stack",37,53,1737105217.0,datascience
1i3a45a,How long did it take you to get a new role when looking for a new job?,"I'm feeling very miserable at my job as well as feeling uneasy with the ethics of my company so I desperately am looking for a new role, but this job market is concerning. I have a BS in Math and MS in DS, been at my job as a data scientist for 1.5 years, worked for 3 years between BS and MS in analyst roles. Is there hope to have something new soon? How many apps per day should I be sending? ",48,46,1737094521.0,datascience
1i2vj0x,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",250,26,1737051786.0,datascience
1i3a227,Google Titans : New LLM architecture with better long term memory,,8,0,1737094288.0,datascience
1i3clrk,Microsoft MatterGen: GenAI model for Material design and discovery ,,2,1,1737105866.0,datascience
1i2vmuv,Introducing mlsynth.,"Hi DS Reddit. For those of who you work in causal inference, you may be interested in a Python library I developed called ""machine learning synthetic control"", or ""mlsynth"" for short.

As I write in its [documentation](https://mlsynth.readthedocs.io), mlsynth is a one-stop shop of sorts for implementing some of the most recent synthetic control based estimators, many of which use machine learning methodologies. Currently, the software is hosted from my GitHub, and it is still undergoing developments (i.e., for computing inference for point-estinates/user friendliness).

mlsynth implements the following methods: [Augmented Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), CLUSTERSCM, [Debiased Convex Regression](https://doi.org/10.1287/inte.2023.0028)  (undocumented at present), the [Factor Model Approach](https://doi.org/10.1177/00222437221137533), [Forward Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), [Forward Selected Panel Data Approach](https://doi.org/10.1016/j.jeconom.2021.04.009), the [L1PDA](https://doi.org/10.1002/jae.1230), the [L2-relaxation PDA](https://doi.org/10.13140/RG.2.2.11670.97609), [Principal Component Regression](https://doi.org/10.1080/01621459.2021.1928513), [Robust PCA Synthetic Control](https://academicworks.cuny.edu/gc_etds/4984), [Synthetic Control Method (Vanilla SCM)](https://doi.org/10.1198/jasa.2009.ap08746), [Two Step Synthetic Control](https://doi.org/10.1287/mnsc.2023.4878)  and finally the two newest methods which are not yet fully documented, [Proximal Inference-SCM](https://arxiv.org/abs/2108.13935) and [Proximal Inference with Surrogates-SCM](https://arxiv.org/abs/2308.09527)  

While each method has their own options (e.g., Bayesian or not, l2 relaxer versus L1), all methods have a common syntax which allows us to switch seamlessly between methods without needing to switch softwares or learn a new syntax for a different library/command. It also brings forth methods which either had no public documentation yet, or were written mostly for/in MATLAB.

The documentation that currently exists explains installation as well as the basic methodology of each method. I also provide worked examples from the academic literature to serve as a reference point for how one may use the code to estimate causal effects.

So, to anybody who uses Python and causal methods on a regular basis, this is an option that may suit your needs better than standard techniques.",22,7,1737052050.0,datascience
1i2qj4j,Books on Machine Learning + in R,"I'm interested in everyone's experience of books based specifically in R on machine learning, deep learning, and more recently LLM modelling, etc.  If you have particular experience to share it would really useful to hear about it.

As a sub-question it would be great to hear about books intended for relative beginners, by which I mean those familiar with R and statistical analysis but with no formal training in AI. There is obviously the well-known *""Introduction to Machine Learning with R""* by Scott V Burger, available as a [free pdf](https://edisciplinas.usp.br/pluginfile.php/8527271/mod_resource/content/0/Burger%2C%20Scott%20V%20-%20Introduction%20to%20machine%20learning%20with%20R_%20rigorous%20mathematical%20analysis-OReilly%20%282018%29.pdf).  But it hasn't been updated in nearly 7 years now, and a quick [scan of Google](https://www.google.co.uk/search?tbm=shop&hl=en-GB&psb=1&ved=2ahUKEwi6jeS9vPqKAxXdQ0ECHRvSDwAQu-kFegQIABAK&q=Machine+Learning+in+R&oq=Machine+Learning+in+R&gs_lp=Egtwcm9kdWN0cy1jYyIVTWFjaGluZSBMZWFybmluZyBpbiBSSABQAFgAcAB4AJABAJgBAKABAKoBALgBA8gBAJgCAKACAJgDAJIHAKAHAA&sclient=products-cc) shows quite a number of others.  Suggestions much appreciated.",27,18,1737038919.0,datascience
1i2jytl,Start freelancing with 0 experience?,"I hear many people have the ambition to start freelancing as soon as they can, ideally before having significant job experience. 
I like the attitude, but I tried myself a few years ago and got burned. So I wanna share my experience. 
   
I am a Data Scientist and tried to start freelancing with just one year job experience in 2017. Did the usual stuff. Set up an Upwork profile, applied to jobs at nights and during weekends and waited for a reply. 
Crickets. I **applied to 11 jobs** and didn't get any. Looking back at that experience I see a few mistakes
1 I didn't have a portfolio of projects that matched the jobs I applied to. 
2 I only used Upwork, without leveraging LInkedIn, Catalant, Fiverr and others. 
3 I gave up too early. Just 11 applications over one month is not enough. I recommend applying to 20-30 jobs per week if possible.
4 I set an unreasonable hourly rate. I set my hourly rate same as my daily job, Freelancing is a market where you are the product. When there is no demand for you (because nobody knows you) it's a smart move to set the price low. Once demand picks up, increase the price accordingly. 

Overall, I think experience is not the number one factor that a client looks for when hiring a freelancer. It's way more important to give the client confidence that you can do the job. So you should always work with that goal in mind, from the way you build your profile, to all the communication with your client. 
Last bit of advice. I found success in my local market at first. In Italy there is not many Data professionals that are also freelancers, and that helped me. People like to work with familiar faces and speaking the same language, sharing the same culture, goes a long way building confidence.

Curious to know your point of view too. ",48,32,1737013947.0,datascience
1i34tao,looking for arts sales data to understand arts pricing dynamics or madness,"I would like to explore datasets of arts sale and auctions, please if anyone has a good source please post below in the link. Just curious to explore if there are any patterns in art prices or just maddness which data science can't understand why a banana and tape would sell for 6 million or perhaps I can learn more about arts from this dataset. 

thanks in advance



Thanks ",0,5,1737076708.0,datascience
1i3bwdj,Can someone help me understand what is the issue exactly?,,0,2,1737102536.0,datascience
1i2mh17,Solution completeness and take home assignments for interviews?,"What is the general consensus about take home interviews and then completeness of solution.

I have around a week and it took me already 2 days just to work with with the data just so I can
1) clean it
2) enhance it with external data
3) feature engineer it
4) establish baselines to capture lift

The whole thing is supposed to be finished around the span of a week. As i was scoping it out the whole thing is essentially potentially 3-4 models in a framework given the complex nature of the work.

How critical is the completeness and assumptions being made regarding these take home assignments. I didnt get a take home that large in scope. Its difficult task but very doable just laborious in the sense that it requires to be well thought out. ",5,14,1737025307.0,datascience
1i28x7i,What do you think about building the pipeline first with bad models to start refining quickly?,"we have to build a computer vision application, I detect 4 main problems, 



get the highest quality training set, it is requiring lots of code and it may require lots of manual work to generate the ground truth

train a classification model, two main orthogonal approaches are being considered and will be tested

train a segmentation model

connect the dots and build the end to end pipeline

  
one teammate is working in the highest quality training set, and three other teammates in the classification models. I think it would be incredibly beneficial to have the pipeline as soon as possible integrated with the extremely simple models, and then iterate taking into account error metrics, as it gives us goals and this lets them test their module/section of the work also taking into account variation of the final metrics.

  
this would also help the other teams that depend on our output, web development can use a model, it is just a bad model, but we'll improve the results, the deployment work could also start now.

  
what do you guys think about this approach? for me it looks like its all benefits and zero problems but I see some teammates are reluctant on building something that definitely fails at the beginning and I'm not definitely the most experienced data scientist.",38,22,1736978111.0,datascience
1i1z6pj,Who is the most hungry for AI / ML talent right now,"I run a job search engine for Data Scientists. This week we added monitoring of the highest paid job openings in the last week. This is what I saw. It seems one company in particular wants to outbid everyone else. And this is not because of lack of competition - we monitor more than 30.000 companies including all of Fortune 100 and most of Fortune 1000. We index more than 60k data science jobs every month. 

Source: [jobs-in-data.com](https://jobs-in-data.com)

https://preview.redd.it/sqxgf9u786de1.png?width=2438&format=png&auto=webp&s=476af7f8ec1456a3d3f0e27f2fea61d4519daa9c

",128,37,1736953054.0,datascience
1i20otn,aspirations of starting a data science consultancy ,"Has anyone ever here thought of how to use their skills to start their own consultancy or some kind of business? Lately ive been kinda feeling that it would be really nice to have something of my own to work one involving analytics. Working for a company is great experience, but part of me would really like to have a business that I own where I help small businesses who have data make sense of it with low hanging fruit solutions.

Just a thought, but I’ve always thought of some sort of consultancy where clients are some sort of local business that collects data but doesn’t use it effectively or does not have the expertise on how to turn their data into insights that can be used. 

For example, suppose you had three clients:

1. Local gyms which have lots of membership data - my consultancy could offer services to measure engagement, etc and use demographic information to further understand gym goers - don’t know what “action” they could take but a thought 

2. Local shop has expenses they track and right now it’s all over the place. A dashboard that can help them view everything in one place

Something where, it’s tasks which are trivial for the average data scientist, but generate a lot of value for local businesses.

But maybe you can go deeper? I’m not sure how genAI works and haven’t played around with like any of these tools, but I’ve thought of ways these can be incorporated too.

Idk, I just find working in the industry sole draining and I just want to be able to have something that I can call my own, work on my own schedule, and it lead to a lot more revenue than working for a company. 

If anyone has any thoughts on what they have done, or how they have tried to do something, please let me know. Ideally I’d try and start this after 3-4 years of experience where I’ve built some niche industry experience. ",37,44,1736957009.0,datascience
1i275yh,WASM-powered codespaces for Python notebooks on GitHub,"During a hackweek, we built this project that allows you to run [marimo](https://github.com/marimo-team/marimo) and Jupyter notebooks directly from GitHub in a Wasm-powered, codespace-like environment. What makes this powerful is that we mount the GitHub repository's contents as a filesystem in the notebook, making it really easy to share notebooks with data.

**All you need to do is prepend** [`https://marimo.app`](https://marimo.app) **to any Python notebook on GitHub.** Some examples:

* Jupyter Notebook: [https://marimo.app/github.com/jakevdp/PythonDataScienceHandb...](https://marimo.app/github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)
* marimo notebook: [https://marimo.app/github.com/marimo-team/marimo/blob/07e8d1...](https://marimo.app/github.com/marimo-team/marimo/blob/07e8d14109f7312f19916fd13e4046a561a740f8/examples/third_party/polars/polars_example.py)

Jupyter notebooks are automatically converted into marimo notebooks using basic static analysis and source code transformations. Our conversion logic assumes the notebook was meant to be run top-down, which is usually but not always true \[2\]. It can convert many notebooks, but there are still some edge cases.

We implemented the filesystem mount using our own FUSE-like adapter that links the GitHub repository’s contents to the Python filesystem, leveraging Emscripten’s filesystem API. The file tree is loaded on startup to avoid waterfall requests when reading many directories deep, but loading the file contents is lazy. For example, when you write Python that looks like

    with open(""./data/cars.csv"") as f:
        print(f.read())
    
    # or
    
    import pandas as pd
    pd.read_csv(""./data/cars.csv"")

behind the scenes, you make a request \[3\] to *https://raw.githubusercontent.com/<org>/<repo>/main/data/cars.csv*

Docs: [https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github](https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github)

\[2\] [https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/)

\[3\] We technically proxy it through the playground [https://marimo.app](https://marimo.app) to fix CORS issues and GitHub rate-limiting.

**Why is this useful?**

Vieiwng notebooks on GitHub pages is limiting. They don't allow external css or scripts so charts and advanced widgets can fail. They also aren't itneractive so you can't tweek a value or pan/zoom a chart. It is also difficult to share your notebook with code - you either need to host it somehwere or embed it inside your notebook. Just append `https://marimo.app/<github_url>`",11,0,1736973521.0,datascience
1i29a6d,"Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?

","**Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?**

Hi everyone,

I’m looking to spark a discussion about **advanced imputation techniques** for datasets with multiple distinct but correlated time series. Imagine a dataset like **energy consumption** or **sales data**, where hundreds of stores or buildings are measured separately. The granularity might be hourly or daily, with varying levels of data completeness across the time series.

Here’s the challenge:

1. Some buildings/stores have **complete or nearly complete data** with only a few missing values. These are straightforward to impute using standard techniques.
2. Others have **partial data**, with gaps ranging from days to months.
3. Finally, there are buildings with **100% missing values** for the target variable across the entire time frame, leaving us reliant on correlated data and features.

The time series show **clear seasonal patterns** (weekly, annual) and dependencies on external factors like weather, customer counts, or building size. While these features are available for all buildings—including those with no target data—the features alone are insufficient to accurately predict the target. Correlations between the time series range from moderate (\~0.3) to very high (\~0.9), making the data situation highly heterogeneous.

# My Current Approach:

For stores/buildings with **few or no data points**, I’m considering an approach that involves:

1. **Using Correlated Stores**: Identify stores with high correlations based on available data (e.g., monthly aggregates). These could serve as a foundation for imputing the missing time series.
2. **Reconciling to Monthly Totals**: If we know the **monthly sums** of the target for stores with missing hourly/daily data, we could constrain the imputed time series to match these totals. For example, adjust the imputed hourly/daily values so that their sum equals the known monthly figure.
3. **Incorporating Known Features**: For stores with missing target data, use additional features (e.g., holidays, temperature, building size, or operational hours) to refine the imputed time series. For example, if a store was closed on a Monday due to repairs or a holiday, the imputation should respect this and redistribute values accordingly.

# Why Just Using Correlated Stores Isn’t Enough:

While using highly correlated stores for imputation seems like a natural approach, it has limitations. For instance:

* A store might be closed on certain days (e.g., repairs or holidays), resulting in zero or drastically reduced energy consumption. Simply copying or scaling values from correlated stores won’t account for this.
* The known features for the missing store (e.g., building size, operational hours, or customer counts) might differ significantly from those of the correlated stores, leading to biased imputations.
* Seasonal patterns (e.g., weekends vs. weekdays) may vary slightly between stores due to operational differences.

# Open Questions:

* **Feature Integration**: How can we better incorporate the available features of stores with 100% missing values into the imputation process while respecting known totals (e.g., monthly sums)?
* **Handling Correlation-Based Imputation**: Are there specific techniques or algorithms that work well for leveraging correlations between time series for imputation?
* **Practical Adjustments**: When reconciling imputed values to match known totals, what methods work best for redistributing values while preserving the overall seasonal and temporal patterns?

From my perspective, this approach seems sensible, but I’m curious about others' experiences with similar problems or opinions on why this might—or might not—work in practice. If you’ve dealt with imputation in datasets with heterogeneous time series and varying levels of completeness, I’d love to hear your insights!

Thanks in advance for your thoughts and ideas!

",9,3,1736979031.0,datascience
1i1wnxj,Leaving Public Sector for Private,"Posting for a friend:

Currently in a an ostensibly manager level DS position in local government. They are in the final stages of interviewing for a Director level role at a private firm. Is the compensation change worth it (posted below) and are there any DS specific aspects they should consider? 

Right now they are an IC who occasionally manages, but it seems this new role might be 80-90% managing. Is that common for the private sector? I told them it doesn't seem worth it (I'm biased as I am also in the public sector), but they said the compensation combined with more interesting work might be worth it.

Public Sector:
Manager
135k
Pension (secure but only okay payout)
Student Loan Forgiveness

Private Sector:
Director
165k
10-15% Bonus
401k 4% Match

",19,18,1736945403.0,datascience
1i2m3mv,What Challenges Do Businesses Face When Developing AI Solutions?,"Hello everyone,

I’m currently working on providing cloud services and looking to better understand the challenges businesses face when developing AI. As a cloud provider, I’m keen to learn about the real-world obstacles organizations encounter when scaling their AI solutions.

For those in the AI industry, what specific issues or limitations have you faced in terms of infrastructure, platform flexibility, or integration challenges? Are there any key challenges in AI development that remain unresolved? What specific support or solutions do AI developers need from cloud providers to overcome current limitations?

Looking forward to hearing your thoughts and learning from your experiences. Thanks in advance!",0,18,1737023704.0,datascience
1i1bjhi,E-values: A modern alternative to p-values,"In many modern applications - A/B testing, clinical trials, quality monitoring - we need to analyze data as it arrives. Traditional statistical tools weren't designed with this sequential analysis in mind, which has led to the development of new approaches.

E-values are one such tool, specifically designed for sequential testing. They provide a natural way to measure evidence that accumulates over time. An e-value of 20 represents 20-to-1 evidence against your null hypothesis - a direct and intuitive interpretation. They're particularly useful when you need to:

- Monitor results in real-time
- Add more samples to ongoing experiments
- Combine evidence from multiple analyses
- Make decisions based on continuous data streams

While p-values remain valuable for fixed-sample scenarios, e-values offer complementary strengths for sequential analysis. They're increasingly used in tech companies for A/B testing and in clinical trials for interim analyses.

If you work with sequential data or continuous monitoring, e-values might be a useful addition to your statistical toolkit. Happy to discuss specific applications or mathematical details in the comments.​​​​​​​​​​​​​​​​

P.S: Above was summarized by an LLM.

Paper: Hypothesis testing with e-values - https://arxiv.org/pdf/2410.23614

Current code libraries:

Python:

- expectation: New library implementing e-values, sequential testing and confidence sequences (https://github.com/jakorostami/expectation)

- confseq: Core library by Howard et al for confidence sequences and uniform bounds (https://github.com/gostevehoward/confseq)


R: 

- confseq: The original R implementation, same authors as above

- safestats: Core library by one of the researchers in this field of Statistics, Alexander Ly. (https://cran.r-project.org/web/packages/safestats/readme/README.html)

",100,63,1736876152.0,datascience
1i0x2pm,Fuck pandas!!! [Rant],"I have been a heavy R user for 9 years and absolutely love R. I can write love letters about the R data.table package. It is fast. It is efficient. it is beautiful. A coder’s dream.
 
But of course all good things must come to an end and given the steady decline of R users decided to switch to python to keep myself relevant.

And let me tell you I have never seen a stinking hot pile of mess than pandas. Everything is 10 layers of stupid? The syntax makes me scream!!!!!! There is no coherence or pattern ? Oh use [] here but no use ({}) here.
Want to do a if else ooops better download numpy. 
Want to filter ooops use loc and then iloc and write 10 lines of code.

It is unfortunate there is no getting rid of this unintuitive maddening, mess of a library, given that every interviewer out there expects it!!! There are much better libraries and it is time the pandas reign ends!!!!! (Python data table even creates pandas data frame faster than pandas!)

Thank you for coming to my Ted talk
I leave you with this datatable comparison article while I sob about learning pandas 

",482,329,1736825814.0,datascience
1i18xcv,Dash Python Incosistence Performance,"I'm currently working on a project using Dash Python. It was light and breezy in the beginning. I changed a few codes while maintaining the error at 0, test-running it once in a while just to check if the code change affected the website, and nothing bad happened. But after I left it for a few hours without changing anything, the website wouldn't run anymore and showed me an ""Internal Server Error"". This happened way too many times, and it stresses me out, as I have to update most of the backend ASAP. Does anyone has any similar experience and manage to solve it? I'd like to know how.",6,4,1736869506.0,datascience
1i13e03,Seeking Advice on Amazon Bedrock and Azure,"Hello everyone. I’m currently exploring AI infrastructure and platform for a new project and I’m trying to decide between Amazon Bedrock and Azure (AI Infrastructure & AI Studio). I’ve been considering both but would love to hear about your real-world experiences with them.

Has anyone used Amazon Bedrock or Azure AI Infrastructure and Azure AI Studio? How would you compare the two in terms of ease of use, performance, and overall flexibility? Are there specific features from either platform that stood out to you, or particular use cases where one was clearly better than the other?

Any advice or insights would be greatly appreciated. Thanks in advance! ",9,5,1736851459.0,datascience
1i0dbaj,Mastering The Poisson Distribution: Intuition and Foundations,,146,17,1736772966.0,datascience
1i03pk7,Where do you go to stay up to date on data analytics/science?,"Are there any people or organizations you follow on Youtube, Twitter, Medium, LinkedIn, or some other website/blog/podcast that you always tend to keep going back to? 

My previous career absolutely lacked all the professional ""content creators"" that data analytics have, so I was wondering what content you guys tend to consume, if any. Previously I'd go to two sources: one to stay up to date on semi-relevant news, and the other was a source that'd do high level summaries of interesting research papers. 

Really, the kind of stuff would be talking about new tools/products that might be of use, tips and tricks, some re-learning of knowledge you might have learned 10+ years ago, deep dives of random but pertinent topics, or someone that consistently puts out unique visualizations and how to recreate them. You can probably see what I'm getting at: sources for stellar information.",308,45,1736735034.0,datascience
1i1951j,exit cmd.exe from R (or python) without admin privilege,"I run:

system(""TASKKILL /F /IM cmd.exe"")

I get

Erreur�: le processus ""cmd.exe"" de PID 10333 n'a pas pu être arrêté.

Raison�: Accès denied.

Erreur�: le processus ""cmd.exe"" de PID 11444 n'a pas pu être arrêté.

Raison�: Accès denied.


I execute a batch file> a cmd open>a shiny open (I do my calculations)> a button on shiny should allow the cmd closing (and the shiny of course)

I can close the cmd from command line but I get access denied when I try to execute it from R. Is there hope? I am on the pc company so I don't have admin privilege",0,2,1736870071.0,datascience
1i0c3x8,Humana Senior DS Position merry-go-round,Anyone in the US apply to the Humana revolving Senior DS position over the last 5 months? They continuously post this position and never seem to fill it. Wondering if anyone has gotten an actual interview. I make it to the prescreen rounds  every single time I apply and then it just gets reposted.  ,26,8,1736768520.0,datascience
1i0m1ts,Advice on stabilizing an autoencoder's representation?,,3,1,1736795972.0,datascience
1i0wxxt,Mistral released Codestral 25.01 : Free to use with VS Code and Jet brains,,0,6,1736825403.0,datascience
1hzpcuv,"How we matured Fisher, our A/B testing library",,64,8,1736696534.0,datascience
1i06k3y,"Weekly Entering & Transitioning - Thread 13 Jan, 2025 - 20 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,42,1736744505.0,datascience
1i0czn6,Sky-T1-32B: Open-sourced reasoning model outperforms OpenAI-o1 on coding and maths benchmarks ,,0,0,1736771828.0,datascience
1i0bhi3,Seeking Advice on GPU Comparison: GreenNode vs FPT,"I’m currently exploring GPU options for my projects and I’m curious if anyone here has experience using GPUs from GreenNode or FPT. I’m looking for real feedback on how they compare in terms of performance, pricing, and overall experience.

Has anyone used GPUs from either of these providers? How do they stack up against each other in terms of power efficiency, speed, and reliability? Are there any specific use cases where one outperforms the other?

I’d love to hear your thoughts, personal experiences, or any suggestions you might have on which GPU might be better for intensive workloads. Thanks in advance!",0,1,1736765919.0,datascience
1hyploh,"200 applications - no response, please help. I have applied for data science (associate or mid-level) positions. Thank you ",,426,168,1736575525.0,datascience
1hyte5x,Feeling stuck in my career. Please help,"I'm in a weird position, where I feel like I'm stuck in my career. I really enjoy mathematics, ML/AI, implemented a lot of algorithms from scratch in C, developed new models for business purposes, presented at some internal/small conferences, and developed entire ML infrastructures for startups, but having no real opportunities to grow more.

At the moment I'm making over 100k$ working remotely from eastern Europe for a FAANG in the US (they have an office here, but my entire data science team is based in the US and I'm working on the same things as them).

When applying to companies in the US/UK I'm receiving zero callbacks (willing to relocate), although companies from the same areas are reaching out with remote offers of \~100k$/year. Those don't have the benefits of my current company, and are not attractive opportunities. I'm looking to relocate and get 200k$+. Current internal transfers to the US are closed, as they are looking to expand in east Europe. I've also asked for more difficult projects, but those are only available for US, not for my region.

The projects that are open to me at the moment offer zero satisfaction and I want to solve more complex problems and continue to expand my skills, but I'm stuck for the only thing that my studies are in eastern Europe and that I don't hold a PhD, even though I've already worked on novel models in industry, and speaking with friends and colleagues that hold a PhD, my skills are on par.

I'm at a point where I feel like skills and projects don't mean absolutely anything, and the only thing that has any weight for getting a job are diplomas and people you know... Maybe I'm exaggerating, but from all of my experiences I'm starting to feel like people from my region without studies abroad are seen only as cheap labor that should never be given the chance to work on real problems and be paid accordingly (a shitty company directly told me that, while another told me explicitly that my skills don't matter and they're only offering bad projects with bad pay in my region). It's like, there's a limit to the level of difficulty I can work on and the pay I can receive, regardless of how much I outcompete others...

At the moment, I'm working on a side research project that I'll be sending to some top tier conferences, and then try getting a PhD in the west... but that will take years, and if I already have the skills it's so frustrating to be stuck for so long just for a diploma and a title...

Or maybe my skills are really not on par, and I'm only good compared to the people in my region? Here's my resume if anyone would be willing to offer me some feedback.",56,35,1736592284.0,datascience
1hy7g0m,SQL Squid Game: Imagine you were a Data Scientist for Squid Games (9 Levels),,525,35,1736524298.0,datascience
1hyaw2t,How to communicate with investors?,"I'm working at a small scale startup and my CEO is always in talks with investors apparently. I'm currently working in different architectures for video classification as well as using large multimodal models to classify video. They want to show how no other model works on our own data (obviously) and how recent architectures are not as good as our own super secret model (videoMAE finetunned on our data...). I'm okay with faking results/showing results that cannot be compared fairly. I mean I'm not but if that's what they want to do then fine, doesn't really involve more work for me.

Now what pisses me off is that now I need to come up with a way to get an accuracy per class in a multilabel classification setting based solely on precision and recall per class because different models were evaluated by different people at different times and I really only have those 2 metrics per class - precision and recall. I don't even know if this is possible, it feels like it isn't, and is an overall dumb metric for our use case. All because investors only know the word ""accuracy""....

Would it not be enough to say: ""This is the F1 score for our most important classes, and as you can see, none of the other models or architectures we've tried are as good as our best model... By the way, if you don't know what F1 means, just know that higher scores are better. If you want, I can explain it in more detail..."" as opposed to getting metrics that do not make any sense...?

I will not present it to the investors, I only need to come up with a document, but wouldn't it be enough for the higher ups in my company to say what I said above in this scenario? ",15,12,1736532961.0,datascience
1hyxec6,Simple Full stack Agentic AI project to please your Business stakeholders,"Since you all refused to share how you are applying gen ai in the real world, I figured I would just share mine.

  
So here it is:  [https://adhoc-insights.takuonline.com/](https://adhoc-insights.takuonline.com/)  
There is a rate limiter, but we will see how it goes.



Tech Stack:

Frontend: Next.js, Tailwind, shadcn

Backend: Django (DRF), langgraph

LLM: Claude 3.5 Sonnet

I am still unsure if l should sell it as a tool for data analysts that makes them more productive or for quick and easy data analysis for business stakeholders to self-serve on low-impact metrics.

So what do you all think?",0,8,1736606856.0,datascience
1hxalxo,Companies are finally hiring,"I applied to 80+ jobs before the new year and got rejected or didn’t hear back from most of them. A few positions were a level or two lower than my currently level. I got only 1 interview and I did accept the offer. 

In the last week, 4 companies reached out for interviews. Just want to put this out there for those who are still looking. Keep going at it. 

Edit - thank you all for the congratulations and I’m sorry I can’t respond to DMs. Here are answers to some common questions. 

1. The technical coding challenge was only SQL. Frankly in my 8 years of analytics, none of my peers use Python regularly unless their role is to automate or data engineering. You’re better off mastering SQL by using leetcode and DataLemur

2. Interviews at all the FAANGs are similar. Call with HR rep, first round is with 1 person and might be technical. Then a final round with a bunch of individual interviews on the same day. Most of the questions will be STAR format. 

3. As for my skillsets, I advertise myself as someone who can build strategy, project manage, and can do deep dive analyses. I’m never going to compete against the recent grads and experts in ML/LLM/AI on technical skills, that’s just an endless grind to stay at the top. I would strongly recommend others to sharpen their soft skills. A video I watched recently is from The Diary of a CEO with Body Language Expert with Vanessa Edwards. I legit used a few tips during my interviews and I thought that helped ",1569,126,1736421422.0,datascience
1hxt0wl,How good are your linear algebra skills?,"Started my masters in computer science in August. Bachelors was in chemistry so I took up to diff eq but never a full linear algebra class. I’m still familiar with a lot of the concepts as they are used in higher level science classes, but in my machine learning class I’m kind of having to teach myself a decent bit as I go. Maybe it’s me over analyzing and wanting to know the deep concepts behind everything I learn, and I’m sure in the real world these pure mathematical ideas are rarely talked about, but I know having a strong understanding of core concepts of a field help you succeed in that field more naturally as it begins becoming second nature.

Should I lighten my course load to take a linear algebra class or do you think my basic understanding (although not knowing how basic that is) will likely be good enough?",83,41,1736472663.0,datascience
1hy8jhq,SAS - SQL question: inobs= vs outobs=,"Just a quick question here regarding PROC SQL in SAS.  Let's say I'm just writing some code and I want to test it.  Since the database I'm querying has over a million records, I don't want it to process my code for all the records.  

My understanding is that I would want to use the inobs= option to limit how much of the table is queried and processed on the server.  Is this correct?

The outobs= option will return however many records I set, but it process every record on the table in the server.  Is this correct?",4,3,1736527127.0,datascience
1hyhm2a,Is it necessary to understand the mathematics for data science anymore?,"The general consensus has been that you need to know the maths behind the models (proofs) in data science and that it’s advantageous to do so. But in this era of LLMs making our work even easier, and all the tools we use having already baked in the math behind the models for us, I wonder if this statement remains true or if it’s outdated advice. For example, in my limited experience of doing DS work, I’m personally yet to come across a situation where I was able to debug something because I knew the deep math proofs behind it (I did stats so know a decent amount of proofs). But I’m also very new to DS work so perhaps I’m missing something. 

Obviously understanding model output and what each of them means such as AUC, residuals, checking for drift etc remains important and will always do so.",0,20,1736550108.0,datascience
1hxxjz6,Microsoft's rStar-Math: 7B LLMs matches OpenAI o1's performance on maths,,4,0,1736487671.0,datascience
1hxplq8,Best resources for CO2 emissions modeling forecasting,"I'm looking for a good textbook or resource to learn about air emissions data modeling and forecasting using statistical methods and especially machine learning. Also, can you discuss your work in the field; id like tonlearn more.",9,14,1736462904.0,datascience
1hx305z,I was penalized in a DS interview for answering that I would use a Generalized Linear Model for an A/B test with an outcome of time on an app... But a linear model with a binary predictor is equivalent to a t-test. Has anyone had occasions where the interviewer was wrong?,"Hi,

I underwent a technical interview for a DS role at a company. The company was nice enough to provide feedback. This reason was not only reason I was rejected, but I wanted to share because it was very surprising to me. 

They said I aced the programming. However, hey gave me feedback that my statistics performance was mixed. I was surprised. The question was what type of model would I use for an A/B test with time spent on an app as an outcome. I suspect many would use a t-test but I believe that would be inappropriate since time is a skewed outcome, with only positive values, so a t-test would not fit the data well (i.e., Gaussian outcome). I suggested a log-normal or log-gamma generalized linear model instead.

  
I later received feedback that I was penalized for suggesting a linear model for the A/B test. However, a linear model with a binary predictor *is equivalent to a t-test*. I don't want to be arrogant or presumptuous that I think the interviewer is wrong and I am right, but I am struggling to have any other interpretation than the interviewer did not realize a linear model with a binary predictor is equivalent to a t-test.

Has anyone else had occasions in DS interviewers where the interviewer may have misunderstood or been wrong in their assessment?",268,123,1736391735.0,datascience
1hy9am1,Spreadsheet first cell debate ,"Settle this debate I'm having with a coworker. 

I say that spreadsheets should always start in row 1, column A. They say row 2, column B, [edit] so that there is an empty row and column before the table starts.

What's your take?",0,25,1736529004.0,datascience
1hxnq3t,Question on quasi-experimental approach for product feature change measurement,"I work in ecommerce analytics and my team runs dozens of traditional, ""clean"" online A/B tests each year. That said, I'm far from an expert in the domain - I'm still working through a part-time master's degree and I've only been doing experimentation (without any real training) for the last 2.5 years. 

One of my product partners wants to run a learning test to help with user flow optimization. But because of some engineering architecture limitations, we can't do a normal experiment. Here are some details:

* Desired outcome is to understand the impact of removing the (outdated) new user onboarding flow in our app. 
* Proposed approach is to release a new app version without the onboarding flow and compare certain engagement, purchase, and retention outcomes.
* ""Control"" group: users in the previous app version who did experience the new user flow
* ""Treatment"" group: users in the new app version who *would have* gotten the new user flow had it not been removed

One major thing throwing me off is how to handle the shifted time series; the 4 weeks of data I'll look at for each group will be different time periods. Another thing is the lack of randomization, but that can't be helped.

Given these parameters, curious what might be the best way to approach this type of ""test""? My initial thought was to use difference-in-difference but I don't think it applies given the specific lack of 'before' for each group. ",6,13,1736457945.0,datascience
1hxi5em,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,,6,2,1736443855.0,datascience
1hx286f,Am I underpaid/underemployed at $65k for a Data Analyst position in a MCOL city?,"I'm in a mcol city. I have a master's in Data Analytics that I finished in October 2024, and I've been working as a Data Analyst for 1.5 years. Before that, I was a study lead Clinical Data Manager for over a year (and before that I was a tax researcher and worked in HR). Currently, I make $65k base salary, but $85k total compensation. 

I keep getting interviews for Data Scientist positions that are well into the $100k+ base salary range, but I haven't landed an offer yet (it's really disheartening). Am I underpaid?

P.S. I'm open to job suggestions lol",71,58,1736389552.0,datascience
1hwmsd2,absolute path to image in shiny ui,"Hello,
Is there a way to get an image from an absolute path in shiny ui, I have my shiny app in a .R and I havn t created any R project or formal shiny app file so I don t want to use a relative paths
for now 
ui <- fluidPage(  tags$div( tags$img(src= absolute path to image).....
doesn t work",4,4,1736350202.0,datascience
1hvzskd,Change my mind: feature stores are needless complexity.,"I started last year at my second full-time data science role. The company I am at uses DBT extensively to transform data. And I mean very extensively. 

The last company I was at the data scientist did not use DBT or any sort of feature store. We just hit the raw data and write sql for our project.

The argument for our extensive feature store seems to be that it allows for reusability of complex logic across projects. And yes, this is occasionally true. But it is just as often true that there is a Table that is used for exactly one project. 

Now that I'm starting to get comfortable with the company, I'm starting to see the crack in all of this; complex tables built on top of complex tables built in to of complex tables built on raw data. Leakage and ambiguity everywhere. Onboarding is a beast. 

I understand there are times when it might be computationally important to pre-compute some calculation when doing real-time inference. But this is, in most cases, the exception, not the rule. Most models can be run on a schedule. 

TLDR; The amount of infrastructure, abstraction, and systems in place to make it so I don't have to copy and paste a few dozen lines of SQL is n or even close to a net positive. It's a huge drag.

Change my mind. ",116,47,1736278466.0,datascience
1hw5s76,As of 2025 which one would you install? Miniforge or Miniconda? ,"As the title says, which one would you install today if having a new computer for Data Science purposes. Miniforge or Miniconda and why?

For TensorFlow, PyTorch, etc.

Used to have both, but used Miniforge more since I got used to it (since 2021). But I am formatting my machine and would like to know what you guys think would be more relevant now.

I will try UV soon but want to install miniforge or miniconda at the moment.",40,77,1736293697.0,datascience
1hvwxzv,People who do DS/Analytics as freelancing any suggestions ,"Hi all

I've been in DS and aligned fields in corporate for 5+ years now. I'm thinking of trying DS freelance to earn additional income as well as learn whatever new things I can by doing more projects. I have few questions for people who have done it or tried it. 

Does it pay well? Do you do it fulltime or along with your job? Is it very difficult with a job?

What are some good platforms?

How do you get started? How much time does it take? How to get your first project? How to build your brand?

If you do it with your current job how much time does it take? Did you take permission from your manager about this?

Other than freelancing are there better options to make additional income?

Thanks!",75,34,1736271429.0,datascience
1hwcayh,CAG : Improved RAG framework using cache,,7,5,1736313595.0,datascience
1hvy3ld,Gradient boosting machine still running after 13 hours - should I terminate?,"I'm running a gradient boosting machine with the caret package in RStudio on a fairly large healthcare dataset, \~700k records, 600+ variables (most are sparse binary) predicting a binary outcome. It's running very slow on my work laptop, over 13 hours.

Given the dimensions of my data, was I too ambitious choosing hyperparameters of 5,000 iterations and a shrinkage parameter of .001? 

  
My code:  
\### Partition into Training and Testing data sets ###

set.seed(123)

inTrain <- createDataPartition(asd\_data2$K\_ASD\_char, p = .80, list = FALSE)

train <- asd\_data2\[ inTrain,\]

test  <- asd\_data2\[-inTrain,\]



\### Fitting Gradient Boosting Machine ###

set.seed(345)

gbmGrid <- expand.grid(interaction.depth=c(1,2,4), n.trees=5000, shrinkage=0.001, n.minobsinnode=c(5,10,15))

gbm\_fit\_brier\_2 <- train(as.factor(K\_ASD\_char) \~ .,

tuneGrid = gbmGrid,

data=train,

trControl=trainControl(method=""cv"", number=5, summaryFunction=BigSummary, classProbs=TRUE, savePredictions=TRUE),

train.fraction = 0.5,

method=""gbm"",

metric=""Brier"", maximize = FALSE,

preProcess=c(""center"",""scale""))

",22,46,1736274260.0,datascience
1huz6ax,This is how l stay up to date with the latest machine learning papers and technics ,"l go for the popular papers l hear about on Twitter and machine learning subreddits(Andrew Ng suggests these as great places to get the latest ml information). It won't cover everything, but it's okay and better to have some coverage than none - just because there are too many papers.

As for why l go for popular(by popular l mean a lot of technical/knowledgeable people are talking about them), well for certain things to be adopted they need some adoption, and l am sure there are great frameworks/architectures out there that just never got adopted and are not used a lot.

I will not write GPU kernels just so l can make this esoteric architecture, which l found on a paper somewhere,  work. Instead, I would use the popular transformer architecture, with lots of documentation and empirical evidence to support performance.

How about you all?",119,37,1736170682.0,datascience
1hvfuwa,What technology should I acquaint myself with next?,"Hey all. First, I'd like to thank everyone for your immense help on my last question. I'm a DS with about ten years experience and had been struggling with learning Python (I've managed to always work at R-shops, never needed it on the job and I'm profoundly lazy). With your suggestions, I've been putting in lots of time and think I'm solidly on the right path to being proficient after just a few days. Just need to keep hammering on different projects. 

At any rate, while hammering away at Python I figure it would be beneficial to try and acquaint myself with another technology so as to broaden my resume and the pool of applicable JDs. My criteria for deciding on what to go with is essentially: 

1. Has as broad of an appeal as possible, particularly for higher paying gigs
2. Isn't a total B to pick up and I can plausibly claim it as within my skillset within a month or two if I'm diligent about learning it

I was leaning towards some sort of big data technology like Spark but I'm curious what you fine folks think. Alternatively I could brush up on a visualization tool like Tableau.",13,23,1736213326.0,datascience
1hurpgg,data experience,,478,30,1736140819.0,datascience
1hv3gn4,Are Medium Articles helpful?,"I read almost every day something from Medium (I do write stuff myself too) though I kind of feel some of the articles even though highly rated are not properly written and to some extent loses its flow from the title to the content.

I want to know your thoughts and how have you found articles helpful on Medium or TDS.",24,44,1736182086.0,datascience
1huk9gq,What's your biggest time sink as a data scientist?,"I've got a few ideas for DS tooling I was thinking of taking on as a side project, so this is a bit of a market research post. I'm curious what data-scientist specific task/problem is the biggest time suck for you at work. I feel like we're often building a new class of software in companies and systems that were designed for web 2.0 (or even 1.0). ",181,98,1736118652.0,datascience
1hv5720,SWE + DS? Is learning both good,"I am doing a bachelor in DS but honestly i been doing full stack on the side (studying 4-5 hours per day and developing) and i think its way cooler.

Can i combine both? Will it give me better skills?",4,31,1736186285.0,datascience
1hvnkbl,"Tried Leetcode problems using DeepSeek-V3, solved 3/4 hard problems in 1st attempt",,0,2,1736241165.0,datascience
1hudtrj,Do you prepare for interviews first or apply for jobs first?,"I’ve started looking for a new job and find myself in a bit of a dilemma that I’m hoping you might have some experience with. Every day, I come across roles that seem like a great fit, but I hesitate to apply because I feel like I’m not fully prepared for an interview. While I know there’s no guarantee I’ll even get an interview, I worry about wasting an opportunity if I’m not ready.

On the other hand, preparing for an interview when you have one lined up seems like the most effective approach, but I’m not sure how to balance it all.

How do you usually handle this?",192,44,1736102433.0,datascience
1huz0m1,Meta's Large Concept Models (LCMs) : LLMs to output concepts ,,4,0,1736170208.0,datascience
1huloe0,"How are these companies building video/image generation tools? From scratch, fine-tuning Llama, or something else?
","There’s an enormous amount of LLM-based tools popping up lately, especially in video/image generation, each tied to a different company. Meanwhile, we only see a handful of really good open-source LLM models available.

So, my question is: How are these companies creating their video/image/avatar-generation tools? Are they building these models entirely from scratch, or are they leveraging existing LLMs like Llama, GPT, or something else?

If they are leveraging a model, are they simply using an API to interact with it, or are they actually fine-tuning those models with new data these companies collected for their specific use case?

If you’re guessing the answer, please let me know you’re guessing, as I’d like to hear from those with first-hand experience as well.

Here are some companies I’m referring to:

* **Video/image generation**:
   * [heygen.com](https://heygen.com)
   * [invideo.io](https://invideo.io)
   * [character.ai](https://character.ai)
   * [kindroid.ai](https://kindroid.ai)
   * [runwayml.com](https://runwayml.com)",19,2,1736122444.0,datascience
1hvk25m,Best LLMs to use ,"So I tried to compile a list of top LLMs (according to me) in different categories like ""Best Open-sourced"", ""Best Coder"", ""Best Audio Cloning"", etc. Check out the full list and the reasons here : https://youtu.be/K_AwlH5iMa0?si=gBcy2a1E3e6CHYCS",0,3,1736226461.0,datascience
1hurdd1,"Weekly Entering & Transitioning - Thread 06 Jan, 2025 - 13 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,86,1736139681.0,datascience
1huoyaf,What schema or data model are you using for your LLM / RAG prototyping?,"How are you organizing your data for your RAG applications? I've searched all over and have found tons of tutorials about how the tech stack works, but very little about how the data is actually stored. I don't want to just create an application that can give an answer, I want something I can use to evaluate my progress as I improve my prompts and retrievals.

This is the kind of stuff that I think needs to be stored:

* Prompt templates (i.e., versioning my prompts)
* Final inputs to and outputs from the LLM provider (and associated metadata)
* Chunks of all my documents to be used in RAG
* The chunks that were retrieved for a given prompt, so that I can evaluate the performance of the retrieval step
* Conversations (or chains?) for when there might be multiple requests sent to an LLM for a given ""question""
* Experiments. This is for the purposes of evaluation. It would associate an experiment ID with a series of inputs/outputs for an evaluation set of questions.

I can't be the first person to hit this issue. I started off with a simple SQLite database with a handful of tables, and now that I'm going to be incorporating RAG into the application (and probably agentic stuff soon), I really want to leverage someone else's learning so I don't rediscover all the same mistakes.",9,5,1736131935.0,datascience
1hu5gha,Announcing Plotlars 0.8.0: Expanding Horizons with New Plot Types! 🦀✨📊,"Hello Data Scientists!

I’m thrilled to announce the release of **Plotlars 0.8.0** — our latest step towards making **data visualization in Rust** more powerful, accessible, and versatile.

With this release, we’ve introduced **four new plot types**, unlocking exciting ways to represent your data visually. Whether you’re working with images, geographical datasets, or matrix data, Plotlars has you covered!

🚀 **New Features in Plotlars 0.8.0**

* 🖼️ Image Plot Support: Visualize **raster data** effortlessly with our new Image plot. Perfect for embedding and displaying image-based datasets directly in your plots.
* 🥧 PieChart Support: Represent **categorical data** using elegant and customizable pie charts. Ideal for showing proportions and category breakdowns.
* 🎨 Array2DPlot for RGB Data: Introducing Array2DPlot for **2D array visualization** using **RGB color values**. Excellent for displaying pixel grids, image previews, or matrix-based visualizations.
* 🌍 ScatterMap for Geographical Data: Plot your **geographical data points** interactively on maps with ScatterMap. Perfect for visualizing cities, sensor locations, or any spatial data.

🌟 **A Big Thank You to Our Supporters!**

Plotlars is nearing an incredible **300 stars on GitHub**. Your support, feedback, and enthusiasm have been instrumental in driving this project forward. If you haven’t already, please consider **leaving a star ⭐️ on GitHub** — it’s a small gesture that means a lot and helps others discover Plotlars.

🔗 Explore More:

📚 [Documentation](https://docs.rs/plotlars/0.8.0)  
💻 [GitHub Repository](https://github.com/alceal/plotlars)

**If you love Plotlars, share it with your friends and colleagues! Let’s build a thriving ecosystem of data science tools in Rust together.**

Thank you all for your continued support, and as always — **happy plotting!** 🎉📊

https://preview.redd.it/dhd0kxqfy5be1.png?width=984&format=png&auto=webp&s=c8059ef9038fada080b033f6d89a9765409fab5b

",34,1,1736077739.0,datascience
1hu86xb,Optimizing Advent of Code D9P2 with High-Performance Rust,,12,0,1736087336.0,datascience
1htlb8y,I don't like my current subfield of DS,"I have been in Data Science for 5 years and working as Senior Data Scientist for a big company.

In my DS journey most of my work are Applied Data Science where I was working on creating and training models, improving models and analysing features and make improvements so on (I worked on both ML, DL models) which I loved. 

Recently I have been moved to marketing data science where it feels like it is not appealing to me as I'm doing Product Data science with designing Experiment, analysing causal impact, Media mix modeling so on (also I'm somewhat not well experienced in Bayesian models or causal inference still learning). 

But in this field what I feel is you do buch of stuff to answer to business stakeholder in 1 or 2 slides and move on to next business question . Also even if you come up with something business always work based on traditional way with their past experience. I'm not feeling motivated and not seeing any of my solution is creating an impact.

Is this common with product data science/ causal inference world or I'm not seeing with correct picture?",94,21,1736016656.0,datascience
1htfjez,Whats the best resources to be better at EDA,"While I understand the math about ML, The one thing I lack is understanding and interpreting the data better.  
What resources could help me understand them?",86,27,1736001067.0,datascience
1htxzrg,Looking for some advice on my career path,,6,7,1736052777.0,datascience
1ht6ztm,I feel useless ,I’m an intern deploying models to google cloud. Everyday I work 9-10 hours debugging GCP crap that has little to no documentation. I feel like I work my ass off and have nothing to show for it because some weeks I make 0 progress because I’m stuck on a google cloud related issue. GCP support is useless and knows even less than me. Our own IT is super inefficient and takes weeks for me to get anything I need and that’s with me having to harass them. I feel like this work is above my pay grade. It’s so frustrating to give my manager the same updates every week and having to push back every deadline and blame it on GCP. I feel lazy sometimes because i’ll sleep in and start work at 10am but then work till 8-9pm to make up for it. I hate logging on to work now besides I know GCP is just going to crash my pipeline again with little to no explanation and documentation to help. Every time I debug a data engineering error I have to wait an hour for the pipeline to run so I just feel very inefficient. I feel like the company is wasting money hiring me. Is this normal when starting out? ,343,44,1735967040.0,datascience
1hti98t,Do you have any tips to keep up to date with all the ML implementations?,"I work as a data scientist, but sometimes i feel so left-behind in the field. do you guys have some tips to keep up to date with the latest breakthrough ML implementations?",37,13,1736008688.0,datascience
1htjd17,"Is there a similar career outperformance to-do list for a DS/DA, given some of the options/approaches aren’t available?",,10,7,1736011551.0,datascience
1htcyqo,How do you find data science internships?,"I am a high school student (grade 12) in a EU country, and if I do well on the national entrance exams, I'll get to the best university in the country which is in the top 200-250 for CS - according to QS. 

My experience with programming/data science is with Kaggle (for the last 2 years), having participated in 10+ competitions (1 bronze medal), and having \~4000 forks for my notebooks/codebases. 

Starting with university, how and when should I look for internships (preferably overseas because my country is lackluster when it comes to tech, let alone AI). Is there anything I can use to my advantage?

What did you guys do when you got your internships? Is it networking/nepotism that makes the difference?",17,16,1735992024.0,datascience
1ht2bbg,Moving to Germany,"Hi, I am a data scientist in Australia with about two years experience building ML models, doing data mining and predictive analysis for a big company. For personal reasons, I am moving to Munich at the end of the year, but am a bit worried about finding a data job abroad. 

I am wondering how difficult it might be to find a job in Germany, and what can I do to make myself competitive in an international market. What skillsets are in demand these days that I can learn and market?

Any advice would be greatly appreciated! ",30,55,1735952305.0,datascience
1hsxfrd,Data Science Job Market in UK vs. USA,"I've seen a worrying number of posts on social media over the past year describing how bad the job market is for recent computer science graduates, particularly in the US. Obviously there are differences between CS grads and those who pursue DS (though the general consensus (as far as I am aware) is that a CS could do a data scientist role but not vice versa).

Firstly, why do you think this is occurring? I've seen a lot of people mention the H-1B visa is a key issue surrounding this though I personally haven't a clue.

Secondly, is there a vast difference in the UK and USA job markets surrounding data science roles and is the market just as bad in the UK as it is in the USA?

Thirdly, are these CS graduates who are unable to get tech jobs migrating to more DS-centred jobs? This will obviously saturate the DS job market significantly.

Finally, as someone who is just starting to transition into the DS field, how worried should I be about job market saturation in the UK?",41,50,1735939373.0,datascience
1hsyiwl,Dicts vs classes: which do you tend to use?,"I’ve been thinking about the trade-offs between using plain Python dicts and more structured options like dataclasses or Pydantic’s BaseModel in my data science work.

On one hand, dicts are super flexible and easy to use, especially when dealing with JSON data or quick prototypes. On the other hand, dataclasses and BaseModels offer structure, type validation, and readability, which can make debugging and scaling more manageable.

I’m curious—what do you all use most often in your projects? Do you prefer the simplicity of dicts, or do you lean towards dataclasses/BaseModels for the added structure?

Would love to hear the community's thoughts!",28,15,1735942121.0,datascience
1hsv9ql,Professor looking for college basketball data similar to Kaggles March Madness,"The last 2 years we have had students enter the March Madness Kaggle comp and the data is amazing,  I even did it myself against the students and within my company (I'm an adjunct professor).  In preparation for this year I think it'd be cool to test with regular season games.  After web scraping and searching, Kenpom, NCAA website etc .. I cannot find anything as in depth as the Kaggle comp as far as just regular season stats, and matchup dataset. Any ideas?  Thanks in advance!",4,6,1735933833.0,datascience
1hsm94k,Data Scientist for Schools/ Chain of Schools,"Hi All,

I’m currently a data manager in a school but my job is mostly just MIS upkeep, data returns and using very basic built in analytics tools to view data. 

I am currently doing a MSc in Data Science and will probably be looking for a career step up upon completion but given the state of the market at the moment I am very aware that I need to be making the most of my current position and getting as much valuable experience as possible (my work are very flexible and they would support me by supplying any data I need). 

I have looked online and apparently there are jobs as data scientists within schools but there are so many prebuilt analytics tools and government performance measures for things like student progress that I am not sure there is any value in trying to build a tool that predicts student performance etc. 

Does anyone work as a data scientist in a school/ chain of schools? If so, what does your job usually entail? Does anyone have any suggestions on the type of project I can undertake, I have access to student performance data (and maybe financial data) across 4 secondary schools (and maybe 2/3 primary schools). 

I’m aware that I should probably be able to plan some projects that create value but I need some inspiration and for someone more experienced to help with whether this is actually viable. 

Thanks in advance. Sorry for the meandering post…",18,9,1735910239.0,datascience
1hslejn,How would you calculate whether to use Open Source LLM vs Vendors?,"Hi folks! I saw a lot of people online comenting on using DeepSeek instead of GPT4o and I was wondering how much are we saving by switching. 

Does anyone know a framework to estimate that?",9,6,1735907334.0,datascience
1hsn3e4,Why doesn't changepoint detection work the way I expect it to?,"I've been experimenting with changepoint detection packages and keep getting results that look like this:

  
[https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png)

  
If you look at 2024-05-26 in that picture, you'll what -- to me -- looks like an obvious changepoint. The line has been going down for a while and has suddenly started going up.

However, the model I'm using here is using the red and blue bands to show where it identified changepoints, and it's putting the changepoint just a little bit after the obvious one.

This particular visualization was made using the Ruptures package in Python, but I'm seeing pretty consistent results with every built-in changepoint model I can find. 

Does anyone know why these models, by default, aren't picking up significant changes in direction and how I need to update the calibration to change their behavior?",6,11,1735912872.0,datascience
1hsgfvp,Fine-Tuning ModernBERT for Classification ,,10,5,1735886939.0,datascience
1hrpb9q,How do you self-identify in this field and what is your justification?,"I've been in this field for many years, holding various titles, and connecting with peers who are unfathomably dissimilar in their roles, education, and skills, despite sharing titles.

I am curious to learn how folks view themselves and the various titles in this field. Assuming Data Science is the umbrella that encompasses computer science, machine learning, statistics, maths, etc., and there is a spectrum of roles within this field, how would you self-identify? The rules are:

1. It doesn't have to be your actual title from your employer or degree major.
2. It doesn't have to be a formally known identity. For example, you can identify as a ""number cruncher"", a ""tableau manager"", a ""deep learning developer"", make up your own, or just use a formal identity, such as ""Data Scientist"" or ""Machine Learning Engineer"".
3. You have to also add your justification. i.e. why do you believe such identity justly represents you/your role?
4. It should be self-explainable, technical, maturely and reasonably justified. So avoid the likes of ""Ninja"", ""Unicorn"", ""Guru"", unless you can maturely make a compelling argument.
5. You must be open to criticism and being challenged. Other redditors are not compelled to agree with your self-identity.

I'll also add my own response in the comments because I do not want it to be the center focus of the discussion. ",38,84,1735806871.0,datascience
1hr8ifj,What was your favorite work/project of 2024 and why was it personally fulfilling?,"I'm curious what the state of data science was in 2024, and what 2025 may bring, based on what data scientists prefer to be working on.

So let us know what project or type of work you most enjoyed last year that you may want to do more of in 2025 :)",104,70,1735754916.0,datascience
1hqx9a4,Finally got my NVIDIA Jetson Orin Nano SuperComputer (NVIDIA sponsored). What are some Data Science specific stuff I should try on it?,"So recently NVIDIA released Jetson Orin Nano, a Nano Supercomputer which is a powerful, affordable platform for developing generative AI models. It has up to 67 TOPS of AI performance, which is 1.7 times faster than its predecessor. 

Has anyone used it? My first time with an AI embedded system so what are some basic things to test on it? Already planned on Ollama and a few games like Crysis, doom, minecraft.",205,68,1735712582.0,datascience
1hqacf9,Duolingo for Data science and Machine learning ,"Edit: Thank you guys for all your recommendations. I really appreciate. Datacamp has exactly what I'm looking for. Brilliant is a close second. Thanks once again.


Is there an app like Duolingo for practicing data science and machine learning?
Solo learn and mimo are both for python and I was wondering if there are any apps like that but tailored for data science.
I installed some from playstore but it's just courses where I have to read things. I don't want to read things. I want to apply the technical coding aspects like in the mimo apps.

I know about kaggle and udemy but I'm looking for something like mimo.",165,77,1735633105.0,datascience
1hqkw4y,Any help for advanced numpy,"I am working on something where I need to process data using numpy. It's a tabular data and I need to convert it to multi dimensional arrays and then perform operations efficiently. 

Can anyone suggest some resources for advanced numpy so that I can understand and visualise numpy arrays, concept of axis, broadcasting etc.? I need to convert my data in such a way that I can do efficient operations on them. For that I need to understand multi dimensional numpy arrays and axis well enough. 
",25,29,1735669948.0,datascience
1hq0s6q,What would be the fastest way for me to get from novice to advanced level Python?,"I'm a data scientist with ten years experience. I've always worked at R shops and haven't been forced to learn Python on the job so my knowledge of the language is just from piddling around with it on my own and distinctly novice. If I was prepared to sink 5+ hours a day into it, what would be my best bet in terms of fastest way to hone my skills?",133,98,1735601417.0,datascience
1hpqjrk,How did you learn Git?,"What resources did you find most helpful when learning to use Git? 

I'm playing with it for a project right now by asking everything to ChatGPT, but still wanted to get a better understanding of it (especially how it's used in combination with GitHub to collaborate with other people).

I'm also reading at the same time the book Git Pocket Guide but it seems written in a foreign language lol",313,126,1735574950.0,datascience
1hp7pim,My Data Science Manifesto from a Self Taught Data Scientist,"**Background**

I’m a self-taught data scientist, with about 5 years of data analyst experience and now about 5 years as a Data Scientist. I’m more math minded than the average person, but I’m not special. I have a bachelor’s degree in mechanical engineering, and have worked alongside 6 data scientists, 4 of which have PHDs and the other 2 have a masters. Despite being probably, the 6th out of 7 in natural ability, I have been the 2nd most productive data scientist out of the group.


**Gatekeeping**

Every day someone on this subreddit asks some derivative of “what do I need to know to get started in ML/DS?” The answers are always smug and give some insane list of courses and topics one must master. As someone who’s been on both sides, this is attitude extremely annoying and rampart in the industry. I don’t think you can be bad at math and have no pre-requisite knowledge, and be successful, but the levels needed are greatly exaggerated. Most of the people telling you these things are just posturing due to insecurity.



As a mechanical engineering student, I had at least 3 calculus courses, a linear algebra course, and a probability course, but it was 10+ years before I attempted to become a DS, and I didn’t remember much at all. This sub, and others like it, made me think I had to be an expert in all these topics and many more to even think about trying to become a data scientist. 



When I started my journey, I would take coding, calculus, stats, linear algebra, etc. courses. I’d take a course, do OK in it, and move onto the next thing. However, eventually I’d get defeated because I realized I couldn’t remember much from the courses I took 3 months prior. It just felt like too much information for me to hold at a single time while working a full-time job. I never got started on actually solving problems because the internet and industry told me I needed to be an expert in all these things.


**What you actually need**

The reality is, 95% of the time you only need a basic understanding of these topics. Projects often require a deeper dive into something else, but that's a case by case basis, and you figure that out as you go.


For calculus, you don't need to know how to integrate multivariable functions by hand. You need to know that derivatives create a function that represents the slope of the original function, and that where the derivative = 0 is a local min/max. You need to know integrals are area under the curve.



For stats, you need to understand what a p value represents. You don't need to know all the different tests, and when to use them. You need to know that they exist and why you need them. When it's time to use one, just google it, and figure out which one best suits your use case.



For linear algebra, you don't need to know how to solve for eigenvectors by hand, or whatever other specific things you do in that class. You need to know how to ‘read’ it. It is also helpful to know properties of linear algebra. Like the cross product of 2 vectors yields a vector perpendicular to both.



For probability, you need to understand basic things, but again, just google your specific problem.



You don't need to be an expert software dev. You need to write ok code, and be able to use chatGPT to help you improve it little by little.



You don't need to know how to build all the algorithms by hand. A general understanding of how they work is enough in 95% of cases.



Of all of those things, the only thing you absolutely NEED to get started is basic coding ability. 



By far the number one technical ability needed to 'master' is understanding how to ""frame"" your problem, and how to test and evaluate and interpret performance. If you can ensure that you're accurately framing the problem and evaluating the model or alogithm, with metrics that correctly align with the use case, that's enough to start providing some real value. I often see people asking things like ""should I do this feature engineering technique for this problem?"" or “which of these algorithms will perform best?”. The answer should usually be, ""I don't know, try it, measure it, and see"". Understanding how the algorithms work can give you clues into what you should try, but at the end of the day, you should just try it and see.   



Despite the posturing in the industry, very few people are actually experts in all these domains. Some people are better at talking the talk than others, but at the end of the day, you WILL have to constantly research and learn on a project by project basis. That’s what makes it fun and interesting. As you gain PRACTICAL experience, you will grow, you will learn, you will improve beyond what you could've ever imagined. Just get the basics down and get started, don't spin your wheels trying and failing to nail all these disciplines before ever applying anything.



The reason I’m near the top in productivity while being near the bottom in natural and technical ability is my 5 years of experience as a data analyst at my company. During this time, I got really good at exploring my companies’ data. When you are stumped on problem, intelligently visualizing the data often reveals the solution. I’ve also had the luxury of analyzing our data from all different perspectives. I’d have assignments from marketing, product, tech support, customer service, software, firmware, and other technical teams. I understand the complete company better than the other data scientists. I’m also just aware of more ‘tips and tricks’ than anyone else.  



Good domain knowledge and data exploration skills with average technical skills will outperform good technical skills with average domain knowledge and data exploration almost every time. 


**Advice for those self taught**


I’ve been on the hiring side of things a few times now, and the market is certainly difficult. I think it would be very difficult for someone to online course and side project themselves directly into a DS job. The side project would have to be EXTREMELY impressive to be considered. However, I think my path is repeatable.



I taught myself basic SQL and Tableau and completed a few side projects. I accepted a job as a data analyst, in a medium sized (100-200 total employees) on a team where DS and DA shared the same boss. The barrier to DA is likely higher than it was ~10 years ago, but it's definitely something achievable. My advice would be to find roles that you have some sort of unique experience with, and tailor your resume to that connection. No connection is too small. For example, my DA role required working with a lot of accelerometer data. In my previous job as a test engineer, I sometimes helped set up accelerometers to record data from the tests. This experience barely helped me at all when actually on the job, but it helped my resume actually get looked at. For entry level jobs employers are looking for ANY connection, because most entry level resumes all look the same.


The first year or two I excelled at my role as a DA. I made my boss aware that I wanted to become a DS eventually. He started to make me a small part of some DS projects, running queries, building dashboards to track performance and things like that. I was also a part of some of the meetings, so I got some insight into how certain problems were approached. 



My boss made me aware that I would need to teach myself to code and machine learning. My role in the data science projects grew over time, but I was ultimately blocked from becoming a DS because I kept trying and failing to learn to code and the 25 areas of expertise reddit tells you that you need by taking MOOCs. 

  

Eventually, I paid up for DataQuest. I naively thought the course would teach me everything I needed to know. While you will not be proficient in anything DS upon completing, the interactive format made it easy to jump into 30-60 minutes of structured coding every day. Like a real language consistency is vital. 



Once I got to the point where I could do some basic coding, I began my own side project. THIS IS THE MOST IMPORTANT THING. ONCE YOU GET THE BASELINE KNOWLEDGE, JUST GET STARTED WORKING ON THINGS. This is where the real learning began. You'll screw things up, and that's ok. Titanic problem is fine for day 1, but you really need a project of your own. I picked a project that I was interested in and had a function that I would personally use (I'm on V3 of this project and it's grown to a level that I never could've dreamed of at the time). This was crucial in ensuring that I stuck with the project, and had real investment in doing it correctly. When I didn’t know how to do something in the project, I would research it and figure it out. This is how it works in the real world.



After 3 months of Dataquest and another 3 of a project (along with 4 years of being a data analyst) I convinced my boss to assign me DS project. I worked alongside another data scientist, but I owned the project, and they were mostly there for guidance, and coded some of the more complex things. I excelled at that project, and was promoted to data scientist, and began getting projects of my own, with less and less oversight. We have a very collaborative work environment, and the data scientists are truly out to help each other. We present our progress to each other often which allows us all to learn and improve. I have been promoted twice since I began DS work.



I'd like to add that you can almost certainly do all this in less time than it took me. I wasted a lot of time spinning my wheels. ChatGPT is also a great resource that could also increase your learning speed. Don't blindly use it, but it's a great resource.


**Tldr:** Sir this is Wendy’s.

**Edit:** I’m not saying to never go deeper into things, I’m literally always learning. I go deeper into things all the time. Often in very niche domains, but you don't need to be a master in all things get started or even excel. Be able to understand generalities of those domains, and dig deeper when the problem calls for it. Learning a concept when you have a direct application is much more likely to stick.


I thought it went without saying, but I’m not saying those things I listed are literally the only things you need to know about those topics, I was just giving examples of where relatively simple concepts were way more important than specifics.

**Edit #2:** I'm not saying schooling is bad. Yes obviously having a masters and/or PhD is better than not. I'm directing this to those who are working a full time job who want to break into the field, but taking years getting a masters while working full time and going another 50K into debt is unrealistic",1977,168,1735511285.0,datascience
1hoy3dm,recommend me the best statistics textbook for data science ,"I am intermediate level student who already studied stats , But i want to revisit it from DS and ML perspective  ",119,53,1735485620.0,datascience
1hpaa3c,Looking for some Senior DS Advice,"Hello everyone,

I think this is okay to be a post since it's not about entering/transitioning, but if I need to repost in the weekly threads please let me know! 

TLDR:

* I started working as a Data Scientist at a medium to large company almost 3 years ago.
* I spent the majority of my time doing more Software Engineering/Data Engineering related tasks with DS projects sprinkled in.
* A reorg changed the entire landscape of my company and potential growth at the company.
* I don't know what to do because I don't know if I got solid enough experience to leave for another DS job, but my current situation is very uncomfortable.
* Looking for any seasoned perspective/advice on the situation to help anchor me since I'm in a bit of a doom spiral. 



I am looking for some career advice. I don't want to write a novel about my journey to this point, but it was a hell of a lot of work. A snippet of my relevant work experience is I worked at various tech startups doing Data Analyst/Engineering work before I found my way to DS. I graduated with my MS in Data Science back in 2021, and I landed a job at a medium/large global business in the retail space. To my surprise, it was the common meme situation where they had no infrastructure put in place for DS work, and on top of that, a former IBM DS had built a Python ""application"" being used by an internal team that was barely hanging on.

**Year 1** 

My boss asked if I'd be able to modernize the application, and since I have a bit of a programming background, I told them I'd be happy to do that to get my feet wet with the org. I am going to way oversimplify the work I did for the sake of time. The important part is this project took around 6 months as the org had everything on-prem, so I had to go through approvals to get the more ""modern"" tech. I refactored a large portion of it, containerized it, and deployed it via an OpenShift (RedHat's Kubernetes product) cluster. The bulk of the program was a massive Jupyter Notebook (5000 lines of code with some custom-built math libraries) that an analyst would execute each cell after a request was made. This notebook housed all the business logic, so I just wrapped all that up to be executed automatically when the internal team interacted with the new app. By the end of it, I had a firm grasp on various business processes and was already talking to my boss about possibilities. Additionally, I found out that I was the only ""Data Scientist"" on staff, and I was a little bummed because I had chosen to work for a larger org in hopes of getting some sort of mentor/learn-by-osmosis going on. However, since my background is in startups I wasn't overly concerned because I knew I could utilize this environment to grow by trailblazing.

The conversation then shifted to the logic in the notebook, and the fact that no one really knew what was happening inside it. This notebook was driving a fairly important piece of the business by analyzing various datapoints, applying business rules, and spitting out results to be used day to day. They asked if I could dissect it, and I readily agreed – really wish LLMs were as commercialized as they are now. I spent the next 2-3 months working out bugs in the newly deployed app, and flow charting out all the business logic inside the notebook into nice Confluence pages. It was fairly spaghettified, so making changes to it was going to prove challenging. I put my ""Product Manager"" hat on and asked what their goals were with this application, the logic, measuring success, etc. I was asked to start a rewrite so that the laundry list of changes they had wanted to make could be done. It was also at this time my boss was super happy with the ideas/work I had done (I had several other smaller projects I did during this time), so they began speaking to me about being promoted up. How we'd get an actual software engineer on my team so I could focus on more of the ""Data Science"" stuff. I was super excited/anxious because I was hoping to get more hands-on DS experience before leading a team. However, once again, I come from startups so sort of par for the course.



**Year 2** 

The IT department announces a ""reorg"" a month before my promotion. By this point I had job descriptions for a few new positions, and we had made plans for who would be shifting to my team. All of this gets put on hold, and there's tons of uncertainty. I spend the next year doing the rewrite by myself. I build a few classification models in the process to help a few other internal teams operate more efficiently.

Basically they come through with a domain-driven design philosophy so that the Software teams can build more efficiently by having more autonomy. They establish practices across the domains, and they had a Data/ML practice initially. That gave me some confidence that I'd at least have ""peers"" when it was all said and done.



**Year 3 – Current year** 

I get moved into a domain, and they establish a separate BI & Analytics domain. They decentralized everything else but anything to do with ""Data Work"". I am given a promotion to DS Manager with a single employee – a Data Engineer. It has been super confusing all year with things taking much longer as the org adjusts for the new bureaucratic processes that have been introduced – tooling now has to be approved, Business analyst, delivery leads, PMO offices, etc. I meet with the head of engineering to ask how I go about getting tools approved (Sage Maker endpoints), and to get a sense of our overall data strategy. I'm basically told there isn't one in place, but they hope to get one together soonish. A lot has happened and it all feels very confusing. Basically no one is empowered to make decisions, the BI domain is leading the charge for their stuff, and me and my team are sort of this island that exists outside of everything else going on.



I tried to keep that as short as possible, and happy to give further detail if you believe it'd help.



**Here's my main issue:** I spent these years doing what needed to be done, but there really isn't a path of ""growth"" because they aren't really accounting for Data Scientists yet – though they say they hope to hire them. It was clear in the first year what the path would probably look like, but with everything becoming more corporate it feels like I could easily get shafted in one way or another. However, because I spent these years being the ""good employee"" and doing what needed to be done instead of what was best for my own experience I think it may be hard for me to get a DS job at another org. I'm hoping to get some perspective from all of you more seasoned professionals.",15,13,1735518438.0,datascience
1hpfkyr,"Weekly Entering & Transitioning - Thread 30 Dec, 2024 - 06 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",3,63,1735534882.0,datascience
1houdgh,What are some of the most interesting applied ml papers/blogs you read in 2024 or projects you worked on,"I am looking for some interesting successful/unsuccessful real-world machine learning applications. You are also free to share experiences building applications with machine learning that have actually had some real world impact.

Something of this type: 

1. LinkedIn has developed a new family of domain-adapted foundation models called Economic Opportunity Network (EON) to enhance their platform's AI capabilities.

https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform


Edit: Just to encourage this conversation here is my own personal SAAS app - this is how l have been applying machine learning in the real world as a machine learning engineer. It's not much, but it's something.
This is a side project(built during weekends and evenings) which flopped and has no users
[Clipbard](https://clipbard.com). I mostly keep it around to enhance my resume.
My main audience were educators would like to improve engagement with the younger 'tiktok' generation. I assumed this would be a better way of sharing things like history in a more memorable way as opposed to a wall of text. I also targeted groups like churches (Sunday school/ Children's church) who want to bring bible stories to life or tell stories with lessons or parents who want to bring bedtime stories to life every evening.


",55,20,1735472512.0,datascience
1hp65ll,"IYE, how does the computational infrastructure for AI models and their cost impact developers and users? Has your org ever bottlenecked development by cost to deploy the AI solution, either for you or in their pricing for clients?","I'm curious how the expense of AI factors into business. It seems like an individual could write code that impacts their cost of employment, and that LLM training algorithms and other AI work would be more expensive. 

 I'm wondering how businesses are governing the cost of a data scientist/software developer's choices with AI.",6,3,1735507199.0,datascience
1hp0cbx,Building Production-Ready AI Agents & LLM programs with DSPy: Tips and Code Snippets,,9,4,1735491867.0,datascience
1hoq8yb,ModernBERT vs BERT ,,12,2,1735454485.0,datascience
1hoc6n8,Seeking Collaborators to Develop Data Engineer and Data Scientist Paths on Data Science Hive,"Data Science Hive is a completely free platform built to help aspiring data professionals break into the field. We use 100% open resources, and there’s no sign-up required—just high-quality learning materials and a community that supports your growth.

Right now, the platform features a Data Analyst Learning Path that you can explore here:  https://www.datasciencehive.com/data_analyst_path 

It’s packed with modules on SQL, Python, data visualization, and inferential statistics - everything someone needs to get Data Science Hive is a completely free platform built to help aspiring data professionals break into the field. We use 100% open resources, and there’s no sign-up required—just high-quality learning materials and a community that supports your growth.

We also have an active Discord community where learners can connect, ask questions, and share advice. Join us here: https://discord.gg/gfjxuZNmN5

But this is just the beginning. I’m looking for serious collaborators to help take Data Science Hive to the next level.

Here’s How You Can Help:

	•	Share Your Story: Talk about your career path in data. Whether you’re an analyst, scientist, or engineer, your experience can inspire others.
	•	Build New Learning Paths: Help expand the site with new tracks like machine learning, data engineering, or other in-demand topics.
	•	Grow the Community: Help bring more people to the platform and grow our Discord to make it a hub for aspiring data professionals.

This is about creating something impactful for the data science community—an open, free platform that anyone can use.

Check out https://www.datasciencehive.com

explore the Data Analyst Path, and join our Discord to see what we’re building and get involved. Let’s collaborate and build the future of data education together!",63,27,1735411445.0,datascience
1ho91f8,Will the official Year End Salary thread be posted for 2024? ,I tried searching for it with the “salary” as the keyword. Usually that thread is up by now. Was just curious as I was looking for comparisons to my own salary. ,52,20,1735402904.0,datascience
1ho4com,Meta's Byte Latent Transformer: new LLM architecture (improved Transformer),Byte Latent Transformer is a new improvised Transformer architecture introduced by Meta which doesn't uses tokenization and can work on raw bytes directly. It introduces the concept of entropy based patches. Understand the full architecture and how it works with example here : https://youtu.be/iWmsYztkdSg,40,2,1735387439.0,datascience
1hnnwf5,Euchre Simulation and Winning Chances,"I tried posting this to r/euchre but it got removed immediately.

I’ve been working on a project that calculates the odds of winning a round of Euchre based on the hand you’re dealt. For example, I used the program to calculate this scenario:

If you in the first seat to the left of the dealer, a hand with the right and left bower, along with the three non-trump 9s wins results in a win 61% of the time. (Based on 1000 simulations)

For the euchre players here:

Would knowing the winning chances for specific hands change how you approach the game?
Could this kind of information improve strategy, or would it take away from the fun of figuring it out on the fly?
What other scenarios or patterns would you find valuable to analyze?
I’m excited about the potential applications of this, but I’d love to hear from any Euchre players. Do you think this kind of data would add to the game, or do you prefer to rely purely on instinct and experience? Here is the github link:

https://github.com/jamesterrell/Euchre_Calculator",25,4,1735330775.0,datascience
1hnl48d,Imputation Use Cases,"I’m wondering how and why people use this technique. I learned about it early on in my career and have avoided it entirely after trying it a few times. If people could provide examples of how they’ve used this in a real life situation it would be very helpful.

I personally think it’s highly problematic in nearly every situation for a variety of reasons. The most important reason for me is that nulls are often very meaningful. Also I think it introduces unnecessary bias into the data itself. So why and when do people use this?",30,52,1735323441.0,datascience
1hn0k9f,I analyzed you guys ,"In my quest for finding an internship and figuring what I want to do with my life workwise I decided to analyze how y'all feel about jobs in data science. One of the fields I am interested in is machine learning/data science so I decided to do a project that would help me see what other people think about this field. 

The project is available here: [Sentiment analysis part 1 | Ted’s cave](https://tedthecaver.github.io/2024/11/29/sentiment_analysis.html)

I would really appreciate any advice on the project itself if anyone bothers to read through it or on the problem of how I'm supposed to figure out what my passions are, and how do i commit to one thing (and how do i land an internship lol). 

Anyways I thought I would share with my dataset the project I did. Thanks y'all. ",146,43,1735254124.0,datascience
1hnlbhw,Pre/Post Implementation Analysis Interpretation,"I am using an interrupted time series to understand whether a certain implementation affected the behavior of the users. We can't do a proper A/B testing since we introduced the feature to all the users.

Lets say we were able to create a model and predict the post implementation daily usage to create the ""counterfactual"" which would be ""What would be the usage look like if there was no implementation?""

Since I have the actual post-implementation usage, now I can use it to find the cumulative difference/residual.

But my question is, since the model is trained on the pre-implementation data doesn't it make sense for the residual error to be high against the counter factual?

The data points in pre-implementation are mostly even across the lower and higher boundary and Its clear that there are more data points in the lower boundaries in the post-implementation but not sure how I would correctly test this. I want to understand the direction so was thinking about using MBE (Mean Bias Deviation)

Any thoughts?",3,3,1735323961.0,datascience
1hn1eqn,What's your 2025 resolution as a DS?,"As 2024 wraps up, it’s time to reflect and plan ahead. What’s your new year resolution as a data scientist? Are you aiming for a promotion, a pay bump, or a new job? Maybe you’re planning to dive into learning a new skill, step into a people manager role, or pivot to a different field.

Curious to hear what's on your radar for 2025 (of course coasting counts too).",82,119,1735256582.0,datascience
1hmuob8,Regression on multiple independent variable,"Hello everyone,

I've come across a use case that's got me stumped, and I'd like your opinion.

I have around 1 million pieces of data representing the profit of various projects over a period of time. Each project has its ID, its profits at the date, the date, and a few other independent variables such as the project manager, city, etc...

So I have projects over years, with monthly granularity. Several projects can be running simultaneously.

I'd like to be able to predict a project's performance at a specific date. (based on profits)

The problem I've encountered is that each project only lasts 1 year on average, which means we have 12 data points per project, so it's impossible to do LSTM per project. As far as I know, you can't generalise LSTM for a case like mine (similar periods of time for different projects).

How do you build a model that could generalise the prediction of the benefits of a project over its lifecycle?

What I've done for the moment is classic regression (xgboost, decision tree) with variables such as the age of the project (in months), the date, the benefits over M-1, M-6, M-12. I've chosen 1 or 0 as the target variable (positive or negative margin at the current month).

I'm afraid that regression won't be enough to capture more complex trends (lagged trend especially). Which kind of model would you advise me to go ? Am I on a good direction ?",31,17,1735238072.0,datascience
1hng96m,Puppy: organize your 2025 python projects,"# TLDR

[https://github.com/liquidcarbon/puppy](https://github.com/liquidcarbon/puppy) is a transparent wrapper around pixi and uv, with simple APIs and recipes for using them to help write reproducible, future-proof scripts and notebooks.

## From 0 to rich toolset in one command:

Start in an empty folder.

```
curl -fsSL ""https://pup-py-fetch.hf.space?python=3.12&pixi=jupyter&env1=duckdb,pandas"" | bash
```

installs python and dependencies, in complete isolation from any existing python on your system.  Mix and match URL [query params](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#one-installer-to-rule-them-all) to specify python version, tools, and venvs to create.

The above also installs puppy's CLI (`pup --help`):

## CLI - kind of like ""uv-lite""
- `pup add myenv pkg1 pkg2` (install packages to ""myenv"" folder using uv)
- `pup list` view what's installed across all projects
- `pup clone` and `pup sync` clone and build external repos (must have buildable `pyproject.toml` files)

## Pup as a Module - no more notebook kernels

The original motivation for writing puppy was to simplify handling kernels, but you might just not need them at all.  Activate/create/modify ""kernels"" interactively with:

```
import pup
pup.fetch(""myenv"")  # ""activate"" - packages in ""myenv"" are now importable
pup.fetch(""myenv"", ""pkg1"", ""pkg2"")  # ""install and activate"" - equivalent to `pup add myenv pkg1 pkg2`  
```

Of course you're welcome to use `!uv pip install`, but after 10 times it's liable to get messy.


## Target Audience

Loosely defining 2 personas:

1. Getting Started with Python (or herding folks who are):
   1. puppy is the easiest way to go from 0 to modern python - one-command installer that lets you specify python version, venvs to build, repos to clone - getting everyone from 0 to 1 in an easy and standardized way
   2. if you're confused about virtual environments and notebook kernels and install full jupyter into every project

2. Competent - check out [Multi-Puppy-Verse](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#multi-puppy-verse) and [Where Pixi Shines](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#where-pixi-shines-) sections:
   1. you have 10 work and hobby projects going at the same time and need a better way to organize them for packaging, deployment, or even to find stuff 6 months later
   2. you need support for conda and non-python stuff - you have many fast-moving external and internal dependencies - check out `pup clone` and `pup sync` workflows and [dockerized examples](https://github.com/liquidcarbon/puppy/tree/main/examples)


## Filesystem is your friend

Puppy recommends a sensible folder structure where each outer folder houses one and only one python executable - in isolation from each other and any other python on your system. Pup is tied to a python executable that is installed by Pixi, along with project-level tools like Jupyter, conda packages, and non-python tools (NodeJS, make, etc.) Puppy commands work the same from anywhere within this folder.

The inner folders are git-ready projects, defined by pyproject.toml, with project-specific packages handled by uv.



```
# ├── puphome/  # python 3.12 lives here
# │   ├── public-project/
# │   │   ├── .git  # this folder may be a git repo (see pup clone)
# │   │   ├── .venv
# │   │   └── pyproject.toml
# │   ├── env2/
# │   │   ├── .venv/  # this one is in pre-git development
# │   │   └── pyproject.toml
# │   ├── pixi.toml
# │   └── pup.py
# ├── pup311torch/  # python 3.11 here
# │   ├── env3/
# │   ├── env4/
# │   ├── pixi.toml
# │   └── pup.py
# └── pup313beta/  # 3.13 here
#     ├── env5/
#     ├── pixi.toml
#     └── pup.py
```

Puppy embraces ""explicit is better than implicit"" from the Zen of python; it logs what it's doing, with absolute paths, so that you always know where you are and how you got there.



PS I've benefited a great deal from the many people's OSS work - now trying to pay it forward. The ideas laid out in puppy's README and implementation have come together after many years of working in different orgs, where average ""how do you rate yourself in python"" ranged from zero (Excel 4ever) to highly sophisticated. The matter of ""how do we build stuff"" is kind of never settled, and this is my take.

Thanks for checking this out! Suggestions and feedback are welcome!",0,12,1735310424.0,datascience
1hmdpjm,Non-technical job alternatives for former data scientist,"Some context, I have a PhD in a hard science and I worked as a data scientist at a medical company for about 4 years and learned quite a bit and felt overall useful, from machine learning to stats, reports, dashboards and python writing. I have good social and communication skills as well, though they were not needed at my position as data scientist.

However, I felt like the amount of work and the nature of work just wasn't a match for me, it felt like manual labour, except with my brain. Constant and never ending work and problem solving -- no where near as difficult as the graduate work but much more abundant and relentless. At some point I guess you could say burnout occurred. I don't mind problem solving and writing code, but at a human pace, with intellectual freedom. Has anyone been in my situation? What sort of jobs aside from management did you transition to? If anyone knows of any specific roles or advice please do share. I would be happy to provide more context if necessary. 

Thank you!",125,59,1735176258.0,datascience
1hmrwcw,DeepSeek-v3 looks the best open-sourced LLM released,,6,0,1735230550.0,datascience
1hm7es6,Updated with 250+ Questions - DS Questions,"Hi everyone, 

Just wanted to give a heads up we updated our list of data science interview questions to now have almost 250 questions for you guys to try out and access for yourselves. Again with a free plan you can access most of the content on the site.

Hope this helps you guys in your interview prep - merry christmas.

[https://www.dsquestions.com/problems](https://www.dsquestions.com/problems)",13,11,1735155586.0,datascience
1hlz38v,Where can I find real-world ML/DS experience? Volunteering works too!,"Hey everyone,

So, I’m trying to get some hands-on experience in machine learning and data science—not just the “do more projects” advice (I’ve already done a bunch), but actual real-world stuff where I can work on meaningful problems. Paid or unpaid, doesn’t really matter to me—I’d even love to volunteer if it means I get to learn and grow.

I recently applied for an Omdena project, and I’m wondering if anyone here has done something with them? What’s it like? Did it actually help you gain valuable experience, or was it just another “group project” kind of thing?

Also, are there other platforms or places where I could jump into something similar? I’m trying to avoid the whole “chasing certifications” rabbit hole. I just want to get better at solving real problems, not stacking credentials.

Would love to hear your thoughts or any experiences you’ve had. Thanks in advance!

bit about me: I’m a 3rd-year undergrad in Computer Science with a minor in Statistics, and I just got an internship for a data role at a pretty big company. Super excited about it, but I want to keep building my skills and exploring different opportunities in ML/DS.",35,15,1735126029.0,datascience
1hm9he8,Am I cooked or is it this job market? ,,0,18,1735162224.0,datascience
1hlup8w,"LangChain In Your Pocket (Generative AI Book, Packt published) : Free Audiobook","Hi everyone,

It's been almost a year now since I published my debut book

>“LangChain In Your Pocket : Beginner’s Guide to Building Generative AI Applications using LLMs”

https://preview.redd.it/lgtj9570ix8e1.png?width=934&format=png&auto=webp&s=8b2a0e87914072d5125551adf830b731afcb293e

And what a journey it has been. The book saw major milestones becoming a **National and even International Bestseller in the AI category**. So to celebrate its success, I’ve released the Free Audiobook version of “LangChain In Your Pocket” making it accessible to all users free of cost. I hope this is useful. The book is currently rated at 4.6 on amazon India and 4.2 on amazon com, making it amongst the top-rated books on LangChain and is published by Packt as well

More details : [https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775](https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775)

# Table of Contents

* Introduction
* Hello World
* Different LangChain Modules
* Models & Prompts
* Chains
* Agents
* OutputParsers & Memory
* Callbacks
* RAG Framework & Vector Databases
* LangChain for NLP problems
* Handling LLM Hallucinations
* Evaluating LLMs
* Advanced Prompt Engineering
* Autonomous AI agents
* LangSmith & LangServe
* Additional Features

**Edit :** Unable to post direct link (maybe Reddit Guidelines), hence posted medium post with the link.",0,1,1735105238.0,datascience
1hk7uvs,"You Get a Dataset and Need to Find a ""Good"" Model Quickly (in Hours or Days), what's your strategy?","**Typical Scenario**: Your friend gives you a dataset and challenges you to beat their model's performance. They don't tell you what they did, but they provide a single CSV file and the performance metric to optimize.

Assumptions:
- Almost always tabular data, so no need learning needed.
- The dataset is typically small-ish (<100k rows, <100 columns), so it fits into memory.
- It's always some kind of classification/regression, sometimes time series forecasting.
- The data is generally ready for modeling (minimal cleaning needed).
- Single data metric to optimize (if they don't have one, I force them to pick one and only one).
- No additional data is available.
- You have 1-2 days to do your best.
- Maybe there's a hold out test set, or maybe you're optimizing repeated k-fold cross-validation.

I've been in this situation perhaps a few dozen times over the years. Typically it's friends of friends, typically it's a work prototype or a grad student project, sometimes it's paid work. Always I feel like my honor is on the line so I go hard and don't sleep for 2 days. Have you been there?

Here's how I typically approach it:

1. **Establish a Test Harness:** If there's a hold out test set, I do a train/test split sensitivity analysis and find a ratio that preserves data/performance distributions (high correlation, no statistical difference in means). If there's no holdout set, I ask them to evaluate their model (if they have one) using 3x10-fold cv and save the result. Sometimes I want to know their result, sometimes not. Having a target to beat is very motivating!
2. **Establish a Baseline:** Start with dummy models get a baseline performance. Anything above this has skill.
3. **Spot Checking:** Run a suite of all scikit-learn models with default configs and default ""sensible"" data prep pipelines.
	- Repeat with  asuite (grid) of standard configs for all models.
	- Spot check more advanced models in third party libs like GBM libs (xgboost, catboost, lightgbm), superlearner, imbalanced learn if needed, etc.
	- I want to know what the performance frontier looks like within a few hours and what looks good out of the box.
4. **Hyperparameter Tuning:** Focus on models that perform well and use grid search or Bayesian optimization for hyperparameter tuning. I setup background grid/random searches to run when I have nothing else going on. I'll try some bayes opt/some tpot/auto sklearn, etc. to see if anything interesting surfaces.
5. **Pipeline Optimization:** Experiment with data preprocessing and feature engineering pipelines. Sometimes you find that a lesser used transform for an unlikely model surfaces something interesting.
6. **Ensemble Methods:** Combine top-performing models using stacking/voting/averaging. I schedule this to run every 30 min and to try look for diverse models in the result set, ensemble them together and try and squeeze out some more performance.
7. **Iterate Until Time Runs Out:** Keep refining and experimenting based on the results. There should always be some kind of hyperparameter/pipeline/ensemble optimization running as background tasks. Foreground is for wild ideas I dream up. Perhaps a 50/50 split of cores, or 30/70 or 20/80 if I'm onto something and need more compute.

Not a ton of time for EDA/feature engineering. I might circle back after we have the performance frontier mapped and the optimizers are grinding. Things are calmer, I have ""something"" to show by then and can burn a few hours on creating clever features.

I dump all configs + results into an sqlite db and have a flask CRUD app that allows me to search/summarize the performance frontier. I don't use tools like mlflow and friends because they didn't really exist when I started doing this a decade ago. Maybe it's time to switch things up. Also, they don't do the ""continuous optimization"" thing I need as far as I know.

I re-hack my scripts for each project. They're a mess. Oh well. I often dream of turning this into an ""auto ml like service"", just to make my life easier in the future :)

What is (or would be) your strategy in this situation? How do you maximize results in such a short timeframe?

Would you do anything differently or in a different order?

Looking forward to hearing your thoughts and ideas!",211,65,1734901944.0,datascience
1hl9xdo,12 days of OpenAI summarized ,,0,5,1735031798.0,datascience
1hjy9nb,tHe wINdoWs mL EcOsYteM,,336,42,1734873281.0,datascience
1hk7fx3,Do data scientists do research and analysis of business problems? Or is that business analysis done by data analysts? What's the distinction?,"Are data scientists, scientists of data itself but not applied analysts producing business analysis for business leaders?

Put another way, are data scientists like drug dealers that don't get high on their own supply? So other people actually use the data to add value? And data scientists add value to the data so analysts can add value to the business with the data?

Where is the distinction? Can someone be both? At large companies does it matter?

I get paid to define and solve business problems with data. I like that advanced statistical business analysis since it feels like scientific discovery. I have an offer to work in a new AI shop at work, but fear that sort of 'data science' is for tool-builders, not researchers 
",28,52,1734900749.0,datascience
1hkgk35,"Weekly Entering & Transitioning - Thread 23 Dec, 2024 - 30 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,53,1734930081.0,datascience
1hk1ot7,ML pipeline questions,"I am building an application that processes videos and that needs to run many tasks (some need to be sequentially and some in parallel). Think audio extraction, ASR, diarization, translation, video classification, etc... Note that this is in supposed to be run online, i.e. this is supposed to be used in a web app where the user uploads a video and this pipeline I just described is run, the output is either stores in a bucket or a database and the results are shown after some time.

When I look up ""ML pipelines"" on goole I get stuff like kubeflow pipelines or vertex ai pipelines, so here is my first question:

1. Are these pipeline tools supposed to be run in production/online like in the use case I just described or are they meant to build ML pipelines for model training (preprocessing data, training a model and building a docker with the model weights, example) that are scheduled every so often? 

  
It feels like these tools are not what I want because they seem to be aimed at building models and not serving them.



After some googling I realized one good option would be to use Ray with Kubernetes. They allow for model composition and allow for node configuration for each task which is exactly what I was looking for, but my second question is: 

2. What else could I use for this task?

Plain kubernetes seems to be another option but more complex at setting up... it seems weird to me that there are no more tools for this purpose (multi model serving with different hardware requirements), unless I can do this with kubeflow or vertex ai pipelines",11,10,1734884282.0,datascience
1hjluem,"Statisticians, Scripts, and Chaos: My Journey Back to the 90s","We often hear a lot about how data science teams can lack statistical expertise and how this can lead to flawed analyses or misinterpretation of results. It’s a valid concern, and the dangers are real. But let me tell you, there’s another side of the coin that had me saying, “Holy bleep.”

This year, I joined a project where the team is dominated by statisticians and economists. Sounds like a data science dream team, right? Not so fast. It feels like I hopped into a time machine and landed in the 90s. Git? Never heard of it. Instead, we’ve got the old-school hierarchy of script_v1, script_final_version_1, script_final_version_2, all the way to script_final_version_n. It's a wild ride.

Code reviews? Absolutely nonexistent. Every script is its own handcrafted masterpiece, riddled with what I can only describe as ""surprise features"" in the preprocessing pipeline. Bugs aren’t bugs, apparently. “If you just pay close attention and read your code twice, you’ll see there’s no issue,” they tell me. Uh, sure. I don’t trust a single output right now because I know that behind every analysis bugs are having the party of their lives. 

Chances are, statisticians have absolutely no idea how a modern database actually works, have never heard of a non-basic data structure like a HyperLogLog, and have likely never wrestled with a truly messy real-world dataset.",177,53,1734823167.0,datascience
1hjrill,Genesis : Physics AI engine for generating 4D robotic simulations ,"One of the trending repos on GitHub for a week, genesis-world is a python package which can generate realistic 4D physics simulations (with no irregularities in any mechanism) given just a prompt. The early samples looks great and the package is open-sourced (except the GenAI part). Check more details here : https://youtu.be/hYjuwnRRhBk?si=i63XDcAlxXu-ZmTR",6,1,1734843022.0,datascience
1hjn3ad,"Data scientist interview(UK) coming soon,  any tips ?","Hi all, 

Final round interview coming up with a Major insurance company in the Uk. So basically they gave me an take-home assessment where I need to do some EDA and come up with an algorithm to predict mental health and also create presentation slides which I did and sent it to them and received an interview invite after, they also gave me some feedback acknowledging the assessment. 

So my questions are:

Tips for the interview on what to keep in mind and what major things should I keep in mind? 

They also told me to do a presentation on the slides I created keeping in mind the ‘Technical audiences and Non-Technical audiences’- Any tips for this will really help me 

Thank you to everyone for reading this post and for upcoming suggestions,

Yours loving Redditor 🫂",13,16,1734827165.0,datascience
1hjc9e0, Doctorate in quantitative marketing / marketing worth it? ,"I’ll be graduating with my MS stats in the spring and then working as a data scientist within the ad tech / retail / marketing space. My current Ms thesis, despite it being statistics (causal inference) focused it’s rooted in applications within business, and my advisors are stats/marketing folks in the business school.

After my first year of graduate school I immediately knew a PhD n statistics would not be for me. That degree is really for me not as interesting as I’m not obsessive about knowing the inner details and theory behind statistics and want to create more theory. I’m motivated towards applications in business, marketing, and “data science” settings. 

Topics of interest of mine have been how statistical methods have been used in the marketing space and its intersection with modern machine learning.

I decided that I’d take a job as a data scientist post graduation to build some experience and frankly make some money.

A few things I’ve thought about regarding my career trajectory:

1. Build a niche skillset as a data scientist within the industry within marketing/experimentation and try and get to a staff DS in FAANG experimentation type roles

- a lot of my masters thesis literature review was on topics like causal inference and online experimentation. These types of roles in industry would be something I’d like to work in 


2. After 3-4 yo experience in my current marketing DS role, go back to academia at a top tier business school and do a PhD in quantitative marketing or marketing with a focus on publishing research regarding statistical methods for marketing applications

- I’ve read through a lot of the research focus of a lot of different quant marketing PhD programs and they seem to align with my interests. My current Ms thesis in ways to estimate CATE functions and heterogenous treatment effect, and these are generally of interest in marketing PhD programs

- I’ve always thought working in an academic setting would give me more freedom to work on problems that interest me, rather than be limited to the scope of industry. If I were to go this route I’d try and make tenure at an R1 business school.


I’d like to hear your thoughts on both of these pathways, and weigh in on:

1. Which of these sounds better, given my goals?

2. Which is the most practical?

3. For anyone whose done a PhD in quantitative marketing and or PhD in marketing with an emphasis in quantitative methods, what that was like and if it’s worth doing especially if I got into a top business school. 




",28,17,1734795465.0,datascience
1hk0yh1,Saw this linkedin post - really think it explains the advances o3 has made well while also showing the room for improvement - check it out ,,0,0,1734882161.0,datascience
1hirjaq,"OpenAI o3 and o3-mini annouced, metrics are crazy",So OpenAI has released o3 and o3-mini which looks great on coding and mathematical tasks. The Arc AGI numbers looks crazy ! Checkout all the details summarized in this post : https://youtu.be/E4wbiMWG1tg?si=lCJLMxo1qWeKrX7c,146,59,1734722705.0,datascience
1hjx3q0,Is OpenAI o3 really AGI? ,,0,6,1734868576.0,datascience
1hisd3a,Advice on Analyzing Geospatial Soil Dataset — How to Connect Data for Better Insights?,"
Hi everyone! I’m working on analyzing a dataset (600,000 rows) containing geospatial and soil measurements collected along a stretch of land. 

The data includes the following fields:

Latitude & Longitude: Geospatial coordinates for each measurement.

Height: Elevation at the measurement point.

Slope: Slope of the land at the point.

Soil Height to Baseline: The difference in soil height relative to a baseline.

Repeated Measurements: Some locations have multiple measurements over time, allowing for variance analysis.

Currently, the data points seem disconnected (not linked by any obvious structure like a continuous line or relationships between points). My challenge is that I believe I need to connect or group this data in some way to perform more meaningful analyses, such as tracking changes over time or identifying spatial trend. 

Aside from my ideas, do you have any thoughts for how this could be a useful dataset? What analysis can be done? ",15,20,1734724937.0,datascience
1hjb4m7,Data Science Interview Prep,"Hi everyone,

My friend Marc and I broke into data science a while back and we 100% understand how hard the job market is. So, we've have been working on a interview prep platform for data science students that we'd enjoy using ourselves.

Right now we have \~200 questions including coding, probability, and statistics questions with most free to answer. We are adding new questions daily and want to grow a community where we can help one another out. [https://dsquestions.com/](https://dsquestions.com/)

All we need now is good feedback - I'd appreciate if you guys could check it out and give us some :)",0,22,1734792021.0,datascience
1hhv4iy,"Project: Hey, wait – is employee performance really Gaussian distributed??
A data scientist’s perspective ",,272,40,1734621459.0,datascience
1hi2gk9,Going back for a BS in Statistics,"Hi! I graduated from a Notre Dame with a BA in Psychology and a Supplementary Major in Statistics (more than a minor, less than a major). I only need 4 more classes to get a BS in Statistics because I did a lot of additional science reqs as pre-med. Does anyone know my options to either go back to school (undergrad) or transfer the credits to another school to get a double degree? I'm currently in a masters program (60%ish done) and working full-time as a DS in a dead-end role, but I'm having so much trouble getting any traction on job apps, and I always wondered if a BS would help.... Is this crazy?",51,44,1734640662.0,datascience
1hhmgvw,GotHub CoPilot gets a free tier for all devs,"GitHub CoPilot has now introduced a free tier with 2000 completions, 50 chat requests and access to models like Claude 3.5 Sonnet and GPT-4o. I just tried the free version and it has access to all the other premium features as well. Worth trying out : https://youtu.be/3oTPrzVTx3I",175,20,1734588609.0,datascience
1hheyol,"I built a free job board that uses ML to find you ML jobs
","**Link:** [**https://www.filtrjobs.com/**](https://www.filtrjobs.com/)

I tried 10+ job boards and was frustrated with irrelevant postings relying on keyword matching -- so i built my own for fun

I'm doing a semantic search with your jobs against embeddings of job postings prioritizing things like working on similar problems/domains

The job board fetches postings daily for ML and SWE roles in the US.

It's **100% free with no ads** for ever as my infra costs are $0

I've been through the job search and I know its so brutal, so feel free to DM and I'm happy to give advice on your job search

My resources to run for free:

* free 5GB postgres via [aiven.io](http://aiven.io/)
* free LLM from [galadriel.com](http://galadriel.com) (free 4M tokens of llama 70B a day)
* free hosting via heroku (24 months for free from [github student perks](https://www.heroku.com/github-students))
* free cerebras LLM parsing (using llama 3.3 70B which runs in half a second - 20x faster than gpt 4o mini)
* Using posthog and sentry for monitoring (both with generous free tiers)",380,88,1734564486.0,datascience
1hhs47t,Looking for Applied Examples or Learning Resources in Operations Research and Statistical Modeling ,"Hi all, 

I'm a working data scientist and I want to study Operations Research and Statistical Modeling, with a focus on chemical manufacturing. 

I’m looking for learning resources that include applied examples as part of the learning path. Alternatively, a simple, beginner-friendly use case (with a solution pathway) would work as well - I can always pick up the theory on my own (in fact, most of what I found was theory without any practice examples - or several months long courses with way too many other topics included).

I'm limited in the time I can spend, so each topic should fit into a half-day (max. 1 day) of learning. The goal here is not to become an expert but to get a foundational skill-level where I can confidently find and conduct use cases without too much external handholding. Upskilling for the future senior title, basically. 😄

Topics are:

 - Linear Programming (LP): e.g. Resource allocation, cost minimization.

 - Integer Programming (IP): e.g. Scheduling, batch production.

- Bayesian Statistics
    
- Monte Carlo Simulation: e.g. Risk and uncertainty analysis.
    
- Stochastic Optimization: Decision-making under uncertainty.
    
- Markov Decision Processes (MDPs): Sequential decision-making (e.g., maintenance strategies).

- Time Series Analysis: e.g. forecasting demand for chemical products.

- Game Theory: e.g. Pricing strategies, competitive dynamics.

Examples or datasets related to chemical production or operations are a plus, but not strictly necessary.

Thanks for any suggestions!",15,14,1734612364.0,datascience
1hhlqh4,Tips on where to access research papers otherwise locked behind paywalls? ,"For example, I want to read papers from IEEEE(eeeeeeeeeee....sorry I can't help it). But they're locked behind a paywall and $33 per paper for me to purchase since I don't have a university/alumni logon. 

I usually try to stick to open source/open access research for this reason but I'm on a really specific rabbit trail right now. Does anyone have any non-$$$$$ ideas for accessing research? ",44,28,1734585869.0,datascience
1hiefi0,"Google's reasoning LLM, Gemini2 Flash Thinking looks good",,0,2,1734678778.0,datascience
1hhqfds,stop script R but not shiny generation,"source ( script.R) in a shiny, I have a trycatch/stop in the script.R. the problem is the stop also prevent my shiny script to continue executing ( cuz I want to display error). how resolve this?
I have several trycatch in script.R",0,2,1734605794.0,datascience
1hgedpk,"a ""data scientist handbook"" for 2025 as a public Github repo","A while back, I created this public GitHub repo with links to resources (e.g. books, YouTube channels, communities, etc..) you can use to learn Data Science, navigate the markt and stay relevant.

Each category includes only 5 resources to ensure you get the most valuable ones without feeling overwhelmed by too many choices.

And I recently made updates in preparation for 2025 (including free resources to learn GenAI and SQL)

Here’s the link:

https://github.com/andresvourakis/data-scientist-handbook

Let me know if there’s anything else you’d like me to include (or make a PR). I’ll vet it and add it if its valuable.

I hope this helps 🙏",801,55,1734452807.0,datascience
1hguh6u,What's it like building models in the Fraud space? Is it a growing domain? ,"I'm interviewing for a Fraud DS role in a smaller bank that's in the F100. At each step of the process, they've mentioned that they're building a Fraud DS team and that there's a lot of opportunity in the space, but also that banks are being paralyzed by fraud losses.

I'm not too interested in classification models. But it pays more than what I currently make. I'm a little worried that there'll be a lot of compliance/MRM things compared to other industries - is that true?

Only reason why I'm hesitant is that I've been focusing on LLM work for a while and it doesn't seem like that's what the Fraud space does.

To sum it up:

1. Is there a ton of red tape/compliance/MRM work with Fraud models?
2. With an increase of Fraud losses every year, is this an area that'll be a hot commodity/good to get experience with?
3. Can you really do LLM work in this space? The VP I interviewed with said that the space was going to do GenAI in a few years, but when I asked him questions on what that meant to him, he had no clue but wanted to get into it
4. Is real-time data used to decline transactions instead of just detection?

EDIT: Definitely came to the conclusion that I want to apply to other banking companies. And that there's  a lot to learn in regards to 3 and 4. ",59,59,1734498672.0,datascience
1hh58jx,"Hiring Cybersecurity focused Data Science Experts - remote, part time",,7,10,1734539038.0,datascience
1hhm39j,I feel like I've peaked,,0,7,1734587171.0,datascience
1hgllx0,"Sales Forecasting for optimizing resource allocation (minimize waste, maximize sales)","Hi All,

To break up the monotony of ""muh job market bad"" (I sympathize don't worry), I wanted to get some input from people here about a problem we come across a lot where I work.  Curious what some advice would be.

So I work for a client that has lots of transactions of low value.  We have TONS of data going back more than a decade for the client and we've recenlty solved some major organizational challenges which means we can do some really interesting stuff with it.

They really want to improve their forecasting but one challenge I noted was that the data we would be training our algorithms on is affected by their attempts to control and optimize, which were often based on voodoo.  Their stock becomes waste pretty quickly if its not distributed properly.  So the data doesn't really reflect how much profit could have been made, because of the clients own attempts to optimize their profits.  Demand is being estimated poorly in other words so the actual sales are of questionable value for training if I were to just use mean squared error, median squared error, because just matching the dynamics of previous sales cycles does not actually optimize the problem.

I have a couple solutions to this and I want the communities opinion.

  
**1) Build a novel optimization algorithm that incorporates waste as a penalty.**    
I am wondering if this already exists somewhere, or 

  
**2) Smooth the data temporally enough and maximize on profit not sales.**

Rather than optimizing on sales daily, we could for instance predict week by week, this would be a more reasonable approach because stock has to be sent out on a particular day in anticipation of being sold.  

  
**3) Use reinforcement learning here, or generative adversarial networks.**

I was thinking of having a network trained to minimize waste, and another designed to maximize sales and have them ""compete"" in a game to find the best actions.  Minimizing waste would involve making it negative.

  
**4) Should I cluster the stores beforehand and train models to predict based on the subclusters, this could weed out bias in the data.**

I was considering that for store-level predictions it may be useful to have an unbiased sample.  This would mean training on data that has been down sampled or up-sampled to for certain outlet types 

  
**Lastly any advice on particular ML approaches would be helpful, was currently considering MAMBA for this as it seems to be fairly computationally efficient and highly accurate.  Explain ability is not really a concern for this task.**

**I look forward to your thoughts a criticism, please share resources (papers, videos, etc) that may be relevant.**

  
",16,28,1734471676.0,datascience
1hgozqm,Asking for help solving a work problem (population health industry),"Struggling with a problem at work. My company is a population health management company. Patients voluntarily enroll in the program through one of two channels. A variety of services and interventions are offered, including in-person specialist care, telehealth, drug prescribing, peer support, and housing assistance. Patients range from high-risk with complex medical and social needs, to lower risk with a specific social or medical need. Patient engagement varies greatly in terms of length, intensity, and type of interventions. Patients may interact with one or many care team staff members.

My goal is to identify what “works” to reduce major health outcomes (hospitalizations, drug overdoses, emergency dept visits, etc). I’m interested in identifying interventions and patient characteristics that tend to be linked with improved outcomes.

I have a sample of 1,000 patients who enrolled over a recent 6-month timeframe. For each patient, I have baseline risk scores (well-calibrated), interventions (binary), patient characteristics (demographics, diagnoses), prior healthcare utilization, care team members, and outcomes captured in the 6 months post-enrollment. Roughly 20-30% are generally considered high risk.

My current approach involves fitting a logistic regression model using baseline risk scores, enrollment channel, patient characteristics, and interventions as independent variables. My outcome is hospitalization (binary 0/1). I know that baseline risk and enrollment channel have significant influence on the outcome, so I’ve baked in many interaction terms involving these. My main effects and interaction effects are all over the map, showing little consistency and very few coefficients that indicate positive impact on risk reduction. 

I’m a bit outside of my comfort zone. Any suggestions on how to fine-tune my logistic regression model, or pursue a different approach?",5,6,1734481000.0,datascience
1hfxs76,Did working in data make you feel more relativistic?,"When I started working in data I feel like I viewed the world as something that could be explained, measured and predicted if you had enough data.

Now after some years I find myself seeing things a little bit different. You can tell different stories based on the same dataset, it just depends on how you look at it. Models can be accurate in different ways in the same context, depending on what you’re measuring.

Nowadays I find myself thinking that objectively is very hard, because most things are just very complex. Data is a tool that can be used in any amount of ways in the same context 

Does anyone else here feel the same?",312,96,1734394381.0,datascience
1hg1k3v,How do you stay up to date with new trends and advancements?,"Hi everyone! I'm getting my first big boy job soon (read: non internship) and one of my job duties is to stay updated in trends in data science and ML, especially with NLP and sentiment analysis in the social sciences

I'd like to do a good job with this and was wondering if anyone has recommendations for *how* to stay up to date. I will basically be the only technical person on my team so I'll need to be able to keep up with industry by myself without hand holding

Does anyone have any suggestions for keeping up to date with this sort of stuff? Besides following this sub and /r/MachineLearning ofc :p

Would love either blogs or journals with creative methodologies or usage of technology, both general DS stuff and places more focused on NLP. Thanks!",109,44,1734405900.0,datascience
1hgfl0t,exact line error trycatch,Is there a way to know line that caused error in trycatch? I have a long R script wrapped in trycatch,0,4,1734455876.0,datascience
1hfmope,Best ML certificate for undergrads to back up their profile?,"I’m an undergrad looking to strengthen my profile for ML internships/co-ops and overall career growth. I know some people might say certificates aren’t worth it, and yeah, I get it—experience and solid projects weigh more. But for those who think certs aren’t the best option, what would you suggest instead?

That said, I’m looking for something comprehensive and valued by employers. Between AWS ML Engineer Associate, ML Specialty, Databricks ML Associate/Professional, or Azure Data Scientist Associate, which one do you think is the most beneficial?

I’m not new to the field—just looking to expand my knowledge and improve my chances of landing a good ML co-op or internship. Any advice on where to learn ML more deeply or what certs actually help is much appreciated!",66,24,1734365802.0,datascience
1hf03dq,Data science is a luxury for almost all companies,"Let's face it, most of the data science project you work on only deliver small incremental improvements. Emphasis on the word ""most"", l don't mean all data science projects.
Increments of 3% - 7% are very common for data science projects.
I believe it's mostly useful for large companies who can benefit from those small increases, but small companies are better of with some very simple ""data science"". They are also better of investing in a website/software products which could create entire sources of income, rather than optimizing their current sources.
",846,211,1734290596.0,datascience
1hfjz1b,"Suggestion about Designing my Elective. Title: ""Text Analytics with LLM"" ","Hi Folks,
I'm a recent PhD graduate in Information Systems with a focus on using the current development in ML, NLP, NLU etc for business problems. I'm designing my first Text Analytics Elective for Management Scholars/Grad Students.

Objective is to given them some background and then help them focus on using the LLMs (open source ofcourse) to solve various type of problems.

I have already Includes 
- Vectorization : Comparing Text in Various Ways
- Concept & Design: Speed*, Coverage etc
- Building Scales: Measuring Emotion, Personality**, Nostalgia etc.


*Compare the Avg distance between consecutive embedding in a movie script or speech. Reference - https://psycnet.apa.org/record/2022-78257-001

**Scale Development with Little Data - https://journals.sagepub.com/doi/abs/10.1177/10944281231155771


It would be great if you guys can suggest some cool use of various text Analytics methods which are new (anything popular since 2020) or something you use often in solving business problems. Reference to a tool/paper would be great.

Would be glad to share the syllabus and resources when it's locked (Feb, 25')

",5,8,1734358398.0,datascience
1hfk7ah,Fine-tuning & synthetic data example: creating 9 fine tuned models from scratch in 18 minutes,"**TL;DR:** I built [Kiln](https://getkiln.ai), a new free tool that makes fine-tuning LLMs easy. In this example, I create 9 fine-tuned models (including Llama 3.x, Mixtral, and GPT-4o-mini) in just 18 minutes for less than $6 total cost. This is completely from scratch, and includes task definition, synthetic dataset generation, and model deployment.

The codebase is all on [GitHub](https://github.com/Kiln-AI/Kiln).

# Walkthrough

For the example I created 9 models in 18 minutes of work (not including waiting for training/data-gen). There's a walkthrough of each step in the [fine-tuning guide](https://github.com/Kiln-AI/Kiln/blob/main/guides/Fine%20Tuning%20LLM%20Models%20Guide.md), but the summary is:

* \[2 mins\]: Define task, goals, and schema
* \[9 mins\]: Synthetic data generation: create 920 high-quality examples using topic trees, large models, chain of thought, and interactive UI
* \[5 mins\]: dispatch 9 fine tuning jobs: Fireworks (Llama 3.2 1b/3b/11b, Llama 3.1 8b/70b, Mixtral 8x7b), OpenAI (GPT 4o-mini & 4o), and Unsloth (Llama 3.2 1b/3b)
* \[2 mins\]: deploy models and test they work

# Results

The result was small models that worked quite well, when the base models previously failed to produce the correct style and structure. The overall cost was less than $6 (excluding GPT 4o, which was $16, and probably wasn’t necessary). The smallest model (Llama 3.2 1B) is about 10x faster and 150x cheaper than the models we used during synthetic data generation. 

# Guide

I wrote a [detailed fine-tuning guide](https://github.com/Kiln-AI/Kiln/blob/main/guides/Fine%20Tuning%20LLM%20Models%20Guide.md), covering more details around deployment, running fully locally with Unsloth/Ollama, exporting to GGUF, data strategies, and next steps like evals.

# Feedback Please!

I’d love feedback on the tooling, UX and idea! And any suggestions for what to add next (RAG? More models? Images? Eval tools?). Feel free to DM if you have any questions.

I'm starting to work on the evals portion of the tool so if folks have requests I'm eager to hear it.

# Try it!

Kiln is 100% free, and the python library is MIT open source. You can [download Kiln here](https://github.com/Kiln-AI/Kiln/releases/latest)

",4,3,1734359045.0,datascience
1hf1180,What projects are you working on and what is the benefit of your efforts? ,"I would really like to hear what you guys are working on, challenges you’re facing and how your project is helping your company. Let’s hear it. ",84,97,1734293116.0,datascience
1hfzbit,I don’t understand AI hype. What am I missing?,"Edit 2: I need to try other models and practice my prompts. Thanks everyone!

Edit: I needed a script to parse a nested JSON file. I asked Chat GPT and it gave me a wrong answer. It only parsed the first layer. I asked a few more times and still no. I googled it and the first result from stack overflow was correct.


Not trolling. I've used ChatGPT about five times and was underwhelmed. What am I doing wrong?

1. Asked it for some simple code I couldn't remember. Nice but it only saved me about 10 minutes of googling.

2. Asked it for some moderately complex code and it didn't know the answer.

3. Asked it for some moderately complex code and the answer it gave was bad and wrong.

4. Asked it to generate an image and it was way off.

5. Asked it for some knowledge about an API and it just said the exact same thing as the official doc.


",0,38,1734398832.0,datascience
1hfbpe1,"Weekly Entering & Transitioning - Thread 16 Dec, 2024 - 23 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",4,43,1734325284.0,datascience
1hewiu9,What’s the point of testing machine learning model knowledge during interviews for non-research data science roles?,"I always make an effort to learn how a model works and how it differs from other similar models whenever I encounter a new model. So it felt natural to me that these topics were brought up in interviews.

However, someone recently asked me a question that I hadn’t given much thought to before: what’s the point of testing machine learning model knowledge during interviews for non-research data science roles?

Interview questions about model knowledge often include the following, especially if a candidate claims to have experience with these models:-

* what's the difference between bagging and boosting? 
* whether LightGBM uses leaf-wise splitting or level-wise splitting?
* what's the underlying assumptions of linear regression?

I learned these concepts because I’m genuinely interested in understanding how models work. But, coming back to the question: How important is it to have deep technical knowledge of machine learning models for someone who isn’t in a research position and primarily uses these tools to solve business problems?

From my experience, knowing how models learn from data has occasionally helped me identify issues during the model training process more quickly. But I couldn’t come up with a convincing argument to justify why it is fair to test this knowledge, other than “the candidate should know it if they are using it.”

What’s your experience with this topic? Do you think understanding the inner workings of machine learning models is critical enough to be tested during interviews?",37,30,1734281055.0,datascience
1hers8s,Visualization Process and Time Management,"At work I make many *exploratory* data visualizations that are fast, rough, and abundant. I want to develop a skill for *explanatory* visualizations that are polished, rich, and curated.

I've read a couple books on design principles and visualzation libraries (i.e. Seaborn and Matplotlib) and have some idea what I am after. But then I'll sit down to draft a paper with my outline and my hand-sketches, and I'll blow through my time budget just tweaking one of the charts!

I've learned a reliable process for writing, but I haven't mastered one for graphics. I'd love to hear what other people are doing. Some rudiments of a process:

- Start with cheap exploratory viz to find your story.
- Outline and revise your explanatory graphics by hand-- seems faster.
- Draft the ""data ink"" completely before tweaking aesthetics.
- Draft 80%-polished versions of graphs before the day you need them.
- Ruthlessly cut and consolidate graphics to the essentials.
- Forego graphics when narrative or tables are equally effective.
- Accept that a given chart typically takes X hours and plan accordingly.
- Practice, practice, practice so at least the tooling comes natural.
",33,9,1734266430.0,datascience
1hexjm9,Best domains for machine learning ?,"What are the best domains for expertise where I can use machine learning ? I don't want to use machine learning as it is I want a domain to use it, for eg: I have read about signal processing, healthcare, finance etc. 

 ",11,53,1734283772.0,datascience
1he2n61,Unexpectedly let go. Best ways to get a job fast?,"Hey all, 

I’m in Germany and was let go at the end of my probation period. 

I was ensured I would make it and actively made money for the company with proof. 

My reasons for termination were unclear and actually not inline with my responsibilities as a data scientist. 

Essentially, I was given peace of mind, and could ensure I needn’t worry. 

Whatever it may be, I’m now out of a job. That’s the way it goes sometimes. 

What are your tips for grabbing that next position fast? I’m not picky, I just want a job in my field, and with a team I enjoy - easier said than done. 

Any tips would be amazing! 

Happy holidays :) 

",95,56,1734182173.0,datascience
1he716d,Applying for Graduate Jobs in the UK.,"I recently graduated with an MSc in Artificial Intelligence in the UK and am currently looking for job opportunities. However, I often feel unsure about whether I’m approaching the job search process effectively. The journey can feel overwhelming and confusing at times, and I wonder if I’m targeting and applying for roles in the right way.

I am specifically targeting roles as a Machine Learning Engineer or Data Scientist. Could you share any proven strategies for job searching in the UK, particularly for these fields? Additionally, I’d like to know which months are crucial for job applications and when companies are most likely to hire graduates.",17,17,1734195181.0,datascience
1he6tyr,What are some things to consider if you wish to develop an experimentation platform? ,"Our company is quite small and we dont have a robust experimentation platform. Campaign measurement tasks are scattered all around the business with no unified set of standards. 6 different data scientists will bring you 6 different numbers of a lift measurement because nobody has a set way of doing things. 

A few of us are thinking of building out an experimentation platform to be a one stop shop for all things measurement. For those of you at places with mature experimentation culture, what kind of things should we consider? I’m a data scientist whose never worked as closely with engineers, but taking on this project is going to force me to do that, so I want to know more about an experimentation platform setup from that side as well. What has worked for you guys and what are things to recommend in building an experimentation platform?",7,6,1734194607.0,datascience
1hdd6yx,"0 based indexing vs 1 based indexing, preferences? ",,858,109,1734099552.0,datascience
1he7o50,plumber api or standalone app (.exe)?,I am thinking about a one click solution for my non coders team. We have one pc where they execute the code ( a shiny app). I can execute it with a command line. the .bat file didn t work we must have admin previleges for every execution. so I think of doing for them a standalone R app (.exe). or the plumber API. wich one is a better choice?,4,6,1734196963.0,datascience
1hdk59i,Help with clustering over time,"I'm dealing with a clustering over time issue.
Our company is a sort of PayPal. We are trying to implement an antifraud process to trigger alerts when a client makes excessive payments compared to its historical behavior.
To do so, I've come up with seven clustering features which are all 365-day-long moving averages of different KPIs (payment frequency, payment amount, etc.). So it goes without saying that, from one day to another, these indicators evolve very slowly. I have about 15k clients, several years of data.
I get rid of outliers (99-percentile of each date, basically) and put them in a cluster-0 by default.
Then, the idea is, for each date, to come up with 8 clusters. I've used a Gaussian Mixture clustering (GMM) but, weirdly enough, the clusters of my clients vary wildly from one day to another.
I have tried to plant the previous mean of my centroids, using the previous day centroid of a client to sort of seed the next day's clustering of a client, but the results still vary a lot. I've read a bit about DynamicC and it seemed like the way to address the issue, but it doesn't help.",9,36,1734118105.0,datascience
1hcw1o5,"Is it ethical to share examples of seed-hacking, p-hacking, test-set pruning, etc.?","I can't tell you the number of times I've been asked ""what random number seed should I use for my model"" and later discover that the questioner has grid searched it like a hyperparameter.

Or worse: grid searched the seed for the train/test split or CV folds that ""gives the best result"".

At best, the results are fragile and optimistically biased. At worst, they know what they're doing and it's intentional fraud. Especially when the project has real stakes/stakeholders.

I was chatting to a colleague about this last week and shared a few examples of ""random seed hacking"" and related ideas of test-set pruning, p-hacking, leader board hacking, train/test split ratio gaming, and so on.

He said I should write a tutorial or something, e.g. to educate managers/stakeholders/reviewers, etc. 

I put a few examples in a github repository (I called it ""[Machine Learning Mischief](https://github.com/Jason2Brownlee/MachineLearningMischief)"", because it feels naughty/playful) but now I'm thinking it reads more like a ""how-to-cheat instruction guide"" for students, rather than a ""how to spot garbage results"" for teachers/managers/etc.

What's the right answer here? 

Do I delete (make private) the repo or push it for wider consideration (e.g. expand as a handbook on how to spot rubbish ml/ds results)? Or perhaps no one cares because it's common knowledge and super obvious?",185,45,1734039253.0,datascience
1hcrjn2,How to Best Prepare for DS Python Interviews at FAANG/Big Companies?,"Have an interivew coming up where the focus will be on Stats, ML, and Modeling with Python at FAANG. I'm expecting that I need to know Pandas from front to back and basics of Python (Leetcode Easy). 

  
For those that have went through interviews like this, what was the structure and what types of questions do they usually ask in a live coding round for DS? What is the best way to prepare? What are we expected to know besides the fundamentals of Python and Stats?",171,43,1734027684.0,datascience
1hcy1cg,"How do you track your models while prototyping? Sharing Skore, your scikit-learn companion.","Hello everyone! 👋

In my work as a data scientist, I’ve often found it challenging to compare models and track them over time. This led me to contribute to a recent open-source library called [**Skore**](https://github.com/probabl-ai/skore), an initiative led by Probabl, a startup with a team comprising of many of the core scikit-learn maintainers.

Our goal is to help data scientists use scikit-learn more effectively, provide the necessary tooling to track metrics and models, and visualize them effectively. Right now, it mostly includes support for model validation. We plan to extend the features to more phases of the ML workflow, such as model analysis and selection.

I’m curious: how do you currently manage your workflow? More specifically, how do you track the evolution of metrics? Have you found something that worked well, or was missing?

If you’ve faced challenges like these, check out [the repo on GitHub](https://github.com/probabl-ai/skore) and give it a try. Also, please star our repo ⭐️ it really helps!

Looking forward to hearing your experiences and ideas—thanks for reading!",20,15,1734044644.0,datascience
1hcxv52,Masters in Applied Stats for an experienced analyst — good idea? Bad idea?,"I’m considering getting a master’s and would love to know what type of opportunities it would open up. I’ve been in the workforce for 12 years, including 5-7 years in growth marketing. 

Somewhere along the line, growth marketing became analyzing growth marketing and being the data/marketing tech guy at a series c company. I did the bootcamp thing. And now I’m a senior data analyst for a fortune 100 company. So: successfully went from marketing to analytics, but not data science.

I’m an expert in SQL, know tableau in and out, okay at Python, solid business presentation skills, and occasionally shoehorn a predictive model into a project. But yeah, it’s analytics.

But I’d like to work on harder, more interesting problems and, frankly, make more money as an IC. 

The master’s would go in depth on a lot of data science topics (multi variable regression, nlp, time series) and I could take comp sci classes as well. Possibly more in depth than I need.

Anyway, thoughts on what could arise from this?",17,42,1734044174.0,datascience
1hbtwbn,The Solitude of Data Science: Looking for a Kindred Spirits,"Hello!



I’ll try to keep this short because I’m terrible at being concise.



I came from a different world—operations and sales. It didn’t take long for me to realize that I wanted to move away from... well, salespeople. I applied for a dev job at my company and got rejected, but they saw potential in my knowledge and experience with machine learning, deep learning, and some other rogue projects I had been working on.



They asked if I could develop a proof of concept (POC) to present to our board of directors. The company had previously attempted to work with three external teams, but none of those efforts were successful. I presented the POC, and it went exceptionally well. We secured funding and created a junior data science position specifically for me. Previously, the company had no such role or anything similar. While the IT team is very strong, they haven’t had the capacity to handle initiatives like this.



Since then, I’ve been obsessed—reading everything I can and taking stats classes for a certificate program at MIT (with plans to continue my education). I’m pretty sure I’ve been driving my wife and friends crazy because I love talking about this stuff. I’m genuinely passionate about it!



That said, I still have so much to learn and need to overcome my imposter syndrome. On top of this fast-moving environment, I’ve never worked in IT before, never used Jira, or been involved in their overall processes, so I’m navigating that learning curve too. I’d love to connect with others here, hear your stories, and get more involved in this r/datascience community!         ",114,37,1733925068.0,datascience
1hcb1gc,Error rates /dirty data can cause sickness? ,"I do remember reading a long time ago that in production lines with high error-rates the motivation of labourers went down and the stress affected the workforce.

I wonder if dirty-data can have the same effect and has been researched as such. I know there are studies into error-rates in software, but that mixes software with data.

I wonder if specifically the stress caused by the unpredictability of the amount of work and the constant pressure dirty data causes has been studied as a health concern/risk.

Thanks.

Y.

edit: added the source [Unraveling Software Engineering Failures: Reasons and Fixes](https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/) [https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/](https://growprogramming.com/engineering-excellence-unraveling-the-reasons-behind-failures-in-software-engineering/) ",3,21,1733970805.0,datascience
1hc964o,CodeSignal companies,"Does anyone have a list of companies that have the codesignal data science assesement? 

Let's list the companies that did codesingal interviews so we can compile a list.

",4,12,1733965120.0,datascience
1hcuaog,Need help standard deviation,"
Hey guys I really need help I love statistics but I don’t know what the standard deviation is. I know I could probably google or chatgpt or open a basic book but I was hoping someone here could spoon feed me a series of statistics videos that are entertaining like Cocomelon or Bluey, something I can relate to.

Also I don’t really understand mean and how it is different from average, and a I’m nervous because I am in my first year of my masters in data science.

Thanks guys 🙏 ",0,18,1734034782.0,datascience
1hb7lcw,"I'm burnt out from constantly being on call where everything is on fire. Are there any good ""research"" or ""data collection"" or ""data interpretation"" roles that offer a more relaxed environment?","As a quick summary, I work as a Site Reliability Engineer and get paid pretty well (especially since I live in rural South Carolina and entirely remote). I juggle tasks like automating deployments, managing Kubernetes clusters in AWS, and scripting in Python and Bash, manage and analyze SQL databases, working with APIs, etc.  
  
**What I like**  
- I get paid well  & have skillsets that makes it more difficult for companies to replace you  
- I need to learn and stay up to date on a variety of technologies (I consider this a plus since you're never really 'out of date' on your role)     
- I enjoy makes graphs and gathering statistics/data to help our team  
- I enjoy interpreting that data to determine the root cause of an issue  
- In terms of scripting, I like making quick and dirty scripts that help my team automate something for us (this doesn't including writing large complicated scripts for other teams)   

    

**Why I hate it and want to leave**  
- The job, by its very nature, means everything is always urgent  
- On call, so a consistent 9-5 is not possible. You're often staying past your shift  
- Have to constantly work with devs and other parties to ensure their services or code gets fixed  
- Rarely any slow days, you're either automating a new large project or jumping on an urgent issue   
 
 
So based on the above, I'm curious if transitioning to a Data Science type role would offer a more laid-back environment, the question is I don't know what. Anyone made this switch or have insights? If not, can you recommend some jobs that I can look into? Preferably jobs that can utilize at least some of what I know.",169,57,1733852955.0,datascience
1hba8s2,Best cross-validation for imbalanced data?,"I'm working on a predictive model in the healthcare field for a relatively rare medical condition, about 5,000 cases in a dataset of 750,000 records, with 660 predictive features.

Given how imbalanced the outcome is, and the large number of variables, I was planning on doing a simple 50/50 train/test data split instead of 5 or 10-fold CV in order to compare the performance of different machine learning models.

Is that the best plan or are there better approaches? Thanks",76,48,1733859583.0,datascience
1hb2qbb,Hierarchical Time Series Forecasting,Anyone here done work for forecasting grouped time series? I checked out the hyndman book but looking for papers or other more technical resources to guide methodology. I’m curious about how you decided on the top down vs bottom up approach to reconciliation. I was originally building out a hierarchical model in STAN but wondering what others use in terms of software as well.,59,22,1733840218.0,datascience
1hc4971,get message markdow: execution ko or ok,"I am working with non developpers. I want them to enter parameters in markdown, execute a script then get the message at the end execution ok or ko on the knitted html ( they ll do it with command line)
I did error=T in the markdown so we ll alwyas get the document open. if I want to specify if execution ko or okay, I have to detect if theres at least a warning or error in my script? how to do that?",0,8,1733951613.0,datascience
1hc0ipg,Love this.,,0,0,1733942269.0,datascience
1haqdhq,Thoughts on the ethics of health insurance companies using Data Science to increase profits based on selective coverage ,"I want to have a good discussion on this topic since no one is talking about it outside of just the context of a CEO making decisions, but as a lot of us know, company decisions and strategy are driven by the suits(board) and the higher ups a lot of times, and that strategy is trickled down to the analysts and other groups forming projects to support the strategic initiative. I think not talking about this from a data science perspective is an ethics violation because we as practitioners can make the decision to not engage or pursue a project just because “I have a boss and they told me I need to because it aligns with our strategy.” I personally have quit a job in the past because the ethics of the CV models we were creating dawned on me and didn’t make me feel right. Sure I could validate it by saying I was only creating a small part of the software system, the reality is I knew the end goal and was actively participating in the development of a system that could be used for an ethically questionable use case. 

The possibility of UHCs actuarial science, analysts, and Data Scientists developing models to contribute to the strategy of increased profits and increased denials should be questioned. And I know “denial rates” aren’t apples to apples as back office rev cycle management people could wrongfully code a claim which can cause it to be denied. I’m talking more from a targeted perspective. Actuaries that work in insurance are very smart, but I want to get some insight about the specifics of what goes on from a health insurance perspective when they are denying a claim. 

I would love to hear perspectives from both sides, especially those who may have worked in the industry. ",266,122,1733794629.0,datascience
1hb0kqh,The pandas MemoryError,"I’ve been programming for data analysis for about 5 years, but I’ve never found an easy way to handle this. 

With my old beat up Dell Latitude, anything over ~100,000 rows if a sparse df tends to throw the dreaded Memory Error, specifically with functions like get dummies, indexing, merging, etc. 

My questions are:
1. Will a better laptop help with this?
2. Are there any modules or helper functions for this out there? 
3. How much does using colab help with this problem? Trying to avoid paying more.

TIA!


Edit: seems like most parallelizing options do not store the df in memory, and so can’t be used to visualize. That’s my main use case. So…
4. Anyone know of any visualization tools that work with large data? Currently using Plotly/Dash.",18,39,1733833468.0,datascience
1hb2048,Master Data science vs Quantitative Finance ,"Major data science vs Quantitative Finance

Hi, 
I am currently studying the bachelor Econometrics in The Netherlands and next year I will need to choose a master to pursue. My main doubt is, as you can see from the title, between data science (which is a bit outside my bachelor) and quantitative finance. 

On the one hand I may be a bit more interested in data science, but on the other hand I have the feeling that I will ‘throw away’ my Econometrics bachelor that is quite unique. From my point of view data science is followed by many people, also people from lower wage countries, while quantitative finance is a master that not many people follow. 

That’s why I’m curious what other people think about this, will I be going the wrong path if I choose data science which is pursued by many students overall, should I stick to the specific field of quantitative finance or will it not matter?",9,31,1733838131.0,datascience
1haneem,Is LeetCode or HackerRank actually worth it for ML/DS jobs?,"I’m an undergrad trying to break into Data Science/ML roles, and I’m not sure if spending time on LeetCode or HackerRank is really worth it. A lot of the problems feel more geared toward software dev interviews, and I’m wondering if that’s the best use of time for DS/ML jobs.

Wouldn’t working on projects or learning tools like TensorFlow or PyTorch be more valuable? Has anyone here actually benefited from doing LeetCode/HackerRank for DS/ML roles, or is it overhyped for this field?",108,57,1733786123.0,datascience
1ha78te,Thoughts? Please enlighten us with your thoughts on what this guy is saying. ,,911,197,1733742026.0,datascience
1hankc7,Real time predictions of custom models & aws,"I am someone who is trying to learn how to deploy machine learning models in real time. As of now the current pain points is that my team uses pmmls and java code to deploy models in production. The problem is that the team develops the code in python then rewrites it in java. I think its a lot of extra work and can get out of hand very quickly. 


My proposal is to try to make a docker container and then try to figure out how to deploy the scoring model with the python code for feature engineering.

We do have a java application that actually decisions on the models and want our solutions to be fast.

Where can i learn more about how to deploy this and what type of format do i need to deploy my models? I heard that json is better for security reasons but i am not sure how flexible it is as pmmls are pretty hard to work with when it comes to running the transformation from python pickle to pmmls for very niche modules/custom transformers. 

If someone can help explain exactly the workflow that would be very helpful. This is all going to use aws at the end to decision on it.",11,1,1733786574.0,datascience
1haeip6,How do you keep up with all the tools?,Plenty of tools are popping on a regular basis. How do you do to keep up with them? Do you test them all the time? do you have a specific team/person/part of your time dedicated to this? Do you listen to podcasts or watch specific youtube chanels?,35,16,1733763907.0,datascience
1haf05c,Customer Life Time Value Applications,"At work I’m developing models to estimate customer lifetime value for a subscription or one-off product. It actually works pretty well. Now, I have found plenty of information on the modeling itself, but not much on how businesses apply these insights. 

The models essentially say, “If nothing changes, here’s what your customers are worth.” I’d love to find examples or resources showing how companies actually use LTV predictions in production and how they turn the results into actionable value. Do you target different deciles of LTV with different campaigns? do you just use it for analytics purposes? ",29,19,1733765096.0,datascience
1hakhka,SUMO/VISSIM for traffic condition simulation,"Hi team!

As I have no experience with AI and predictive models for trafic management, I’m not sure how to simulate current traffic conditions in an urban city (or portion of it) without VS with implementation of IoT and AI.

Any good resources or advice?

Also, if anyone with first hand experience is interested, I would love to have a quick interview discussion, 15-20mins max, for qualitative analysis :)",5,2,1733778588.0,datascience
1haig7e,How can a webdev help DS?,"Hello y'all. My expertise is between DS and full stack dev, but usually its been one or the other. 

What would your ideas be on how I can leverage my webdev skills to collaborate with other DSs in my team?

Context is supply chain, and there's some reasonable freedom to initiate projects",4,6,1733773519.0,datascience
1hacmoc,entering parameters+executing R without accessing R,"I am preparing a script for my team  (shiny or rmarkdown) where they have to enter some parameters then execute it ( and have maybe executions steps shown). I don t want them to open R or access the script.
1) How can I do that?
2) is it dangerous security wise with a markdown knit to html? and with shiny is it safe? I don t know exactly what happens with the online, server thing?
3) is it okay to have a password passed in the parameters, I know about the Rprofile, but what are the risks?
thanks",5,4,1733759138.0,datascience
1han2ml,Low classification accuracy,"Hello
And when i do regression it gives me zero, whoever could help please contact me it’s so urgent 
",1,1,1733785231.0,datascience
1h9v7pe,Is your org treating the rollout of LLMs as an IT or data science problem? ,"Our org has given all resource (and limited all API access) to LLMs to a dedicated team in the IT department, which has no prior data experience. So far no data scientist has been engaged for feedback on design or practicality of use-cases. I'm wondering is this standard in other orgs? ",77,32,1733698440.0,datascience
1h9m492,Are certifications even worth it these days?,"So, I’m a cs major stats minor undergrad, and I’ve done a couple of certifications—AWS Cloud Practitioner and IBM Data Science. Honestly, I’m not sure if they added much value. In one interview, I mentioned my certifications right at the end, and they didn’t even seem to notice.

From what I’ve seen, well-defined projects seem to carry more weight than a cert. Projects show real skills, while certs sometimes feel like just ticking a box.

What’s your take? Are there any certs you’ve done that actually helped you stand out, or do you think the focus should shift more toward solid project work?

Also, which one is more valuable or more worth it, AWS, Azure, GCP or Databricks for Data Science/ML??",152,64,1733674230.0,datascience
1ha28aj,"Weekly Entering & Transitioning - Thread 09 Dec, 2024 - 16 Dec, 2024"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,67,1733720481.0,datascience
1h9o1r5,Timeseries pattern detection problem,"I've never dealt with any time series data - please help me understand if I'm reinventing the wheel or on the right track.

I'm building a little hobby app, which is a habit tracker of sorts. The idea is that it lets the user record things they've done, on a daily basis, like ""brush teeth"", ""walk the dog"", ""go for a run"", ""meet with friends"" etc, and then tracks the frequency of those and helps do certain things more or less often.

Now I want to add a feature that would suggest some cadence for each individual habit based on past data - e.g. ""2 times a day"", ""once a week"", ""every Tuesday and Thursday"", ""once a month"", etc.

My first thought here is to create some number of parametrized ""templates"" and then infer parameters and rank them via MLE, and suggest the top one(s).

Is this how that's commonly done? Is there a standard name for this, or even some standard method/implementation I could use?",14,5,1733679369.0,datascience
1h9f8hr,How to find freelance opportunities - what is the most typical troupe of project you do as freelance ,"Hi all,

I have 5+ years of experience. I’m based in Europe

Lately I’m thinking switch from full time employee to contractor, doing freelancing and working for different companies at the same time.

I think that freelancing for data scientists is harder than freelancing for software developers. I imagine a front end developer can easily get a project to build form scratch a website, or add a functionality to the existent one. Data scientists instead need already data and infrastructure to perform their job.


- How do data scientists find freelance jobs, I’m based in Europe which platform/website do you use?
- What is the most typical project you worked on?
- How is the market now, is there a good demand? ",30,12,1733650061.0,datascience
1h8xo0m,Is the data job market as badly affected as software engineering?,Everyone knows the market is bad right now for software engineers. Probably as bad as it's every been. What is the consensus on the job market for data professionals right now?,270,113,1733593122.0,datascience
1h8dlz9,Classification threshold cost optimisation,"Say you’ve selected the best classifier for a particular problem, using threshold invariant metrics such as AUROC, Brier score, or log loss. 

It’s now time to choose the classification threshold. This will clearly depend on the use case and the cost/ benefits associated with true positives, false positives, etc. 

Often I see people advising to choose a threshold by looking at metrics such precision and recall. 

What I don’t see very often is people explicitly defining relative (or absolute, if possible) costs/ benefits of each cell in the confusion matrix (or more precisely the action that will be taken as a result). For example a true positive is worth $1000, a false positive -$500 and the other cells $0. 

You then optimise the threshold based on maximum benefit using a cost-threshold curve. The precision and recall can also be reported, but they are secondary to the benefit optimisation and not used directly in the choice. I find this much more intuitive and is my go-to.

Does anyone else regularly use this approach? In what situations might this approach not make sense?",29,25,1733524141.0,datascience
1h8j6tq,Llama3.3 free API,,9,4,1733540941.0,datascience
1h87a9m,Meta released Llama3.3,,25,1,1733507631.0,datascience
1h81878,Deploying Niche R Bayesian Stats Packages into Production Software,"Hoping to see if I can find any recommendations or suggestions into deploying R alongside other code (probably JavaScript) for commercial software. 

Hard to give away specifics as it is an extremely niche industry and I will dox myself immediately, but we need to use a Bayesian package that has primary been developed in R.   
  
Issue is, from my perspective, the package is poorly developed. No unit tests. poor/non-existent documentation, plus practically impossible to understand unless you have a PhD in Statistics along with a deep understanding of the niche industry I am in. Also, the values provided have to be ""correct""... lawyers await us if not...  
  
While I am okay with statistics / maths, I am not at the level of the people that created this package, nor do I know anyone that would be in my immediate circle. The tested JAGS and untested STAN models are freely provided along with their papers.

It is either I refactor the R package myself to allow for easier documentation / unit testing / maintainability, or I recreate it in Python (I am more confident with Python), or just utilise the package as is and pray to Thomas Bays for (probable) luck.

Any feedback would be appreciated. ",38,18,1733491339.0,datascience
1h7j7ry,"The ""method chaining"" is the best way to write Pandas code that is clear to design, read, maintain and debug: here is a CheatSheet from my practical experience after more than one year of using it for all my projects",,251,42,1733431204.0,datascience
1i4oujz,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",1,17,1737256529.0,MachineLearning
1hq5o1z,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",38,12,1735615814.0,MachineLearning
1i7nvix,[D] Comment on CVPR reviews and ICLR decisions.,"Hey everyone,  
We all know how reviews and decisions can be controversial, and I’m sure many of you are feeling disappointed with the results (My rating from CVPR is all 2 😅). But remember, it’s not the end of the world!

Rejection doesn’t mean you’re at fault—it’s often just bad luck (though, of course, we should always strive to improve our work).

Take a break—grab some chicken and beers, get a good night’s sleep, and gear up to submit your work to another venue. You’ve got this! 💪",24,1,1737584777.0,MachineLearning
1i7v6q9,"Building and Testing an AI pipeline using Open AI, Firecrawl and Athina AI [P]","While building a production-grade LLM application, it is critical to test your AI pipeline on a dataset specific to your use case/domain.

It takes a lot of iterations across multiple combinations of prompts, models, retrievals, and other advanced techniques.

Here's a step-by-step breakdown of how a large healthcare company built its AI-powered copilot for medical practitioners.

It covers how to set up the multi-step AI pipeline and evaluate it using custom evaluations.

Link to the entire pipeline and blog in the comments 👇

https://preview.redd.it/9dwt18xe6oee1.png?width=2940&format=png&auto=webp&s=c7bac5a079e09a6604ce07205df3424ccce58f23

",7,1,1737605889.0,MachineLearning
1i7g04y,[R] Learning to Continually Learn with the Bayesian Principle,,36,1,1737565525.0,MachineLearning
1i7dqlh,[D] CVPR 2025 Reviews,"Reviews should be out in less than 24 hours (Jan 23 '25 01:59 AM CST).

Good luck everyone.",44,108,1737559916.0,MachineLearning
1i7x4dw,[D] Machine Learning Engineer Skills 2025,"Hi

I would like to know what skills any MLE should have in 2025 ? Lets collect  exhaustive list here. One may not have all skills but if we create exhaustive list, it helps to upskill someone.

\- Fine Tuning

\- Prompt Engineering

\- Should be able to deploy any models with real time or batch inferencing

\- Training Large language model from Scratch

\- Distributed training of models

\-GPU (CUDA/Triton) code optimisation  
\-Problem Solving (DS/algo)  
\-Should be able to interact with DB/vectorDB etc",2,1,1737613055.0,MachineLearning
1i6zh6p,[D]: A 3blue1brown Video that Explains Attention Mechanism in Detail,"- YouTube [video](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [subtitle](https://downsub.com/?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DeMlx5fFNoYc)

  
# Timestamps

  
02:21 : token embedding

02:33 : in the embedding space \
there are multiple distinct directions for a word \
encoding the multiple distinct meanings for the word.

02:40 : a well-trained attention block \
calculates what you need to add to the generic embedding \
to move it to one of these specific directions, \
as a function of the context. \

07:55 : Conceptually think of the Ks as potentially answering the Qs.

11:22 : ( did not understand )


",329,13,1737509923.0,MachineLearning
1i7i1d4,[D] ICLR 2025 Decisions,The decisions seem to have appeared. What are your thoughts?,7,21,1737570469.0,MachineLearning
1i7wcrg,[D] Where can I find the best Machine Translation (MT) models?,Specifically looking for encoder-decoder models but machine translation models in general work. ,0,1,1737610044.0,MachineLearning
1i7l666,Have You Used AI Tools for Your Research? Which Ones Are Your Favorite and Why?,"Over a decade ago, I wrote two articles: *""A B\*ginner’s Guide to Computer Science Research""* and *""How to Start a Research Work in Computer Science""*. These articles were widely used in universities worldwide to help students and early-career researchers navigate academic research in Computer Science (CS).

Fast forward to 2025, the research landscape has evolved significantly, especially in AI and CS, with the advent of AI-powered research tools, open-access repositories, and real-time collaboration platforms. These tools have made research more accessible, enabling students and professionals to work more efficiently while focusing on real innovation.

I recently published an updated article in **The Times of India**, presenting an *Eight-Step Approach to Research* framework designed for modern AI and CS research. This framework integrates **AI-powered literature review tools, reference management systems, open science platforms, and collaborative research methods** to enhance the research workflow.

🚀 **Would love to hear from the ML research community:**

  
1️⃣ Have you used **any AI-powered tools** or **automation techniques** in your research? Which ones do you find most useful?  
2️⃣ Do you have recommendations for **other AI tools** that weren’t covered in the article but could benefit researchers?  
3️⃣ How do you think AI will shape **the future of academic research and discovery?**

📖 **Read the article here:** [How to Start Research in Computer Science & AI in 2025 – An Updated Framework](https://timesofindia.indiatimes.com/blogs/a-window-to-the-tech-world/how-to-start-a-research-work-in-computer-science-and-ai-in-2025-an-updated-framework/)

[Block Diagram of “Eight-Step Approach to Research” in 2025](https://preview.redd.it/a0a4s8ftulee1.jpg?width=3200&format=pjpg&auto=webp&s=30e6ec7cc3e3d7f4775f8543005e4ba797b76888)

Let’s discuss! What are your go-to tools for making research more efficient in 2025?",6,11,1737577968.0,MachineLearning
1i7viki,[D] Turning an ML inference into an Inference server/pipeline,"This might be a noob and stupid question, so I apologize in advance. But is there a well known python based framework or library that one could refer to, to learn how to take an inference based setup (ex inference.py) and turn it into a server application that can accept requests?",0,2,1737607010.0,MachineLearning
1i7i9kd,[D] CVPR 2025 Reviews are out!! How did it go?,"I got a Reject (1), Borderline (3) and Accept (5), with confidence (3,3,4)! Quite stochastic I'd say!! But the Reject reviewer is not quite bad actually. ",4,2,1737571011.0,MachineLearning
1i7tmfs,[D] Encoding mix symbols + numeric tokens,"I find myself often thinking about tokenizers for problems where the input space is a vast array of numbers associated to symbols.

A good example here would be proteomics data, where one might often have dozens of thousands to millions of types of proteins - and a quantity associated with each.

There are a lot of clever ways to feed this sort of data into a model, but what I care about is more so training a w2v style tokenizer that is able to look for similarities between semantic and numeric pairs.

  
An example:

Protein-A:50k-500k -- only occurs happens when Protein-B:20k-40k (but, not always)

Protein-A: 0-50k -- seems uncorrelated with Protein-B in any numeric range

So ideally I'd want a tokenizer that, when I apply, say, cosine similarity (or some other function, I don't mind if the tokens make sense only with more complex distance functions):

Pa(65k) sim Pb(25k) = high

Pa(20k) sim Pb(45k) = low

Pa(80k) sim Pb(5k) = low

\---

  
That might be a bit too dumb, but I'm trying to be very explicit about the intent here.

  
I tried looking for papers on this topic but I come up mainly blank, like e.g.

[https://www.jaypujara.org/pubs/2021/thawani-naacl21/thawani-naacl21.pdf](https://www.jaypujara.org/pubs/2021/thawani-naacl21/thawani-naacl21.pdf)

[https://aclanthology.org/2023.findings-emnlp.662.pdf](https://aclanthology.org/2023.findings-emnlp.662.pdf)

[https://arxiv.org/pdf/2411.0208](https://arxiv.org/pdf/2411.0208)

Are not == 0 relevance but are, like, np.isclose(relevance, 0)

Since most of what they think about is representing numbers as tokens in the case of LLMs, and my concern is more with a problem space where every symbol will have a number associated with them.",1,0,1737600905.0,MachineLearning
1i7ovl0,"[D][P] How are you handling ""memory"" and personalization in your end-user AI apps?","With apps like **ChatGPT** and **Gemini** supporting ""memory"" and frameworks like **mem0** offering customizable memory layers, how are folks approaching personalization in your own apps? As foundational AI models become more standardized, the *context* and *UX layers* built on top (like user-specific memory, preferences, or behavioral data) seem critical for differentiation. 

RAG itself is in some ways personalizing the response for you, but other than ChatGPT, I don't think I have come across any other AI apps that actually handle memory or personalization well. i.e., I can't just ask them to tell me about what they know about me based on past interactions. ",2,0,1737587333.0,MachineLearning
1i7afnz,[D] A day in the life of fraud detection in production,"Hello. Could someone here who works with fraud models in production answer a few questions, please?

\- How would you describe a typical day of work?

\- You do mostly transactional models?

\- How often do you retrain your models?

\- how often do you retrain the same model? What is the average time window of your training base? And when do you know you need to train a new model? What changes between them? Audience, variables? (Because they are much more frequent than in credit).

\- how do you test the performance of a new model in production?

\- how much do you need to know about MLEng/architecture/infrastructure?

For example, say I work as a data scientist for a fintech (I actually do). They already have fraud detection models running. When should I retrain the current model and when should I retrain a new one?

I am exploring new opportunities in the area, but I find difficult to collect information on actual experiences of work.

Thank you very much.",10,4,1737550406.0,MachineLearning
1i74pni,[D] A little late but interesting talk by Fei-Fei Li at NeurIPS 2024,"Great talk by Fei-Fei Li on Visual Intelligence and what the future holds for AI. Wanted to share it here in case anyone wants to check it out on their [website](https://neurips.cc/virtual/2024/invited-talk/101127). 


TL;DR: At NeurIPS 2024, Fei-Fei Li delivered a talk titled ""From Seeing to Doing: Ascending the Ladder of Visual Intelligence,"" where she emphasized the progression from visual perception to actionable intelligence in Al systems. She highlighted the limitations of current Al, particularly its reliance on two-dimensional data, and advocated for the development of ""spatial intelligence"" to enable machines to comprehend and interact with the three-dimensional physical world. Li discussed her work at World Labs, focusing on creating Al models that understand 3D environments, which is crucial for applications in robotics, autonomous vehicles, and augmented reality. She concluded by expressing optimism that Al advancements will augment human capabilities rather than replace them.",32,4,1737526338.0,MachineLearning
1i7i2zl,[R] Any good resources for search algorithms?,What methods are ypu using for search algorithms? If you know good repositories / sources I'd be more than happy! Thanks,3,0,1737570578.0,MachineLearning
1i7i32i,From Deep Blue to AlphaZero: Exploring the Legacy of AI in Chess [P],"Hi All,

I’ve always been fascinated by the story of Deep Blue, IBM’s legendary chess computer, and its iconic matches against Garry Kasparov in the 90s. The intersection of chess and technology is a story that resonates deeply with me, and I wanted to create something that captures that magic for others.

I’ve put together a google doc that collects and organizes some of the best long-form resources on the topic. It’s designed to serve as a comprehensive guide for anyone interested in exploring this moment in artificial intelligence history.

If this way of exploring the Deep Blue story resonates with you, I’d love to hear your thoughts in the comments.

Thank you for taking the time to read this post. Cheers!

Link to the Google doc: https://docs.google.com/spreadsheets/d/1bZGQWR7zBPAyGVPlw6tu37FYF60w33m6gRsSlPNT5u0/edit?usp=sharing",2,0,1737570584.0,MachineLearning
1i7ahbb,"[P] Built a free API wrapper for ML models at our lab - deploy sklearn/pytorch models with just Python code, no devops needed","Hey everyone,

We built **Jaqpot** at our research lab because we needed a simple way to deploy machine learning models through APIs without dealing with the deployment headaches. Basically, you train your model (sklearn, pytorch geometric), and with a few lines of code it gets wrapped in an API you can call from your applications or visit on our dashboard.

Example in Python:

    from jaqpotpy.models import SklearnModel
    from jaqpotpy import Jaqpot
    
    # Train your model as usual
    model = SklearnModel(dataset=dataset, model=LogisticRegression())
    model.fit()
    
    # Deploy it
    jaqpot = Jaqpot()
    jaqpot.login()
    model.deploy_on_jaqpot(
        name=""My Model"",
        description=""Simple classifier""
    )

Then you can call it from Python:

    prediction = jaqpot.predict_sync(model_id=model_id, dataset=input_data)

or Java:

    Dataset prediction = jaqpotApiClient.predictSync(modelId, inputData);

Some key points:

* Free to use (research project funded through our lab)
* Supports scikit-learn models and pytorch geometric
* Python and Java/Kotlin SDKs available
* Handles all the ONNX conversion and API setup
* Private or public model deployment options
* Includes specialized features for predictive modeling in chemistry and materials science

We're making this available to researchers and developers while we have funding to run the infrastructure. We want to see how people actually use it and what features they need. Long term we'll probably need usage tiers to keep the servers running, but for now it's completely free and open-source.

Check out our [documentation](https://jaqpot.org/docs) or try it at [app.jaqpot.org](http://app.jaqpot.org) if you're interested.

Happy to answer any questions!

https://preview.redd.it/685xnheuljee1.png?width=1582&format=png&auto=webp&s=957a61576ca14a52cee33a1877331c61bb214905

https://preview.redd.it/tho37heuljee1.png?width=1289&format=png&auto=webp&s=4704e95d03e70c8f02c01d228a445c96f0f81c37

Edit: Since people are asking about the tech stack - it uses ONNX for model conversion and runs on AWS. Models are versioned and you can track usage through the web interface.",4,0,1737550562.0,MachineLearning
1i7wyur,[D] Machine Learning PhD intern salary at a startup in North Carolina,">[Machine Learning PhD intern salary at a startup in North Carolina](https://www.reddit.com/r/PhD/comments/1i7wiz4/machine_learning_phd_intern_salary_at_a_startup/)  
by[u/InstinctsInFlow](https://www.reddit.com/user/InstinctsInFlow/) in[PhD](https://www.reddit.com/r/PhD/)",0,0,1737612442.0,MachineLearning
1i7doq7,[R] Learning Complex Knowledge from Raw Video: VideoWorld's Success in Go and Robotic Control,"This paper presents an approach for learning world knowledge directly from unlabeled video data through a self-supervised framework called VideoWorld. The core technical contribution is a multi-stage architecture that processes videos to extract both visual and temporal relationships without requiring manual annotation.

Key technical points:
- Uses contrastive learning between video segments to capture temporal dynamics
- Implements cross-modal alignment between visual and motion features
- Employs temporal consistency learning to understand event sequences
- Introduces a hierarchical attention mechanism for long-range dependencies

Results demonstrate improvement over existing methods:
- 12% increase in video QA performance on HowTo100M
- 15% better temporal relationship understanding on ActivityNet
- 8% improvement in next-frame prediction tasks
- Effective zero-shot transfer to unseen video domains

I think this approach could significantly change how we train video understanding models. By removing the need for expensive manual labeling, we could potentially train on much larger and more diverse video datasets. I'm particularly interested in how this could improve robotic learning systems that need to understand physical interactions and causality.

The results suggest good potential for real-world applications, though I think there are still important challenges around computational efficiency and handling of abstract concepts that need to be addressed.

TLDR: New self-supervised framework learns world knowledge from unlabeled videos, showing strong improvements on video understanding tasks without requiring manual annotations.

[Full summary is here](https://aimodels.fyi/papers/arxiv/videoworld-exploring-knowledge-learning-from-unlabeled-videos). Paper [here](https://arxiv.org/abs/2501.09781).",2,0,1737559790.0,MachineLearning
1i73gxr,[D]: Andrej Karpathy lecture: Building makemore Part 2: MLP,"Youtube [video](https://www.youtube.com/watch?v=TCH_1BHY58I)


# Timestamps

00:01:38 : 3 character context ( 27*27*27 = 19683 ) . Too much possibilities. \
Introduce Multi-Layer Perception model.

00:02:09 - 00:09:00 : [00-02-03-bengio-2003-paper.md](https://github.com/xuyuji9000/machine-learning-playground/blob/ffee4b430c9ae40a1aecdb1aee9af83b9e60901c/10-learning-materials/01-videos/01-andrej-karpathy/02-Building%20makemore%20Part%202%3A%20MLP/00-02-03-bengio-2003-paper.md)
",12,0,1737521788.0,MachineLearning
1i7fdf9,[D] Suggestion for image embedding model fine-tuning,"I am trying to build a image search app, like how Google's image search works. I am considering SigLip for this task (maybe Jina CLIP if this succeeds). It's a domain specific data, and I have the image, description and a label for classification. How do I fine-tune the model? What should be the approach here?",0,1,1737564010.0,MachineLearning
1i727qm,[R] Future-Guided Learning: A Predictive Approach To Enhance Time-Series Forecasting,"Hello everybody! My name is Skye and I am the first author of this work! This paper demonstrates that forecasting and event prediction can be enhanced by taking inspiration from the brain, specifically predictive coding theory. I am posting the abstract, code, and arXiv link for anybody curious! Please feel free to leave any comments below, as this is my first full-length paper and I would appreciate any feedback!

**Abstract:** Accurate time-series forecasting is crucial in various scientific and industrial domains, yet deep learning models often struggle to capture long-term dependencies and adapt to data distribution drifts over time. We introduce Future-Guided Learning, an approach that enhances time-series event forecasting through a dynamic feedback mechanism inspired by predictive coding. Our method involves two models: a detection model that analyzes future data to identify critical events and a forecasting model that predicts these events based on current data. When discrepancies occur between the forecasting and detection models, a more significant update is applied to the forecasting model, effectively minimizing surprise and adapting to shifts in the data distribution by aligning its predictions with actual future outcomes. This feedback loop allows the forecasting model to dynamically adjust its parameters, focusing on persistent features despite changes in the data. We validate our approach on a variety of tasks, demonstrating a 44.8% increase in AUC-ROC for seizure prediction using EEG data, and a 48.7% reduction in MSE for forecasting in nonlinear dynamical systems. By incorporating a predictive feedback mechanism adaptable to data drift, Future-Guided Learning advances how deep learning is applied to time-series forecasting. 

**Our code is publicly available at:** https://github.com/SkyeGunasekaran/FutureGuidedLearning.

**arXiv:** [https://arxiv.org/pdf/2410.15217](https://arxiv.org/pdf/2410.15217)",7,3,1737517691.0,MachineLearning
1i7p4mx,[D] Can someone explain this value embeddings technique?,"Title.

Hi folks I am an MLE at a certain big social media company. Saw an interesting tweet that wrote something about how they trained an LLM faster than GPT2. 

I work a lot on sequence modeling and transformers so it got me curious but I am quite confused.

This is what the guy wrote

“Changes: Multihead Latent Attention, and value embeddings on only the first + last 3 layers (instead of all layers), plus various perf optimizations.”

What is this value embeddings on first last 3 layers mean? Quite confused? Some kind of pooling operation?? Value matrix don’t multiply in the middle?? What…",0,3,1737588010.0,MachineLearning
1i6lgoo,[D] AISTATS 2025 Paper Acceptance Result,AISTATS 2025 paper acceptance results are supposed to be released today. Creating a discussion thread for this year's results.,45,61,1737474232.0,MachineLearning
1i6os2n,[R] Multivariate Time Series Prediction with Transformers,"I am working on a model that I want to be able to take in a multivariate time series of weather and river height data, and output a series of predictions for one of the river gauge heights (Essentially, I feed in timesteps 20-40 and expect to receive timesteps 41-61). I have previously been using an LSTM for this, but I got pretty subpar results with several different architectures. I'm now looking at using a transformer encoder network, and I have this recurring issue I can't seem to figure out.

For almost any context length, model size, positional encoding, training time, etc.; the model seems to be incapable of distinguishing between timesteps on the outputs. It always learns to predict a good average for the gauge height across the timesteps, but there's no variation in its outputs. On an example case where the target gauge height is \[0.2, 0.3, 0.7, 0.8, 0.6\] it would output something like \[0.4, 0.45, 0.4, 0.45, 0.5\].

In fact, the model performs almost exactly the same without any positional encoding at all.

Here's an example of what an output might look like from several continuous tests:

[Several prediction lines, showing a similar trend regardless of actual position on the graph.](https://preview.redd.it/ukhktk9nxdee1.png?width=1252&format=png&auto=webp&s=fbc276b432ff28dc104c24afc9da1d2829d82cd9)

I have tried both relative positional encoding and absolute positional encoding and adjusting the loss function to add a term that focuses on the slope between timesteps, but I can't seem to enforce differentiation between timesteps.

The extra loss term:

    class TemporalDeregularization(nn.Module):
        def __init__(self, epsilon):     
            super().__init__() 
            self.epsilon = epsilon 
            self.mse = nn.MSELoss()
    
        def forward(self, yPred, yTrue):
            predDiff = yPred[:, 1:] - yPred[:, :-1]
            targetDiff = yTrue[:, 1:] - yTrue[:, :-1]
            return self.epsilon * self.mse(predDiff, targetDiff)

My positional encoding scheme:

    class PositionalEncoding(nn.Module):
        def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first=False):
            super().__init__()
            self.batch_first = batch_first
            self.dropout = nn.Dropout(p=dropout)
    
            position = torch.arange(max_len).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
            pe = torch.zeros(max_len, 1, d_model)
            pe[:, 0, 0::2] = torch.sin(position * div_term)
            pe[:, 0, 1::2] = torch.cos(position * div_term)
            self.register_buffer('pe', pe)
    
        def forward(self, x: Tensor) -> Tensor:
            if self.batch_first:
                x = x + self.pe[:x.size(1)].permute(1, 0, 2)
            else:
                x = x + self.pe[:x.size(0)]
            return self.dropout(x)

Here's a diagram of my architecture that's more explicit:

[Image containing transformer network architecture, including a linear projection, positional encoding, transformer encoder, and another projection in series.](https://preview.redd.it/emh3s8ozvdee1.png?width=521&format=png&auto=webp&s=ba586df018bdfa653747e98bf03d88a871e0a9ce)

I understand that this isn't exactly a common use case or architecture for this use case, but I'm not sure why the model isn't capable of making the distinction between timesteps. I've considered adding a bidirectional LSTM before the final projection to force time differentiation.

For reference, I have found that this model performs well with a dModel of 64, feedForward of 128, 6 layers, and 8 heads. The other term in the loss function is a standard MSE. Also, I don't apply masking as all of the inputs should be used to calculate the outputs in my case.

I can't post much code as this is related to my job, but I would like to learn more about what is wrong with my approach.

Any help or advice is appreciated, I'm getting my master's currently but I have yet to encounter any machine learning classes despite years of work experience with it, so I may just be missing something. (Also sorry for the dog ass Google drawings)",19,13,1737482509.0,MachineLearning
1i6p1ja,Apple AIML Residency Program 2025 [R],"Hello!

Has anyone participated in Apple's AIML residency in the past and is willing to share their experience?

I'm mostly curious about the interview process, the program itself (was it tough? fun?), also future opportunities within Apple as a permanent employee. Thanks in advance!",14,6,1737483137.0,MachineLearning
1i6s0bb,[R] Language Model Mind Evolution: An Evolutionary Search Strategy for Scaling LLM Inference,"A really interesting technical advancement in using evolutionary algorithms to enhance LLM reasoning capabilities. The core methodology combines genetic algorithms with LLM outputs to evolve better reasoning patterns.

Key technical points:
- Implements genetic algorithm framework operating on LLM solution attempts
- Uses specialized evaluator models to assess reasoning quality and guide evolution
- Performs crossover and mutation operations on successful reasoning patterns
- Iteratively optimizes solutions across generations focusing on correctness and depth

Results from their experiments:
- 15-20% improvement in reasoning accuracy on test cases
- Enhanced step-by-step solution generation
- Reduced logical gaps and errors in complex reasoning tasks
- Maintained performance improvements across different reasoning domains

I think this approach could be particularly valuable for improving LLM performance on structured reasoning tasks like mathematical proofs and logical deductions. The evolutionary optimization framework provides a systematic way to discover and refine better reasoning patterns.

I think the computational costs will need to be addressed before widespread adoption, but the methodology shows promise for automated improvement of AI capabilities. The ability to evolve more sophisticated reasoning strategies could help develop more reliable AI systems.

TLDR: Research demonstrates evolutionary algorithms can optimize LLM reasoning patterns, showing 15-20% accuracy improvements through automated evolution of solution approaches.

[Full summary is here](https://aimodels.fyi/papers/arxiv/evolving-deeper-llm-thinking). Paper [here](https://arxiv.org/abs/2501.09891).",8,1,1737490342.0,MachineLearning
1i6xkdh,[D] Unsure if I am overfitting,"I am trained a machine learning model and I am unsure as to whether it is overfitting. The accuracy, precision, recall and f1-score when predicting with the training set is all 1.0, and for test set it is \~0.9 for all. I know overfitting happens when it can't generalise well for test set, but my results are pretty high for test set. I am not sure whether it is overfitting as the test scores are still quiet high.",2,10,1737504554.0,MachineLearning
1i6h40i,[D] Understanding predictive coding networks,"Hi all, 

I'm trying to understand predictive coding networks like described in [Rao & Ballard](https://www.researchgate.net/publication/13103385_Predictive_Coding_in_the_Visual_Cortex_a_Functional_Interpretation_of_Some_Extra-classical_Receptive-field_Effects).   
  
So far I understand that training the network is done through setting the input (and output if training is supervised) and first modifying the activity of the neurons to reduce prediction errors, then modifying the synaptic weights. 

What I don't understand is that it seems the activity of a hidden layer ""r"" seems to be a function of the difference between the prediction and the input (see figure 1.b), it seems implied here that \`r\` is the product of the transposed weights U^(T) and the prediction error which confuse me : I understand that we want to propagate the prediction error to the next layer, but how can we minimize (I - f(Ur)) if r = U^(T) (I - f(Ur))? 

I think I still haven't fully grasped the overall architecture and would really appreciate if someone could help. ",12,4,1737461165.0,MachineLearning
1i5z6rd,[D] ICLR 2025 paper decisions,Excited and anxious about the results! ,82,243,1737402639.0,MachineLearning
1i716g6,[D]: An Article Explains Self-Attention (code snippet included),"[article](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)

- single-head attention
- multi-head attention
- cross-attention 

explanations included.
",0,0,1737514831.0,MachineLearning
1i726eh,[R] Tensor Product Attention is All You Need,"Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available",0,3,1737517592.0,MachineLearning
1i69l1i,"[Research] Who publish this gene expression dataset? 7070 genes, 69 samples, 5 classes: EPD, JPA, MED, MGL, RHB","Hi, my goal is to reference the original author and understand what is EPD, JPA, MED, MGL, RHB. The oldest reference I can found:

1. 2008's paper \[1\], and the author's paper cite Dr. Gregory Piatetsky-Shapiro from KDnuggets and Prof. Gary Parker from Connecticut College. The most information I can get out of is it's a pediatric tumor dataset.
2. 2009's paper \[2\], and the author's paper cite \[3\]. However, the paper mentioned only 42 patients samples. Meanwhile, the dataset I have 69 labeled samples and 23 unlabeled samples.

Although I doubt it's the same paper, since paper \[3\] mentioned it's a 6,817 genes instead of 7,070 genes. But paper \[2\] add the complete name of each class based on paper \[3\]. So, I used archive website to check the dataset but it didn't archive the zip file. As of right now, I cannot check whether it is the same dataset.

The last page I am visiting: [https://web.archive.org/web/20060907191641/http://www.broad.mit.edu/mpr/CNS/](https://web.archive.org/web/20060907191641/http://www.broad.mit.edu/mpr/CNS/)

The link that I need: [http://www.broad.mit.edu/mpr/CNS/#:\~:text=Pomeroy\_et\_al\_0G04850\_11142001\_datasets.zip](http://www.broad.mit.edu/mpr/CNS/#:~:text=Pomeroy_et_al_0G04850_11142001_datasets.zip)

\[1\]N. E. Ling and Y. A. Hasan, “Evaluation Method in Random Forest as Applied to Microarray Data,” *Malaysian Journal of Mathematical Sciences*, vol. 2, no. 2, pp. 73–81, 2008.

\[2\]S. L. Pomeroy *et al.*, “Prediction of central nervous system embryonal tumour outcome based on gene expression,” *Nature*, vol. 415, no. 6870, pp. 436–442, 2002, doi: 10.1038/415436a.

\[3\]N. LING, “CLASSIFICATION OF MICROARRAY DATASETS USING RANDOM FOREST,” 2009.",11,5,1737430636.0,MachineLearning
1i5tm4j,[D] - Most Engaging ML Podcasts?,Looking for good podcasts to stay on top of ML news. Specifically looking for ones that are able to tell a good story or narrative like Planet Money or Freakonomics rather than sounding like a lecture,78,28,1737389192.0,MachineLearning
1i5rcqn,[R] Do generative video models learn physical principles from watching videos? Not yet,"A new benchmark for physics understanding of generative video models that tests models such as Sora, VideoPoet, Lumiere, Pika, Runway. From the authors; ""We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism""  
paper: [https://arxiv.org/abs/2501.09038](https://arxiv.org/abs/2501.09038)",96,14,1737383353.0,MachineLearning
1i6brew,[D] Accumulation error,"Can anyone give me some work that has theorem/insight, about possible bounds or method to approximate error accumulation of sequential model? Something like the changes in distribution/error after each steps?",1,1,1737438089.0,MachineLearning
1i68w3u,[D] Useful software development practices for ML?,"I am teaching a workshop on ML and I want to dedicate 2 hours to the software development part of building an ML system. My audience are technical undergraduate students that know python and command line. **Any software practices (with links) you wish you knew when you were younger?**

Currently thinking of talking about git, code tests, validation (pydantic) and in terms of principles: [YAGNI](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it), [KISS](https://en.wikipedia.org/wiki/KISS_principle) and [DRY/WET](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) code. Could also cover [technical debt](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf).",0,11,1737428449.0,MachineLearning
1i5i073,[R] Evolving Deeper LLM Thinking,,38,4,1737347031.0,MachineLearning
1i5mku4,"[Discussion] How to Build a Knowledge Graph from Full Text Without Predefined Entities?
","I'm building a knowledge graph from a large set of industry documents without predefined entities. How can I handle semantically duplicate entities and relationships effectively? Also, since I can't process all documents at once, how can I ensure consistency in extracted relationships when working in chunks?

PS - Will be using GPT for processing",13,7,1737365813.0,MachineLearning
1i58qk2,Any gift ideas for someone into ML?  [D],"Hello everyone, I need help for a really special gift for someone who is really into Machine Learning and related fields and is doing research/a career in it. 

I know very little about Machine Learning, but I still want to get them something either really cool or practical for their work. Anything from buying them a new computer specifically for work or some cool collectible item. Anything including pointing me in a good direction would be appreciated, thank you!",26,35,1737319990.0,MachineLearning
1i51cks,[P] Noteworthy LLM Research Papers of 2024 (Part Two): July to December,,37,0,1737301590.0,MachineLearning
1i5xv29,[D]  Uncertinity Quantificationfor time seriese prediction (RNN)?,"I have a time series that predicts one of two classes at each step (0 or 1) using RNN, so it's sequence to sequence. I'm new to the topic of Uncertainty Quantification (UQ). Can I directly apply common methods such as deep-ensemble or MC dropout and simply expect everything to work? Are there any caveats?

I have checked two libraries: [torch-uncertinity](https://github.com/ENSTA-U2IS-AI/torch-uncertainty) and [UQ-BOX](https://github.com/lightning-uq-box/lightning-uq-box) but nothing is mentioned about time series.",0,4,1737399463.0,MachineLearning
1i5h6n6,[R] Looking for retrieval datasets built from real documentation and queries,"Retrieval as in `(query, passage)` pairs where `passage` is a chunk of text from the documentation which is relevant to `query`.

BeIR has good datasets, but the ""documentation"" is often pretty wide, e.g., any Wikipedia or PubMed article. I'm looking for a dataset where the documentation is more focused, something like scikit-learn's docs.

[StaRD](https://github.com/oneal2000/STARD) is a high quality dataset, but it doesn't have enough queries for my purposes. Ideally, there are ≥5k unique queries.",3,0,1737344239.0,MachineLearning
1i5veu2,[P] Anyone Experienced with Charting and Backtesting in Futures Trading?,"Hello everyone,

I’ve been working on backtesting a theory related to trading futures around news events. The results so far have been promising, but I’d like to take things to the next level, potentially by incorporating machine learning or more advanced techniques.

Does anyone here have experience with backtesting and integrating machine learning into trading strategies? Specifically for futures or similar instruments?

I’d love to hear your insights, tips, or even resources that could help refine and expand this approach.

Thanks in advance!",0,5,1737393593.0,MachineLearning
1i5azv1,[D] The Case for Open Models,Why openness matters in AI,6,0,1737325641.0,MachineLearning
1i5pk0d,Pre-trained models on faces/skin tones? [D],"I am doing a project that involves rPPG and I was woandering if there are any good pre-trained models on faces/skin tones that I can build on top.

Thanks",0,4,1737377798.0,MachineLearning
1i5dizh,[D] Looking for NLP annotation tool with custom column view,"Hi everyone! I'm working on a document revision project that requires NLP data annotation. I need a tool that can:

1. Display the dataset in a standard tabular view
2. Show git-style diffs between source and revised texts in a custom column

I've already tried Argilla and Label Studio, but neither supports custom columns. Does anyone know of annotation tools that offer this functionality?

Thanks in advance!",0,5,1737332645.0,MachineLearning
1i5mk5e,[D] Llama3.2 model adds racial annotation,"This is really interesting, I was conversing with Llama 3.2 3B model, I found out that it automatically appends or greets you based on your name. Maybe others already know this, but I just paid attention to this detail just now. Could it be because of the training dataset, or is this injected.

[check this out](https://preview.redd.it/8vlginagc4ee1.png?width=1352&format=png&auto=webp&s=07b36347e77fd357f2375e96eef4462efc9c3c52)

  
Edit: the racial annotation is not added to put a negative spin on this, it’s just an observation that salutations are customised based on the name. Which is an attention to detail",0,10,1737365726.0,MachineLearning
1i4ltt6,[D] Suggestions for topics for a PhD level ML focused programming course?,"Some background: I work as a data scientist/ML engineer for a small startup. I also adjunct for the department from which I got my PhD(in statistics).

For the last few years, I’ve been teaching a series of statistical programming courses for masters students, and early PhD‘s. This semester, my class unfortunately got canceled due to low enrollment, which I was told is due to poor recruitment last fall and poor advertising.  We are thinking to offer that course every other year. I would like to propose a third course in the series with more advanced topics.

First course: programming fundamentals for both R and Python. Some basic analytical stuff for each.

Second course: Python based analysis course (many R courses exist already) which touches on statistical routines from basics to mixed modeling and Bayesian analysis.  Also we go through the classic models with PyTorch as well as a few transformer based applications.  Also work in some explainable AI techniques 

Third course: optimization, variational inference, other Bayesian deep learning approaches, MLops concepts, ????

The thing is I need to work in a fair amount of stochastic approaches because it’s a statistics department after all. 

Hope that’s clear. I would like to provide relevant information especially to PhD students who would like to live at the cutting edge with an emphasis on experimentation and implementation.  I know there is a lot out there but at work I need to focus on my specific tasks.

Thanks so much for any advice!",26,18,1737247026.0,MachineLearning
1i4rz3r,[P] Speech recognition using MLP ,"So we have this assignment where we have to classify the words spoken in the audio file. We are restricted to using spectrograms as input, and only simple MLPs no cnn nothing. The input features are around 16k, and  width is restricted to 512, depth 100, any activation function of our choice. We have tried a lot of architectures, with 2 or 3 layers, with and without dropout, and with and without batch normal but best val accuracy we could find is 47% with 2 layers of 512 and 256, no dropout, no batch normal and SELU activation fucntion. We need 80+ for it to hold any value. Can someone please suggest a good architecture which doesn't over fit?",9,42,1737266802.0,MachineLearning
1i44h5v,[D] I hate softmax ,"This is a half joke, and the core concepts are quite easy, but I'm sure the community will cite lots of evidence to both support and dismiss the claim that softmax sucks, and actually make it into a serious and interesting discussion.

What is softmax? It's the operation of applying an element-wise exponential function, and normalizing by the sum of activations.
What does it do intuitively? One point is that outputs sum to 1. Another is that the the relatively larger outputs become *more* relatively larger wrt the smaller ones: big and small activations are teared apart.

One problem is you never get zero outputs if inputs are finite (e.g. without masking you can't attribute 0 attention to some elements).
The one that makes me go crazy is that for most of applications, magnitudes and ratios of magnitudes are meaningful, but in softmax they are not: softmax cares for differences.
Take softmax([0.1, 0.9]) and softmax([1,9]), or softmax([1000.1,1000.9]). Which do you think are equal? In what applications that is the more natural way to go?

Numerical instabilities, strange gradients, embedding norms are all things affected by such simple cores. Of course in the meantime softmax is one of the workhorses of deep learning, it does quite a job.

Is someone else such a hater? Is someone keen to redeem softmax in my eyes?",261,92,1737194715.0,MachineLearning
1i4ho23,[D] Refactoring notebooks for prod,"I do a lot of experimentation in Jupyter notebooks, and for most projects, I end up with multiple notebooks: one for EDA, one for data transformations, and several for different experiments. This workflow works great until it’s time to take the model to production.

At that point I have to take all the code from my notebooks and refactor for production. This can take weeks sometimes. It feels like I'm duplicating effort and losing momentum.

Is there something I'm missing that I could be using to make my life easier? Or is this a problem y'all have too?

\*Not a huge fan of nbdev because it presupposes a particular structure",30,25,1737235091.0,MachineLearning
1i4fvqn,[R] VortexNet: Neural Computing through Fluid Dynamics ,,29,7,1737230204.0,MachineLearning
1i4n01i,[R] Tensor and Fully Sharded Data Parallelism,"In this series, we continue exploring distributed training algorithms, focusing on tensor parallelism (TP), which distributes layer computations across multiple GPUs, and fully sharded data parallelism (FSDP), which shards model parameters, gradients, and optimizer states to optimize memory usage. Today, these strategies are integral to massive model training, and we will examine the properties they exhibit when scaling to models with 1 trillion parameters.

[https://martynassubonis.substack.com/p/tensor-and-fully-sharded-data-parallelism](https://martynassubonis.substack.com/p/tensor-and-fully-sharded-data-parallelism)",9,0,1737250575.0,MachineLearning
1i455gs,[R] Causal Inference Meets Deep Learning: A Comprehensive Survey,,28,0,1737197680.0,MachineLearning
1i3t4c3,[P] Building an Reinforcement Learning Agent to play The Legend of Zelda,"A year go I started trying to use PPO to play the original Legend of Zelda, and I was able to train a model to beat the first boss after a few months of work. I wanted to share the project just for show and tell. I'd love to hear feedback and suggestions as this is just a hobby project. I don't do this for a living. The code for that lives in the [original-design branch](https://github.com/DarkAutumn/triforce/tree/original-design) of my [Triforce repo](https://github.com/DarkAutumn/triforce). I'm currently tinkering with new designs so the main branch is much less stable.

Here's a video of the agent [beating the first dungeon](https://www.youtube.com/watch?v=yERh3IJ54dU), which was trained with 5,000,000+ steps. At 38 seconds, you can see it learned that it's invulnerable at the screen edge, and it exploits that to avoid damage from a projectile. At 53 seconds it steps up to avoid damage from an unblockable projectile, even though it takes a -0.06 penalty for moving the wrong way (taking damage would be a larger penalty.) At 55 seconds it walks towards the rock projectile to block it. And so on, lots of little things the model does is easy to miss if you don't know the game inside and out.

As a TLDR, [here's an early version of my new (single) model](https://youtu.be/3AJXfBnmgVk). This doesn't make it quite as far, but if you watch closely it's combat is already *far* better, and is only trained on 320,000 steps (~6% of the steps the first model was trained on).

This is pretty far along from my [very first model](https://www.youtube.com/watch?v=KXPMwehTOf0).

# Original Design

I got the original project working using stable-baselines's PPO and default neural network (Shared NatureCNN, I believe). SB was great to get started but ultimately stifling. In the new version of the project I've implemented PPO from scratch with torch with my own simple neural network similar to stable-baseline's default. I'm playing with all kinds of changes and designs now that I have more flexibility and control. Here is my rough original design:

## Overall Strategy

My first pass through this project was basically ""imagine playing Zelda with your older sibling telling you where to go and what to do"". I give the model an objective vector which points to where I want it to go on the screen (as a bird flies, the agent still had to learn path finding to avoid damage and navigate around the map). This includes either point at the nearest enemy I want it to kill or a NSEW vector if it's supposed to move to the next room.

Due a few limitations with stable-baselines (especially around action masking), I ended up training unique models for traversing the overworld vs the dungeon (since they have entirely different tilesets). I also trained a different model for when we have sword beams vs not. In the video above you can see what model is being used onscreen.

In my current project I've removed this objective vector as it felt too much like cheating. Instead I give it a one-hot encoded objective (move north to the next room, pickup items, kill enemies, etc). So far it's working quite well without that crutch. The new project also does a much better job of combat even without multiple models to handle beams vs not.

## Observation/Action Space

**Image** - The standard neural network had a really tough time being fed the entire screen. No amount of training seemed to help. I solved this by creating a [viewport around Link](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/notebooks/torch_viewport.ipynb) that keeps him centered. This REALLY helped the model learn.

I also had absolutely zero success with stacking frames to give Link a way to see enemy/projectile movement. The model simply never trained with stable-baselines when I implemented frame stacking and I never figured out why. I just added it to my current neural network and it seems to be working...

Though my early experiments show that giving it 3 frames (skipping two in between, so frames curr, curr-3, curr-6) doesn't *really* give us that much better performance. It might if I took away some of the vectors. We'll see.

**Vectors** - Since the model cannot see beyond its little viewport, I gave the model a vector to the closest item, enemy, and projectile onscreen. This made it so the model can shoot enemies across the room outside of its viewport. My new model gives it multiple enemies/items/projectiles and I plan to try to use an attention mechanism as part of the network to see if I can just feed it all of that data.

**Information** - It also gets a couple of one-off datapoints like whether it currently has sword beams. The new model also gives it a ""source"" room (to help better understand dungeons where we have to backtrack), and a one-hot encoded objective.

**Action Space**

My original project just has a few actions, 4 for moving in the cardinal directions and 4 for attacking in each direction (I also added bombs but never spent any time training it). I had an idea to use masking to help speed up training. I.E. if link bumps into a wall, don't let him move in that direction again until he moves elsewhere, as the model would often spend an entire memory buffer running headlong straight into a wall before an update...better to do it once and get a huge negative penalty which is essentially the same result but faster.

Unfortunately SB made it really annoying architecturally to pass that info down to the policy layer. I could have hacked it together, but eventually I just reimplemented PPO and my own neural network so I could properly mask actions in the new version. For example, when we start training a fresh model, it cannot attack when there aren't enemies on screen and I can disallow it from leaving certain areas.

The new model actually understands splitting swinging the sword short range vs firing sword beams as two different actions, though I haven't yet had a chance to fully train with the split yet.

**Frameskip/Cooldowns** - In the game I don't use a fixed frame skip for actions. Instead I use the internal ram state of game to know when Link is animation locked or not and only allow the agent to take actions when it's actually possible to give meaningful input to the game. This greatly sped up training. We also force movement to be between tiles on the game map. This means that when the agent decides to move it loses control for longer than a player would...a player can make more split second decisions. This made it easier to implement movement rewards though and might be something to clean up in the future.

## Other interesting details

**Pathfinding** - To facilitate rewards, the original version of this project used A* to pathfind from link to what he should be doing. [Here's a video of it in action](https://www.youtube.com/watch?v=HFsHpex4OvM). This information wasn't giving to the model directly but instead the agent would only be given the rewards if it exactly followed that path or the transposed version of it. It would also pathfind around enemies and not walk through them.

This was a nightmare though. The [corner cases were significant](https://github.com/DarkAutumn/triforce/blob/original-design/triforce/critics.py#L343-L502), and pushing Link towards enemies but not *into* them was really tricky. The new verison just uses a wavefront algorithm. I calculate a [wave from the tiles we want to get to outwards](https://github.com/DarkAutumn/triforce/blob/main/triforce/wavefront.py), then make sure [we are following the gradient](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/critics.py#L281-L329). Also calculating the A* around enemies every frame (even with caching) was super slow. Wavefront was faster, especially because I give the new model no special rewards for walking around enemies...faster to compute and it has to learn from taking damage or not.

Either way, the both the old and new models successfully learned how to pathfind around danger and obstacles, with or without the cheaty objective vector.

**Rewards** - I programmed very dense rewards in both the [old](https://github.com/DarkAutumn/triforce/blob/original-design/triforce/critics.py) and [new](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/critics.py) model. At basically every step, the model is getting rewarded or punished for something. I actually have some ideas I can't wait to try out to make the rewards more sparse. Or maybe we start with dense rewards for the first training, then fine-tune the model with sparser rewards. We'll see.

**Predicting the Future** - Speaking of rewards. One interesting wrinkle is that the agent can do a lot of things that will eventually deal damage but not on that frame. For example, when Link sets a bomb it takes several seconds before it explodes, killing things. This can be a massive reward or penalty since he spent an extremely valuable resource, but may have done massive damage. PPO and other RL propagates rewards backwards, of course, but that spike in reward could land on a weird frame where we took damage or moved in the wrong direction.

I probably could have just *not* solved that problem and let it shake out over time, but instead I used the fact that we are in an emulator to just see what the outcome of every decision is. When planting a bomb, shooting sword beams, etc, we [let the game run forward](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/state_change_wrapper.py#L140-L192) until impact, then rewind time and reward the agent appropriately, continuing on from when we first paused. This greatly speeds up training, even if it's expensive to do this savestate, play forward, restore state.

**Neural Networks** - When I first started this project (knowing very little about ML and RL), I thought most of my time would be tuning the shape of the neural network that we are using. In reality, the default provided by stable-baselines [and my eventual reimplemnentation](https://github.com/DarkAutumn/triforce/blob/dea241219ff17b386e368bc25adfbc171207888a/triforce/models.py#L19-L255) has been enough to make *massive* progress. Now that I have a solid codebase though, I really want to revisit this.  I'd like to see if trying CoordConvs and similar networks might make the viewport unncessary.

## Less interesting details/thoughts

**Hyperparameters** - Setting the entropy coefficinet way lower helped a TON in training stable models.  My new PPO implementation is way less stable than stable-baselines (ha, imagine that), but still converges most of the time.

**Infinite Rewards** - As with all reinforcement learning, if you give some way for the model to get infinite rewards, it will do just that and nothing else. I spent days, or maybe weeks tweaking reward functions to just get it to train and not find a spot on the wall it could hump for infinite rewards.  Even just neutral rewards, like +0.5 moving forward and -0.5 for moving backwards, would often result in a model that just stepped left, then right infinitely.  There has to be a real reward or punishment (non-neutral) for forward progress.

**Debugging Rewards** - In fact, building a rewards debugger was the only way I made progress in this project.  If you are tackling something this big, do that very early.

**Stable-Retro is pretty great** - Couldn't be happier with the clean design for implementing emulation for AI.

**Torch is Awesome** - My early versions heavily used numpy and relied on stable-baselines, with its multiproc parallelization support. It worked great. Moving the project over to torch was night and day though. It gave me so much more flexibility, instant multithreading for matrix operations. I have a pretty beefy computer and I'm *almost* at the same steps per second as 20 proc stable-retro/numpy.

## Future Ideas

This has already gone on too long. I have some ideas for future projects, but maybe I'll just make them another post when I actually do them.

## Special Thanks

A special thanks to [Brad Flaugher](https://bradflaugher.com/) for help with the early version of this, Fiskbit from the Zelda1 speedrunning community for help pulling apart the raw assembly to build this thing, and MatPoliquin for maintaining Stable-Retro.

Happy to answer any questions, really I just love nerding out about this stuff.",160,22,1737154326.0,MachineLearning
1i458ut,[R] Liquid Neural Networks exhibit robust navigation in OOD environments.,,11,1,1737198072.0,MachineLearning
1i3qxy4,[D] Am I actually a machine learning engineer?,"For the past few years I've had a job with the official title ""machine learning engineer"", but as I hunt for other jobs online, I wonder if that's actually accurate. Based on the experience requirements and responsibilities listed, it doesn't seem to match up with what I do.

I have a master's with a focus in ML (though that was pre LLM-boom, so things have changed a lot) but struggled to find work in my area pertaining to that out of college. Post-COVID when everyone went remote I got my current job. In it, I work on a team building and deploying software that utilize machine learning to accomplish tasks. However, I'm never the one actually building the learning models (there's a researcher on our team who does that); just creating the systems around them. I'm actually pretty happy in my ""machine learning adjacent"" role, but should I be searching for different job titles to find something similar?

EDIT: a bunch of people keep replying thinking I'm looking for validation about my title. I don't care about that. I only care about knowing what job titles I should be searching for when looking for something similar.",128,58,1737148506.0,MachineLearning
1i44wlr,[P] Launch a Federation of robots that collaboratively train an object manipulation model,"Using [Flower](https://flower.ai/)  and [LeRobot](https://github.com/huggingface/lerobot), I put together a  [quickstart example](https://github.com/adap/flower/tree/main/examples/quickstart-lerobot) that demonstrates how to train a diffusion model collaboratively across 10 individual nodes (each with its own dataset partition!). This example uses the`push-t` dataset, where the task is to move a letter T object on top of another that is to remain static.

The example it's pretty easy to run, and can do so efficiently if you have access to a recent gaming GPU. Although the diffusion model only takes 2GB of VRAM (of course you can decide to scale it up), the compute needed to train them isn't negligible. For context, running the example until convergence takes 40mins on a dual RTX 3090 setup. It takes about 30rounds of federated learning (FL) to do so although the example runs for 50 rounds by default.

[Evaluation of the global model at different rounds. After just a few rounds of collaboratively AI training the model successfully completes the task \(and it does so pretty fast!!!\)](https://i.redd.it/lp13jn70dqde1.gif)

  
The example runs each node/robot in simulation by default (i.e. each node is a Python process and there is some clever scheduling to run the jobs in a resource-aware manner). But it is straight forward to run it as a real deployment where each node is, for example, a different device (e.g. NVIDIA Jetson). If someone is interested in doing this, checkout the links added at the bottom of the example readme

  
Learn more about the Action Diffusion policy method -> [https://arxiv.org/abs/2303.04137](https://arxiv.org/abs/2303.04137)

",10,1,1737196643.0,MachineLearning
1i3ym76,[R] Any paper recommendations for Bayesian methods in ML and causal inference?,"Hey guys,

So I am very new to Bayesian methods and am curious about it from a data science and modelling point of view and how it could determine causal relationships.

I don't really know where to start, but I've read some papers on Bayesian Networks and have heard interesting things about Bayesian Deep Learning so would be happy to see any recommendations on those topics.

I would also be happy to hear about any papers you may have recently read, but am looking for anything you guys have found interesting and not an application on any specific domain, just interested in learning the theory for now (unless you suggest that I pick a domain first).

Many thanks",31,12,1737170980.0,MachineLearning
1i40viz,[D] Dynamic Neuron-Controller-Based Transformer Architecture: Feedback Wanted,"**Dynamic Neuron-Controller-Based Transformer Architecture by Shanmukh Ram**

# Abstract

This white paper presents an innovative architecture that integrates dynamic neuron-controller systems with transformer models to create a continuously adaptive and resource-efficient AI framework. The proposed architecture utilizes neuron or batch controllers to dynamically adjust the weights and operations of a shared transformer architecture in real time.

By responding to signals generated by individual or grouped neurons, the system continuously adapts to changing demands. This adaptability enables efficient multi-tasking and optimizes resource sharing, ensuring high performance across diverse contexts. These features establish the architecture as a groundbreaking innovation in AI, unlocking advancements in applications such as general intelligence, personalized systems, and multi-agent collaboration.

# 1. Introduction

# 1.1 Background

Transformer architectures have revolutionized natural language processing and other domains, owing to their scalability, attention mechanisms, and ability to model long-range dependencies. However, transformers remain largely static post-training, with fine-tuning or retraining required to adapt to new tasks or shifting environments.

# 1.2 Motivation

Real-world applications often involve dynamic and unpredictable environments. Traditional transformer models, though powerful, are inefficient in adapting to real-time changes without significant retraining. This gap motivates the design of a system where neurons act as adaptive controllers, dynamically modifying the transformer’s behavior to optimize performance across varying tasks and inputs.

# 2. Proposed Architecture

# 2.1 Core Components

The architecture consists of the following core components:

1. **Neuron-Controllers**:
   * Independent neurons or batches of neurons act as dynamic agents within the system, controlling and optimizing the transformer’s performance. These controllers receive input signals from various sources, including real-time environmental data, user feedback, or task-specific objectives. Upon processing these inputs, the controllers generate precise control signals to dynamically modify transformer parameters such as attention weights, layer activations, or embeddings. For instance, in a natural language processing task, the controllers might adjust attention weights to focus on critical phrases in a document, ensuring more accurate summarization. Similarly, in image recognition tasks, layer activations could be optimized to emphasize edges or textures, improving classification accuracy.
   * These targeted adjustments significantly enhance the system’s ability to adapt to diverse tasks while maintaining high performance and efficiency. This dynamic adjustment ensures the system remains highly adaptive, continuously optimizing its responses to suit specific tasks or contexts.
2. **Shared Transformer Framework**:
   * A modular transformer architecture forms the backbone of the system, meticulously crafted to support real-time adjustments to its operational parameters. This modularity allows each core component, such as attention heads, transformer layers, or embeddings to be dynamically reconfigured based on control signals generated by neuron-controller batches. By enabling real-time adaptability, the system ensures that computational resources can be scaled efficiently or concentrated on specific areas of importance, depending on the complexity and requirements of the task. For instance, attention heads may be activated selectively for high-priority inputs, while layers or embeddings can be modified dynamically to fine-tune task-specific outputs. This approach not only enhances scalability but also optimizes performance, making the architecture capable of handling both simple and complex tasks with remarkable efficiency.
3. **Feedback Loop**:
   * The architecture integrates a continuous feedback mechanism wherein the transformer's outputs are systematically analyzed and fed back to the neuron-controllers. This iterative process allows the neuron-controllers to refine their strategies based on real-time performance metrics and contextual outcomes. By dynamically adjusting control parameters, the system ensures alignment with evolving task objectives and operational efficiency. This feedback loop not only enhances adaptability but also fosters a robust learning environment where both controllers and the transformer progressively improve in tandem.
   * This loop refines the controllers’ strategies in real time, ensuring constant performance improvement and alignment with task objectives.
   * By iteratively optimizing both the controllers and the transformer, the system achieves a closed-loop learning environment.
4. **Coordinator Mechanism**:
   * A centralized or decentralized coordinator mechanism is designed to ensure seamless interactions among multiple neuron-controller batches. This mechanism prioritizes resource allocation and balances task assignments, mitigating potential conflicts that may arise when neuron batches manage separate transformers or collaborate on shared tasks. By enabling effective coordination, the architecture prevents inefficiencies and ensures that all tasks are executed optimally, maintaining synergy across the entire system.

# 2.2 Key Features

1. **Dynamic Weight Adjustment**:

Dynamic weight adjustment represents the core capability of the system where controllers fine-tune specific transformer weights in real time. These adjustments are informed by contextual signals, which include environmental data, user feedback, and task-specific objectives. For example, in autonomous driving, the controllers can adjust attention weights to prioritize critical inputs like pedestrian detection over less immediate data, such as road signage in clear weather. In healthcare applications, layer activations might be fine-tuned dynamically to focus on anomalies in medical imaging, ensuring accurate diagnostics. When an input signal is received, the neuron-controllers analyze it and generate precise commands to recalibrate the transformer's internal parameters, such as attention weights or activation thresholds. This process ensures that the architecture adapts seamlessly to the demands of diverse tasks and dynamic environments. The ability to perform these real-time optimizations not only enhances task-specific performance but also maximizes resource efficiency, as only the necessary components of the transformer are engaged at any given time. This dynamic adaptability is crucial for handling complex, real-world scenarios where static models would fail to perform optimally, thereby positioning this system as a significant advancement in AI adaptability and responsiveness.

1. **Batch-Based Control**:
   * Groups of neurons manage different tasks or modules, each acting as specialized agents to oversee specific functionalities within the system. This allows simultaneous optimization across multiple frameworks by dynamically distributing computational resources and responsibilities. For example, one group of neurons may control language modeling tasks while another focuses on vision-based analysis, enabling these processes to run concurrently without interfering with each other. This approach enhances efficiency and ensures that the transformer system remains scalable and adaptable, bringing the value of multitasking without compromising performance.
2. **Task-Specific Adaptation**:
   * Each neuron batch can specialize in controlling a subset of the transformer for task-specific performance by dynamically focusing on the specific layers, attention mechanisms, or embeddings that are most relevant to the task. For example, in a multi-task learning setup, one neuron batch could fine-tune the transformer’s attention weights for language modeling, while another batch might adjust embedding layers for visual data processing. This specialization ensures that the system can effectively handle diverse tasks in parallel without sacrificing efficiency or performance. By leveraging this dynamic specialization, the architecture optimizes resource utilization, minimizes interference between tasks, and enhances the accuracy and responsiveness of each transformer subset to its assigned task.
3. **Multi-Agent Collaboration**:
   * Neuron batches play a pivotal role in enhancing the system's overall performance by engaging in collaborative or competitive dynamics tailored to complex, multi-dimensional tasks. For example, in a multi-modal AI system, one neuron batch could specialize in processing textual data, while another focuses on visual inputs. Collaboration between these batches ensures that insights from both modalities are integrated effectively, leading to more accurate and coherent outcomes, such as in video summarization or multimedia content analysis. Similarly, competition among neuron batches could prioritize critical tasks, ensuring time-sensitive objectives like anomaly detection in real-time surveillance are addressed promptly. These batches act as specialized agents, dynamically adjusting their behaviors to maximize task outcomes based on the broader system’s objectives. For instance, collaboration between neuron batches may involve sharing insights or control signals to optimize resource allocation across different sections of the transformer. In contrast, competitive dynamics could arise in scenarios where distinct neuron batches vie to prioritize their assigned tasks, ensuring critical objectives receive adequate focus.
   * By allowing both collaboration and competition, the architecture fosters a balance between efficiency and task-specific precision. This mechanism integrates seamlessly with the feedback and coordination systems, ensuring that neuron batches remain aligned with the overarching goals of the system while dynamically optimizing their strategies. The value of this approach lies in its ability to handle multi-tasking demands with enhanced adaptability and responsiveness, making it an essential component of the architecture's design.

# 3. Implementation

# 3.1 Input Signals

Neuron-controllers process a variety of inputs, such as:

* **Environmental Data**: Real-time data streams from external sensors or APIs.
* **Feedback Signals**: Outputs from transformers or user interaction data.
* **Predefined Objectives**: Task-specific goals encoded during training.

# 3.2 Dynamic Controllers

Neuron-controllers utilize advanced reinforcement learning (RL) techniques and optimization algorithms to determine the most effective adjustments for the transformer. These adjustments include recalibrating attention weights to focus on the most relevant features of the input, selectively activating or deactivating layers to optimize computational efficiency, and dynamically modifying positional encodings or embeddings to enhance the transformer's contextual understanding. By analyzing input signals and system feedback in real-time, neuron-controllers ensure that the architecture remains highly adaptive and aligned with task-specific objectives, enabling superior performance across diverse and complex tasks.

# 3.3 Transformer Modularity

The transformer is designed with modularity in mind:

* **Adapters**: Lightweight modules inserted into transformer layers to enable task-specific adjustments.
* **Sparse Activation**: Only parts of the transformer are activated based on control signals.
* **Mixture of Experts (MoE)**: Controllers determine which expert modules to activate for a given input.

# 3.4 Feedback Mechanism

A feedback loop evaluates the transformer’s output and updates the neuron-controllers’ strategies, creating a continuous learning environment.

# 4. Applications

# 4.1 Multi-Task Learning

Dynamic controllers empower a single transformer architecture to manage multiple tasks simultaneously by dynamically redistributing resources to optimize for each task's specific requirements. These controllers act as task-specialized agents, analyzing the contextual demands of each input and directing computational focus to the most relevant sections of the transformer such as attention heads, embeddings, or specific layers. For example, when handling a combination of natural language processing and vision-based tasks, the dynamic controllers can assign priority resources to textual embeddings for language inputs while activating vision-specific modules for image data.

This simultaneous multi-task optimization ensures that each task benefits from the transformer's shared architecture without compromising performance. The ability to dynamically allocate resources not only reduces computational redundancy but also enhances scalability, allowing the system to adapt seamlessly to complex, real-world scenarios. By maintaining task-specific precision while sharing computational infrastructure, this architecture represents a significant step forward in creating efficient and robust AI systems capable of managing diverse workloads.

# 4.2 Personalized Systems

Dynamic controllers allow the transformer to adapt its behavior to individual users or specific contexts, enabling highly tailored and responsive applications. By analyzing real-time user data, such as preferences, historical interactions, or contextual inputs, these controllers dynamically modify the transformer's parameters to deliver personalized outputs. For example, in a virtual assistant application, the controller might adjust the transformer's attention mechanisms to prioritize the user's current needs or focus on topics of interest based on prior interactions. This capability ensures that the system evolves alongside the user, providing a more engaging and effective experience. The ability to personalize outputs in real-time is critical for applications in education, healthcare, and customer service, where individualized solutions add significant value.

# 4.3 Collaborative AI

Neuron-controller batches enhance the system's ability to handle complex, multi-dimensional problems by fostering collaboration among multiple transformers. For instance, in a multi-modal AI system integrating text, images, and audio, one batch of neuron-controllers could process and extract key textual information, another batch could analyze visual data, and a third could handle audio signals. Collaboration ensures that insights from each modality are synthesized into a unified understanding, significantly improving outcomes such as multimedia content analysis or real-time event summarization.

This collaborative potential enables the system to leverage diverse data types effectively, ensuring comprehensive and accurate results. These controllers dynamically allocate resources and share insights between transformers, enabling them to work together seamlessly. For instance, in multi-modal AI applications that integrate text, images, and audio, one transformer might specialize in processing textual data while another focuses on visual analysis.

Through real-time communication and coordination, the system ensures that insights from each modality contribute to a cohesive and accurate result. This collaborative approach not only improves task performance but also enables the system to tackle problems that require integrated knowledge from multiple domains.

# 4.4 General Intelligence

The architecture's dynamic adaptability, real-time resource allocation, and collaborative mechanisms represent a significant step toward achieving general artificial intelligence. By allowing neuron-controller batches to manage diverse tasks and contexts dynamically, the system creates a foundation for cross-domain learning and decision-making. Unlike traditional AI systems that require retraining for new tasks, this architecture can rapidly adapt to novel scenarios, demonstrating a level of flexibility and generalization that closely mirrors human intelligence. The ability to integrate knowledge across tasks and respond effectively to unforeseen challenges positions this architecture as a cornerstone in the pursuit of general AI.

# 5. Societal Impacts

# 5.1 Positive Outcomes

* **Efficiency**: Reduced computational costs through dynamic resource sharing.
* **Adaptability**: Better handling of real-world variability and user-specific needs.
* **Innovation**: New AI applications and use cases become feasible.

# 5.2 Risks

* **Unpredictability**: Dynamic systems may produce unforeseen behaviors.
* **Security**: Systems must be robust against adversarial inputs or misuse.
* **Ethical Concerns**: Continuous learning raises questions about accountability and transparency.

# 6. Future Directions

The dynamic neuron-controller-based transformer architecture opens up several avenues for research and practical advancements. The focus must be on refining the foundational mechanisms to further enhance scalability, adaptability, and safety.

# 6.1 Enhancing Controller Intelligence

Research should prioritize the development of neuron-controllers capable of understanding higher-level abstractions, contextual nuances, and complex task hierarchies. By integrating advanced algorithms such as meta-learning and neural architecture search, these controllers can evolve into highly intelligent agents that adapt seamlessly to diverse and unforeseen challenges. This advancement will make the system more robust in managing a wider array of applications.

# 6.2 Scaling to Larger Architectures

Efforts must be directed toward designing and managing larger systems that integrate multiple controllers and transformers. However, scaling such architectures presents significant challenges, including increased computational overhead, potential bottlenecks in communication between controllers, and the risk of degraded performance in highly complex systems. Addressing these limitations is crucial to unlock the full potential of this approach and ensure seamless scalability in real-world applications. Techniques such as distributed computing, modular design, and sparse activations will be critical to maintain performance and efficiency at scale. This scaling capability will empower the architecture to handle increasingly complex tasks across industries, from healthcare diagnostics to autonomous systems.

# 6.3 Safety and Robustness

Ensuring the safety and reliability of dynamically adaptive systems is paramount. Specific strategies to achieve this include the integration of robust adversarial defense mechanisms to counter malicious inputs, the development of fail-safe protocols to handle unexpected failures, and the implementation of comprehensive ethical oversight frameworks. Additionally, employing techniques such as explainability in AI and real-time monitoring systems can ensure transparency and accountability, further reinforcing the trustworthiness of these architectures. This requires the implementation of fail-safes, ethical oversight mechanisms, and robust adversarial defenses.

By addressing these concerns, the architecture can operate confidently in critical applications, including finance, defense, and public safety. For example, in finance, the system could dynamically adapt to market changes by prioritizing critical data streams for fraud detection or risk assessment. In defense, collaborative neuron-controller batches could integrate intelligence from multiple data modalities such as satellite imagery, intercepted communications, and real-time ground reports to provide actionable insights for decision-makers. Similarly, in public safety, the architecture could manage resources dynamically during emergencies, such as optimizing response times for disaster management or ensuring accurate predictions for crowd control. Safety-focused research will also ensure that the system remains compliant with evolving regulations and ethical standards.

# 8. Conclusion

The proposed dynamic neuron-controller-based transformer architecture represents a paradigm shift in AI development. By enabling real-time adaptability, efficient resource sharing, and multi-tasking capabilities, this system has the potential to revolutionize AI applications across industries. While challenges remain, the opportunities for innovation and societal benefit are immense, making this a promising direction for future research and development.",12,17,1737179120.0,MachineLearning
1i45h9r,[D] Few-shot Learning with prototypical networks - help to understand the concept ,"Hi, probably quite simple questions for those who know the concept but still tricky for me to realize.

Let's say I have a dataset with 200 labeled samples and I have 10 classes. However, not all 200 examples contain all 10 classes, but only some of them while the rest samples contain a combination of them. Meaning that a sample might be labeled for classes 0, 1, 5, 8, while another for 0, 3, 7, and so on. Which also means that the prevalence of the classes varies a lot.

How do I split my dataset for few-shot learning with prototypical networks? Do I need to train and validate on samples that include all classes, so the network learns to compute prototypes for every class? Also, given that the prevalence of the classes varies, do I need to balance the sampling so it sees each class equally on the number of training and validation episodes?

During testing do I need to include on my test set a few labeled samples for each class? Can I do inference without any labeled samples? Is that zero-shot learning? Also, can I train a model that generalizes to unseen classes during training? 

Thanks in advance for your time and help!",3,8,1737199033.0,MachineLearning
1i4cybu,[P] How to Highlight Entire Articles Based on Keywords in a Multi-column Newspaper PDF,"Hi everyone,

I'm working on a project where I need to search for a keyword within a newspaper PDF and highlight the entire article that contains the keyword. The challenge arises because:

1. **Multiple Articles Per Page:** Each page may have multiple articles, and I need to distinguish which parts of the page belong to which article.
2. **Multi-column Layout:** Articles are often spread across multiple columns, which makes it tricky to identify the full content of an article when it spans across these columns.

My goal is to:

* Search for a keyword in the text.
* Identify and highlight the entire article that contains the keyword, even if the article spans across multiple columns or pages.

I’m currently using **PyMuPDF4LLM** to process the PDFs and extract text, but I’m not sure how to accurately map the text to specific articles and handle multi-column text.

Has anyone worked on something similar or have suggestions on how to approach this problem?

I’m attaching a sample page of the newspaper for context.

https://preview.redd.it/07gc46adhsde1.png?width=692&format=png&auto=webp&s=f0f6cd925013c67ea5b0ad2cef72a84cc2430c35

",0,1,1737222373.0,MachineLearning
1i4c5ao,[D] Fine tuning FinBert,"I want to solve use case for categorising transactions from bank statements. 

Can fine tuning finbert a good choice?",0,1,1737220250.0,MachineLearning
1i4aqxu,[D] Looking for a specific youtube video,There is a video of an asian guy who is explaining backpropogation on pen and paper using the minst dataset can anyone send me the url to the video? Thanks,0,2,1737216478.0,MachineLearning
1i39iuh,[D] Recommendations of noteworthy AI papers for starters in 2025,"Hi I’m devising up a list of papers to recommend students just starting out in compsci.

What are some must-read papers to give that is not too deep?

These days all the statistic learning theories are within reach with online courses but I want them to grow to read academic papers.

I’m starting off with ilya Sutskever's reading list.

A brief explanation of why you’re recommending the paper would be welcome too!",72,18,1737092193.0,MachineLearning
1i34keg,Grokking at the Edge of Numerical Stability [Research],">Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the naïve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ⊥Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods.

Paper: [https://arxiv.org/abs/2501.04697](https://arxiv.org/abs/2501.04697)

(not my paper, just something that was recommended to me)  
",131,24,1737075960.0,MachineLearning
1i3rrkx,[P] Are there any formal references to this dataset?,"Hi all!

I'm working on a project about Multitouch Attribution Modeling using Tensor flow to predict conversion over different channels.

In the project, we are using this dataset ([https://www.kaggle.com/code/hughhuyton/multitouch-attribution-modelling](https://www.kaggle.com/code/hughhuyton/multitouch-attribution-modelling)). However, we cannot find any formal reference (published paper or something similar) to make a proper citation. I have searched on Google a lot… really, a lot.

Does anyone know what is the origin of the data or if is it referenced somewhere?

Thanks for the help.",1,3,1737150696.0,MachineLearning
1i3enos,Automate Deep learning model with camera(inception -Tensorflow) [P],So i have been working with a deep learning project the aim is to detect objects. My main goal was to detect plastic from water and pick it up using a conveyor belt attached with a boat so i took code from GitHub and made sufficient changes and now the model is working but one problem is i have to manually add photo and change its name to test.jpeg(which i have given) so in my model the boat have a camera how will i make a project that can took the photo automatically when it detects a object and automatically load to my already made model and for all this process which development board will be sufficient.i hope someone answers my question 🙂,6,1,1737114704.0,MachineLearning
1i39cia,[D] share your most frequent embarrassingly parallel tasks,"Hey all,

I’m curious about the most common embarrassingly parallel tasks you encounter in the wild. In the ML and DS world, I’ve noticed many workflows tend to follow this general pattern:

* Pull a bunch of data from cloud storage
* Process that data through a series of functions
* Run an analysis, use the data for training, or pass it into a model for inference

What workloads do you have that follow this process or something similar? I’ve been tinkering with a cloud abstraction to make large-scale parallel processing easier, and I’m trying to identify common use cases to build tutorials around.

Any ideas, advice, or feedback would be super helpful",11,8,1737091537.0,MachineLearning
1i3baxp,[D] Concerns about review process at TPAMI,"I submitted a paper to TPAMI on June 25, 2024. It was a significant extension of our work that was accepted as an oral presentation at AAAI 2023. I know the reviews at TPAMI are rigorous and can take months, but I was just wondering what the longest time it has taken in your experience, since it has been 6 months and 3 days with no news. Also, would the reviewers take into account works that were published after the submission date? I am just worried that with the (understandably) slow reviews, I will be asked by the reviewer why I am not comparing against method XYZ, and asked to compare against said method, which could potentially outperform mine due to how fast the field progresses, and make revision and acceptance complicated.",8,6,1737099768.0,MachineLearning
1i3tr80,[P] How to import and deploy a pre-trained text-to-image model on Google Cloud for a high-traffic e-commerce project?,"Hello, I am working on an e-commerce project and I need a text-to-image model. I want to deploy this model on Google Cloud Platform (GCP), but this process seems quite new and complicated for me. Since I have limited time, I would like to know which of the following scenarios is more suitable:

Using ready-made GitHub models: For example, pre-trained models like Stable Diffusion. Can I import and use these models on GCP? If possible, can you share the recommended steps for this?

Google Cloud Marketplace: Would it be easier to buy a ready-made solution from GCP Marketplace? If so, what are the recommended APIs or services?

My goal:

To take inputs from user data (e.g. a string array) in the backend and return output via a text-to-image API.

Since I have an e-commerce project, I need a scalable solution for high traffic.

Information:

Backend: Requests will come via REST API.

My project allows users to create customized visuals (e.g. product designs).

Instead of training a model from scratch, I prefer ready-made solutions that will save time.

My questions:

Which way is more practical and faster? A ready-made model from GitHub or a solution from Google Cloud Marketplace?

If I prefer a model from GitHub, what steps should I follow to import these models to GCP?

How can I optimize a scalable text-to-image solution on GCP for a high-traffic application?

What platforms am I asking about:

If you have experience with Stable Diffusion or similar models, can you share them?

I would like to get suggestions from those who have started such a project on Google Cloud.",0,0,1737156112.0,MachineLearning
1i3han7,[P] Virtual Orientation session on EY Open Science AI & Data Challenge 2025,"[Join the upcoming Open Science AI & Data Challenge Virtual Orientation session on January 22nd 2025.](https://form.ey.com/243104302127945) Let's work together to cool down our cities and create healthier, more sustainable urban environments. Learn how the [2025 EY Open Science AI & Data Challenge](https://challenge.ey.com/2025) will help tackle the problem of urban heat islands through the application of AI and technology-based solutions. Winners are eligible for cash prizes and attendance at an exciting awards ceremony. [Register today!](https://challenge.ey.com/register)",0,0,1737123412.0,MachineLearning
1i31dwr,[P] I made a script to create GSM problems of any complexity.,"[Project github link](https://github.com/dattasid/grade-school-math-procedural)

[Here is a example](https://github.com/dattasid/grade-school-math-procedural/blob/main/datasets/examples/price_normal.md).

[Here is a example](https://github.com/dattasid/grade-school-math-procedural/blob/main/datasets/examples/price_easy.md) which uses simpler language, for testing if it is the confusing language that causes a model to fail.

Edit: Detailed post keeps getting removed. Please ask questions, hope someone finds this tool helpful.",13,4,1737066918.0,MachineLearning
1i2l0ey,[D] Titans: a new seminal architectural development?,"What are the initial impressions about their work? Can it be a game changer? How quickly can this be incorporated into new products?
Looking forward to the conversation!",90,53,1737018747.0,MachineLearning
1i2q6t9,[R] Multimodal Visualization-of-Thought: Enhancing MLLM Reasoning Through Visual Thinking,"The key innovation here is combining large language models with image generation to create a system that can ""visually think"" while solving problems. The approach, called Multimodal Visualization-of-Thought (MVoT), generates relevant visualizations during its reasoning process, similar to how humans might sketch diagrams to better understand a problem.

Main technical points:
- System architecture integrates LLMs for reasoning with image generation models
- Uses **spatial-semantic alignment** to ensure generated visuals match reasoning steps
- Implements an iterative process where each reasoning step can trigger visualization
- Maintains consistency between visual and textual representations through multimodal chain-of-thought

Results:
- 12% improvement on visual reasoning benchmarks compared to baseline approaches
- Particularly strong performance on tasks involving spatial relationships
- Generated visualizations showed clear alignment with reasoning steps
- Works with different combinations of language and image generation models

I think this approach could meaningfully improve AI systems' ability to reason about physical and spatial problems. By incorporating visual thinking into the reasoning process, we might see better performance on tasks that humans typically solve through visualization - from physics problems to architectural design. However, the computational overhead of generating images during reasoning could limit practical applications.

I think the most interesting aspect is how this mimics human cognitive processes - we often sketch or visualize to understand complex problems. This could lead to AI systems that reason in more intuitive and interpretable ways.

TLDR: New method combines language models with image generation to create AI systems that can ""think visually"" while reasoning, showing 12% improvement on visual reasoning tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/imagine-while-reasoning-space-multimodal-visualization-thought). Paper [here](https://arxiv.org/abs/2501.07542).",16,1,1737037918.0,MachineLearning
1i2h315,"Best way to classify NSFW text - BERT, small LLM like llama 3.2 3B or something else? [D]","I'm working on a project where I need to classify text as either nsfw or sfw. I know there are some BERT-based classifiers out there that are specifically trained for this kind of task. I've also seen people using smaller LLMs.  
What's the best approach for this? Since the underlying complexity of detecting NSFW text isn't that high, I'm thinking maybe a full blown LLM is overkill. What are your recommendations?",83,40,1737002280.0,MachineLearning
1i2nu5q,CIFAR 100 with MLP mixer. [P],Recently took part in a hackathon where was tasked with achieving a high accuracy without using Convolution and transformer models. Even though mlp mixers can be argued being similar to convolution they were allowed. Even after a lot of tries i could not take the accuracy above 60percent. Is there a way to do it either with mlp or with anything else to reach somewhere near the 90s.,14,17,1737030619.0,MachineLearning
1i23zbo,[P] How I found & fixed 4 bugs in Microsoft's Phi-4 model,"Hey r/MachineLearning! Last week, Microsoft released Phi-4, a 14B open-source model that rivals OpenAI's GPT-4-o-mini. I managed to find & fix 4 bugs impacting its output quality. You might remember me previously from [fixing 8 bugs](https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/) in Google's Gemma model! :)

I'm going to walk you through how I found & fixed the bugs. Phi-4's benchmarks were amazing, however many users reported weird or just wrong outputs. Since I maintain the open-source project called '[Unsloth](https://github.com/unslothai/unsloth)' (fine-tuning LLMs 2x faster with 70% less VRAM) with my brother, I firstly tested Phi-4 for inference and found many errors. Our GitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

This time, the model had no implementation issues (unlike Gemma 2) but did have problems in the model card. For my first inference run, I randomly found an extra token which is obviously incorrect (2 eos tokens is never a good idea). Also during more runs, I found there was an extra assistant prompt which is once again incorrect. And, lastly, from past experience with Unsloth's bug fixes, I already knew fine-tuning was wrong when I read the code.

These bugs caused Phi-4 to have some drop in accuracy and also broke fine-tuning runs. Our fixes are now [under review by Microsoft](https://huggingface.co/microsoft/phi-4/discussions/21) to be officially added to Hugging Face. We uploaded the fixed versions to [https://huggingface.co/unsloth/phi-4-GGUF](https://huggingface.co/unsloth/phi-4-GGUF)

Here’s a breakdown of the bugs and their fixes:

***1. Tokenizer bug fixes***

The Phi-4 tokenizer interestingly uses <|endoftext|> as the BOS (beginning of sentence), EOS (end of sentence) and PAD (padding) tokens. The main issue is the EOS token is wrong - it should be <|im\_end|>. Otherwise, you will get <|im\_end|><|endoftext|> in generations.

***2. Fine-tuning bug fixes***

The padding token should be a designated pad token like in Llama (<|finetune\_right\_pad\_id|>) or we can use an untrained token - for example we use <|dummy\_87|>, fixing infinite generations and outputs.

***3. Chat template issues***

The Phi-4 tokenizer always adds an assistant prompt - it should only do this if prompted by add\_generation\_prompt. Most LLM serving libraries expect non auto assistant additions, and this might cause issues during serving.

**We dive deeper into the bugs in our blog:** [**https://unsloth.ai/blog/phi4**](https://unsloth.ai/blog/phi4)

# Do our Fixes Work?

Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4).

https://preview.redd.it/d8hew26e06ce1.png?width=2366&format=png&auto=webp&s=173c23feacc625566271470839fe7a5e25eb860e

Some redditors even tested our fixes to show greatly improved results in:

* Example 1: [Multiple-choice tasks](https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/)

https://preview.redd.it/qx50pkq706ce1.png?width=1579&format=png&auto=webp&s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316

* Example 2: [ASCII art generation](https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/)

https://preview.redd.it/sw1o3a3yt4de1.png?width=2326&format=png&auto=webp&s=fc6bfc45d14134d45f332ba58bbd1de049f5776b

We also made a Colab notebook fine-tune Phi-4 completely for free using Google's free Tesla T4 (16GB) GPUs: [https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi\_4-Conversational.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)

Thank you for reading this long post and hope you all found this insightful! If you have any questions, please feel free to ask! :)

**How I found the bugs:**

1. I first downloaded the original Phi-4 from [https://huggingface.co/microsoft/phi-4](https://huggingface.co/microsoft/phi-4), and tested inference out. Weirdly I found `<|im_start|>assistant<|im_sep|>` to be appended at the even with `add_generation_prompt = False` in Hugging Face, so I theorized there was a chat template problem. Adding assistant prompts by default can break serving libraries.
2. And yes, [https://huggingface.co/microsoft/phi-4/blob/f957856cd926f9d681b14153374d755dd97e45ed/tokenizer\_config.json#L774](https://huggingface.co/microsoft/phi-4/blob/f957856cd926f9d681b14153374d755dd97e45ed/tokenizer_config.json#L774) had by default added the assistant prompt - I first fixed this!
3. I then found `<|endoftext|>` to be used for the BOS, EOS and PAD tokens, which is a common issue amongst models - I ignored the BOS, since Phi-4 did not have one anyways, but changed the PAD token to `<|dummy_87|>`. You can select any of the tokens since they're empty and not trained. This counteracts issues of infinite generations during finetuning.
4. For Llama-fication, I used torch.allclose to confirm all tensors are in fact equivalent. I also used some fake random data to check all activations are also mostly similar bitwise. I also uploaded the model to the HF [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4) to confirm if the original Phi-4 arch and the new Llama-fied models are equivalent.
5. Finally I verified all finetuning runs with Unsloth in a [Colab Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb) to confirm all runs were correct.

",305,28,1736965368.0,MachineLearning
1i2qmdf,"[R] Virgo: A Preliminary Exploration on
Reproducing o1-like MLLM","**TL;DR:** A reasoning multimodal model built from Qwen2-VL-72B. Surprisingly, beats QVQ in evals.

**Paper:** [https://arxiv.org/pdf/2501.01904](https://arxiv.org/pdf/2501.01904)

**Abstract:**

>Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.  
To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at [this https URL](https://github.com/RUCAIBox/Virgo).

**Highlights:**

>\[W\]e obtain approximately 5K long thought instruction instances distilled from two open slow-thinking reasoning systems: DeepSeek-R1-Lite-Preview \[2\] (abbreviated as R1) and QwQ-32B-preview \[3\] (abbreviated as QwQ). The statistics of the collected instruction data are categorized by domain as follows: math (3.7K), science (0.9K), code (0.2K) and puzzle (0.1K). \[...\]

>After collecting instruction data for long-form reasoning, we fine-tune the base MLLM to emulate slow-thinking reasoning behavior. \[...\]

>The second approach we explore is the direct distillation of multimodal long thought data from slow-thinking MLLMs (e.g., QVQ). \[...\]

>As another alternative approach, we design a multi-stage tuning method for self-distillation. Specifically, we first fine-tune the selected MLLM (i.e., Qwen2-VL-72B-Instruct) on the textual long thought instruction set DT, obtaining model M0. Next, we use M0 to generate the visual long thought instruction set by self-distillation DSD, which can be subsequently used for fine-tuning the original MLLM.

**Visual Highlights:**

https://preview.redd.it/2o4v0y2tcdde1.png?width=1123&format=png&auto=webp&s=b3ac8bbb69248d8c326a73553cbccb0c5ab12d46

https://preview.redd.it/j3wyeg0ucdde1.png?width=967&format=png&auto=webp&s=98b2ca3dedb6c99c078491e67e53df6343f074c3

https://preview.redd.it/m6385vuucdde1.png?width=1191&format=png&auto=webp&s=4447b132fb7484d1375403916c3dcd8a9c2b8e17

[Looks a bit chaotic, if you ask me](https://preview.redd.it/q2dgxrlvcdde1.png?width=929&format=png&auto=webp&s=0f8ec5a05dc7feee71ebe57cb0ae2cb18dfd2051)

",6,0,1737039166.0,MachineLearning
1i2n50v,[D] Best Text-to-Sound-Effects model (MIT license or equivalent),"Hi there !
I've been looking around for a MIT (commercially available) model for Text-to-Sound-Effects (Text-to-Audio) and haven't found much, besides the traditional stable-Audio-Open (with its special license)

Do you know any other ?
",8,3,1737027976.0,MachineLearning
1i389km,[D] How to analysis memory and computation cost by parts in LLM fine-tune?,"[Flash-attention paper](https://arxiv.org/pdf/2205.14135) show that “most operation in Transformers are botlenecked by memory accesses”

[Cut cross entropy](https://arxiv.org/abs/2411.09009) show that “The cross-entropy loss is responsible for up to 90% of the memory footprint of modern LLM training”


How to get these data, is there a tool or platform which can show the cost by parts in LLM, like embedding, attention, layer normalazation, loss computation?

Purpose: After know that, we will know which part we should accelerate at first and can pay more attention on it. 

Thanks for any suggestion",0,2,1737087716.0,MachineLearning
1i2lk5n,[P] AutoResearch: A new open-source LLM-driven research automation tool,"Hello, everyone

I recently developed a new open-source LLM-driven research automation tool, called AutoResearch. It can automatically conduct various tasks related to machine learning research, the key function is:

[Topic-to-Survey Automation](https://jlx0.github.io/auto_research/_examples_gallery/top_to_survey.html) \- In one sentence, **it converts a topic or research question into a comprehensive survey of relevant papers.** It generates keywords, retrieves articles for each keyword, merges duplicate articles, ranks articles based on their [impacts](https://jlx0.github.io/auto_research/target_code/auto_research.search.core.html#auto_research.search.core.AutoSearch.score_threshold), summarizes the articles from the topic, method, to results, and optionally checks code availability. It also organizes and zips results for easy access.

When searching for research papers, the results from a search engine can vary significantly depending on the specific keywords used, even if those keywords are conceptually similar. For instance, searching for ""LLMs"" versus ""Large Language Models"" may yield different sets of papers. Additionally, when experimenting with new keywords, it can be challenging to remember whether a particular paper has already been checked. Furthermore, the process of downloading papers and organizing them with appropriate filenames can be tedious and time-consuming.

This tool streamlines the entire process by automating several key tasks. It suggests multiple related keywords to ensure comprehensive coverage of the topic, merges duplicate results to avoid redundancy, and automatically names downloaded files using the paper titles for easy reference. Moreover, it leverages LLMs to generate summaries of each paper, saving researchers valuable time and effort in uploading it to ChatGPT and then conversing with it in a repetitive process.

Additionally, there are some basic functionalities:

* [Automated Paper Search](https://jlx0.github.io/auto_research/_examples_gallery/search_papers.html) \- Search for academic papers using keywords and retrieve metadata from Google Scholar, Semantic Scholar, and arXiv. Organize results by relevance or date, apply filters, and save articles to a specified folder.
* [Paper Summarization](https://jlx0.github.io/auto_research/_examples_gallery/summarize_a_paper.html) \- Summarize individual papers or all papers in a folder. Extract key sections (abstract, introduction, discussion, conclusion) and generate summaries using GPT models. Track and display the total cost of summarization.
* [Explain a Paper with LLMs](https://jlx0.github.io/auto_research/_examples_gallery/explain_a_paper.html) \- Interactively explain concepts, methodologies, or results from a selected paper using LLMs. Supports user queries and detailed explanations of specific sections.
* [Code Availability Check](https://jlx0.github.io/auto_research/_examples_gallery/get_github_link.html) \- Check for GitHub links in papers and validate their availability.

This tool is still under active development, I will add much more functionalities later on.

I know there are many existing tools for it. But here are the **key distinctions and advantages** of the tool:

* [Free and open-source](https://github.com/JLX0/auto_research/)
* Pure Python code-base, which enables convenient deployment, such as [Google Colab notebook](https://colab.research.google.com/drive/1Xj0xTpHvpnPfmK9tYnI8Ep7oRKrQ9gn7?usp=sharing)
* [API documentation](https://jlx0.github.io/auto_research/target_code/auto_research.html#module-auto_research) are available
* No additional API keys besides LLM API keys are required (No API keys, such as Semantic Scholar keys, are needed for literature search and downloading papers)
* Support multiple search keywords.
* Rank the papers based on their [impacts](https://jlx0.github.io/auto_research/target_code/auto_research.search.core.html#auto_research.search.core.AutoSearch.score_threshold), and consider the most important papers first.
* Fast literature search process. It only takes about 3 seconds to automatically download a paper.

**------Here is a quick installation-free** [**Google Colab demo**](https://colab.research.google.com/drive/1Xj0xTpHvpnPfmK9tYnI8Ep7oRKrQ9gn7?usp=sharing)**------**

Here is the [official website of AutoResearch](https://jlx0.github.io/auto_research/).

Here is the [GitHub link to AutoResearch](https://github.com/JLX0/auto_research/).

**------Please star the** [**repository**](https://github.com/JLX0/auto_research/) **and share it if you like the tool!------**

Please DM me or reply in the post if you are interested in collaborating to develop this project!",5,7,1737021315.0,MachineLearning
1i33y8v,[R] Seeking a Knowledgeable Co-Author for Time Series Foundation Models Research,"Hello,

I am conducting research that I plan to submit to the AHLI Conference on Health, Inference, and Learning (CHIL) (H5-index 26, h5-median 43). However, the submission deadline is approaching quickly—February 10.

My advisor has suggested adding other professors as co-authors, but they would primarily review and provide feedback rather than directly contributing to the writing. Therefore, I am reaching out to see if anyone with expertise in time series foundation models would be interested in collaborating as a co-author.

The research involves comparing time series foundation models across different datasets. The experiments are nearly complete, but I need support in writing the theoretical foundation for each model. If you have the necessary knowledge, time, and interest in contributing meaningfully to this work, please send me a private message so we can discuss this opportunity further.

Thank you!",0,3,1737074146.0,MachineLearning
1i210hp,"Kaggle dataset: one of the input features has a >0.99 correlation with the target, yet most/all notebooks (20+) do not care? [D]","There is this dataset (won't link here as I don't want my kaggle and reddit associated) with a few input features (5-6) used to predict one target value.

But one of the features is basically perfectly linearly correlated with the target (>0.99).

An example would be data from a trucking company with a single model of trucks:

Target: truck fuel consumption / year Features: driver's age, tires type, truck age, DISTANCE TRAVELED / year

Obviously in average the fuel consumption will be linearly proportional with the nb of miles traveled. I mean normally you'd just use that to calculate a new target like fuel/distance.

Yet not a single person/notebook did this kind of normalization. So everyone's model has >.99 accuracy, as that one feature drowns out everything else.

Is that something other people noticed: more and more the code looks fine (Data loading, training many types of models), maybe thanks to LLMs. But the decision making process is often quite bad?",97,26,1736957850.0,MachineLearning
1i2mcy1,"Good People of this reddit who worked with multiple adapters on the same model, guide me with your wisdom [D]",How do you deal with multiple adapters created for different tasks? I understand task id based dynamic loading of the appropriate adapter is obvious but is there a better way? I am especially asking for whisper,3,1,1737024846.0,MachineLearning
1i2t0gj,[D] Platform for Multimodal Dataset Upload?,"What do you guys use to upload Multimodal Dataset?

I want it to be convenient for the people who use it. For the text, huggingface dataset is the best convenient solution, but I cant find any convenient solution for Multimodal (Image + Video + Audio + Text) datast.

Thanks in advance.",0,2,1737045440.0,MachineLearning
1i351hc,"[D] How often are you babysitting your models? 🤔
","Hey yall! I'm curious, how often are you kicking off a new training runs?   
Once a week? Twice a week? Everyday?   
Would love to hear about your experience! ",0,7,1737077393.0,MachineLearning
1i279gb,[R] Imagine while Reasoning in Space: Multimodal Visualization-of-thought,"Abstract:

Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high- quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.

Arxiv link: https://arxiv.org/pdf/2501.07542",31,0,1736973785.0,MachineLearning
1i1xgp2,[R] MathReader: A Text-to-Speech System for Mathematical Documents Using OCR and Fine-tuned T5,"Interesting new text-to-speech system that tackles mathematical content by combining OCR and language models. The key innovation is treating mathematical notation as a specialized language that needs translation, using a multi-stage pipeline to convert equations into natural speech.

Technical approach:
* Custom OCR model trained specifically on mathematical documents
* T5-based language model fine-tuned for math-to-text translation
* Three-stage pipeline: recognition → translation → synthesis
* Integration with LaTeX parsing for handling complex mathematical typography

Key results:
* 95% accuracy in mathematical expression recognition
* Successful handling of complex notation including fractions, integrals, matrices
* User testing showed preference over existing math TTS systems
* Natural language output matches human descriptions

I think this could be impactful for making technical education more accessible. Being able to convert mathematical documents to clear speech opens up some possibilities for learning and working with technical content. The combination of OCR and NLP seems maybe like a robust approach that could extend beyond just mathematics to other technical domains with specialized notation.

I see some limitations around context-dependent notation and complex proofs, but these seem like natural areas for future work rather than fundamental flaws in the approach.

TLDR: New TTS system combines specialized OCR and language models to convert mathematical documents to natural speech, achieving 95% accuracy in math recognition and producing human-like descriptions.

[Full summary is here](https://aimodels.fyi/papers/arxiv/mathreader-text-to-speech-mathematical-documents). Paper [here](https://arxiv.org/abs/2501.07088).",19,0,1736947990.0,MachineLearning
1i1l8d4,[R] Transformer²: Self-Adaptive LLMs,"Paper: https://arxiv.org/abs/2501.06252

**Abstract**

Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer², a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer² employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific ""expert"" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer² demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer² represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. 

Blog Summary: https://sakana.ai/transformer-squared/

GitHub: https://github.com/SakanaAI/self-adaptive-llms",186,13,1736901669.0,MachineLearning
1i26qvt,Best AutoML for Supervised Regression of Tabular Data [P],"I have a simple dataset that I want to train a prediction model on for a pretty low stakes project (more for fun), but I have no experience training ML models. Simple linear regression didn't have great performance when I tried it and I suspect there is a more complex interaction between the variables.

Training Dataset: 25K observations of 5 numerical predictor variables with one 1 numerical outcome variable.

What is the best AutoML platform that I can run this with minimal code, just to see if ML models can perform better than simple regression can? Thanks!",4,4,1736972414.0,MachineLearning
1i21t17,[R] Explainable GNNs in Job Recommender Systems: Tackling Multi-Stakeholder Challenges,"Can explainable AI balance competing needs in job recommendation systems? Models like OKRA, powered by GNNs, deliver stakeholder-specific insights - text explanations for candidates, skill alignment for recruiters, and visualizations for companies. They address biases (e.g. rural underrepresentation) and challenges like integrating explanations with source data (CVs, vacancies). 

Future directions focus on refining explanation coherence, fairness metrics, and real-world validation, pushing explainable multi-stakeholder AI towards equitable, context-aware job matching.

We unpack *""Explainable Multi-Stakeholder Job Recommender Systems""* by *Roan Schellingerhout* here: [https://www.shaped.ai/blog/decoding-job-recommendations-the-future-of-explainable-multi-stakeholder-ai](https://www.shaped.ai/blog/decoding-job-recommendations-the-future-of-explainable-multi-stakeholder-ai)",5,0,1736959904.0,MachineLearning
1i236iq,[D] Dilemma b/w JAIR vs Pattern Recognition Journal,"Dear All, 

I am a UG student and I want to submit my manuscript to any of these two journals; the work is on the interplay of privacy and explainability in machine learning (would be more than happy to send you the arXived version of the same on request). I have previously published in a very reputed workshop of EMNLP and came to know that mostly ML nowadays is a conference-centric discipline. I want to know which of these two will be better to submit my work (due to the length and scope, I am unable to submit to conferences this time). I cannot submit it to tmlr until it's Scopus-indexed and not considering AIJ and Machine Learning Journal at this moment. 

I just want to make sure that if the paper gets accepted, I want this to be at least comparable with a borderline A\* paper (in terms of the so-called *prestige* of the venue). Also, let me know if you have any other suggestions; I am new to journals and I appreciate your opinion.

P.S.: My guide slightly prefers PR to JAIR due to its higher IF but nevertheless, he is open JAIR or any other Scopus-indexed journals as long as it is comparable with at least a borderline A\* or very strong A conf paper as said. ",4,11,1736963354.0,MachineLearning
1i1lg6o,[R] Titans: Learning to Memorize at Test Time,"Abstract: “Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps an attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of a fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.”

Arxiv: https://arxiv.org/abs/2501.00663",68,4,1736902312.0,MachineLearning
1i1hz8c,[D] How are people searching for papers in ArXiv? ,"Hello,

I am wondering what is the usual way people search for or discover new papers in ArXiv? Do you just use their search engine? Any tips/hints?",45,29,1736892699.0,MachineLearning
1i18421,[D] Machine Learning Engineer vs AI Research Scientist: Future Prospects?,"Some people say that AI research scientists (PhD holders) are pretty much irreplaceable because of their ability to push the boundaries of knowledge and come up with groundbreaking methods and algorithms. But let’s be real—tech companies don’t need a ton of researchers, especially if their work doesn’t directly boost profits.

On the flip side, Machine Learning Engineers are the ones putting those algorithms into action, scaling systems, and keeping production pipelines running—all things that directly bring in the $$$. That’s why some people think MLE roles will grow faster than AI research scientist roles in the future.

What do you think? Are there trends or experiences you’ve seen that suggest one of these roles will be more in demand down the line?

For a fair comparison, let’s assume both roles are at a FAANG company.",126,72,1736867312.0,MachineLearning
1i22zdu,What percent of your paper is written by AI and is it unethical?[D],"Hello folks,

Just started contributing into the writing for research, previously I just used to experiment and work on results, tables and plots.

Obviously using AI to generate content for paper is unethical and wrong in many aspect. But what about using it to correct your grammar and comprehension. Technically it will also considered as AI written but is it okay to do this atleast in the literature review, introduction and description for the experiment?

To be honest, I like writing and when I asked AI (chatgpt and others) I see that it is much easier to read and interpret, which I think is good for the community and on the other hand, it may be considered unethical by many.

When I ran a 'AI-text detector' on many of paper I'm using as reference from last 1~ year, I usually get a 50-70% score. 

What do you all think?

",0,28,1736962864.0,MachineLearning
1i1gr9y,[D]How do you measure improvements of your AI pipeline?,"I am very creative when it comes to adding improvements to my embedding or inference workflows, but  I am having problems when it comes to measuring whether those improvements really make the end result better for my use case. It always comes down to gut feeling.

How do you all measure...

..if this new embedding model if better than the previous?

..if this semantic chunker is better than a split based one?

..if shorter chunks are better than longer ones?

..if this new reranker really makes a difference?

..if this new agentic evaluator workflow creates better results?

**Is there a scientific way to measure this?**",6,5,1736889549.0,MachineLearning
1i1act4,Pre-trained models for Sentiment classification and analysis [Discussion],"Hi. I am working on a project which requires me to identify sentiments from English text and then quantify those sentiments as percentage. I need to run six models on the text and then compare the classifications.

So far, I have explored some BERT and RoBERTa based models in Huggingface, which are trained using the GoEmotion dataset provided by Google. I was curios, are there any better models that I am missing? Please leave the name of some pre-trained models which can give some good results.

TIA!",5,1,1736873202.0,MachineLearning
1i0hfsd,[R] Cosine Similarity Isn't the Silver Bullet We Thought It Was,"Netflix and Cornell University researchers have exposed significant flaws in cosine similarity. Their study reveals that regularization in linear matrix factorization models introduces arbitrary scaling, leading to unreliable or meaningless cosine similarity results. These issues stem from the flexibility of embedding rescaling, affecting downstream tasks like recommendation systems. The research highlights the need for alternatives, such as Euclidean distance, dot products, or normalization techniques, and suggests task-specific evaluations to ensure robustness. 

Read the full paper review of 'Is Cosine-Similarity of Embeddings Really About Similarity?' here: [https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was](https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was)",444,52,1736784660.0,MachineLearning
1i194ja,[D] Non-Person Action Recognition,"I am working on getting object tracking working for a sports game, and would like to take the next step and be able to detect when an action has taken place (like a soccer ball has gone out of bounds, of a bowling ball has hit pins, or a ball has been thrown (as opposed to a practice throw or pump fake).  I have been doing these by hand coding heuristics for how to detect, but I would like to be more flexible.  All the libraries for action recognition seem to be about human skeleton actions.  That makes me think I am looking at the wrong problem space.  Is there existing art for taking locations of objects over time and learning when an action is taking place given training data?",2,1,1736870033.0,MachineLearning
1i1aarq,[D] Predicting the probability of default for a credit card user,"I have an imbalanced dataset of about 100,000 rows 1500 of them are of defaultes, which has more than 1000 features and has lots of missing values. Alsothe name of the features are anonymized (like bureau\_1, bureau\_2)  so it also seems difficult and these feaures had max correlation of 0.1 with the target variable

I want to predict the probability of a customer who might default based on the data but am not able to make much progress in terms of metrics like recall (0.25), f1 and auprc.  
I have tried various tree based models like lgbm, xgboost etc with various class balance attributes but its not giving me that good of results.

If anyone of you have such prior experience of handling such datasets, can you suggest me what should i do in terms of feature engineering, modelling etc. All of your help will mean a lot to me.",0,15,1736873056.0,MachineLearning
1i13aot,[D]  Correlation clustering?,"I wanted to apply clustering algorithms on a similarity matrix. Is that possible? If yes, how?",3,8,1736851054.0,MachineLearning
1i0vrg3,LLM Distributed Training [R],"What are the approaches to access datasets during training? Are they downloaded to the machines/pods before starting the training process or are they network mounted?

Similarly for large models how do the models are deployed for inference? ( for auto scaling or for updating the model version)",6,5,1736821786.0,MachineLearning
1i1fwsg,[D] Prove me wrong…,"I’ve built an LGBM model to classify Parkinson’s patients using a dataset of 2,562 patients with 37 features selected through P value and correlation analysis and my own domain knowledge, questions can be binary, continuous or ordinal eg do they have Urinary Problems yes/no = 0/1, all questions are numerical answers.  The dataset was split into 70% training (1,793 samples), 15% validation (384 samples), and 15% hold-out test (385 samples). I performed 5-fold stratified cross-validation on the training set, with approximately 1,434 samples for training and 359 for validation in each fold. The dataset contains 1085 PD patients and 1477 non-PD patients. I think the performance is really good, I'm wondering if anyone has any additional tests or methods to assess whether it's a big fantasy or have I a good model on my hands?

.=== Cross-Validation Metrics ===

Mean F1 Score: 0.8860 ± 0.0210

Mean AUC: 0.9565 ± 0.0095

Mean Recall: 0.8814 ± 0.0239

Mean Precision: 0.8911 ± 0.0251 

=== Hold-Out Set Metrics ===

F1 Score: 0.8875

AUC: 0.9505

Recall: 0.8957

Precision: 0.8795",0,33,1736887221.0,MachineLearning
1i0u5sv,NannyML chunking [D],"Does anyone have experience with the NannyML library? I am having a difficult time fully grasping the reasoning behind forcing users to split data into chunks. I haven’t seen any other drift detection libraries do this. 

Let’s say I have a model on which I would like to perform drift detection. I have some reference feature data from some time ago, and some analysis feature data from today. It seems that to use this library, I am required to split these 2 datasets into arbitrary chunks (they recommend at least 6). I would actually like to perform drift detection by comparing both sets of data to each other as a whole, however. This doesn’t work - forcing the chunk size to 1 results in the upper_threshold value to be set to 0 and every feature gets alerted on.

It seems like the library is geared towards comparing some number of reference datasets  across time vs some equal number of analysis datasets across time… but doesn’t work if there is only have 1 analysis dataset (for 1 date). What am I missing here? Any help much appreciated!",6,2,1736817040.0,MachineLearning
1i0g71d,[Project] Hallucination Detection Benchmarks,"Hi Everyone, I recently noticed most LLM observability providers (Arize AI, Galileo AI, LangSmith) use a simple LLM-as-a-Judge framework to detect hallucinations for deployed RAG applications. There's a ton of hallucination detection research out there like [this](https://arxiv.org/abs/2311.05232) or [this](https://arxiv.org/abs/2403.16527) survey, so I wondered why aren't any of these providers offering more advanced research-backed methods? Given the user input query, retrieved context, and LLM output, one can pass this data to another LLM to evaluate whether the output is grounded in the context. So I benchmarked this LLM-as-a-Judge framework against a couple of research methods on the HaluBench dataset - and turns out they're probably right! A strong base model with chain-of-thought prompting seems to work better than various research methods. [Code here](https://github.com/liuzihe02/halu/tree/main). Partial results:

|Framework|Accuracy|F1 Score|Precision|Recall|
|:-|:-|:-|:-|:-|
|Base (GPT-4o)|0.754|0.760|0.742|0.778|
|Base (GPT-4o-mini)|0.717|0.734|0.692|0.781|
|Base (GPT-4o, sampling)|0.765|0.766|0.762|0.770|
|CoT (GPT-4o)|**0.833**|**0.831**|**0.840**|0.822|
|CoT (GPT-4o, sampling)|0.823|0.820|0.833|0.808|
|Fewshot (GPT-4o)|0.737|0.773|0.680|**0.896**|
|Lynx|0.766|0.780|0.728|0.840|
|RAGAS Faithfulness (GPT-4o)|0.660|0.684|0.639|0.736|
|RAGAS Faithfulness (HHEM)|0.588|0.644|0.567|0.744|
|G-Eval Hallucination (GPT-4o)|0.686|0.623|0.783|0.517|",28,12,1736781378.0,MachineLearning
1i0ju9b,[P] Geometric Intuition for Dot Product,"Hi Community,

First, I want to thank you for reading my earlier posts on geometric intuition and receiving with worms! I didn't expect to receive so much good feedback and also different explanations in the comment. I learned so much!

Motived by this, I wrote another post for geometric intuition and this time about ""**Dot Product**"". Here is the link [https://maitbayev.github.io/posts/dot-product/](https://maitbayev.github.io/posts/dot-product/)

Let me know what you think",13,1,1736790606.0,MachineLearning
1i0cd4n,[P] Fast Semantic Text Deduplication,"Hi! A friend and I have been working on a project called [SemHash](https://github.com/MinishLab/semhash) which I wanted to share. We found that text deduplication is more complex than it appears, so we built this to simplify the process.

Duplicate samples can skew model training, return redundant samples in RAG workflows, reduce generalization, and cause train-test leakage—leading to unreliable results. Techniques like minhash handle exact or near-exact duplicates, but semantic deduplication also catches semantically redundant samples, which we believe is an important aspect of deduplication. Furthermore, it’s not trivial to see why something was removed with minhash, which we also believe is important. For this reason. we’ve added explainability features as well so that you can inspect why something was removed. We already found some interesting results on some well known datasets in our benchmarks which are included in the repo.

The package can be installed with `pip install semhash`, and the basic usage looks like this (this example assumes you have the `datasets` library installed):

    from datasets import load_dataset
    from semhash import SemHash
    
    # Load a dataset to deduplicate
    train = load_dataset(""ag_news"", split=""train"")[""text""]
    test = load_dataset(""ag_news"", split=""test"")[""text""]
    
    # Initialize a SemHash instance
    semhash = SemHash.from_records(records=train)
    
    # Deduplicate the train set
    deduplicated_train = semhash.self_deduplicate().deduplicated
    
    # Or deduplicate the test set against the train set
    deduplicated_test = semhash.deduplicate(records=test).deduplicated

I’m very interested in hearing your thoughts on this! Is deduplication a part of your current ML workflows, and if so, what techniques do you use?",22,3,1736769562.0,MachineLearning
1i16ud7,[D] How to convince the stakeholders that our ML solutions is good enough? ,"Over the past year, we developed a solution designed to be a companion for data analysts, helping them manage and analyze their data. However, I’m struggling to demonstrate its reliability, as it occasionally fails to function properly.",0,11,1736863713.0,MachineLearning
1i162tn,[P] What is RF and How to Implement it?,"If you're building an LLM application that handles complex or ambiguous user queries and find that response quality is inconsistent, you should try **RAG Fusion**!

The standard RAG works well for straightforward queries: retrieve ***k*** documents for each query, construct a prompt, and generate a response. But for complex or ambiguous queries, this approach often falls short:

* Documents fetched may not fully address the nuances of the query.
* The information might be scattered or insufficient to provide a good response.

This is where **RAG Fusion** could be useful! Here’s how it works:

1. **Breaks Down Complex Queries:** It generates multiple sub-queries to cover different aspects of the user's input.
2. **Retrieves Smarter:** Fetches *k*\-relevant documents for each sub-query to ensure comprehensive coverage.
3. **Ranks for Relevance:** Uses a method called **Reciprocal Rank Fusion** to score and reorder documents based on their overall relevance.
4. **Optimizes the Prompt:** Selects the top-ranked documents to construct a prompt that leads to more accurate and contextually rich responses.

We wrote a detailed blog about this and published a Colab notebook that you can use to implement RAG Fusion - **Link in comments!**",0,4,1736861424.0,MachineLearning
1i16f1c,[R] Mastering Machine Learning System Design: A Comprehensive Guide for Scalable AI Solutions,"**Key Highlights**

1. **What to Expect in ML Interviews**

• Problem-solving, system design, and hands-on ML experience.

• Real-world examples from top tech companies like Google and LinkedIn.

2. **Why ML System Design Matters**

• Addresses **scalability**, **reliability**, and **optimization** for millions of users.

• Explores scenarios like LinkedIn’s Feed Ranking and YouTube’s Recommendation System.

3. **Step-by-Step Guide to ML System Design**

• **Define the Problem Statement**: Clarify goals and assumptions.

• **Identify Metrics**: Choose relevant metrics (e.g., AUC, CTR).

• **Determine Requirements**: Training and inference needs.

• **Design High-Level Systems**: Outline components and data flow.

• **Scale the Design**: Optimize for bottlenecks and high traffic.

4. **Real-World Example**: YouTube Recommendation System

• Candidate Generation Service, Ranking Model, and Recommendation API.



**Key Takeaways**

• **Modular Design**: Ensure components can scale or be replaced independently.

• **Real-Time Inference**: Build low-latency systems (<100ms).

• **Bottleneck Identification**: Proactively address system limitations.

• **Monitoring & Maintenance**: Automate model drift detection and retraining.

[🔗 Machine Learning System Design Introduction](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)🔗 [**Machine Learning System Design Introduction**](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)

This article is a must-read for mastering ML system design and preparing for interviews at top tech firms.",0,2,1736862461.0,MachineLearning
1hzshvp,[P] I made pkld – a cache for expensive/slow Python functions that persists across runs of your code,,129,41,1736704664.0,MachineLearning
1i0nmax,"[D] In ""Speculations on Test-Time Scaling (o1)"", shouldn't this equation be E_(y~p(·|,z_(1:t),x))[Ver(y)]? Adding z_(1:t) into the expectation value equation's subscript. Because it depends on it.","In ""Speculations on Test-Time Scaling (o1)"" https://youtu.be/6PEJ96k1kiw?si=-bA2KTKbc0kPJqYX&t=1085 , in the context of https://imgur.com/2t94rWF , shouldn't the equation in https://imgur.com/6AODoeq be E\_(y~p(·|,z\_(1:t),x))[Ver(y)]? Adding z\_(1:t) into the expectation value equation's subscript. Because it depends on it.",1,0,1736799841.0,MachineLearning
1i0et7v,[D] Anisotropic periodic kernel in Python with Sklearn,"Hello,

I am using sklearn in Python to perform Gaussian Process Regression (GPR) on some ocean variables through the GaussianProcessRegressor class. The domain of the parameters is a 3D spacetime domain (latitude, longitude, and time), so I am using an anisotropic kernel for the regression since the three dimensions are quite different. For example:

\# Define the kernel kernel = C(1.0, (1e-3, 1e3)) \* Matern( nu=1.5, length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=\[(1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)\] )

\# Initialize the GPR

gpr = GaussianProcessRegressor(kernel=kernel, n\_restarts\_optimizer=5, alpha=alpha)

Watching the results at a specific location in time (fixed latitude and longitude, looking at the time series) of the predicted versus the real values, I think adding a periodic kernel in time may improve the results. This assumption makes sense as the parameters could exhibit time periodicity (e.g., wind speed).

I tried implementing this using an ExpSineSquared kernel, but it doesn't allow for anisotropy (I was thinking of setting it with very high bounds for periodicity in latitude and longitude so that it would effectively be neglected). However, the documentation states that the function does not support different length scales and periodicity for different dimensions.

Here is an example of what I tried:

\# Define the Matern kernel

matern\_3d = Matern( length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), nu=1.5 )

\# Define the ExpSineSquared kernel

expsine\_3d = ExpSineSquared( length\_scale=\[1.0, 1.0, 1.0\], periodicity=\[1e6, 1e6, 24.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), periodicity\_bounds=((1e5, 1e8), (1e5, 1e8), (12.0, 48.0)) )

\# Combine the kernels

kernel = (C(1.0, (1e-3, 1e3)) \* matern\_3d) + (C(1.0, (1e-3, 1e3)) \* expsine\_3d)

However, this results in an error since ExpSineSquared does not support different length scales and periodicities across dimensions. Has anyone encountered this problem before? Do you know of another function or library that could allow this kind of anisotropic periodic kernel? Thanks in advance!",3,0,1736777492.0,MachineLearning
1hzn0gg,[D] Have transformers won in Computer Vision?,"Hi,

Transformers have reigned supreme in Natural Language Processing applications, both written and spoken, since BERT and GPT-1 came out in 2018.

For Computer Vision, last I checked it was starting to gain momentum in 2020 with [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) but the sentiment then was ""Yeah transformers might be good for CV, for now I'll keep using my resnets""

Has this changed in 2025? Are Vision Transformers the preferred backbone for Computer Visions?

Put another way, if you were to start a new project from scratch to do image classification (medical diagnosis, etc), how would you approach it in terms of architecture and training objective?

I'm mainly an NLP guy so pardon my lack of exposure to CV problems in industry.  ",188,82,1736689650.0,MachineLearning
1hzyjw1,[R] Search-o1: Agentic Search-Enhanced Large Reasoning Models - Renmin University of China,,37,4,1736720043.0,MachineLearning
1hzychc,"[R] optimizing looser bounds on train data, achieves better generalization","I have encountered times that when optimizing with looser bounds, one can get better performance on test data. For example, in this paper:

[https://arxiv.org/pdf/2005.07186](https://arxiv.org/pdf/2005.07186)

authors state: ""It seems that, at least for misspecified models such as overparametrized neural networks, training a looser bound on the log-likelihood leads to improved predictive performance. We conjecture that this might simply be a case of ease of optimization allowing the model to explore more distinct modes throughout the training procedure.""

more details can be found below eq 14 in the appendix.

are there other problems where one has drawn a similar observation?

thanks!",20,3,1736719487.0,MachineLearning
1i0d26d,[P] Is it viable to use customer-declared information as proxy ML labels?,"**CONTEXT:**

Sort of a high-level hypothethical ML training data question: Let's say a company has adult customers and child customers. 90% of customers are adults, and 10% of them are children.\*

The problem is that whether a customer is an adult or child is declared by the customer, the company has no way of knowing the truth. Some children pretend to be adults, as it benefits them, but no adults pretend to be children. Thus the company wants to use ML to find the children pretending to be adults, using various other customer details as features.

**QUESTION:**

The question is, is it worth training a model with this proxy label of how they declared themselves, even though the training set will include children pretending to be adults? (Worth noting that we know that only about 1% of those declared as adults are actually children, ie. about 9% of children are pretending to be adults)

Obviously a MUCH better way to do this would be to have a labelled training set of confirmed adults and children, but there's no way of getting a labelled dataset, all we have is whether customers declared themselves as adults or children.

So what do we think? Is it a non-starter? Or might the 99% of true adults effectively drown-out the 1% of false adults, resulting in a viable model? Asuming the features and model type are otherwise apropriate.

Needless to say we're never going to get a great model, but we just need a model that will give us substantially higher than the 9% baseline, since the alternative is doing blind checks on small samples of customers. It feels wrong but I can't think of an alternative given the data at our disposal.

Would appreciate any thoughts, thanks

\*(Please ignore the fact that age is a continuous variable, the actual characteristic we're using is a binary variable)",0,9,1736772072.0,MachineLearning
1hzupbd,[D] Is a ViT with local window attention (SAM-style) not that much more efficient than a vanilla ViT with global attention in all layers? Especially at high resolution where global attention should be super expensive.,"I was reading this blog post by Lucas Beyer: [https://lucasb.eyer.be/articles/vit\_cnn\_speed.html](https://lucasb.eyer.be/articles/vit_cnn_speed.html)

When he compares ViTB/16 and the SAM variant with mostly local attention (window size 14), it was a bit surprised that throughput improvements are slight (left) and that the SAM variant requires more peak memory.

Now this is inference only, so maybe during training the difference is larger, but I naively would have thought that local attention is much faster still, especially at high resolutions.

At 1024x1024, we should have 1024/16=64x64 patches - so the global attention operation should be extremely expensive? Am I missing something?

https://preview.redd.it/es7oj0ky6mce1.png?width=1425&format=png&auto=webp&s=5241198e5bb7129eae3d79e77f3a1dd136d64c2b

",20,8,1736710224.0,MachineLearning
1hzy2ox,[D] At which floating point precision gradient descent training or inference breaks down,"We consider NNs as a ""differentiable"" model, i.e. assume that we use continuous differentiable functions. However, we use floating point representations which technically discrete. At some precision, the models start to break down. I.e. consider fp64 model. It might not work as well on fp16 precision, etc.


Could anyone point me to resources (papers) which investigate this, investigate failure modes, ways to work them around, etc.


P.S. This question is inspired by NVidia announcement, where they mentioned that Blackwell supports fp4 precision. I am now interested in how it is possible to do anything useful with such a low precision, and what is used to achieve it.",6,9,1736718824.0,MachineLearning
1hz1l2j,[P] Built a Snake game with a Diffusion model as the game engine. It runs in near real-time 🤖 It predicts next frame based on user input and current frames.,,519,32,1736618225.0,MachineLearning
1hzq0ac,[D] Cheaper alternative to modal.com?,"Are there any other good services that let you instantly spin up a docker image on an 8xH100 machine? Modal is twice the price per hour of lambda labs or voltage park, but I kind of need the quick up/down.

Update 3 days later: ori, celium, and shadeform are all real working services and all work quite well.",5,13,1736698271.0,MachineLearning
1hzsm1q,[R] FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers (https://arxiv.org/pdf/2411.14507v1),"Is this paper any good? I am having trouble grokking its essence, for instance what are blocks, group-level, etc. I was looking for a paper that talks about fusing multiple transformer blocks, but this paper doesn't seem to go into the technical implementation details.",3,3,1736704945.0,MachineLearning
1hze3vs,[P] Llama3 Inference Engine - CUDA C,"Hey r/MachineLearning, recently I took inspiration from llama.cpp, ollama, and similar tools that enable inference of LLMs locally, and I just finished building a Llama inference engine for the 8B model in CUDA C.

As part of my explorative work in building optimized GPGPU software, I decided to build this from scratch. This project only makes use of the native CUDA runtime api and cuda_fp16. The inference takes place in fp16, so it requires around 17-18GB of VRAM (~16GB for model params and some more for intermediary caches). 

It doesn’t use cuBLAS or any similar libraries since I wanted to be exposed to the least amount of abstraction. Hence, it isn’t as optimized as a cuBLAS implementation or other inference engines like the ones that inspired the project.

## **A brief overview of the implementation** 

I used CUDA C. It reads a .safetensor file of the model that you can pull from HuggingFace. The actual kernels are fairly straightforward for normalizations, skip connections, RoPE, and activation functions (SiLU). 

For GEMM, I got as far as implementing tiled matrix multiplication with vectorized retrieval for each thread. The GEMM kernel is also written in such a way that the second matrix is not required to be pre-transposed while still achieving coalesced memory access to HBM.

There are some kernels like the one for RoPE and GEMM that use vectorized memory access. Parts of the SwiGLU feedforward computation takes place within a custom fused kernel.

Feel free to have a look at the project repo and try it out if you’re interested. If you like what you see, feel free to star the repo too!

I highly appreciate any feedback, good or constructive.",37,8,1736653881.0,MachineLearning
1hzprm8,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",5,22,1736697638.0,MachineLearning
1hz1xks,[N] I don't get LORA,"People keep giving me one line statements like decomposition of dW =A B, therefore vram and compute efficient, but I don't get this argument at all.

1. In order to compute dA and dB, don't you first need to compute dW then propagate them to dA and dB? At which point don't you need as much vram as required for computing dW? And more compute than back propagating the entire W?

2. During forward run: do you recompute the entire W with W= W' +A B after every step? Because how else do you compute the loss with the updated parameters?

Please no raging, I don't want to hear
1. This is too simple you should not ask
2. The question is unclear

Please just let me know what aspect is unclear instead.
Thanks",53,32,1736619107.0,MachineLearning
1hz4gdy,[P] A hard algorithmic benchmark for future reasoning models,"Hi, I've been toying with a simple idea for developing a future-proof, dynamic, AI model benchmark. The idea is pretty simple. A hidden function transforms data, and the model only gets to see the before and after, and has to deduce the hidden logic. I've carefully curated several levels of slightly increasing difficulty, and I've been surprised to see most current models I can access (GTP, o1, Sonet, Gemini) suck at it.

For instance, the first puzzle simply does \^=0x55 to the bytes on the input buffers, yet most models struggle to see it or deduce it.

I've spin up a opensource MIT repo with a live demo, so others can give this idea a try or contribute. I appreciate any feedback. Thanks!",24,16,1736625768.0,MachineLearning
1hz541n,[D] Which library is good for diffusion model research?,"I wanted to play around with diffusion models and switch out different parts of the pipeline (such as samplers, models, data modalities etc or use custom ones). I had a look at some libraries such as modular_diffusion or diffusor, but they don't seem to be very mature yet or very high-level. What kind of libraries do you use to experiment with diffusion models in your research?",9,3,1736627543.0,MachineLearning
1hz2dfp,[D] Thoughts on Google Paxml (aka Pax)?,"I just discovered [Pax](https://github.com/google/paxml), a *framework to configure and run machine learning experiments on top of Jax*. Did you know about this? It could be a better solution than Pytorch for large-scale models.",9,2,1736620254.0,MachineLearning
1hzpqgl,[D] Do I require to Overclock my RTX 4090 for AI Training Tasks?,"Hello,
I mostly run AI training and experiments on my PC and these experiments sometimes last multiple days non-stop and this machine keeps running 24/7. 
Do you think overclocking is required for my use case to get better performance? I don't want to end up bricking the GPU or end up reducing its lifespan as well. Can OC affect that?
The reason Im asking this is because my GPU is ZOTAC GAMING GeForce RTX 4090 Trinity and it has 3 fans on it. Ive noticed that for all my AI experiments the fans never go above 30% and the GPU temperature is also around 50 - 55°C. Since the GPU can handle higher temperatures and also there is the possibility of the fan going above 30%, I feel like I can possibly get more juice from GPU? What do you recommend, will it be a good idea?",0,11,1736697552.0,MachineLearning
1hznd9q,"[D] Which, in your opinion, is better for cost-saving while maintaining quality?","I have a scenario where I need to feed PDFs of text data to a Generative AI model in order to summarize and fetch only information of interest from each PDF individually. Now, I was first thinking of using the OpenAI API (GPT-4o), but I was wondering if another solution may be cheaper while also maintaining the level of quality for the text comprehension and generation:

* Install a model locally on my machine to do this.
* Install a model on a cloud server, like an EC2 instance in AWS.
* Use a different GenAI offering, like Amazon Bedrock

I don't have experience with downloading a model and using it, as I've only used APIs of popular providers before. But I want to learn how it works and whether you believe these options are realistic.",0,2,1736690749.0,MachineLearning
1hznbmr,[D] Why do we use RLFH instead of Gumbel softmax?,"My question is fairly simple. RLHF is used to fine-tune LLMs because sampled tokens are not differentiable. Why don't we just use Gumbel softmax sampling to achieve differentiable sampling and directly optimize the LLM?

The whole RLHF feel like so much overhead and I do not see why it is necessary.",0,25,1736690615.0,MachineLearning
1hzjlvy,[D] Discrepancy in no. of slices in multimodal segmentation,"Hey I’m using DTI and conventional MRI scans for my segmentation task. DTI has 60 slices, MRI has 23 slices, the segmentation mask was produced based on MRI so it has 23 slices. Any advice how do I go about doing so? There’s a discrepancy in no. of slices",0,3,1736676187.0,MachineLearning
1hz2ct5,[D] Finding optimal hyper parameter for neural network,"I have been trying to find optimal hyperparameter for LSTM model using gray wolf algorithm(GWO) and particle swarm optimizer(PSO). Its taking alot of time. Below is description for what I am doing.

I have a LSTM model wrapped in a objective function to be optimized. This function build model based on parameter passed to it, then it trains the model and find MSE on test data. This test data is returned based on which GWO optimizer will calculate fitness.

This process takes hours. Is there any other way to find optimum parameter?",4,2,1736620209.0,MachineLearning
1hzhrxb,[Discussion] Unclear problem statement,"The following is a problem statement for a use case.  
  
""The nature of fraud is dynamic and ever-changing. Finding patterns and identifying anomalies are essential in this industry. Given a set of mobile device attributes (for example, brand, model) data, design a model to find patterns or anomalies in these data. 

Take into account that not all device attributes are readily available all the time and there is no historical data available.""

There is no dataset provided, I'll have to find it myself. I was thinking of obtaining the Kaggle mobile price dataset and do some basic anomaly checks (Z-score, IQR) + isolation forest to detect fraudulent postings. However, not sure what no historical data means? I interpreted it as having no time series information + unlabelled (to be safe). ",0,4,1736668030.0,MachineLearning
1hzbtak,[R] Which Forecasting library should I be using for this task since all I've tried don't do what I need!,"Hi all,

I'm trying to forecast a single column in my dataset by using multivariate inputs: Fuel % left in car depending on current fuel %, speed, radiator temperature. I need to train a model that can approximate the fuel consumption curve in real-time, therefore it has to predict on unseen data based on what it learnt, however the libraries I've tried don't do that, instead they just train on the previous data and predict the exact next n (fh). I don't need that, I don't want the next n steps of my training data, I want the next n steps of my testing data which is unseen. I built my own pytorch model and it works well, but I need to compare it against other methods to see how to improve the model.

I tried Facebook Prohpet, Nixtla, SKTime, Pytorch Forecasting, GluonTS, but they don't seem to do what I want and/or lack one of the requirements. I've read about TSAI, Darts, Kats, but I'm afraid that I'm wasting time that I might not have testing too many libraries only to find out that they don't do what I need.

Any recommendation that I can look into that can do what I need?

tl;dr

I need a library/model that can take multivariate input to predict a univariate output for the next n steps in real time (unseen data).",0,2,1736646378.0,MachineLearning
1hyxijp,[D] Does softmax tend to result in unconstrained euclidean weight norms?,"Bit of a silly question. While I was in the middle of analyzing neural network dynamics geometrically, I realized something about softmax. When using categorical cross entropy, it results in a lower loss value for pre-softmax vectors within the output layer that have a high positive magnitudes for the correct label-axis, and high negative magnitudes for the non-correct label-axes. I know that regularization techniques keeps weight updates bounded to a degree, but I can't help thinking that softmax + cross entropy is not really a good objective for classifiers, even if the argument that it results in a probability distribution as the output so it's ""more interpretable"".

  
Just me?",7,18,1736607201.0,MachineLearning
1hyfaoc,"[Dataset][R] 19,762 Garbage Images for Building AI Recycling Solutions","Hi ML community!

I’m excited to share the **Garbage Classification V2 Dataset**, featuring **19,762 high-quality images** of garbage categorized into **10 distinct classes** (e.g., metal, plastic, clothes, and paper).

# Why this matters:

* Train AI models for **automated waste sorting and recycling**.
* Develop **waste segregation apps** or sustainability-focused tools.
* Create innovative **computer vision projects** for environmental impact.

🔗 **Dataset Link:** [Garbage Classification V2](https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2/)

This dataset has been used in the research paper, *""Managing Household Waste Through Transfer Learning,""* proving its utility in real-world applications.

Looking forward to seeing how you can use it to promote sustainability!",111,15,1736543993.0,MachineLearning
1hz7n71,[D] Image segmentation with SAM,"Is there somewhere I can segment an image with SAM exactly the same way they do in their website by simply clicking on different parts of the image to add to the mask (or shift click to remove) and download the mask in the end?

I've tested a few labeling tools but I found none of them worked as well as the meta demo. The problem with the meta website is that I can't download the mask, I can just get a cut out of the image.",1,3,1736634363.0,MachineLearning
1hyskcn,[D] Where can I find Machine Learning Engineer/AI Engineer interview Experiences?,"I need to go through some interview experiences of candidates other than glassdoor.
I want resources that tell me like there were so many rounds and what happened in each round.
Let me know if you have such resources.",8,2,1736588560.0,MachineLearning
1hypeqa,How is the job market for machine learning and Al in Australia? [D],"
Hi all. I am a Researcher based in Australia and if possible I would like to hear your opinion regarding ML market.
I've found a post from 2yo ago, and want to have an updated point of view.
Thank you all in advance. ",15,7,1736574772.0,MachineLearning
1hz43vl,"[P] [D] Audio Analysis Project Using PCEN (per channel energy normalization). I would greatly appreciate help and feedback, please DM me if you have additional insight.","My project involves various audio preprocessing techniques for classifying lung sounds, particularly on Per-Channel Energy Normalization (PCEN). To create a comprehensive set of labeled audio clips covering a range of respiratory conditions, we combined and augmented two primary datasets: one from the ICBHI 2017 Challenge and another from Kaggle. Using these datasets, we pursued three classification tasks: multi-diagnosis (classification between ), distinguishing between wheezes, crackles, and everyday sounds, and differentiating between normal and abnormal lung sounds. Each dataset was processed using several methods, including log-mel spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), and PCEN spectrograms. These were then fed into a convolutional neural network (CNN) for training and evaluation. Given PCEN’s noise suppression and enhancement of transient features, I hypothesized it would outperform spectrograms and MFCCs in capturing subtle lung sound patterns. While validation loss during training was often better with PCEN, evaluation metrics (precision, recall, F1-score) were unexpectedly lower compared to spectrograms. This discrepancy raised questions about why PCEN might not be performing as well in this context.

For a video explaining PCEN, here's a video by the creator of PCEN explaining it a bit further: [https://www.youtube.com/watch?v=qop0NvV2gjc](https://www.youtube.com/watch?v=qop0NvV2gjc)

I did a bit more research and was particularly intrigued by an approach to gradient descent self-calibration for PCEN’s five coefficients. I’d like to explore implementing this in my project but am unsure how to apply it effectively. I made it work, but the val accuracy and loss are stuck around 88% which is substantially lower than all the other methods.

Some potential reasons for PCEN not performing as well include:

1. Data imbalance between diagnostic categories may skew results.
2. Suboptimal parameter values for PCEN coefficients that might not align with the nuances of lung sound data. (The parameters I have currently for PCEN are, α=0.98, 𝛿=2.0, r=0.5, ε=1×10\^-6, and T=0.03.)
3. Given the unexpected validation vs. evaluation performance gap, there may be possible inaccuracies in my actual evaluation metrics.

I would be incredibly grateful for your insights on applying gradient-based optimization to PCEN coefficients or any recommendations to improve its application to this dataset. I also have a GitHub repo for the project if you would like to take a look at it. DM me if you're interested in seeing it.

Thank you all for your time, and I look forward to hearing your thoughts. If you have any questions please let me know.",1,2,1736624860.0,MachineLearning
1hypgxp,[P] Check your scholar stats,,11,3,1736575015.0,MachineLearning
1hzdd2h,"[R] Numeric Features: An In-Depth Guide for Machine Learning Enthusiasts 
","📊 **Understanding Numeric Features in Machine Learning**

Numeric features are the backbone of many machine learning models, powering algorithms with the quantitative data they need to make accurate predictions. From healthcare analytics to financial modeling, they’re indispensable in today’s data-driven world.



**Key Insights from the Guide:**



✔️ **What Are Numeric Features?**

• Continuous vs. Discrete features, and why they’re crucial in ML.



✔️ **Handling Numeric Data**:

• Techniques like scaling, normalization, and handling missing values.

• Outlier detection and transformations to improve model performance.



✔️ **Real-World Applications**:

• Financial ratios, healthcare metrics, retail trends, and more.



✔️ **Algorithm Considerations**:

• Why features need scaling for KNN, neural networks, and others.



✔️ **Practical Tips**:

• Visualizing data, selecting features, and avoiding overfitting with polynomial features.



  
**🌐 Why Read This?**



If you’re working in data science, machine learning, or feature engineering, this article is packed with practical insights and examples that you can apply to your projects today.



📖 Check out the full article here: [Numeric Features: An In-Depth Guide](https://medium.com/p/dbd41e6fbd70) 

  
\#MachineLearning #FeatureEngineering #DataScience #NumericFeatures #DataEngineering #AI #DataPreprocessing #BigData #MLTips",0,0,1736651401.0,MachineLearning
1hye3gm,[R] Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought,,46,1,1736540946.0,MachineLearning
1hycnb4,[R] Small Language Models Master Complex Math Through Self-Evolved Monte Carlo Tree Search,"The key innovation here is a self-evolution mechanism that enables small language models to perform complex mathematical reasoning through iterative refinement and self-correction. The approach, called rStar-Math, uses structured decomposition and verification steps to achieve performance comparable to much larger models while using significantly fewer parameters.

Key technical points:
- Multi-step reasoning framework that generates, evaluates, and refines solutions
- Self-evolution mechanism that develops more sophisticated reasoning patterns over time
- Implementation of verification steps to catch and correct errors
- Structured decomposition of complex problems into manageable sub-tasks
- Specialized components for mathematical reasoning and solution verification

Results:
- Achieved 80%+ accuracy on complex math problems
- Matched performance of models with 10x more parameters
- Self-correction improved accuracy by ~25%
- Effective across multiple mathematical domains
- Demonstrated consistent performance on both numerical and word problems

I think this approach could be transformative for deploying capable ML systems in resource-constrained environments. The ability to achieve strong performance with smaller models opens up possibilities for edge devices and scenarios where computational resources are limited. The self-evolution mechanism could also be adapted for other domains requiring complex reasoning.

I think the most interesting aspect is how the system learns to catch its own mistakes and improve its reasoning process, similar to how humans develop mathematical problem-solving skills. This could lead to more robust and reliable AI systems that can explain their thinking and correct errors autonomously.

TLDR: Small language models can achieve strong mathematical reasoning capabilities through self-evolution and structured verification steps, matching larger models while using fewer resources.

[Full summary is here](https://aimodels.fyi/papers/arxiv/rstar-math-small-llms-can-master-math). Paper [here](https://arxiv.org/abs/2501.04519).",48,1,1736537292.0,MachineLearning
1hyefw5,"[D] Resources for how neural nets learn to warp latent space to make predictions?
","What are some good resources to further read on how neural networks build their decision surfaces?

I recently read Chris Olah's post on ""[**Neural Networks, Manifolds and Toplogy**](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"" and also ""[On the Number of Linear Regions of Deep Neural Networks](https://arxiv.org/abs/1402.1869)"" (ICLR ‘14).

Was intrigued with the idea about how neural networks ""learn to fold latent spaces"" to make predictions. 

My intuition for a simple MLP layer is that each component of plays a different role in this geometric warping:  

* The activation function basically works as a gating mechanism (relu)
* The bias vector is a translation operation
* The matrix multiply Wx can be understood via SVD (W=USV):
   * U,V are rotation/reflections matrices
   * S is a scaling matrix

The combination and stacking of these operations leads to this great figure: 

[https:\/\/arxiv.org\/abs\/1402.1869](https://preview.redd.it/ahhmxka9a8ce1.png?width=1689&format=png&auto=webp&s=fa1352a0e2874a10aa013bbd79b47ccd09914a12)

Any other insights, resources to read up on these ideas?",32,3,1736541807.0,MachineLearning
1hxzij5,"[D] Creating Proper LLM Summaries is Surprisingly Expensive
",,83,32,1736495825.0,MachineLearning
1hyn587,[News] Introcuding kokoro-onnx TTS,"Hey everyone!

I recently worked on the *kokoro-onnx* package, which is a TTS (text-to-speech) system built with onnxruntime, based on the new *kokoro* model (https://huggingface.co/hexgrad/Kokoro-82M)

The model is really cool and includes multiple voices, including a whispering feature similar to Eleven Labs.

It works faster than real-time on macOS M1. The package supports Linux, Windows, macOS x86-64, and arm64!

You can find the package here:

[https://github.com/thewh1teagle/kokoro-onnx](https://github.com/thewh1teagle/kokoro-onnx)

Demo for podcast created with it:

I can't add videos here, but I recommend see the example podcast created in the readme of the repository!",1,0,1736566749.0,MachineLearning
1hy3xn7,[R] [P] Cohere For AI launches new LLM cohort focused on Multilingual Long-Context Understanding,"From the BIRDS(Beginners in Research Driven Studies) group of r/CohereAI, Cohere Open Science Community, we're thrilled to announce our new LLM Cohort! 🎉 🚀

This isn't just another learning program; it's a hands-on, collaborative research initiative designed to push the boundaries of what's possible with Large Language Models in multilingual, long-context settings 💡 📚

We'll be diving deep into two exciting tracks:  
  
🔬 **Track 1**: Multilingual Long Context - Enhancing Processing with Advanced Techniques  
🤖 *Led by*: Mayank Bhaskar and Madhava Prasath  
🎯 **Focus**: Exploring cutting-edge methods like RoPE(Rotatory Positional Embedding), NoPE(No Position Encoding), LongROPE, SSMs(State Space Models), and Hybrid Transformer-SSM models to overcome long-context challenges in multilingual NLP, enhancing scalability, efficiency, and the ability to process extended sequences while addressing limitations of traditional Transformers.  
🧠 **Challenge**: Develop a novel method to integrate SSMs with Transformers, optimizing for long-context multilingual understanding. Demonstrate superior performance over RoPE, NoPE, and LongRoPE on synthetic tasks, emphasizing generalization to sequences exceeding training lengths and minimal computational overhead.

https://preview.redd.it/x43aruufz5ce1.png?width=632&format=png&auto=webp&s=9fc3e08f80f5a8713cdc4b39c7cb0284a2baf195

🔬 **Track 2**: Evaluating Multilingual Long Context Generation and Reasoning   
🤖 *Led by*: Guneet Singh Kohli and Shivalika Singh  
🎯 **Focus**: Build a benchmark to assess ability of multilingual LLMs to handle long context tasks involving complex reasoning.  
🧠 **Challenge**: How do we ensure accurate, contextually relevant responses across languages for Long Context Tasks? Evaluating capabilities of existing LLMs for such tasks and coming up with a data creation pipeline to build a Multilingual Long Context Benchmark.

https://preview.redd.it/i2r6z4ahz5ce1.png?width=680&format=png&auto=webp&s=93dabba03df32da12fa577dc02516b9ad6048c3d

Why Join?  
💼 Gain practical research experience: Work on a real-world project from start to finish.  
🤝 Collaborate with experts: Learn from and alongside experienced researchers.  
🌐 Shape the future of LLMs: Contribute to advancements in a rapidly evolving field.

📅 Kick-off Call: Join us this Friday, January 10th at 10:00 am PT for a detailed overview of the cohort and to meet the track leads!

https://preview.redd.it/1uyvx9fiz5ce1.png?width=680&format=png&auto=webp&s=fc43363fb8cc277e7bcdf3725e9c66f5f92b9df7

2025 is shaping up to be a year of groundbreaking research and let's embark on this exciting journey of discovery together! 

https://i.redd.it/01luy73mz5ce1.gif

",8,1,1736514227.0,MachineLearning
1hy05iu,[D] I am trying to find common sense problems that most humans can solve but reasoning models find it hard. Here's an example,"Okay I am at the risk of making absolutely no sense in this post. However, I will try my best. In his book, Godel Escher Bach, David Hofstadter talks about the concept of isomorphism as a characteristic trait of intelligence. We say two systems are isomorphic if they are not equivalent but parts of the system are structurally similar. Example:

\- DNA sequences encode genetic instructions for living organisms using a ""language"" of four nucleotides (A, T, C, G). These sequences specify how proteins are built, which in turn determine biological functions.

\- Computer programs encode instructions for machines using a ""language"" of binary digits (0 and 1). These sequences specify operations, which determine the machine's behavior.

We can say that these systems are isomorphic.

That being said, I want to come up with test cases where I can test and hopefully quantify the ability of reasoning LLMs (like o1) to identify similarities between non-identical objects, structures, observations or experiences and use the identified insight for problem solving. Here's an example test case:

Prompt: There is something wrong with the given storyline. Identify what is wrong. Change the narrative of the story (minimum changes) to fix the problem.

>""Once I was sick and I didnt feel like playing. My mother told me that I had to play or else I wont be able to recover. She said if I dont feel like playing properly, I can play less but I must play. I said what do you mean by play less? She suggested that I can play catch today instead of playing soccer. I said I can try. She gave me the ball. It was shiny and it looked bouncy. As I threw the ball for the first time, it was a rather sweet feeling. Within a few minutes I threw the ball quickly and at the end I did feel better. My mum put me to bed.""

Reply from o1: The mother’s advice that the child must *play* (as in physically exert themselves) to recover from an illness contradicts common sense and typical medical advice, which usually suggests *rest* as a key factor in recovery.

Revised Narrative (Minimal Changes)

>""Once I was sick and I didn’t feel like doing anything. My mother told me I should *rest* so that I could recover faster. She said if I didn’t feel like running around, I could still do something light to keep my spirits up—like playing a gentle game of catch. I said, 'What do you mean by something light?' She suggested we toss the ball softly back and forth instead of playing soccer. I said I could try. She gave me the ball. It was shiny and it looked bouncy. As I threw the ball for the first time, it felt good to move just a little. We only played for a few minutes, and afterward I did feel better. My mum then put me to bed so I could rest more.""

Prompt (follow-up): Here's a hint for you, Change playing to something else

Reply from o1: Revised Story (Minimal Changes: Changed “playing” to physical therapy)

>""Once I was sick and I didn’t feel like doing my physical therapy. My mother told me that I had to do it or else I wouldn’t be able to recover. She said if I didn’t feel like doing the full routine, I could try something simpler, but I must do something. I asked, ‘What do you mean by simpler?’ She suggested I could just practice gentle tossing with the therapy ball instead of completing all my exercises. I said I’d give it a try. She gave me the therapy ball. It was shiny and looked a bit bouncy. As I tossed the ball for the first time, it was a rather sweet feeling. Within a few minutes, I felt a little stronger, and by the end I did feel better. My mum then put me to bed.""

Prompt (follow-up): do you think if I change play to eat it will make sense?

Reply from o1: Revised Story with ""Eat""

>""Once I was sick and I didn’t feel like eating. My mother told me that I had to eat or else I wouldn’t be able to recover. She said if I didn’t feel like having a full meal, I could eat something light but I must eat. I asked, ‘What do you mean by eat less?’ She suggested I could try just some soup today instead of a big dinner. I said I’d give it a try. She gave me a bowl of warm soup. It smelled comforting, and as I took the first spoonful, it was a rather sweet feeling. Within a few minutes, I ate a bit more, and at the end I did feel better. My mum then put me to bed.""

Obviously, the initial story contradicts common sense. Why would the mother ask the child to play when the child is sick. o1 correctly identifies this contradiction. However, o1 is not able to identify a similar situation where such a narrative is possible (a mother asking a sick child to do something that the child is not up to). To me, based on personal experience, the story makes sense if you change ""play"" to ""eat"". I asked a few of my friends and their first response was always change play to eat. Not sure if yall would feel the same way perhaps you prefer one of o1's responses.

This story does not make sense but is isomorphic to a story that makes sense and is a common occurrence (at least in my part of the world). o1 identified that the story doesn't make sense but it was not able to identify an isomorphic situation that does make sense. Most of my friends were able to get this in first try (Within 10 minutes) without nudging.

Now this is a fairly simple example (and some might say its a useless one) but I think that this example suggests that maybe reasoning LLMs are not all that good with isomorphism. If so, this example suggests that we might be able to come up with a set of test cases where humans are able to identify similarities between non-identical entities (in a creative and useful manner) but the most sophisticated LLMs fail to do so. Perhaps in this set of test cases there will be cases that are more useful than the example I provided. Nonetheless, I intend to find more such test cases.

Moreover, I want to know what you think about all this? Can you come up with more examples? Does all this make sense to you? I believe that identifying and clearly defining the limitations of current LLM systems paves the way for new research frontiers to enhance their performance.",14,11,1736498761.0,MachineLearning
1hygkbr,[R] Joint training of conditional diffusion with embedding model?,"Does anyone know the effectiveness of training a conditional diffusion model with the embedding model at the same time? I am aiming to train a diffusion model conditioned on a state and time to produced that state advanced n time steps through time, but the embedding model can’t be known or pretrained ahead of time. 

The closest I have seen to what I need is GenCast but I can’t simply concat the previous state.",1,5,1736547325.0,MachineLearning
1hxk2ab,[R] rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,,132,28,1736448672.0,MachineLearning
1hye9ne,[D] Do MLPs trained for next character prediction require causal masking?,"Suppose we have some data `X = [seq_len, batch_size]` and corresponding labels `Y = [seq_len, batch_size, vocab_size/num/classes]` , one-hot encoded.

And, now we want to train an MLP for next character prediction.

Question: Do we need to apply a causal masking to restrict the model from peaking at future tokens? If so where to you apply it on which layer or output?

During training the model sees the entire sequence and predicts the corresponding one-hot encoded label.

Usually the examples that I’ve seen most of them use `X` and the shifted version of it \`Y = X'\` as labels to train for next character prediction but this doesn't match my case since I already have one-hot encoded labels.",0,18,1736541363.0,MachineLearning
1hy7d3c,[D] using SHAP values to produce a strength of directionality metric for random forests ,"Hello folks. I am hoping somebody can help me out here as I am just an ecologist who dabbles in machine learning when needed.


I have run a bunch of random forest models, one for each group of an animal species, that measures the probability of that group occurring in a particular place given a set of environmental predictor variables. I need to determine the directionality for the top performing predictor variables. Normally I would use PDPs for this, but I  have many groups and it would become completely unwieldy and unsightly. Ideally, I want to build a table to store all this information, using a metric to store the average directionality information, including sign and value. 

Is there a way using SHAP values to build such a metric? I can use SHAP or another metric like mean decrease in accuracy to get at variable importance, but I’d like to pair it with a metric that represents the average directionality of the response as the specific predictor increases. So if a variable has an overall positive relationship, the metric would be a high value and positive, opposite for negative. Importantly, if a variable was very important but had a complex relationship (example, positive then negative) as the predictor increases, it would probably have a low value. The beehive plot outputs you often see with SHAP values tells me that this is probably possible and not that complicated. 
 ",1,0,1736524088.0,MachineLearning
1hxleaa,[R] Agent Laboratory: Using LLM Agents as Research Assistants - Autonomous LLM-based Framework Capable of Completing the Entire Research Process ,"Paper: [https://arxiv.org/pdf/2501.04227](https://arxiv.org/pdf/2501.04227)

Github: [https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file](https://github.com/SamuelSchmidgall/AgentLaboratory?tab=readme-ov-file)

Blog: [https://agentlaboratory.github.io/](https://agentlaboratory.github.io/)

Abstract:

>Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: **(1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.**

https://preview.redd.it/oop8omfvt0ce1.jpg?width=1591&format=pjpg&auto=webp&s=e588146997010797bbe75ea9d575bd65ce0d8fc6",23,1,1736452019.0,MachineLearning
1hykl7s,🌟 Introducing DarwinAI: An Open-Source Platform for the Evolution of Intelligent Agents 🚀 [Project],"# 🌱 The Vision: Evolutionary AI at Your Fingertips

Imagine a world where AI agents aren't just programmed to perform tasks but evolve over time, adapting and improving through generations, much like living organisms. Welcome to DarwinAI, an open-source platform inspired by biological evolution, designed to breed, train, and evolve AI agents that can tackle complex, dynamic, and unpredictable challenges.

# 🧬 The Genetic Blueprint: Building Blocks of Intelligence

At the core of DarwinAI is the concept of a digital DNA for each AI agent. This DNA is a modular structure that defines the agent's capabilities, behaviors, and adaptability. Here's what makes up this digital DNA:

* Genes of Ability: These are snippets of code that represent specific functions, like data classification, text analysis, or optimization. Think of them as the skills your AI agent possesses.
* Genes of Adaptation: These genes control how the agent responds to different environments or contexts. They determine its flexibility and resilience in the face of changing conditions.
* Genes of Connection: These define how the agent interacts with other agents or external resources. They are the social and collaborative aspects of the agent.

This digital DNA is stored in a structured, version-controlled database, allowing us to track the evolution of each agent and ensure that beneficial mutations are preserved over time.

# 🛠️ The Evolutionary Process: From Genesis to Mastery

The evolution of AI agents in DarwinAI happens through a series of generations, each building upon the strengths of the previous one:

* Selection of Parents: The fittest agents, those that excel at specific tasks, are chosen as parents. These agents have proven their worth in the simulated environment and are prime candidates for breeding the next generation.
* Genetic Crossover: The digital DNA of these parent agents is combined to create new agents. This can happen in two ways:
   * Direct Crossover: Where entire genes are copied from the parents.
   * Combinatorial Crossover: Where parts of different genes are fused to create entirely new abilities.
* Mutations: Random, small changes are introduced into the genes to promote diversity and explore new solutions. These mutations are the wildcards that can lead to breakthrough abilities.

# 🌍 The Simulated Environment: A Playground for Evolution

Agents don't just exist in a vacuum; they operate in a dynamic, simulated environment where they must adapt and survive. This environment is designed to challenge the agents with:

* Evolutionary Tasks: Problems that agents must solve, such as data classification, prediction, or content generation.
* Changing Contexts: Factors like noisy data, resource constraints, or new rules that force agents to adapt on the fly.

# 🐣 The Life Cycle of an Agent: From Birth to Legacy

Each agent goes through a life cycle that mirrors the process of natural selection:

* Initial Learning: Agents receive initial training based on their digital DNA.
* Task Execution: They perform tasks in the simulated environment, where their abilities are put to the test.
* Performance Evaluation: Their effectiveness, adaptability, and efficiency are measured.
* Reproduction: The top-performing agents produce offspring with improved genetic traits.
* Discard and Archive: Less effective agents are archived for future analysis, ensuring that their lessons are not lost.

# 🧩 Knowledge Transfer: Passing the Torch

One of the key aspects of DarwinAI is the ability for agents to pass on their learned knowledge to future generations:

* Weight Persistence: Trained models retain their learned weights, allowing them to inherit capabilities from their ancestors.
* Modular Transfer: Optimized ability genes can be directly copied to new generations, ensuring that valuable skills are preserved.

# 🛠️ Modularity and Extensibility: Build, Mix, and Evolve

DarwinAI is designed to be highly modular and extensible, allowing for:

* New Capabilities: Easily incorporate new genes to expand the agents' abilities over time.
* Hybridization: Combine agents from different specializations to create more complex and versatile agents.
* Directed Evolution: Introduce controlled mutations to address specific problems or challenges.

# 🚀 Innovative Use Cases: The Future is Bright

The potential applications of DarwinAI are vast and varied:

* Adaptive Automation: Create agents that can adapt to new market conditions or evolving industrial requirements.
* Collaborative Robots: Develop robots that evolve to improve teamwork in dynamic environments.
* Scientific Discovery: Agents that combine skills to uncover patterns or solutions that were previously unknown.

# 🚀 Vision for the Future: An Ecosystem of Evolving Intelligence

By fostering an ecosystem where knowledge is accumulated and adaptability is paramount, DarwinAI aims to produce agents that are not only intelligent but also diverse and efficient. These agents will be equipped to handle complex, unpredictable challenges, opening up new frontiers in AI research and application.

# 🌐 Join Us in Shaping the Future of AI!

DarwinAI is more than just a project; it's a community-driven movement towards a new era of AI. We invite you to join us, contribute your ideas, and help shape the future of evolutionary AI. Whether you're a developer, researcher, or simply someone excited about the potential of AI, there's a place for you in this journey.

Let's evolve together! 🌱💻

",0,4,1736558638.0,MachineLearning
1hx6q8r,[D] Why does training LLMs suck so much?,"I work in hardware acceleration and have been slowly trying to move my focus into LLM/GenAI acceleration, but training LLMs literally sucks so much... Even just 100M parameter ones takes forever on 4 A6000 Adas, and while I don't spend idle time watching these, it gets so frustrating having to retrain realizing the LR is too high or some other small issue preventing convergence or general causal language understanding... 

I know the more you do something, the better you get at it, but as a GRA by myself with an idea I want to implement, I truly feel that the overhead to train even a small LM is far from worth the time and care you have to put in

It just sucks because deadlines are always coming, and once you're done with pretraining, you still have to fine-tune and likely do some kind of outlier-aware quantization or even train LoRA adapters for higher accuracy

I really hope to never do pretraining again, but needing a model that abides to your specific size constraints to fit into (for example) your NPU's scratchpad RAM means I'm always stuck pretraining

Hopefully in the future, I can have undergrads do my pretraining for me, but for now, any tips to make pretraining LLMs less like slave work? Thanks!

",147,54,1736404134.0,MachineLearning
1hxa6u6,[R] ObliqueTree: Advanced Decision Tree Implementation,"# obliquetree

`obliquetree` is an advanced decision tree implementation designed to provide high-performance and interpretable models. It supports both classification and regression tasks, enabling a wide range of applications. By offering traditional and oblique splits, it ensures flexibility and improved generalization with shallow trees. This makes it a powerful alternative to regular decision trees.

  
You can access the project from here: [ObliqueTree GitHub Repository](https://github.com/sametcopur/obliquetree)

[Tree Visualization](https://preview.redd.it/1u6o7d5e6ybe1.png?width=1412&format=png&auto=webp&s=f64d8838cf8d6ef20878cdac7e32514c4777c8c5)

# Getting Started

`obliquetree` combines advanced capabilities with efficient performance. It supports **oblique splits**, leveraging **L-BFGS optimization** to determine the best linear weights for splits, ensuring both speed and accuracy.

In **traditional mode**, without oblique splits, `obliquetree` outperforms `scikit-learn` in terms of speed and adds support for **categorical variables**, providing a significant advantage over many traditional decision tree implementations.

When the **oblique feature** is enabled, `obliquetree` dynamically selects the optimal split type between oblique and traditional splits. If no weights can be found to reduce impurity, it defaults to an **axis-aligned split**, ensuring robustness and adaptability in various scenarios.

In very large trees (e.g., depth 10 or more), the performance of `obliquetree` may converge closely with **traditional trees**. The true strength of `obliquetree` lies in their ability to perform exceptionally well at **shallower depths**, offering improved generalization with fewer splits. Moreover, thanks to linear projections, `obliquetree` significantly outperform traditional trees when working with datasets that exhibit **linear relationships**.

# Installation

To install `obliquetree`, use the following pip command:

    pip install obliquetree

Using the `obliquetree` library is simple and intuitive. Here's a more generic example that works for both classification and regression:

    from obliquetree import Classifier, Regressor
    
    # Initialize the model (Classifier or Regressor)
    model = Classifier(  # Replace ""Classifier"" with ""Regressor"" if performing regression
        use_oblique=True,       # Enable oblique splits
        max_depth=2,            # Set the maximum depth of the tree
        n_pair=2,               # Number of feature pairs for optimization
        random_state=42,        # Set a random state for reproducibility
        categories=[0, 10, 32], # Specify which features are categorical
    )
    
    # Train the model on the training dataset
    model.fit(X_train, y_train)
    
    # Predict on the test dataset
    y_pred = model.predict(X_test)

# Documentation

For example usage, API details, comparisons with axis-aligned trees, and in-depth insights into the algorithmic foundation, we **strongly recommend** referring to the full [documentation](https://obliquetree.readthedocs.io/en/latest/).



# Key Features

* **Oblique Splits** Perform oblique splits using linear combinations of features to capture complex patterns in data. Supports both linear and soft decision tree objectives for flexible and accurate modeling.
* **Axis-Aligned Splits** Offers conventional (axis-aligned) splits, enabling users to leverage standard decision tree behavior for simplicity and interpretability.
* **Feature Constraints** Limit the number of features used in oblique splits with the `n_pair` parameter, promoting simpler, more interpretable tree structures while retaining predictive power.
* **Seamless Categorical Feature Handling** Natively supports categorical columns with minimal preprocessing. Only label encoding is required, removing the need for extensive data transformation.
* **Robust Handling of Missing Values** Automatically assigns `NaN` values to the optimal leaf for axis-aligned splits.
* **Customizable Tree Structures** The flexible API empowers users to design their own tree architectures easily.
* **Exact Equivalence with** `scikit-learn` Guarantees results identical to `scikit-learn`'s decision trees when oblique and categorical splitting are disabled.
* **Optimized Performance** Outperforms `scikit-learn` in terms of speed and efficiency when oblique and categorical splitting are disabled:
   * Up to **50% faster** for datasets with float columns.
   * Up to **200% faster** for datasets with integer columns.

[Performance Comparison \(Float\)](https://preview.redd.it/6fgh7x6m6ybe1.png?width=2969&format=png&auto=webp&s=bf158ef8ca1128b34ff31cfa00de41a79f3e9375)

[Performance Comparison \(Integer\)](https://preview.redd.it/ic6n6x6m6ybe1.png?width=2969&format=png&auto=webp&s=05ba53fb43fdd5c462187df69381e2ea3ab973fd)

",48,8,1736419670.0,MachineLearning
1hxyx9v,[D] What Are the Most Important Results in Machine Learning Theory in the First Quarter of the 21st Century?,"The 21st century is already a quarter complete. Over the past 25 years, machine learning has made tremendous progress. It is quite easy to find surveys summarizing the most important results in machine learning in the past 25 years. However, most of these results are not theory results.

With this in mind, what do you think are the most important results in machine learning theory in the first quarter of the 21st century? Here are some of my nominations:

1. **Finite-time analysis of multi-armed bandits**. Initiated by \[Auer et al., 2002\], a line of research (\[Audibert and Bubeck, 2010\], \[Zimmert and Seldin, 2018\]) has led to a nearly full understanding of the finite-time behavior of multi-armed bandits.
2. **Establishment of the field of online optimization**. Initiated by \[Zinkevich, 2003\], a line of research defines the fundamental problems (adversarial bandits, online convex optimization) and establishes the fundamental techniques (Online Subgradient Descent, OMD, FTRL, FTPL...) in this field.
3. **The notion of differential privacy**. This notion generates a bunch of new problems and research directions. What particularly interesting is the connection between differential privacy, generalization, and adaptive data analysis (\[Dwork et al., 2015\]).

These are my nominations for the most important results in learning theory in the first 1/4 of the 21st century. What are other important results in your mind? I would love to hear your thoughts.",1,0,1736493136.0,MachineLearning
1hxbvgf,[D] Questionable high score paper in ICLR 2025 (on Diffusion LM),"The new diffusion LLM methodology presented in the ICLR 2025 paper ""**Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning**""(sub **4441**) is no different from the D3PM suggested in 2021. While they rebranded the architecture as ""MDM"" under a new name, the only difference from D3PM is nothing more than the addition of a common technique that slightly weights higher-loss data.

This paper scored **8,6,6,5 at ICLR 2025,** far surpassing the acceptance threshold. It seems like there was a problem with the review system. How do you think?

p.s. I see the claimable contribution of this work is applying an existing discrete diffusion model to bidirectional inference tasks (e.g., Sudoku) and reporting that the diffusion model outperforms autoregressive LMs in such tasks (which is trivial in some points). However, the paper exaggerated its contribution as if it proposed a new diffusion method (MDM), and this was not sufficiently validated in the review process.",25,10,1736426309.0,MachineLearning
1hxgovs,[R] Seminar on Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,[Nice seminar which dives also a bit on the proofs of the paper](https://www.youtube.com/watch?v=E2qBhMmjspU),7,0,1736440176.0,MachineLearning
1hxbldq,[P] I built a library that builds tensors from reusable blueprints using pydantic,"Cyantic lets you build complex objects from simple blueprints during the pydantic build process, with type-safety and validation built in.  

**[Cyantic Github Repo](https://github.com/flywhl/cyantic)**

* Define custom, type-safe blueprints with validation (since they are pydantic models).
* Reference other values using `@value:x.y.z`.
* Import objects using `@import:x.y.z`.
* Load data from environment variables using `@env:VAR`.
* Define custom `@hook` handlers (see tests)

# Example

E.g. add a `data: Tensor` field to a pydantic model, then call `thing.validate_model({..., ""mean"": 0.0, ""std"": 0.1, ...})` and receive the built tensor.

    from cyantic import Blueprint, blueprint, CyanticModel, hook
    ...
    
    # 1. Create and register some useful parameterisations
    #       (or soon install from PyPi, i.e. `rye add cyantic-torch`)
    
    @blueprint(Tensor)
    class NormalTensor(Blueprint[Tensor]):
    
        mean: float
        std: float
        size: tuple[int, ...]
    
        def build(self) -> Tensor:
            return torch.normal(self.mean, self.std, size=self.size)
    
    
    # 2. Write pydantic models using `CyanticModel` base class
    
    class MyModel(CyanticModel):
        normal_tensor: Tensor
        uniform_tensor: Tensor
    
    # 3. Validate from YAML files that specify the parameterisation
    
    some_yaml = """"""common:
        size: [3, 5]
    normal_tensor:
        mean: 0.0
        std: 0.1
        size: @value:common.size
    """"""
    
    # 4. Receive built objects.
    
    my_model = MyModel.model_validate(yaml.safe_load(some_yaml))
    assert isinstance(my_model.normal_tensor, Tensor)

# Why I made it

I do theoretical neuroscience research, so I have to instantiate a lot of Tensors. I wanted a way to do this from YAML (how I specify models), so I built a kind of middleware which uses intermediary pydantic models as blueprints for building full objects during pydantic's build process. Now I can pass in parameters (e.g. mean and standard deviation), and get a fully-built Tensor in a pydantic model.

This is now a library, Cyantic - named after cyanotype photography (i.e. the ""blueprint"").",16,0,1736425278.0,MachineLearning
1hx4o3z,[D] [R] First PhD paper decision: IJCAI or ICML,"I’m a second-year PhD student. I withdrew my first paper from ICLR after receiving ratings below the acceptance threshold and have since made some improvements. Now, I need to decide which conference to target for submission. Both conferences have equal acceptance rates, and the area of my work aligns well with both. I'm unsure which one offers a better chance for success.",39,29,1736396641.0,MachineLearning
1hwvk9x,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,"TabPFN v2, a pretrained transformer which outperforms existing SOTA for small tabular data, is live and just published in 🔗 [**Nature**](https://www.nature.com/articles/s41586-024-08328-6).

Some key highlights:

* It outperforms an ensemble of strong baselines tuned for 4 hours in 2.8 seconds for classification and 4.8 seconds for regression tasks, for datasets up to 10,000 samples and 500 features
* It is robust to uninformative features and can natively handle numerical and categorical features as well as missing values.
* Pretrained on 130 million synthetically generated datasets, it is a generative transformer model which allows for fine-tuning, data generation and density estimation.
* TabPFN v2 performs as well with half the data as the next best baseline (CatBoost) with all the data.
* TabPFN v2 was compared to the SOTA AutoML system AutoGluon 1.0. Standard TabPFN already outperforms AutoGluon on classification and ties on regression, but ensembling multiple TabPFNs in TabPFN v2 (PHE) is even better.

TabPFN v2 is available under an [open license](https://github.com/PriorLabs/TabPFN): a derivative of the Apache 2 license with a single modification, adding an enhanced attribution requirement inspired by the Llama 3 license. You can also try it via [API](https://github.com/PriorLabs/tabpfn-client).

We welcome your feedback and discussion! You can also join the discord [here](https://discord.com/invite/VJRuU3bSxt).",84,28,1736371920.0,MachineLearning
1hxn0w2,[R] How to train StyleGAN3 with classes?,"I was reading the documentation of the [train.py](http://train.py) on stylegan3 github and it mentioned that by setting the cond=True and providing a dataset.json that contains the structure of the classes then you can conduct the image generation with classes.

This all seemed fine until I began training but I encountered the following error:

    The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1
    
    I believe this is happening because I'm using a pre-trained model to fine-tune and avoid training from scratch and that pretrained model possibly didn't contain classes. If my assumption is true, does anyone know where I can find a pretrained model that was trained with classes on a 512x512 resolution?I was reading the documentation of the train.py on stylegan3 github and it mentioned that by setting the cond=True and providing a dataset.json that contains the structure of the classes then you can conduct the image generation with classes.This all seemed fine until I began training but I encountered the following error:The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1
    
    I believe this is happening because I'm using a pre-trained model to fine-tune and avoid training from scratch and that pretrained model possibly didn't contain classes. If my assumption is true, does anyone know where I can find a pretrained model that was trained with classes on a 512x512 resolution?",1,2,1736456152.0,MachineLearning
1hx88ip,[R] Dynamic Time Warping on animal vocalizations,"Hopefully it's alright to ask this question here. I know DTW isn't ML, but I thought I may find some insight on this sub. I'm still a newcomer to time-series analysis and audio signal processing, and I'm having some difficulty with DTW implementation. Thank you in advance for any help/insight.

*Here's my problem:* I'm working on rat ultrasonic vocalizations (USVs). These vocalizations were recorded from a rather noisy, naturalistic colony environment. My data consists of a subset of USVs which I believe may constitute 3-4 ""new"" (previously unreported) classes of USVs. I want to use DTW to assess the accuracy of my call classification scheme: are same-type calls more similar (less warping, lower DTW cost) to one another than when compared different-type calls?

*Broad overview of my approach:* I take the raw waveforms and transform them to a frequency-domain representations using stft. I convert the amplitude spectrogram to a dB-scaled spectrogram, and then plot the spectrograms. This is where I encounter my first problem - I get some noisy spectrograms. My data contains lots of non-stationary noise, making noise reduction difficult. I've tried different non-stationary noise reduction algorithms (e.g, [noisereduce.py](http://noisereduce.py), per channel energy normalization), but the results are sub-optimal. In the future I may try some more custom implementations, but I have a deadline to meet, so that's not feasible right now.

* My current stft parameters are nfft = 2048, hop\_length = nfft // 8, window = 'hamming'. From what I've tested so far, these parameters produce the cleanest spectrograms.

I've also tried interpolating the data to have the same lengths, but for a reason I'm yet to understand, this results in no warping whatsoever - all time series are perfectly aligned, even when this clearly should not be the case. However, as I understand it, DTW can work on different-length time series, so it's not necessary to resample my time-series to the same lengths.

I compute DTW using the tslearn library. My current dtw parameters: metric = 'cosine', global\_constraint=""sakoe\_chiba"", sakoe\_chiba\_radius=15.  I haven't implemented further constraints yet.

Here are some sample results, the warping in this first plot seems reasonable?

https://preview.redd.it/lwtl42laexbe1.png?width=695&format=png&auto=webp&s=582d4fcd8d76ab16e6581d2b56a61118f17ddf31

However in this example, the flat regions of the query and comparison spectrograms are being warped to 'fit' one another.

https://preview.redd.it/e3t3akoeexbe1.png?width=695&format=png&auto=webp&s=3225798404078812d0c8ba864e4bb3cd249911a6

and why is there warping along the front edge here? these calls are highly similar. can this be mitigated with boundary conditions?

https://preview.redd.it/adfbujqiexbe1.png?width=695&format=png&auto=webp&s=cea88d69e0111df0f79144a72d136cd26d298832

Minimal warping here but the query and comparison spectrograms have opposing directions of frequency modulation:

https://preview.redd.it/ftk5yp2lexbe1.png?width=695&format=png&auto=webp&s=36f6c92a72e4bd76ddd581ec65322b97802eb65f

I'd really appreciate any help, and I'm sorry if this is an inappropriate place to ask this question (please delete in that case). Thank you.",9,6,1736410722.0,MachineLearning
1hx5166,[R] [P] WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting,"A new long-term time series forecasting model, **WPMixer**, has been proposed. The model incorporates patching, embedding, and multiple mixing modules. It compares the results with state-of-the-earth TSMixer, TimeMixer, iTransformer, PatchTST, Crossformer, Dlinear, and TimesNet. The paper has been accepted in **AAAI-2025**.

Paper link: [WPMixer](https://arxiv.org/abs/2412.17176)

Code: [git](https://github.com/Secure-and-Intelligent-Systems-Lab/WPMixer)

https://preview.redd.it/0lwtmhkcewbe1.png?width=935&format=png&auto=webp&s=095cd9507b11171d4709783323ad86d8c43b7290

",10,0,1736397896.0,MachineLearning
1hwsl22,Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition,,28,1,1736364467.0,MachineLearning
1hxbs08,[D] Help Needed with Text Detection on Thumbnails Using EasyOCR,"\[D\] Hi everyone,

I'm currently working on a project where I need to use **EasyOCR** (Python) to detect text from a set of **thumbnail images**. However, I'm running into an issue where the text detection is **not 100% accurate**. It struggles to correctly recognize some parts of the text, and this affects the overall accuracy of my results.

Has anyone experienced similar issues with EasyOCR? Do you have any recommendations for:

1. **Improving text detection accuracy**?
2. Any **pre-processing techniques** I should try before running the images through EasyOCR?
3. Suggestions for **alternative OCR tools or libraries** that might work better with thumbnails?

The text on these thumbnails varies in **font styles**, **colors**, and **backgrounds**, so any advice on handling this would be greatly appreciated.

Thanks in advance for your help!",1,0,1736425958.0,MachineLearning
1hwkmwm,[R][D] What are the most important papers that provide entry to your domain of research?,"Please mention what domain (niche) of machine learning you work in for your research?

Why did you chose that particular domain?

If someone with basic understanding of machine learning and deep learning wants to get involved in your field, which papers/blogs/tools should they consider reading/implementing?",48,3,1736344129.0,MachineLearning
1hwh8um,[D] To Fellow researchers: What are your top 3 challenges in research?,"As researchers, we all face various hurdles in our journey. What are the top 3 challenges you encounter most often? Do you have any suggestions for improving these areas?

Your challenges could include:

* Finding a problem statement or refining your research question
* Accessing resources, datasets, or tools
* Managing time effectively or overcoming administrative tasks
* Writing, revising, and publishing papers
* Collaborating with others or finding research assistants

We’d love to hear your experiences! If possible, please share an anecdote or specific example about a problem that consumes most of your time but could be streamlined to improve efficiency.

We're a team of young researchers working to build an open community and FOSS AI tools (with ""bring your own key"" functionality) to simplify the end-to-end research process. Your input will help us better understand and address these pain points.",45,33,1736332718.0,MachineLearning
1hx67c5,[R] Dense Reward View on RLHF for Text-to-Image Diffusion Models,"ICML'24 paper: ""***A Dense Reward View on Aligning Text-to-Image Diffusion with Preference***""! (No, it hasn't outdated!)

In this paper, we take on a **dense-reward perspective** and develop a novel alignment objective that **breaks the temporal symmetry in DPO**\-style alignment loss. Our method particularly **suits the generation hierarchy of text-to-image diffusion models** (*e.g.* Stable Diffusion) by emphasizing the initial steps of the diffusion reverse chain/process --- **Beginnings Are Rocky**!

Experimentally, **our dense-reward objective significantly outperforms the classical DPO loss** (derived from assuming sparse reward) **in both the effectiveness and efficiency** of aligning text-to-image diffusion models with human/AI preference.

* Paper: [https://arxiv.org/abs/2402.08265](https://arxiv.org/abs/2402.08265)
* Poster: [https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204](https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204)
* Code: [https://github.com/Shentao-YANG/Dense\_Reward\_T2I](https://github.com/Shentao-YANG/Dense_Reward_T2I)",1,0,1736402073.0,MachineLearning
1hwbhuj,"[D] ML Engineers, what's the most annoying part of your job?",i just know a phd just inspecting datasets and that sounds super sad,97,122,1736310819.0,MachineLearning
1hwpbnh,[D][R] What conferences are on your list this year?,"What conferences are you planning to go to this year? On my list for computer vision / machine learning is:

* Nvidia GTC - March 17-24, San Jose CA
* CVPR, June 11-15, Nashville TN
* ICCV, October 20-24, Honolulu Hawaii
* Supercompute 25, Nov 16-21, St Louis MO
* Neuroips, Dec 9-15, San Diego CA

What's on yours?",9,4,1736356449.0,MachineLearning
1hwxgqj,[D] How is developing internal LLMs going? ,"a lot of yall have this task. I used to have this task. i want to create this thread to share insights and frustrations. hopefully shared solutions will help people in the same boat out.

please share:

1. vaguely what you're working on (""internal LLM for {use case}"")
2. your hurdles in getting the training data you needed
3. how much faith you have in how it's going/any rant material",2,3,1736376742.0,MachineLearning
1hwfs48,[R] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks,"**Paper:** [https://arxiv.org/pdf/2412.15204](https://arxiv.org/pdf/2412.15204)

**Abstract:**

>This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at [this https URL](https://longbench2.github.io/).

**Highlights:**

>**Single-Doc QA.** We integrate subtask categories from previous datasets (Bai et al., 2024b; An et al., 2024) and expand them to include QA for academic, literary, legal, financial, and governmental documents. Considering that detective QA (Xu et al., 2024) requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels. We also include Event ordering, where the goal is to order minor events according to the timeline of a novel. 

>**Multi-Doc QA.** To distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents. Besides the categories in single-doc QA, multi-doc QA also includes multinews QA, which involves reasoning across multiple news articles, events, and timelines.

>**Long In-context Learning**. \[...\] LongBench v2 includes several key tasks, including User guide QA, which answers questions with information learnt from user guides for electronic devices, software, etc.; New language translation (Tanzer et al., 2024; Zhang et al., 2024a), which involves learning to translate an unseen language from a vocabulary book; Many-shot learning (Agarwal et al., 2024), which involves learning to label new data from a handful of examples. 

>**Long-dialogue History Understanding.** \[...\] These tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., Agent history QA (Huang et al., 2024), and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., Dialogue history QA (Wu et al., 2024a).

>**Code Repository Understanding.** Code repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task. 

>**Long Structured Data Understanding.** \[...I\].e., Table QA (Zhang et al., 2024c), and answering complex queries on knowledge graphs (KGs), i.e., Knowledge graph reasoning (Cao et al., 2022; Bai et al., 2023). We anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.

**Visual Highlights:**

https://preview.redd.it/c93xt3zeiqbe1.png?width=947&format=png&auto=webp&s=6790ef5e76ccd5e943990a089d1501297531aad4

https://preview.redd.it/uruwiqtfiqbe1.png?width=771&format=png&auto=webp&s=b6df076c68f06a1cc62b2acc2e45d8df9ec04d53

https://preview.redd.it/v5w6y9jhiqbe1.png?width=915&format=png&auto=webp&s=1e71a19d3853da7dd015308d8b7fa341af843e0c

https://preview.redd.it/x8a0i4aiiqbe1.png?width=649&format=png&auto=webp&s=c91afeeb17822624eda40d534474c4059eeae845

[The up-to-date top of the leaderboard \(the interactive version is available at the linked repo\). Notably, includes DeepSeek v3 result. Note also substantial GPT-4o nerfing going from 2024-08-06 ver. to 2024-11-20 ver.](https://preview.redd.it/2wafoijjiqbe1.png?width=1192&format=png&auto=webp&s=ff0cd16148eded0bef383d2e7c52ccbc37119ac9)

",13,2,1736326882.0,MachineLearning
1hvy385,[R][D] White Box Transformers,"Opening a thread on this line of research:
https://ma-lab-berkeley.github.io/CRATE/

As I understand it, the authors basically have framed the process of learning effective representations of data as the problem of finding a dictionary of multivariate gaussians that cover the data distribution with parsimony.
In particular, with sparse coding in terms of features/gaussians.

Building an architecture which takes multiple alternate steps of ""clustering"" similar vectors and respectively orthogonalizing the vectors from different clusters, they end up with a structure analogous to Vision Transformer.
A MultiHead Attention-like module clusters vectors, brings them closer to local principal directions or manifolds, and a MLP-like module moves this vectors along axes that are mutually more orthogonal.
Mathematically they are approximating a well defined sparse coding rate, hence the white box algorithm, however I can't say the math is more intuitive than that of Transformers.

Indeed, the CLS attention heads of the last layer have interpretable preferences under image classification supervised training, as in DINO (self-supervised) or with SimPool.
This is directly connected to the interpretation of the process, and opens up to explanations of the interpretability and dynamics of DINO.
It is also referred to an architecture blueprint for visual intelligence by George Hinton, the GLOM transformer.

I think the clustering effect of attention is somehow under appreciated in the literature, as much as the action of FFNs in Transformers is under studied.
I wonder if there's a third way mathematically as straightforward as the MLP and as intuitive as the gaussian dictionary of features.",62,6,1736274233.0,MachineLearning
1hwmmze,How do the real time TTS models work? [Discussion],I was was wondering what models are used for the real-time text-to-speech programs or if it was just a really fast input model and output model put together. ,0,11,1736349808.0,MachineLearning
1hwdw40,[D] Anyone tried predibase/lorax?,"[https://github.com/predibase/lorax](https://github.com/predibase/lorax)

Predibase/Lorax is really an interesting repo. It solves major problem of using an adapters, i.e., assigning an adapter dynamically. Did anyone try it out?",4,6,1736319362.0,MachineLearning
1hw0mn9,[R][P] distillKitPlus: High Performent Knowledge Distillation for LLMs,"An open-source toolkit for LLM KLD with **LoRA Fine-Tuning** and **Quantization Support**

Larger LLMs generalize better and faster. You leverage leaverage this and then transfer the best of 70B model to a 7B model without breaking the bank or sacrificing performance.

GitHub Link: [https://github.com/agokrani/distillkitplus](https://github.com/agokrani/distillkitplus)",17,2,1736280508.0,MachineLearning
1hvx2pq,"[D] ML engineers, what is the most rewarding thing about your job?","Some people tell me that it's the paycheck, but I think it depends on your experience level and who you work for? Is there more to this job?",27,26,1736271753.0,MachineLearning
1hvqdvt,"[D] What is the most fascinating aspect of machine learning for you?
",Title. You can interpret this question as subjectively as you would like. ,51,63,1736253054.0,MachineLearning
1hvxpcc,[D] Positional Embeddings in Embedding Space,"How are the original Position Encodings distributed in feature space?
How are RPE distributed?
What is the interplay of these embeddings and LayerNorm (which removes the component parallel to the uniform vector, the vector of ones)?",14,1,1736273271.0,MachineLearning
1hwal1h,[R][P] Open-sourced Project and Paper on Denser Reward for RLHF PPO Training,"In this paper, **the granularity of action space in RLHF PPO training is studied**, assuming only binary preference labels. **Segment-level RLHF PPO and its Token-level PPO variant outperform bandit PPO** across AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks under various backbone LLMs.

1. Paper: [https://arxiv.org/pdf/2501.02790](https://arxiv.org/pdf/2501.02790)
2. Code: [https://github.com/yinyueqin/DenseRewardRLHF-PPO](https://github.com/yinyueqin/DenseRewardRLHF-PPO)
3. Prior work on token-level reward model for RLHF: [https://arxiv.org/abs/2306.00398](https://arxiv.org/abs/2306.00398)",1,0,1736307802.0,MachineLearning
1hvwnde,"[N] ESwML 2025 Call For Papers [March 31, 2025] (In Conjunction with ASPLOS-25/EuroSys-25)","This is a CALL FOR PAPERS for:

**ESwML 2025**

**The Second International Workshop on Empowering Software Development through Machine Learning**

[https://eswml.github.io/2025/2025.html](https://eswml.github.io/2025/2025.html)

**Important Deadlines:**

Submission due date: February 7, 2025 (AoE)  
Author notification: February 21, 2025  
Workshop scheduled date: March 31, 2025

**Call For Papers**

The software of tomorrow will heavily rely on the use of machine learning models.   
This will span various aspects including using Machine Learning (ML) models during   
software development time to enhance developer productivity, designing ML   
heuristics to improve application execution, and adopting surrogate Neural   
Networks (NN) models within applications to replace expensive computations  
 and accelerate their performance. However, several challenges limit the   
broad adoption of ML in today’s software. The goal of Empowering Software  
 Development through Machine Learning (ESwML) half-day workshop is to establish   
a platform where researchers, scientists, application developers, computing center staff,  
 and industry professionals can come together to exchange ideas and explore how artificial  
 intelligence can help in effective and efficient use of future systems.

This workshop will actively drive discussion and aim to answer the following questions:

This workshop will actively drive discussion and aim to answer the following questions:

\* How can we leverage the advances in Machine Learning to ease the software development process?  
\* What tools are missing to bridge the interaction with ML models during application development?  
\* Can we improve the accuracy and efficiency of ML models by exposing to them existing analytical tools? For example, enabling Large Language Models to interact with memory sanitizers etc.  
\* How can we seamlessly integrate ML models into applications to improve their performance while  
   ensuring the correctness of the generated outputs?

Paper and abstract submission

We seek abstracts describing recent or ongoing research related to the research topics  
 in the ESwML workshop. All researchers and practitioners are welcome to submit their  
work for presentation at this workshop. This is an in-person workshop and only the slides   
will optionally be posted on the workshop website.

Short papers must be submitted electronically as PDF files. Format is 1-4 double-column   
Pages excluding references. Submissions should be printable on US Letter or A4 paper.  
Please submit your manuscripts through hotcrp.  
[https://eswml25.hotcrp.com/](https://eswml25.hotcrp.com/)

Note: Presentations and short papers will be made available online only with the explicit consent   
of the authors. Authors who wish to share their presentations are encouraged to inform the workshop organizers.

Workshop Co-chairs  
\* Florina Ciorba (University of Basel, Switzerland), florina.ciorba at [unibas.ch](http://unibas.ch)  
\* Harshitha Menon (Lawrence Livermore National Laboratory, USA), harshitha at [llnl.gov](http://llnl.gov)  
\* Konstantinos Parasyris (Lawrence Livermore National Laboratory, USA) parasyris1 at [llnl.gov](http://llnl.gov)",3,0,1736270718.0,MachineLearning
1hvj7fx,[D] Optimization techniques in NLP/LLM that also works in transformers based sequence modeling?,"Title.

Trying to brainstorm if there are techniques that work in NLP use cases that I can apply in sequence modeling.

Specifically, I am trying to optimize the transformers used in recommender systems (user representation modeling).

So far the basics I can think of are: flash attention, efficient/linear transformers, fused kernel embedding, mixed precision/quantization for training/serving.

Anything else or any other papers come to mind?

I think the main problem sometimes is that the concept of a token in something like user sequence representation or rec sys is drastically different from that of LLM. We also deal with embeddings that are much more sparse…

Thanks in advance!",21,2,1736223592.0,MachineLearning
1hvotzx, [D][R] How to stay up-to date in Neural Architecture Search,"Greetings all, specifically I am looking for recommendations into venues which publish literature for the field of Neural Architecture Search, aside from [AutoML](https://2025.automl.cc/) and [NeurIPS](https://neurips.cc/). Any newsletters or blogs and the like would also be highly appreciated (aside from [automl itself](https://www.automl.org/) ofc).



Other than the aforementioned info my interests lie in the intersection of NAS techniques into Computer Vision and RL if it helps in any way.



Thank you in advance and cheers!",6,2,1736246997.0,MachineLearning
1hw04bx, [D] Hyperparameter Optimization with Metaheuristic algorithms ,"I'm currently working on my thesis on this topic, I started off with image classification with CNN's as my professor suggested it. However apparently I can not run more than 25-30 iterations because it's heavy on ram. There are not much papers about this area too. I see that there are much faster algorithms like Bayesian Optimization, and they yield similar results.  
Is this is a dead area of research? Where can I go from here?",1,0,1736279262.0,MachineLearning
1hvvto8,[D] Which model is best for training on flattened street-level images?,"TL;DR:
I’m working on a school project to recognize locations in a small town using flattened 360° images captured with an Insta360 camera, labeled with GPS coordinates. The goal is to predict the GPS location of a regular phone photo (not 360°) by training a visual place recognition model. I’m considering DELF, LoFTR, vision transformers (ViT/DINO), or fine-tuning ResNet/EfficientNet, but I’m unsure which is best for handling equirectangular projections and this specific task. Any advice on model selection or dataset preparation would be greatly appreciated!

Hi everyone!

I’m currently working on a school project where I’m trying to recognize specific locations in a small town based on street-level images. To collect the data, I’m using an Insta360 camera and capturing 360° images at regular intervals. I’m also ensuring that the data includes images taken at different times of the day and under various weather conditions to make the model more robust.

To prepare the data for training, I’m converting the 360° images into flattened equirectangular projections. In some cases, I may also crop these into smaller views, like cube map projections. Each of these processed images is labeled with GPS coordinates, which I want the model to predict later when given a new query image. The query images would be regular photos taken with a phone, so they won’t be 360° images but instead just standard portrait or landscape shots.

I’ve been researching possible models for this task and have come across DELF, LoFTR, and vision transformers like ViT or DINO. I’m not sure which model would be the most suitable for my project, as I need something that can handle visual place recognition based on flattened or cropped 360° images. I’m also considering whether fine-tuning a pretrained model like ResNet or EfficientNet might be a better approach.

I would really appreciate any advice or recommendations on which model might work best for this kind of problem. If anyone has experience working with equirectangular projections or training datasets for visual place recognition, I’d love to hear your thoughts. Thank you in advance for your help!",0,10,1736268694.0,MachineLearning
1hvk8m0,[D] ACL ARR public anonymous preprint,I submitted my paper to ARR dec cycle and checked the box to publish public anonymous preprint. I still couldnt find a preprint link after 3 weeks. Does any one know when do i get the link for public anonymous preprint?,7,0,1736227095.0,MachineLearning
1huxrd2,[D] Misinformation about LLMs,Is anyone else startled by the proportion of bad information in Reddit comments regarding LLMs? It can be dicey for any advanced topics but the discussion surrounding LLMs has just gone completely off the rails it seems. It’s honestly a bit bizarre to me. Bad information is upvoted like crazy while informed comments are at best ignored. What surprises me isn’t that it’s happening but that it’s so consistently “confidently incorrect” territory,137,210,1736166043.0,MachineLearning
1hv5qoe,[P] Interactive and geometric visualization of Jensen's inequality,"Hi Community,

I have been learning Jensen's inequality in the last week. I was not satisfied with most algebraic explanations given throughout the internet. Hence, I wrote a post that explains a geometric visualization, which I haven't seen a similar explanation so far. I used interactive visualizations to show how I visualize it in my mind. 

  
Here is the post [https://maitbayev.github.io/posts/jensens-inequality/](https://maitbayev.github.io/posts/jensens-inequality/)

  
Let me know what you think",39,8,1736187606.0,MachineLearning
1hveq4q,[D] Mathematical proofs as benchmarks for novel reasoning?,"I'm not an expert but I have been following the academic discussion about LLMs and reasoning pretty closely and I don't think there has been any sufficient benchmarks to demonstrate reasoning as opposed to simply applying information directly from the training data (iteratively in the case of CoT).

An ideal benchmark would have 3 properties:
1. A *clear* demonstration of novel reasoning, not simply the solving of a difficult problem or the application of advanced techniques 
2. Easy (or as close to easy as possible) to verify the correctness and existence of reasoning 
3. Easy to control contamination of the training or tuning data

As for point 1 it's clear that generally the only way we can ensure novel reasoning is to use academic topics, because novel reasoning is the bulk of their purpose 
Point 2 makes a lot of fields where what constitutes correctness or reasoning is hard to determine poor choices, ie is using historical context and a list of plot points reasoning in literature? Probably not but how can you tell what is when those are key parts of analysis? How can we say what is correct in history when historians disagree on what a few artifacts from the bronze age imply?
Point 3 also eliminates many fields that are directly discussed in a wide variety of possible training material or where their general techniques are, making it infeasible to curate training data that has no contamination 

From my knowledge the only type of problem that fits is mathematical proof, specifically we can more easily isolate what is novel in a proof, more easily verify the correctness of a proof (1 expert giving a pass could detect most major errors as opposed to teams with non definitive answers), and make sure the training data is free of both the actual proof or the direct steps to it (my understanding is that o3's frontier math score was due to iteratively finding mathematical techniques that already existed and fit the knowledge it has at that stage)

Specifically I propose that the best proof for a benchmark would be one that was very significant and required the invention of new mathematics (so that it definitely requires multiple steps of novel reasoning and has a length long enough to not just guess), is no longer the state of the art (we can control contamination by using a general training set that almost certainly won't have expert mathematics and hand picked mathematics up until the proof in question, plus by having further generalization in the field it will be easy to verify alternative approachs to the proof for validity), and should be more abstract in nature, ie abstract algebra or group theory or fermat's last theorem instead of differential equation techniques so that less existing techniques directly apply

I would suspect that without novel reasoning any answers would be wrong in obvious ways and easy to detect, and any answers with only subtle errors would be easy to retry with slight differences in tuning/training to get right

So I would like to know: is this idea at all plausible? If so what proofs would be best?",9,27,1736210166.0,MachineLearning
1huvvqk,[Discussion] Embeddings for real numbers?,"Hello everyone. I am working on an idea I had and at some point I encounter a sequence of real numbers. I need to learn an embedding for each real number. Up until now I tried to just multiply the scalar with a learnable vector but it didn't work (as expected). So, any more interesting ways to do so?

Thanks",20,20,1736158515.0,MachineLearning
1hv6zb4,[P][D] Cuda-torch compatibility issue for older driver versions despite installing cuda-compat,"Hello,   
I am working on a older-version of GPU machine (due to my office not actually updating the os and GPU drivers). The Nvidia driver is Version 470.233.xx.x and it's CUDA version is 11.4

I was limited to using \`torch==2.0.1\` for the last few years. But the problem arose when I wanted to fine-tune a Gemma model for a project, whose minimum requirement is torch>=2.3. To run this, I need a latest CUDA version and GPU driver upgrade. 

The problem is that I can't actually update anything. So, I looked into a cuda-compat approach, which is a forward-compatibility layer for R470 drivers. Can I use this for bypassing the requirements? If so, my torch2.5 is still unable to detect any GPU device. 

I need help with this issue. Please!",5,2,1736190606.0,MachineLearning
1hvsouv,[D] why do we need regularization if we have learning rate. ,I know everything about both the topics but i want some solid proof or some example where i can see benefits of regularization. Please share it if you have any,0,19,1736260351.0,MachineLearning
1hux0dx,[P] Churn Prediction Two Months in the Future – Need Advice on Dataset and Model,"Hi everyone!

I recently started working as a data scientist, and I've been assigned to a project to create a churn prediction model. Specifically, the goal is to predict the probability of a customer churning precisely two months in the future

Since I'm the only one in the team and it's my first time working with real-world data, I'm not entirely sure how to approach this and make the right decisions.

For now, I structured the dataset by taking six months of historical data (e.g., customer X, 202401, features (related to that month), churn flag, customer X, 202402, features (related to that month), churn flag, etc...).

Once I did that, I used this disaggregated data and applied a Random Forest classification model. However, I ended up with very poor performance metrics.

So, I have a few questions:

* For a dataset containing monthly historical data, which model would be more appropriate to apply (in this case, for churn prediction)? Should I use Aggregation, Disaggregation with lag, Time series, Survival analysis, or something else? And in that case, how should I arrange the dataset?
* Currently, the dataset includes flags indicating whether the customer performed certain actions during that month. Is there a better way to handle this type of information?
* Do you have any tips for handling imbalanced data and which metrics to consider? I used SMOTE on the training set to balance the minority class and looked at the F1-score as a metric.
* If you suggest keeping the dataset as is or aggregating it, should the churn flag refer to two months ahead from the row’s month (e.g., customer x, 202401, features (related to that month), churn flag (churn in 202403))? Currently, I recreate the target month (two months ahead) by updating the time-varying features from the last month of the historical data.

Thanks a lot!",11,2,1736163195.0,MachineLearning
1hurxls, Self-supervised Learning - measure distribution on n-sphere [D] [R],"Most of self-supervised learning methods (SimCLR, MoCo, BYOL, SimSiam, SwAV, MS BYOL, etc.) use an n-sphere hypersphere where the extracted features (after encoder + projection/prediction head) are distributed. The loss function then uses the features distributed on this hypersphere for its loss computation.

Papers such as:

* Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, Tongzhou Wang et al.; ICML 2020
* Align Representations with Base: A New Approach to Self-Supervised Learning, Shaofeng Zhang et al; CVPR 2022
* Rethinking the Uniformity Metric in Self-Supervised Learning, Xianghong Fang et al.; ICLR 2024

and others show that these features are distributed all over the n-sphere for each class.

What are the different ways in which we can measure the distribution of these embedded features in this hypersphere? Say, if I were to randomly choose a class from ImageNet/CIFAR-100 dataset, how can I measure the distribution of all images belonging to this class on this n-sphere?",26,2,1736141613.0,MachineLearning
1hupxdg,[D] Discrete diffusion models,"What are the most promising and most recent achievements in the diffusion for discrete distributions?

So far, I have taken a look at:

* https://arxiv.org/abs/2107.03006
* https://arxiv.org/abs/2310.16834v2

Is there anything more recent or more promising?",37,4,1736134964.0,MachineLearning
1hux0p0,[D] What (human) languages to learn?,"Hi,

This is not the typical LLM doomerism post, but rather an ML-specific career discussion. 

I am an avid learner of new languages (human spoken languages), especially Latin and Romance ones.

Wanted to know if there are languages that open up interesting opportunities for ML practitioners.

Are there non-English regions with demand for ML practitioners but not enough supply of skilled, native practitioners?",7,23,1736163230.0,MachineLearning
1hubl11,[P] I made a CLI for improving prompts using a genetic algorithm,,231,22,1736096673.0,MachineLearning
1hvacrc,[D] Working With Multidimensional NPZ/PKL Data [SMPL/AMASS],"I am working on a project that involves fine tuning with human motion related data. For that, I was advised to work with the SMPL/AMASS databases which are stored in npz/pkl files. I have never worked with similar data types, but one of the groups has 3 dimensional data, which is not possible with csv. Can someone please help me how I can work with these databases.",1,3,1736198743.0,MachineLearning
1husa4d,[R] 3D Vision-Language-Action Generative World Model,,19,2,1736142867.0,MachineLearning
1huqwji,[N] In-Memory Vector Store powered by HNSW Graph,"Hey folks! I’ve now **added a fully command-based vector store** in Treds, powered by an HNSW graph for approximate nearest-neighbor searches. Here’s a quick look at the four commands:

1. **VCREATE** – Initializes a vector index, specifying parameters like maxNeighbors, layer factor, and efSearch.
2. **VINSERT** – Inserts vectors into that index.
3. **VSEARCH** – Searches for the k nearest neighbors to a given vector.
4. **VDELETE** – Deletes a vector from the index by its ID.

Commands can be executed in redis-cli, as Treds is RESP compliant. A simple session might look like

    VCREATE vec 6 0.5 100
    VINSERT vec 1.0 2.0
    VINSERT vec 2.0 3.0
    VINSERT vec 3.0 4.0
    VSEARCH vec 1.5 2.5 2

This creates an index named `vec`, inserts some 2D vectors, searches for the 2 nearest neighbors to `[1.5, 2.5].`**Vectors can be N-Dimension as well**.

If you checked out Treds before, I’d love to hear your thoughts on the new vector store addition. If you haven’t, feel free to give it a look and let me know if you have any suggestions or questions. Thanks for reading, and happy hacking!

[https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store](https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store)  
[https://github.com/absolutelightning/treds](https://github.com/absolutelightning/treds)",12,2,1736138111.0,MachineLearning
1hum9p4,[D] Any background removal models trained on FOSS data?,"I'll be contributing to a project that is very strict on copyright, down to the ML tools used. Many of the models I've found don't specify what data they're trained on (and some are trained on images generated by scrape-trained models, which isn't allowed in my case).

The closest I've found are those [BiRefNet](https://arxiv.org/abs/2401.03407) models that are trained solely on [DIS5K](https://arxiv.org/abs/2203.03041); the images are ""commercial use and mods allowed"" (presumably CC BY and/or BY-SA), but the dataset itself has [terms of use](https://github.com/xuebinqin/DIS/blob/main/DIS5K-Dataset-Terms-of-Use.pdf) that prohibit commercial usage.",7,2,1736124092.0,MachineLearning
1htz91k,"[D] Does human intelligence reside in big data regime, or small data regime?","The frontier LLMs of today have trillion+ parameters and are trained on 500 trillion+ tokens.

Human brain has 86 billion neurons and 100 trillion+ synapses.

The amount of textual information any person consumes is several orders of magnitude less than what LLMs are trained on. However, the human eye captures visual information at an approximate rate of 10Mbps. Add other senses like hearing, touch, balance, smell, and a human child consumes more information in the first few years of their life than any LLM has ever seen.

This seems to suggest that human intelligence requires big data.

But what about people who were blind from birth? What about congenital deaf-blindedness (no documented cases)?",56,60,1736057245.0,MachineLearning
1htpuuv,[R] How Barlow Twins avoid embeddings that differ by affine transformation?,"I am reading the [Barlow Twins (BT) paper](https://arxiv.org/pdf/2103.03230) and just don't get how it can avoid the following scenario.

The BT loss is minimized when the cross-correlation matrix equals the identity matrix. A necessary condition for this to happen is that the diagonal elements *C\_ii* are 1. This can be achieved in 2 different ways. For each *x*:

1. *zA*=*zB*

2. *zA*=*a*⋅*zB*\+*b*

where *zA* and *zB* are embeddings of different augmentations of the same input *x*.  In other words, embeddings can differ but this difference is masked due to: *corr(X,aX+b)=corr(X,X)=1*.

Intuitively, if our aim is to learn representations invariant to distortions, then the 2nd solution should be avoided. Are there any ideas on what drives the network to avoid this scenario?",54,6,1736028628.0,MachineLearning
1hu168k,[D] Randomised SVD/PCA for Efficient Attention Mechanisms - any potential?,"I've had this idea rattling in my brain for a little now, and would love some input on whether it has potential - there's so many proposed efficiency improvements to attention, I've lost track of what has and hasn't been tried!

The process would be something to the effect of:

1. First compute the Keys and Queries as normal
2. Then, conduct randomised PCA on the queries to identify the D largest components of the Query space. 
3. For each of the D largest components, keep the Key vector that best matches that component
4. Do regular attention on those Keys.

  
Given typical attention for a sequence of length N has complexity O(N\^2), while randomised PCA is O(D\^2), there's potentially some pretty big inference time savings here.

  
I can't see any existing research into whether this has legs. LoRA and Linformers come close in that they also use lower-rank approximations, but I think what i'm proposing is unique. Any insights?

",6,2,1736063750.0,MachineLearning
1hty2jr,"[R] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",,8,4,1736053053.0,MachineLearning
1htocnj,[R] I’ve built a big ass dataset ,"I’ve cleaned/processed and merged lots of datasets of patient information, each dataset asks the patients various questions about themselves. I also have whether they have the disease or not. I have their answers to all the questions 10 years ago and their answers now or recently, as well as their disease status now and ten yrs ago. I can’t find any papers that have done it before to this scale and I feel like I’m sitting on a bag of diamonds but I don’t know how to open the bag. What are your thoughts on the best approach with this? To get the most out of it? I know a lot of it is about what my end goals are but I really wanna know what everyone else would do first! (I have 2500 patients and 27 datasets with an earliest record and latest record. So 366 features, one latest one earliest of each and approx 2 million cells.) Interested to know your thoughts",37,37,1736024617.0,MachineLearning
1htp9tv,"[Project] Finding inputs where deep learning models fail
","Hi there! Last month at NeurIPS (an ML conference), I read an interesting paper ""Human Expertise in Algorithmic Prediction"" that describes a framework for determining where ML models are outperformed by human experts. I found the authors' work to be very interesting. Below, I explore their framework further and extend it to multiclass classification. My results are pretty surprising, showing that a group of modern model architectures have trouble with dogs and cats in CIFAR-10.

GitHub Link: [https://github.com/sunildkumar/model\_indistinguishability](https://github.com/sunildkumar/model_indistinguishability)

Paper Link: [https://arxiv.org/abs/2402.00793](https://arxiv.org/abs/2402.00793)",29,6,1736027048.0,MachineLearning
1htw7hw,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",11,9,1736046909.0,MachineLearning
1htohxj,[D] will NeurIPS invited talks be made public?,"Hi all,

The neurips 2024 has yet to make invited talks public and accessible to those not registered:  
[https://neurips.cc/virtual/2024/eventlistwithbios/invited%20talk](https://neurips.cc/virtual/2024/eventlistwithbios/invited%20talk)

people who attended the last neurips: can you access the talks online? if yes, does this mean the talks will not be made public this year? 2023, 2022 made it public:

[https://neurips.cc/virtual/2023/eventlistwithbios/invited%20talk](https://neurips.cc/virtual/2023/eventlistwithbios/invited%20talk)

[https://neurips.cc/virtual/2022/events/Invited%20Talk](https://neurips.cc/virtual/2022/events/Invited%20Talk)

thanks!



",26,2,1736024994.0,MachineLearning
1htedss,[P] Noteworthy AI Research Papers of 2024 (Part One),,81,22,1735997334.0,MachineLearning
1htm15z,[P] Implementing the StyleGAN2,"\[P\] Hi all, I've been working on a blog series recently called the path to StyleGAN2 and I finally got to the StyleGAN2. I have a writeup here: [https://ym2132.github.io/StyleGAN2](https://ym2132.github.io/StyleGAN2)

My aim is to walk through the paper the code and the training process. I hope you find it useful and I would appreciate any feedback :)",28,2,1736018530.0,MachineLearning
1hugszn,[N] SemiKong: The World’s First Open-Source Semiconductor-Focused LLM,"Anyone else heard about SemiKong? apparently its the first open-source LLM made specifically for semiconductor R&D. They’re saying it can speed up chip design by like 30% by directly integrating stuff like design protocols and simulation data into its workflow.

This seems like a pretty big deal for chip design which is usually super resource-heavy and kind of slow. Do you think more niche domain-specific LLM's like this could be the future? or are there too many challenges in integrating something like this into existing workflows?

[https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/](https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/)",0,1,1736109934.0,MachineLearning
1htkg5l,[R] Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,"Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.

[Arxiv Paper](https://arxiv.org/abs/2409.17270)",20,1,1736014408.0,MachineLearning
1htehx6,[R]Infinity ∞ : Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis,"We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024×1024 image in 0.8 seconds, making it 2.6× faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.

* Opensource：https://github.com/FoundationVision/Infinity
* Paper link: [https://arxiv.org/abs/2412.04431](https://arxiv.org/abs/2412.04431)
* Project Page：https://foundationvision.github.io/infinity.project/
* Demo website：https://opensource.bytedance.com/gmpt/t2i/invite

[Text-to-Image results from Infinity.](https://preview.redd.it/dxt24ng3ezae1.png?width=1842&format=png&auto=webp&s=8b4233e4dc05bc84fbeb797c8a46422fd9cbdaa3)

 Building on the prediction of the next resolution level, Infinity models the image space with a finer-grained bitwise tokenizer. They have expanded the vocabulary size to infinity, significantly increasing the representation space of the image tokenizer and raising the upper limits of autoregressive text-to-image generation. The model sizes have been scaled up to 20B. Currently, both the models and the code are open-sourced, and they also provide an online experience website.

What kind of chemical reaction will an infinite vocabulary and large models ignite? Experimental data shows that this new text-to-image method, named Infinity, not only directly defeats Stable Diffusion 3 in image generation quality, but also fully inherits the speed advantages of VAR. The 2B model is 3 times faster than SD3, and the 8.5B model's inference speed is 8 times faster. As a purely discrete autoregressive text-to-image model, Infinity stands out among autoregressive methods, vastly outperforming approaches like HART, LlamaGen, and Emu3, thereby establishing itself as the new king in the field of autoregressive text-to-image generation. Additionally, Infinity surpasses diffusion-based state-of-the-art methods like SDXL and Stable Diffusion 3, reclaiming ground in the battle between autoregressive and diffusion models.

[Evaluation on the GenEval and DPG benchmark. ](https://preview.redd.it/0jm384cgfzae1.png?width=2366&format=png&auto=webp&s=46552654b87625e10ed25ed8ce3520768205e5f6)

In human evaluations, users conducted double-blind comparisons of images generated by Infinity versus HART, PixArt-Sigma, SD-XL, and SD3-Medium, assessing overall appearance, instruction adherence, and aesthetic quality. HART is also based on the VAR architecture and combines diffusion and autoregressive methods, while PixArt-Sigma, SD-XL, and SD3-Medium are SOTA diffusion models. The results showed that Infinity defeated the HART model with a beat rate of nearly 90%, demonstrating Infinity's strong position among autoregressive models. Additionally, Infinity outperformed SOTA diffusion models such as PixArt-Sigma, SD-XL, and SD3-Medium with beat rates of 75%, 80%, and 65% respectively, proving that Infinity can surpass diffusion models of the same size.

[Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.](https://preview.redd.it/soxna03rfzae1.png?width=1280&format=png&auto=webp&s=4c140bf4e66d486ebfe522acb507a66f621f8f4c)

# Bitwise Token Autoregressive Modeling Enhances High-Frequency Representation

Simplicity at its finest, Infinity's core innovation lies in proposing a bitwise token autoregressive framework. By discarding the traditional ""index-wise token"" and utilizing fine-grained ""bitwise tokens"" composed of +1 or -1 for predicting the next resolution level, Infinity shows strong scaling properties. Under this framework, Infinity achieves better performance by continuously scaling the visual encoder (Visual Tokenizer) and transformer.Bitwise Token Autoregressive Modeling Enhances High-Frequency Representation

[Framework of Infinity. Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier \(IVC\), and Bitwise Self-Correction.](https://preview.redd.it/azq1z6syfzae1.png?width=2724&format=png&auto=webp&s=cede27b05c24b1e49d1d52ca08b8d74041f4a8eb)

# The infinite vocabulary extends the representation space of the Tokenizer.

From the perspective of information theory, the continuous Visual Tokenizer used by diffusion models has an infinite representation space, while the discrete Visual Tokenizer used by autoregressive models has a finite representation space. This leads to a higher compression of images by the Tokenizer used in autoregressive models, resulting in a poorer ability to reproduce high-frequency details. To improve the upper limit of autoregressive image generation, researchers have attempted to expand the vocabulary to enhance the effectiveness of the Visual Tokenizer. However, the autoregressive framework based on Index-wise Tokens is very unsuitable for expanding the vocabulary. The prediction method of Tokens in autoregressive models based on Index-wise Tokens is shown on the left side of the figure below, where the model's parameter count is directly proportional to the size of the vocabulary. When \\( d = 32 \\), the vocabulary size is \\( 2\^{32} \\), and the transformer classifier predicting Index-wise Tokens requires \\( 2048 \\times 2\^{32} = 8.8 \\times 10\^{12} \\) = 8.8T parameters! The parameter count of just one classifier reaches the parameter count of 50 GPT3 models, making it obviously impossible to expand the vocabulary to infinity in this situation.

https://preview.redd.it/sq7somspgzae1.png?width=1280&format=png&auto=webp&s=dc72e3ed7aebc94d51ee4edec1523cb042b43876

Speed

In addition to its superior performance, Infinity fully inherits the speed advantage of VAR in predicting the next resolution level, significantly outpacing diffusion models in inference speed. The 2B model generates a 1024x1024 image in just 0.8 seconds, which is 3 times faster than the similarly-sized SD3-Medium and 14 times faster than the 12B Flux Dev. The 8B model is 7 times faster than the similar-sized SD 3.5. The 20B model generates a 1024x1024 image in 3 seconds, still nearly 4 times faster than the 12B Flux Dev.

https://preview.redd.it/61i1mq8sgzae1.png?width=1280&format=png&auto=webp&s=b27959f9cbf4f5a8e3d1526dfb234e82ff4871b5

",20,2,1735997722.0,MachineLearning
1htch73,[P] I wrote optimizers for TensorFlow and Keras,"Hello everyone, I wrote optimizers for TensorFlow and Keras, and they are used in the same way as Keras optimizers.

[https://github.com/NoteDance/optimizers](https://github.com/NoteDance/optimizers)",24,2,1735989921.0,MachineLearning
1htpnp4,"[P] A Pure-Python, Dependency-Free Neural Network Inference Framework","Hello everyone, I’ve been working on a framework that enables the inference of small pre-trained PyTorch neural networks without requiring the installation of dependencies. The entire framework is in a single file to be easily copied into projects.

Obviously, the performance is terrible compared to PyTorch (\~500x slower), so the purpose of the framework is, firstly, when installing dependencies is impossible and, secondly, for educational purposes.

As of right now, the basic functionality is working (reading PNG images, loading model weights, and running inference of CNNs), but more advanced features are not yet implemented. If anyone is interested in using or contributing, here is the link. [Github Repo](https://github.com/manu12121999/ctrl_c_nn)",2,4,1736028093.0,MachineLearning
1ht2m3y,[D] Can LLMs write better code if you keep asking them to “write better code”?,"https://minimaxir.com/2025/01/write-better-code/

This was a thereotical experiment which had *interesting* results. tl;dr, the answer is yes, depending on your definition of ""better.""",115,39,1735953150.0,MachineLearning
1ht19tf,[R] High-performance deep spiking neural networks with 0.3 spikes per neuron,"# Abstract

Communication by rare, binary spikes is a key factor for the energy efficiency of biological brains. However, it is harder to train biologically-inspired spiking neural networks than artificial neural networks. This is puzzling given that theoretical results provide exact mapping algorithms from artificial to spiking neural networks with time-to-first-spike coding. In this paper we analyze in theory and simulation the learning dynamics of time-to-first-spike-networks and identify a specific instance of the vanishing-or-exploding gradient problem. While two choices of spiking neural network mappings solve this problem at initialization, only the one with a constant slope of the neuron membrane potential at threshold guarantees the equivalence of the training trajectory between spiking and artificial neural networks with rectified linear units. For specific image classification architectures comprising feed-forward dense or convolutional layers, we demonstrate that deep spiking neural network models can be effectively trained from scratch on MNIST and Fashion-MNIST datasets, or fine-tuned on large-scale datasets, such as CIFAR10, CIFAR100 and PLACES365, to achieve the exact same performance as that of artificial neural networks, surpassing previous spiking neural networks. Our approach accomplishes high-performance classification with less than 0.3 spikes per neuron, lending itself for an energy-efficient implementation. We also show that fine-tuning spiking neural networks with our robust gradient descent algorithm enables their optimization for hardware implementations with low latency and resilience to noise and quantization.

[https://www.nature.com/articles/s41467-024-51110-5](https://www.nature.com/articles/s41467-024-51110-5)

",72,3,1735949404.0,MachineLearning
1hsxkkk,[Discussion] I trained an AI model to generate Pokemon,"The past few months I have been working on a project to utilize deep learning to generate Pokemon images/names and predict typing. Wanted to share my results here. 

Implementation Details: [https://github.com/smaley02/Pokemon-Generation/tree/main?tab=readme-ov-file](https://github.com/smaley02/Pokemon-Generation/tree/main?tab=readme-ov-file)

All 900 Fake Pokemon: [https://smaley02.github.io/gallery.html](https://smaley02.github.io/gallery.html)

https://preview.redd.it/fzefyqu0kuae1.png?width=748&format=png&auto=webp&s=e5723c9d4696e083c7049f7e36605e9f0f51443f

  
",120,19,1735939721.0,MachineLearning
1hsp3mo,[R] / [N] Recent paper recommendations,"Hello, as the new year came, I expect many research teams to have released their work for that juicy ""et al. 2024"". I am very interested in papers regarding transformers and theoretical machine learning, but if you have a good paper to share, I will never say no to that.

Thank you all in advance and have a great day :)",22,14,1735918424.0,MachineLearning
1hso6rf,[D] ReLU + linear layers aa conic hulls,"In a neural network with ReLU activations, a composition of linear layer with matrix P onto ReLU, maps the inputs into the conic hull of the columns of P. 

Are there any papers exploiting this fact for interesting insights?",20,9,1735916020.0,MachineLearning
1htbe7x,[Discussion]  Agentic AI: Yet another hyped interface or a paradigm shift? ,"This post is for discussing the radius of impact of Agentic AI.  
Agentic AI is being served as something new on the plate, while looking deeply it looks like a conventional system which interacts with some other APIs through a framework.

Looking through different lenses:

**Developer**  
Not much deviation from conventional development. Hence minimal learning curve

**Customers**

Agentic AI might shift focus from web surfaces to chatbots or probably some new kind of surfaces. Given this happens, the role of intuitive/interative UIs may reduce

**Business**

Increase in efficiency for some, while loss for business for others. Service based companies might spearhead the development initially.

**Radius**

B2B or B2C, which will be impacted more.",0,24,1735985199.0,MachineLearning
1hsv7if,[D] Thoughts and suggestions ,"I have a project that need a real time object detection by using Al, currently i am planning to use the raspberry pi 4b 8gb ram but i notice that when i use the laptop i found it quite heavy to run it so maybe raspberry pi might not have enough power to run it due to absence of gpu, so in your opinion does the handheld gaming console (steam deck, rog ally) is good enough to train and run the Al because i need a device that have a compact size but powerful enough, i have consider the jetson nano and mini pc but both of them is quite pricey. i am looking for the 2nd hand model only. Thank you",3,3,1735933674.0,MachineLearning
1hs41pt,[Discussion] How is LLM changing your job as a ML engineer ,"I just watched Andrew Ng’s talk on AI agents. He talked about how traditional ML tasks could take 6 months but now it only needs a weekend with LLMs. 

It’s at 2-4mins into this talk. https://youtu.be/KrRD7r7y7NY?si=XDCAm7NFTMO3ayn3 

Specifically, I guess he’s saying you can do zero shot learning with LLMs instead of gathering large amounts of labelled data, build and deploy a model. He used the example of sentiment analysis tasks. 

I wonder if any one is experiencing this shift in productivity at work as a ML scientist. 

My experience is companies don’t want to use chatGPT directly and try to build their own in house LLMs, I guess for data privacy and cost concerns. 

Please share your experience. ",127,50,1735850958.0,MachineLearning
1hsplsc,[D] / [R] What are your thoughts on LLMs 'understanding' their domain and enhancing domain understanding?,"Hello everyone,

I've been thinking about studying the effects of trying to enhance an LLMs understanding of the domain it is applied to, but I'm unsure if it's worthwhile and if there's enough to go off.

Without explaining too much and boring you guys: Basically, during my last project I fine-tuned LLama by throwing a dataset with 200 examples for two classes at it (400 examples in total), and got an F1 of around 76%. This also included a few-shot prompt.

But I can't help but wonder what if the LLM was taught the domain context more properly, maybe through ontologies and knowledge graphs? And could custom tokenization improve its ability to understand and generate better responses?

I'm thankful for any input you might have and if anything comes to mind that I could look into to enhance a models understanding of its domain. If you think this isn't worthwhile, I'd also be happy to hear it and maybe why you think so.",1,6,1735919765.0,MachineLearning
1hs7syq,[Project] Making a chess engine visualization that lets you see how a neural network based chess engine thinks,"Hey everyone, I'm a hs student working on this chess visualization tool for a school project that uses lc0, featuring neural network evaluation heatmaps made through the verbose output mode and engine analysis. You can play against the engine or use it as an analysis tool to see how a NN based engine to see how it ""thinks"". link to

youtube preview: [https://www.youtube.com/watch?v=7nbWr8TR6nA](https://www.youtube.com/watch?v=7nbWr8TR6nA)

[opening screen of game](https://preview.redd.it/ic9ksgr5znae1.png?width=1784&format=png&auto=webp&s=cf23028e3deee99b7e048461953006c0fd13dd13)

github: [https://github.com/jay63683/BlackBox-Chess-a-XAI-leela-chess-GUI](https://github.com/jay63683/BlackBox-Chess-a-XAI-leela-chess-GUI) Requires Processing to run. Or you can just watch the video tutorial if you dont want to download processing. Planning switching engine to ONNX  for future updates that allow me to explain processes much more in depth using ONNX tools. Would appreciate any feedback.",38,4,1735860388.0,MachineLearning
1i7szvv,"Curated gallery of well-funded, early-stage startups + jobs","Here's **600**\+ manually curated, VC-backed startups (AI, fintech, devtools, cybersecurity, analytics) that are growing and hiring. FYI this isn't another spreadsheet or list. Lots of Series A-C startups building out their data science teams (go to /jobs and type ""data scientist""). And yes, I know startups aren't for everyone, but hopefully these are the better ones: [https://startups.gallery/](https://startups.gallery/)",2,1,1737598983.0,bigdata
1i79nxh,AI DATA SCIENTIST- A NEW CLASS OF SPECIALIST ROUTINE,"Gain an insight into the life of a **data science** professional as you understand the top skills needed including **data labeling, AI, and machine learning**. Read now!

https://preview.redd.it/cuc9iyzedjee1.jpg?width=1200&format=pjpg&auto=webp&s=56c83cdee2d1e155d2191ece9711b1e57e505de9

",0,0,1737547720.0,bigdata
1i6i3aj,How AI Agents & Data Products Work Together to Support Cross-Domain Queries & Decisions for Businesses,,1,0,1737464575.0,bigdata
1i62y89,"Hey friends! Have you heard about this awesome tool for business analysts in the VC scene? It streams live data on all the startups that scored VC funding globally, with loads of historical info! If you're curious or want to try it out, just drop a comment!",,0,0,1737411781.0,bigdata
1i5ohu9,Explore How Python is Revolutionizing Healthcare Technology,"Python is transforming healthcare technology, and here's why it’s the perfect fit! From data analysis to machine learning, Python is the go-to language for healthcare innovation. Curious to see how it’s changing the game?

https://preview.redd.it/543rf79n05ee1.jpg?width=1080&format=pjpg&auto=webp&s=d89e25b31da4989119ea82ff055be98d70feae81

",0,2,1737373924.0,bigdata
1i5693h,"Hey friends, you’ve got to check out this amazing tool that tracks VC investments in real-time! 🌐💸 It’s super useful for seeing which companies are getting funding and even offers detailed insights into industries and key players. A fantastic resource if you're diving into the VC world!",,0,0,1737313856.0,bigdata
1i4vwb3,"Hey friends, if you're curious about the VC world, I just found this amazing live investment tracker that shows all the VC funding happening globally! It's super insightful for data analysis on companies and decision makers. A game-changer if you're looking to learn the ins and outs of venture capit",,1,1,1737283678.0,bigdata
1i4j95u,Solidus AI Tech - Among Binance's Top 5 Alpha Projects!,"Everything starts again TRUMP launching her own token before she becomes president and BTC is a good start and we should not forget artificial intelligence projects

Solidus AI Tech u AITECH has solidified its leadership in Web3 and AI innovations and gained the trust of global investors by being ranked among Binance's Top 5 Alpha Projects.

https://preview.redd.it/30gbjjvswtde1.png?width=1750&format=png&auto=webp&s=514c3218a7027b993cafefd80544c267f6c6999d

Why This Matters

Visibility and Recognition: AITECH's recognition by Binance puts the project on the radar of global investors and increases investor confidence.

Adoption and Growth: Such recognition can accelerate Solidus AI Tech's adoption and support growth in its ecosystem.

Leadership: Being featured on a major platform like Binance helps Solidus AI Tech position itself as a leader in Web3 AI innovations.

What's Next ?

This achievement increases the potential for Solidus AI Tech to attract more collaboration and investment in its future projects. The Solidus AI Tech community celebrates this significant milestone and looks forward to the future.",6,0,1737239474.0,bigdata
1i3dkwk,Cancer Immunotherapy & Big Data/AI Technology,"Cancer touches millions of lives, and the journey to better treatments is one we take together. On January 23rd, 2025, at 11:00 AM EDT / 09:30 PM IST, join us for a thought-provoking webinar, The Intersection of Cancer Immunotherapy & Big Data/AI Technology. 

Link to Register: [https://www.senzmate.com/publish/webinar-7/](https://www.senzmate.com/publish/webinar-7/)",2,2,1737110300.0,bigdata
1i2vmec,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",5,2,1737052018.0,bigdata
1i2yqv1,Exploring Database Isolation Levels,,2,1,1737059971.0,bigdata
1i366ao,"High-key, if you’ve got a service to sell, I totally recommend pitching to fresh VC-funded startups! I hit $5k in monthly recurring revenue in just a month using this clever app to find decision-makers and dropping them a DM. Trust me, it’s way easier than it sounds!",,0,1,1737080871.0,bigdata
1i25j32,Connect Power BI to PowerPoint and Google Slides with Rollstack (www.Rollstack.com),,5,0,1736969263.0,bigdata
1i1wid5,Evolving Data Models: Backbone of Rich User Experiences (UX) for Data Citizens,,3,0,1736944848.0,bigdata
1i1qzdd,Free Webinar: Accelerate AI Value with Teradata and Google Cloud,"📅 Date: 01/15/2025  
⏰ Time: 7:30 AM PT / 4:30 PM CET  
🔗 Register here: [https://www.brighttalk.com/webcast/19856/632920?utm\_source=TDDev&utm\_medium=brighttalk&utm\_campaign=632920](https://www.brighttalk.com/webcast/19856/632920?utm_source=TDDev&utm_medium=brighttalk&utm_campaign=632920)

As a data professional, you want to build solutions that help your company and customers.

There is significant value in unstructured data stored in formats such as text, audio, and more, which you can leverage to achieve this goal.

Advanced Large Language Models (LLMs), like Google’s Gemini, can simplify the process of introducing structure into unstructured data, enabling individuals and organizations to derive insights that better serve their customers.

Join Janeth Graziani, Developer Advocate, Teradata and Merlin Yamssi, Lead Solutions Consultant AI/ML CoE, Google Cloud, as they explore, demo, and discuss how data analysts, engineers, and scientists, can leverage Teradata VantageCloud and Google Cloud to accelerate your AI innovation from development to production.

Janeth and Merlin are excited to share how you can:

\- Get faster results from your AI/ML initiatives by quickly building and training ML models with Vertex AI and the powerful in-database analytics functions of ClearScape Analytics  
\- Easily build and deploy powerful gen AI solutions with Teradata VantageCloud Lake, Vertex AI, and Gemini  
\- Transform customer complaint management through advanced generative AI for precise and automated classification. Janeth will give a complaints classification demo which leverages Teradata Vantage and Google Gemini.

Kate Russell, technology journalist, will moderate this webinar and make sure your questions are addressed by our experts.

https://reddit.com/link/1i1qzdd/video/wokg2qjpk3de1/player

",1,0,1736920647.0,bigdata
1i1b9pe,Just announced: Tableau Conference #TC25 Registration is Open! Who is going? ,,1,0,1736875452.0,bigdata
1i0v37l,Hey friends! I just stumbled upon this awesome tool that gathers info on VC funded startups and helps you find contacts of key decision-makers. It’s a game changer for anyone looking to pitch services! Let me know if you're curious to give it a whirl!,,0,2,1736819780.0,bigdata
1hxdk90,I learned how big data fuels AI on platforms like Instagram and Pinterest,"I wrote an article about how **AI influences social media**, deciding what we see in our feeds, ads, and content. Key points:

* **Facebook and Instagram** use Meta AI to figure out what shows up in your feed based on what you like, comment on, or share.
* **TikTok’s Monolith AI** studies what you watch and interact with to fine-tune your For You Page.
* **LinkedIn** suggests jobs, articles, and connections that match your career goals.
* **YouTube** recommends videos and even picks when ads pop up during what you watch.
* **Pinterest’s PinSage AI** suggests pins and products based on your searches and saves.

It’s remarkable how much AI controls our online experience, but sometimes it can feel a little too spot-on.

If you want to tweak what you see:

* Check your privacy settings regularly to see what data is being used.
* Use tools like “Not Interested” to refine your feed.
* Be mindful of what you interact with—it directly affects future recommendations.

If you’re curious about how it all works, here is the full article: [https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/](https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/)

Have you noticed how accurate your feeds are lately? Do you find it helpful, or is it over the top?",3,1,1736431711.0,bigdata
1hxacjv,Federated Modeling: When and Why to Adopt,,3,0,1736420337.0,bigdata
1hvjyig,"Optimizing Retrieval Speeds for Fast, Real-Time Complex Queries","Dear big data geniuses:

  
I'm using snowflake to do complex muliti-hundred line queries with many joins and window functions. These queries can take up to 20 seconds. I need them to take <1 second. The queries are fully optimized on snowflake and cant be optimized further. What do you recommend?",6,3,1736226116.0,bigdata
1hurwxn,How to create HIVE Table with multi character delimiter? (Hands On) ,,2,0,1736141542.0,bigdata
1hthw2p,"50+ Incredible Big Data Statistics for 2025: Facts, Market Size & Industry Growth",,6,0,1736007688.0,bigdata
1htc6wj,25 Best Project Management software in 2025,,0,0,1735988681.0,bigdata
1hsv9pr,About go get into Big Data ,"About to get into Big Data 

Hey there 

I’m 29 with background experience in farming, biology and nature with some skills related to tech and computers, looking forward to learn more about #BigData as I want to develop another career. 

What are your recommendations, tips, advices, etc.? 


p.s. Also my first time posting in Reddit, greetings from México🌮🌶️🇲🇽",8,3,1735933831.0,bigdata
1hsyr6i,"Hey folks! If you're in VC or a business analyst, you’ve got to check out this tool. It streams live data of VC-funded startups globally and gives you quick access to tons of company history (there's even a CSV or API option). Let me know if you want to give it a shot!",,1,0,1735942713.0,bigdata
1hs7pyr,[Poll] Has anyone used dbt's AI (dbt copilot) yet? What has your experience been?,,2,0,1735860170.0,bigdata
1hqgfr8,guidance for finish and review my first mini-project,"Hello guys , could anyone help me with reviewing and guide me thoughout my mini-project for big data ? ,this involves designing a (textual) information search engine and analyzing user reviews of your search engine.

here is the link : [https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0](https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0)",4,0,1735657345.0,bigdata
1hq0ym3,How automation and AI advanced data-driven reporting in 2024 [LinkedIn Post] ,,2,0,1735601913.0,bigdata
1hpq5vk,"Hey friends, if you're looking for a simple way to make some sales, you should consider selling to new startups that just landed venture capital! I found this awesome app that tracks real-time funding announcements, gathers verified emails of decision-makers, and even summarizes their buying hints w",,0,0,1735573971.0,bigdata
1houh90,Hadoop vs. Spark: Which One Should Beginners Learn First?,,6,6,1735472946.0,bigdata
1hotwap,Welcome to r/BigDataEngineer: Let’s Build and Grow Together!,,0,1,1735470480.0,bigdata
1hkfzpa, Big data Hadoop and Spark Analytics Projects (End to End) ,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",26,1,1734927955.0,bigdata
1hkkq2k,Searching For Hive Alternatives,"My current setup is Hive on Tez, running on YARN with data stored in HDFS.  
I feel like this setup is a bit outdated, and that the performance is not great. However I can't find alternatives.  
Every technology I found so far fails in one of the requirements that I'll mention.

I have the following requirements:

1. Be able to handle huge analytical batch jobs, with multiple heavy joins
2. Scalable (Petabytes)
3. Fault-tolerant, jobs must finish
4. On-premise

Would like to hear your suggestions!",2,3,1734948441.0,bigdata
1hkukvg,"Don't make the CFO wait. Use Rollstack to automate recurring reports (QBRs, Annual Reports, MBRs, etc.,) ",,0,1,1734980002.0,bigdata
1hj6p76,Will Data Science be a big deal in 2025?,"# 1. Getting to know Data Science

# Explaining Data Science

Think of data science as a high-tech detective blending stats, math, and code skills to sniff out cool clues and crack tough puzzles in humongous data piles.

# Why Data Science Rocks Today

Nowadays, with all our lives so wrapped up in data, data science is pretty much a magic element. It's what makes your Netflix picks so spot on, forecasts trends, and helps companies make super-smart choices.

# 2. What's Hot in Data Science

# All About Big Data Analytics

Imagine big data as an all-you-can-eat info spread. Data scientists are like skilled foodies who know how to fill their plates picking out the tasty bits of knowledge that can spice up business plans and spark new ideas.

# Machine Learning and AI Uses

Self-driving automobiles and digital helpers are causing a revolution in our tech interactions, and data scientists are the wizards working magic to make it happen.

# Ways to Present Data

Data visualization turns snooze-fest tables into enthralling masterpieces. It allows a quick grasp of intricate data and shares knowledge with others super .

# 3. What Makes Data Science So In-Demand

# The Rise of Making Choices Based on Data

Since data's become the hot commodity, companies are super eager for data pros. They need these smart folks to transform basic digits into powerful wisdom to guide top-level choices and help their biz expand.

# AI and Automation Demand More Data Pros

The demand for data scientists to create and improve algorithms for AI and automation is soaring. These skills are becoming red-hot in the employment sphere.

# Meeting the Bar for Regulatory Stuff

In our super connected era where keeping data safe is huge, companies want data scientists to help them wade through the complex rules to make sure they play fair and keep data use on the up-and-up.

# 4. The Tough and Good Stuff in Data Science

# Keeping Data Safe and Sound

With data mishaps popping up in the news, data scientists have the tough job. They've got to dig out the good stuff from the data while making sure none of the secret info gets into the wrong hands. They're juggling keeping things fresh and new with making sure everything stays locked down tight.

# Lack of Data Science Experts

As more people want data experts than there are available, this creates a tough spot but also a huge chance for folks aiming to jump into this area offering great jobs and fat paychecks.

# Data Science Rocks Various Sectors

Whether it's in health or money stuff, data science is causing a stir across different work areas. It's leading cool things like making meds just for you spotting cons, and figuring out groups of buyers, proving just how much it can do and how cool it can be.

# 5. What Data Science Might Look Like in 2025

# What to Expect in the Data Science Work Scene

Heading into 2025, folks can expect the data science job scene to keep on climbing. With companies in all sorts of businesses getting how critical data-informed decisions are, there's gonna be a huge ask for data science whizzes. Anyone in data science is looking at some pretty sweet career moves and loads of chances to snag a job.

# Tech Upgrades Making Waves in What's Next

Tech upgrades are huge in deciding [what's next for data science](https://www.usdsi.org/data-science-insights/future-of-data-science-10-predictions-you-should-know). All the cool stuff like artificial intelligence learning machines, and big-time data studies will push forward new stuff for data scientists to do in 2025. Jumping on the tech bandwagon is super important to not fall behind in data science's fast-paced world.

# 6. Tech Stuff Changing the Data Scene

# Blending Blockchain with Crunching Numbers

Blockchain is about to make a big splash in the number-crunching game. It's gonna ramp up security and make sure everything is clear and trackable when it comes to moving digits around. Merging this tech with the brainy science of data could start a whole new game for keeping our online facts straight and real when everything is linked up.

# Making Sense of Internet of Things (IoT) Stats

Okay so all these Internet of Things gadgets are spitting out crazy amounts of info that's got some real golden nuggets hidden in there. By 2025, the brainiacs working with numbers will gotta dig in with some fancy figuring-out tricks to pull out the gems from this data gush. Getting a grip on this IoT number crunching is key for groups looking to smarten up their choices and spark some fresh ideas.

# 7. What You Gotta Have to Be a Data Scientist in 2025

# Know Your Coding and Gadget Game

Data scientists waiting for 2025 got to know their stuff with a bunch of coding languages and gadgets. You gotta be tight with Python, R, SQL, and TensorFlow. Being a wizard with these allows you to mess with big complex data, cook up some solid predictive stuff, and pull out the kind of know-how that makes businesses rock and roll.

# ",0,7,1734774582.0,bigdata
1hhsukf,"Build Real-Time Systems with NATS and Pathway, Scalable Alternatives to Apache Kafka and Flink","Hey everyone! I wanted to share a tutorial created by a member of the Pathway community that explores using [NATS](https://docs.nats.io/) and [Pathway](https://pathway.com/) as an alternative to a Kafka + Flink setup.

The tutorial includes step-by-step instructions, sample code, and a real-world fleet monitoring example to show how you can simplify data pipelines while still handling large volumes of streaming data. It walks through setting up basic publishers and subscribers in Python with NATS, then integrates Pathway for real-time stream processing and alerting on anomalies.  
  
**App template link (with code and details):**  
[https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink](https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink) 

**Key Takeaways:**

* **Seamless Integration:** Pathway’s native NATS connectors allow direct ingestion from NATS subjects, reducing integration overhead.
* **High Performance & Low Latency:** NATS delivers messages quickly, while Pathway processes and analyzes data in real time, enabling near-instant alerts.
* **Scalability & Reliability:** With NATS clustering and Pathway’s distributed workloads, scaling is straightforward. Message acknowledgment and state recovery help maintain reliability.
* **Flexible Data Formats:** Pathway handles JSON, plaintext, and raw bytes, so you can choose the data format that suits your needs.
* **Lightweight & Efficient:** NATS’s simple pub/sub model is well-suited for asynchronous, cloud-native systems—without the added complexity of a Kafka cluster.
* **Advanced Analytics:** Pathway supports real-time machine learning, dynamic graph processing, and complex transformations, enabling a wide range of analytical use cases.

Would love to know what you think—any feedback or suggestions.",11,0,1734614782.0,bigdata
1hhq92i,MASTER DATA SCIENCE ACCELERATE YOUR FUTURE,"https://preview.redd.it/iebxtn3dbs7e1.jpg?width=800&format=pjpg&auto=webp&s=36f9d0083def416dadc2e309f6e0544dab469776

    Organizations need data-driven leaders. With the USDSI® Certification, master data science skills that unlock insights, fuel decisions, and accelerate business growth. Become the data expert companies trust.
    ",2,0,1734605056.0,bigdata
1hhc8b5,I built an end-to-end data pipeline tool in Go called Bruin ,"Hi all, I have been pretty frustrated with how I had to bring together bunch of different tools together, so I built a CLI tool that brings together data ingestion, data transformation using SQL and Python and data quality in a single tool called Bruin:

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)

Bruin is written in Golang, and has quite a few features that makes it a daily driver:

* it can ingest data from many different sources using [ingestr](https://github.com/bruin-data/ingestr)
* it can run SQL & Python transformations with built-in materialization & Jinja templating
* it runs Python fully locally using the amazing [uv](https://github.com/astral-sh/uv), setting up isolated environments locally, mix and match Python versions even within the same pipeline
* it can run data quality checks against the data assets
* it has an open-source [VS Code extension](https://bruin-data.github.io/bruin/vscode-extension/overview.html) that can do things like syntax highlighting, lineage, and more.

We had a small pool of beta testers for quite some time and I am really excited to launch Bruin CLI to the rest of the world and get feedback from you all. I know it is not often to build data tooling in Go but I believe we found ourselves in a nice spot in terms of features, speed, and stability.

Looking forward to hearing your feedback!

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)",8,0,1734557142.0,bigdata
1hg9u75,The Art of Discoverability and Reverse Engineering User Happiness,,2,0,1734439533.0,bigdata
1hgdqtn,String to number in case of having millions of unique values,"Hello,  
I am currently working on preprocessing big data dataset for ML purposes. I am struggling with encoding strings as numbers. I have a dataset of multiple blockchain transactions and I have addresses of sender and receivers for these transactions. I use pyspark.

I've tried String Indexer but it throws out of memory errors due to number of unique values. How should I approach it? Is hasing with SHA256 and casting to big int good approach? Wouldn't big numbers influence ML methods too much? (i will try different methods ex. random forests, gan, some based on distance etc)",1,0,1734451126.0,bigdata
1hg8twp,Data Science Projects for Beginners | Infographic,"One way to excel above your competitors in the race for top data science jobs is by showcasing your practical experience and a strong portfolio to demonstrate your data science skills and knowledge practically. Check out our detailed infographic to learn about popular [data science projects](https://www.usdsi.org/data-science-insights/role-of-major-components-in-data-science-projects) for beginners that you can work on to apply your theoretical data science knowledge practically and build a strong portfolio. 

https://preview.redd.it/qn8ilmoxbe7e1.jpg?width=1080&format=pjpg&auto=webp&s=ff704d8e8a7fe6909dae083229e1c34771b95507

",1,0,1734435744.0,bigdata
1hfup8a,Step-by-Step Tutorial: Setting Up Apache Spark with Docker (Beginner Friendly),"Hi everyone! I recently published a video tutorial on setting up Apache Spark using Docker. If you're new to Big Data or Data Engineering, this video will guide you through creating a local Spark environment.

📺 Watch it here: [https://www.youtube.com/watch?v=xnEXAD9kBeo](https://www.youtube.com/watch?v=xnEXAD9kBeo)

Feedback is welcome! Let me know if this helped or if you’d like me to cover more topics.",2,0,1734386073.0,bigdata
1hfn1h0,Free Ungated Whitepaper: Personalized healthcare reporting with data and AI,,2,0,1734366719.0,bigdata
1hdzmhe,Data-Driven Recruitment The WorkWolf Revolution,"Discover how WorkWolf is transforming the recruitment game by reducing bias and enhancing efficiency with data-driven solutions. As the future of work becomes more data-centric, HR professionals must adapt to ensure ethical and fair hiring practices. [WorkWolf Revolution](https://www.usdsi.org/data-science-insights/data-driven-recruitment-using-workwolf-to-reduce-bias-and-increase-efficiency)

https://preview.redd.it/yvoj2h33cs6e1.jpg?width=1080&format=pjpg&auto=webp&s=58d3f24e9d25fbc390dab129366db0ebb290f53b

",0,0,1734169453.0,bigdata
1hdckux,30 Best IDE Software for Developers in 2025,,0,0,1734097729.0,bigdata
1hdbw7e,"DATA VISUALIZATION IN R: CHEATSHEET AHEAD OF 2025 | INFOGRAPHIC

","Understanding data science has never been this convenient as it amalgamates with the R programming language. [Data science in R](https://www.usdsi.org/data-science-insights/data-visualization-in-r-cheatsheet-ahead-of-2025) is turning tables for deeper data-driven business insights to guide a better business landscape ahead. 

https://preview.redd.it/kehq19q88m6e1.jpg?width=1200&format=pjpg&auto=webp&s=545aa2e49a1d1c962d8e8067575aaffe1ed33ab6

",0,1,1734095544.0,bigdata
1hch7ch,Data Science Roadmap 2025,"Explore the evolutionary journey of data science as it intertwines human intelligence with cutting-edge technology. This roadmap delves into essential skills, tools, and adaptations required to thrive in the ever-changing analytics landscape of 2025. [Data Science Roadmap 2025](https://www.usdsi.org/data-science-insights/data-science-roadmap-2025-a-darwinian-evolution-of-analytics)

https://preview.redd.it/06sw992fwd6e1.jpg?width=1920&format=pjpg&auto=webp&s=d82a7a1fd59ac6a5022323e7b4d717b0feacfa3a

",4,1,1733994686.0,bigdata
1hcai3m,How Do You Do Data?,"Just curious about the types of infrastructure you folks use. Specifically, what kind of chips are you using to train/fine-tune/run your deep models?

I appreciate you filling out this  survey.

[https://forms.gle/uiAmfG9K7MpFvQtK7](https://forms.gle/uiAmfG9K7MpFvQtK7)",0,0,1733969122.0,bigdata
1hc5gm9,For those like me who like to have music on the background while working ,"I often need background music to help me increase my productivity while working. I created these playlists which I update regularly They help me stay calm, focused and productive. Perfect academia playlists! 



Ambient, chill & downtempo trip (a tasty mix of ambient, downtempo, IDM, trip-hop, electronica, jazz house music and more. Chill, hypnotic, trippy and atmospheric grooves for focus, relaxation, and deep listening) [https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg](https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg) 



Mental food (A bit of the same atmosphere as the previous one) [https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow](https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow) 



Something else (atmospheric, poetic, calm, soothing, cinematic and ambient soundscapes with a touch of mystery. Relaxing instrumental music for focus, relaxation, introspection, reading, writing, studying, meditation and mindfulness practice.) [https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS\_JvzkUYUw](https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS_JvzkUYUw) 



Pure ambient (calming ambient music designed to enhance focus, relaxation, study, meditation, sleep, and mindfulness) [https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ](https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ) 



Chill lofi day (mix of smooth lofi hip-hop beats, chillhop, jazzhop and soothing vibes. Chill background music for studying, working, reading or just unwinding) [https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ\_vPqdYQc-idTOg-kt5Vg](https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ_vPqdYQc-idTOg-kt5Vg) 


French Producers (dedicated to new independent French producers.  Several electronic genres covered but mostly chill) [https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ](https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ) 



Jrapzz (the latest in modern jazz with a mix of Nu-Jazz, Jazzhop, Acid Jazz, Jazz UK, Ambient Jazz, Jazztronica, Jazz House, Nu-Soul, Hip-Hop Jazz, rather chill) [https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA](https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA) 



Cool stuff (chill indie pop & rock fresh finds, from emerging independent artists and few recognized talents) [https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA](https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA) 



Enjoy! 


\- 


H-Music",0,4,1733954734.0,bigdata
1hbsznu,Governance for AI Agents with Data Developer Platforms,,2,0,1733922232.0,bigdata
1hbocvq,Data Science Command the Future of Businesses in 2025?,"Data science has been transforming businesses for a long time now. But are these technologies capable of changing the future of the world? Download our comprehensive resource to understand the impact of data science on the world's future. To [download](https://www.usdsi.org/data-science-insights/resources/can-data-science-command-the-future-of-businesses-in-2025), click below.

https://preview.redd.it/x9f6awrqa66e1.jpg?width=1050&format=pjpg&auto=webp&s=0bf5a1720f7671c964936a13a2921a876a025fe1

",2,0,1733902639.0,bigdata
1hb2w3b,2025 Guide to Architecting an Iceberg Lakehouse,,3,2,1733840675.0,bigdata
1hb0upu,"Hey, I collected IMO the best product analytics tools for 2025","Helloo, I made a blogpost about the possible best product analytics tools (warehouse native and traditionals). Feel free to add any experience or comment. Thank youu

https://medium.com/@pambrus7/6-product-analytics-tool-for-2025-ab9766510551",3,0,1733834425.0,bigdata
1haepvt,Has anyone tried this analytics automation tool yet? (Rollstack) What did you think? ,,4,0,1733764394.0,bigdata
1ha3ecw,Any good sources of Social Media/Search Engine Keyword Usage by Day?,"Hey there,

After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the Data Science Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat.",2,8,1733724850.0,bigdata
1ha3bc9,Certified Lead Data Scientist  2025,"Enhance your data science skills and knowledge to drive innovation, build efficient data science models, and manage data science projects effectively with the best data science certification from USDSI® for [CERTIFIED LEAD DATA SCIENTIST  - CLDS™](https://www.usdsi.org/data-science-certifications/certified-lead-data-scientist).

https://preview.redd.it/whm05s4pkr5e1.jpg?width=1080&format=pjpg&auto=webp&s=65f6ec8e5cbf2dba071249f7ca88e7b07da65971

",0,0,1733724516.0,bigdata
1h9lu8v,🚀 Quant Interview Prep - New Videos Added! 🚀,"To all aspiring Quants out there, I’ve restarted my journey of creating content around quantitative interview questions and brain teasers! These videos will help you get familiar with the types of questions typically asked in interviews for roles like quantitative analyst, data scientist, and more.📹

Check out my latest video here: [https://www.youtube.com/@prakarshduhoon1116](https://www.youtube.com/@prakarshduhoon1116)

Here is my LI: [https://www.linkedin.com/in/prakarshd/](https://www.linkedin.com/in/prakarshd/); I am ex Quant with 7 years of exp, working at top funds like Millennium and WorldQuant

If you find the content useful, feel free to like, share, and spread the word with your network. Together, we can make interview prep easier and more effective! Let's crush those interviews! 💪

[\#Quant](https://www.linkedin.com/feed/hashtag/?keywords=quant&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantInterviews](https://www.linkedin.com/feed/hashtag/?keywords=quantinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#InterviewPrep](https://www.linkedin.com/feed/hashtag/?keywords=interviewprep&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#TechInterviews](https://www.linkedin.com/feed/hashtag/?keywords=techinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#Finance](https://www.linkedin.com/feed/hashtag/?keywords=finance&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#BrainTeasers](https://www.linkedin.com/feed/hashtag/?keywords=brainteasers&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantitativeAnalysis](https://www.linkedin.com/feed/hashtag/?keywords=quantitativeanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#CareerGrowth](https://www.linkedin.com/feed/hashtag/?keywords=careergrowth&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296)",2,0,1733673485.0,bigdata
1h8tc2f,Certified Data Science Professional  2025,"Certified data science professionals are in huge demand because of the rapid adoption of data science technologies. So, kickstart your data science journey by mastering the fundamentals and building a strong foundation with the best #beginner-level CERTIFIED DATA SCIENCE PROFESSIONAL CDSP™.

https://preview.redd.it/p9v7vz6qpf5e1.jpg?width=1080&format=pjpg&auto=webp&s=72198191ab9bb6f1068cfa14c652e3767ca58d80

",0,0,1733580855.0,bigdata
1h6x0jf,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  


Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website,](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]",4,0,1733361485.0,bigdata
1h6x4jv,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  
  
Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]",2,0,1733361797.0,bigdata
1h6bm3x,Future of Data Science Technologies and Trends,"This read caters to deciphering the [future of data science](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends). Make it a priority to understand these core nuances before diving in as a seasoned data scientist! Explore the  to know more.

https://preview.redd.it/83az58h8os4e1.jpg?width=1080&format=pjpg&auto=webp&s=ff2a6e5367de2ea4028036402e0f66613ca013d7

",1,0,1733301829.0,bigdata
1h5uqly,"Rollstack Product Updates December 2024, AI-Powered Data Insights, Collections, and More",,5,1,1733251620.0,bigdata
1h5lev5,Amazon EKS Auto Mode: What It Is and How to Optimize Kubernetes Clusters,,2,0,1733225677.0,bigdata
1h5itgm,10 Essential Conda Commands for Data Science - KDnuggets,,3,0,1733214217.0,bigdata
1h4ty8p,The Rise of Open-Source Data Catalogs: A New Opportunity For Implementing Data Mesh,,1,0,1733142872.0,bigdata
1h4rugm,HOW TO BUILD YOUR ORGANIZATION DATA MATURE?,"Take your organization from data exploring to data transformed with this comprehensive guide to data maturity. Discover the four key elements that determine data maturity and how to develop a data-driven culture within your organization. Start your journey to data transformation with this insightful guide. Become [USDSI® Certified](https://www.usdsi.org/data-science-certifications) to lead your team in creating a data-driven culture. 

https://reddit.com/link/1h4rugm/video/mxjthjhrue4e1/player

",0,0,1733134555.0,bigdata
1h4mprc,TRANSFORM YOUR CAREER PATH WITH USDSI®'s DATA SCIENCE CERTIFICATION PROGRAM,"Take your data science career to the next level with USDSI’s industry relevant certification program. Whether you're a students, professionals, and career switchers, our program offers practical skills and knowledge with minimal time commitment. 

https://preview.redd.it/n3q7rz344d4e1.jpg?width=1080&format=pjpg&auto=webp&s=9ffd2bb36307533cde2c6cf2240aa820e1862960

",0,0,1733113536.0,bigdata
1h44frf,Web scraping booking,"Hi folks, I’m working on a data project with a deadline today, and I urgently need help scraping [Booking.com](http://Booking.com) for hotel data in the top 20 cities in France. Objective: I need to scrape hotel information such as: Hotel names Average ratings Number of reviews Locations (latitude and longitude, if possible) Issues I’m Facing: My script only fetches results for one city (e.g., Lyon), even though I’m iterating through 20 cities. Some requests return unexpected content, likely due to session or cookie-handling issues. I suspect [Booking.com](http://Booking.com)’s anti-scraping measures may be blocking my script. What I’ve Tried: Sending city names dynamically via query parameters with requests.get(). Using headers and cookies to mimic a real browser. Adding delays between requests to reduce the chances of being blocked. What I Need: Guidance on why my requests aren’t fetching results for all cities. Advice on handling anti-scraping measures effectively (e.g., proxies, better headers). Suggestions on switching to Selenium or sticking with Requests and BeautifulSoup. My Environment: Language: Python Libraries used so far: Requests, BeautifulSoup Target cities: python Copier le code \[""Paris"", ""Marseille"", ""Lyon"", ""Toulouse"", ""Nice"", ""Nantes"", ""Strasbourg"", ""Montpellier"", ""Bordeaux"", ""Lille"", ""Rennes"", ""Reims"", ""Toulon"", ""Saint-Étienne"", ""Le Havre"", ""Grenoble"", ""Dijon"", ""Angers"", ""Nîmes"", ""Villeurbanne""\] I urgently need assistance since my deadline is today. Any advice, code examples, or alternative approaches would be incredibly appreciated. Thank you so much for your help!",0,1,1733063340.0,bigdata
1h39503,Unfolding the Role of Black Box and Explainable AI in Data Science,"Drive greater progression with [Black Box and Explainable AI in Data Science](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science); facilitating data-driven decision-making for business worldwide. Enhance with popular machine learning models today.

https://preview.redd.it/bluysafwd04e1.jpg?width=1922&format=pjpg&auto=webp&s=1a5939b533219fbdd8ae738e907f2b6e880357c6

",1,0,1732959370.0,bigdata
1h32p1o,Hive Setting Lookup,"Setting up hive queries  in .HQL file , anyone have recommendations how I can look up all Setting options and explanations ? 

Example : SET mapreduce.job.reduces=10;



",1,0,1732934634.0,bigdata
1h2cq3v,I have a data processing scenario. suggested architectural choices,"The total amount of data is expected to be around 2-4 billion/hour. 

I need to `GROUP BY` by hour. the result after `GROUP BY` will be insert into the repository(or file system). It is expected that there will be 2-4 aggregations that will use all of the data, and 10 aggregations that will use part of the data (estimated 1/4). 

The result data will be used in subsequent calculations (it is not clear how much the data will be compressed). Raw data will no longer be required.



The current scenario I have in mind:

1. use Spark, but need to build distributed file system, scheduling service.

2. use OLAP database (e.g. Clickhouse) and utilize `Insert select` inside the database.



The company is expected to provide only 13 processing nodes (SSD), so it feels difficult to deploy both Spark and OLAP at the same time?



It is still in the preliminary research stage. Anything is possible.



Want to hear some experience advice.",4,1,1732851137.0,bigdata
1h27wg7,Domain,"Hi everyone, I have a domain name called bigdataexplained.com

The idea was to create a website to talk about big data, but I don't have time. It's a premium domain and I'm selling it for a very good price. If anyone is interested, just go to the website. There you can find instructions on how to buy everything correctly. I thought it would be interesting to post on this forum. Thanks!",1,2,1732834540.0,bigdata
1h1y0zf,Geospatial Data Analysis with Teradata Vantage ,"Discover how data scientists and developers use Teradata Vantage to perform geospatial and time-series analysis for telecommunication providers!

What is covered: 

Seamless integration of geospatial and customer data in Teradata Vantage. 

Advanced geospatial analysis with Python tools like GeoPandas, Plotly, and TeradataML. 

Quick insights and visualization of customer journeys and signal strength using Teradata’s Unbound Analytics Framework (UAF) and TrackingOp function 

Hands-On Demo: 

Analyzing cell tower locations and signal quality. 

Visualizing coverage areas with GeoJSON data. 

Mapping customer journeys with Teradata's UAF TrackingOp function. ",0,0,1732806715.0,bigdata
1h1upu0,Boost Your Credibility with Data Science Certifications,"Earning a data science certification can significantly boost your data science career. they help you gain new data science skills that are most in demand among industries, validate your knowledge, and show your commitment to lifelong learning.

There are other untold advantages a student or professional gets after earning certification from top institutes that are credible and recognized among employers. However, you need to be able to find a suitable certification program that matches your career goals and aspirations.

Check out our detailed guide about data science certification, its advantages, factors to consider while choosing the right one for you, [top data science certifications](https://www.usdsi.org/data-science-certifications), and other interesting facts. [Download](https://www.usdsi.org/data-science-insights/resources/boost-your-credibility-with-data-science-certifications) your copy now - 

https://preview.redd.it/2a13ix1ywm3e1.jpg?width=1920&format=pjpg&auto=webp&s=01686a954ae3168d3d4b02caf869a5109ef4e3e1

",0,0,1732796271.0,bigdata
1h19gg9,Achieving Sub-Second Latency with S3 Storage—A Kafka Alternative Using Pathway,"Hey everyone,

I've been working on simplifying streaming architectures in big data applications and wanted to share an approach that serves as a Kafka alternative, especially if you're already using S3-compatible storage.

You can skip description and jump to the code here: [https://pathway.com/developers/templates/kafka-alternative#building-your-streaming-pipeline-without-kafka](https://pathway.com/developers/templates/kafka-alternative#building-your-streaming-pipeline-without-kafka)

**The Identified Gap Addressed Here**

While Apache Kafka is a go-to for real-time data streaming, it comes with complexities and costs—setting up and managing clusters, incurring high costs in Confluent cloud (\~2k monthly for the use case here).

**Getting Streaming Performance with your Existing S3 Storage without Kafka**

Instead of Kafka, you can leverage [Pathway](https://github.com/pathwaycom/pathway) alongside Delta Tables on S3-compatible storage like MinIO. Pathway is a Pythonic stream processing engine with an underlying Rust engine.  
  
**Detailed Guide:**

For the technical details, including code walkthrough and benchmarks, check out this article: [Python Kafka Alternative: Achieve Sub-Second Latency with Your S3 Storage Without Kafka Using Pathway](https://pathway.com/developers/templates/kafka-alternative)

**Why Consider This Setup?**

* **Sub-Second Latency**: Benchmarks show that you can get stable sub-second latency for workloads up to 60,000 messages per second.
* **Cost-Effective**: Eliminates the need for Kafka clusters, reducing both complexity and operational costs.
* **Simplified Architecture**: Fewer components to manage, leveraging your existing S3 storage.
* **Scalable Performance**: Handles up to 250,000 messages per second with near-real-time latency (\~3-4 seconds).

**Use Cases**

This setup is suitable for various applications:

* **IoT and Logistics**: Collecting data from numerous sensors or devices.
* **Financial Services**: Real-time transaction processing and fraud detection.
* **Web and Mobile Analytics**: Monitoring user interactions and ad impressions.",9,0,1732727781.0,bigdata
1h1gjfx,10 Use Cases for Dremio in Your Data Architecture,,0,0,1732745756.0,bigdata
1h1bm38,Looking for API/Database to Identify Companies Behind IP Addresses (Not ISPs),"We’re building a tool that needs to identify specific companies behind IP addresses, but we’re running into a common issue: most services, like IPinfo, only return the ISP (e.g., Ziggo, Telenet) instead of the actual business using the IP address.

The Challenge:

For larger organizations, it's easier to identify the company behind the IP, but when it comes to smaller businesses using common ISPs or shared/dynamic IPs, we only get the ISP information. We're specifically after the company data, not just the internet service provider.

What We Need:

We need an API or a database that can accurately identify the company behind an IP address, even when that company is using a dynamic IP provided by an ISP.

Self-hosted or independent solutions are preferred. We're not interested in using another service like Leadfeeder. Instead, we want control over the data and how it integrates into our tool.

We want to find a solution that offers the best balance between price and quality.


What We’ve Tried:

We’ve used IPinfo.io, which aggregates data from sources like WHOIS records, but it often returns only the ISP for smaller businesses. We even tried the IP-to-company data API.

Reverse DNS lookups similarly lead back to the ISP instead of the company.


Our Goal:

We want to find an API or data source that provides the actual business behind an IP, not the ISP.

Alternatively, we’re open to building our own database if there's a reliable method to aggregate and map business information to IP addresses.


Questions:

1. Does anyone know of an API or data provider that can reliably return company-level data behind IP addresses?


2. Has anyone had success in creating a custom database to map businesses to IPs? If so, how did you gather and maintain this data?


3. Are there any other data sources or techniques we should be looking at to solve this problem?

Any advice or recommendations would be greatly appreciated. Thanks in advance for your help!",1,0,1732733140.0,bigdata
1h0yv0p,Unlock the Power of Data Science Framework for Business Growth,"[Data science frameworks](https://www.usdsi.org/data-science-insights/powerful-data-science-frameworks) are pivotal in managing the vast amounts of data generated today. With tools like Python and R at the forefront, they enable organizations to automate tasks and extract valuable insights that drive business decisions.

https://preview.redd.it/1mud01prbe3e1.jpg?width=1080&format=pjpg&auto=webp&s=24566620436b5112c3a0be9f576c83ef8c3063cc

",1,0,1732692306.0,bigdata
1gzrlre,BLOG: 10 Future Apache Iceberg Developments to Look forward to in 2025,,4,0,1732563290.0,bigdata
1gzb2tw,Unlock the Power of Machine Learning in Data Science,"Discover the immense potential of Machine Learning in Data Science! ML automates analysis, from simple linear models to complex neural networks, unlocking valuable insights as data grows. Embrace ML's power for a data-driven future. Master Data Science and ML through our comprehensive courses, earn Data Science certifications and start your career transformation today. Enroll now to become USDSI® certified. Register today.

https://reddit.com/link/1gzb2tw/video/vx7v7us7bz2e1/player

",2,0,1732510513.0,bigdata
1gykl8h,NameNode not working,Hi im trying to download hadoop for my exam and the namenode part in hdfs isnt working in cloudera. youtube is of no help either. pls help if anyone knows what to do.,2,4,1732429532.0,bigdata
1gy3qfb,Your opinion on entertaining educational content.,"I am trying to create educational videos striking a balance between entertainment and learning. Your feedback will valuable for further development.

Please check the videos.

Thanks.",2,0,1732379931.0,bigdata
1gy0l77,Powerful Data Science Frameworks,"Data science technology needs no introduction. Organizations have been using it for a long time now to make data-driven decisions and boost their business. Students aspiring to become successful **data scientists** know the importance of this technology in transforming industries and their applications.

However, it is rare among beginners that they are aware of the heart of data science – the powerful [***data science frameworks***](https://www.usdsi.org/data-science-insights/top-5-must-know-data-science-frameworks). These are the tools that streamline complex processes and make the life of data science professionals easier to explore and analyze data and build efficient models.

Data science frameworks, to put simply, are the collection of **data science tools** and libraries that make various kinds of data science tasks easier. Whether it is data collection, data processing, or **data visualization**, **data science professionals** can utilize popular data science frameworks to accomplish their tasks easily.

USDSI® brings a detailed infographic guide highlighting the importance of data science frameworks, their benefits, top data science frameworks, and various factors that one must consider while choosing one.

Check out the [infographic](https://www.usdsi.org/data-science-insights/powerful-data-science-frameworks) below, and learn from **TensorFlow** to **PyTorch**, what they are and what they are best suitable for. Moreover, [**data science certifications**](https://www.usdsi.org/data-science-certifications) from USDSI® can boost your data science learning endeavors. Explore these too.

https://preview.redd.it/fjxoidqltn2e1.jpg?width=1200&format=pjpg&auto=webp&s=a41a7324a8e11031d75a23896a873d8be4b5629f

",1,0,1732371426.0,bigdata
1gvvr1c,Introducing Rollstack AI-Powered Insights for Analytics and Business Intelligence Reporting,,1,0,1732125703.0,bigdata
1gvg4wn,Descarga Datos de Precios y Productos de Google Shopping,"[El comercio electrónico](https://www.octoparse.es/blog/c%C3%B3mo-extraer-datos-de-una-web-de-comercio-electr%C3%B3nico) es un campo que siempre será competitivo. Hemos tratado varios temas relacionados con el raspado de datos de determinados sitios de comercio electrónico como Amazon, Shopify, eBay, etc. Sin embargo, la realidad es que muchos minoristas pueden tener varias estrategias de marketing en diferentes plataformas, incluso para un solo artículo. Si desea comparar la información de los productos en diversas plataformas, el scraping de Google Shopping le ayudará a ahorrar mucho tiempo.

Conocido anteriormente como Product Listing Ads, Google Shopping es un servicio online proporcionado por Google que permite a los consumidores buscar y comparar productos a través de plataformas de compra online. Google Shopping permite a los usuarios comparar fácilmente los detalles de varios productos y sus precios de diferentes proveedores. Este post mostrará lo que ofrece y cómo se pueden **extraer datos de Google Shopping**.

Hablando de extracción de datos web, mucha gente podría asumir que la extracción de datos web requiere conocimientos de codificación. Con el avance de las herramientas de raspado web, este punto de vista podría ser alterado. Ahora la gente puede extraer datos fácilmente con estas herramientas, independientemente de la experiencia de codificación.

Si es la primera vez que utiliza Octoparse, puede registrarse para obtener una cuenta gratuita e iniciar sesión. Octoparse es una herramienta fácil de usar diseñada para que todo el mundo pueda extraer datos. Puede descargarla e instalarla en su dispositivo para su futuro viaje de extracción de datos. A continuación, puede seguir los pasos que se indican a continuación para extraer información de productos de Google Shopping con Octoparse.

# Plantilla de raspado de datos en línea de Google Shopping

Puede encontrar plantillas de raspado de datos en línea de Octoparse, que le permiten extraer datos directamente introduciendo varios parámetros. No necesita descargar e instalar ningún software en su dispositivo, simplemente pruebe el siguiente enlace para raspar datos de listados de productos de Google Shopping fácilmente.

[plantilla de scraping google shopping](https://preview.redd.it/mb8ygtfx2z1e1.png?width=943&format=png&auto=webp&s=020fefc786b002fe041f4c8b662d4918d3eb1822)

Con Google Shopping, puede detectar fácilmente las tendencias del mercado. Puede utilizarlo para recopilar datos sobre su mercado objetivo, sus consumidores y sus competidores. Ofrece información sobre tantas plataformas distintas, en particular, que es posible que tenga que dedicar mucho tiempo a recopilar el mismo tipo de datos de varios sitios web. Con sólo CUATRO pasos, puede raspar Google Shopping con Octoparse. Esta herramienta también está disponible en una amplia gama de sitios web de comercio electrónico. Consulte los artículos siguientes para obtener más guías.

>[*Ref: Cómo Extraer Datos de Precios y Productos de Google Shopping*](https://www.octoparse.es/blog/como-extraer-datos-de-productos-de-google-shopping)",0,0,1732072952.0,bigdata
1guw2js,A Data Manager’s True Priority Isn’t Data,,7,0,1732019610.0,bigdata
1gt15wq,Newbie in Big data,"As I’m a 23 yr old grad student in data science, my question professor given me a project where I must use databricks community edition and pysprak for applying machine learning algorithms. I’m very near to the deadline I need some project ideas and help as I’m a beginner.",2,1,1731803122.0,bigdata
1grzjff,Deep Dive into Dremio's File-based Auto Ingestion into Apache Iceberg Tables,,3,0,1731686307.0,bigdata
1gs1td8,Avoid Costly Data Migrations: 10 Factors for Choosing the Right Partner,"Most data migrations are complex and high-stakes. While it may not be an everyday task, as a data engineer, it’s important to be aware of the potential risks and rewards. We’ve seen firsthand how choosing the right partner can lead to smooth success, while the wrong choice can result in data loss, hidden costs, compliance failures, and overall headaches.

Based on our experience, we’ve put together a list of the 10 most crucial factors to consider when selecting a data migration partner: 🔗 [Full List Here](https://datacoves.com/post/data-migration-plan)

A couple of examples:

* **Proven Track Record:** Do they have case studies and references that show consistent results?
* **Deep Technical Expertise:** Data migration is more than moving data—it’s about transforming processes to unlock potential.

What factors do you consider essential in a data migration partner? Check out our full list, and let’s hear your thoughts!",1,0,1731692074.0,bigdata
1grsdrh,Newbie to Big Data,"Hi! As the title suggest I'm currently a chemical engineering undergraduate who needs to create a big data simulation using matlab so I really need help on this subject. I went through some research article but I'm still quite confused.

My professor instructed us to create a simple big data simulation using matlab which she wants next week. Any resources which could help me? ",1,1,1731661646.0,bigdata
1gro6mg,Pandas Difference Between loc[] vs iloc[],,1,0,1731644587.0,bigdata
1gr3gxd,Introducing Hive 4.0.1 on MR3,"Hello everyone,

If you are looking for stable data warehouse solutions, I would like to introduce Hive on MR3. For its git repository, please see:

[https://github.com/mr3project/hive-mr3](https://github.com/mr3project/hive-mr3)

Apache Hive continues to make consistent progress in adding new features  
and optimizations. For example, Hive 4.0.1 was recently released and it provides strong support for Iceberg. However, its execution engine Tez is currently not adding new features to adapt to changing environments.  
  
Hive on MR3 replaces Tez with another fault-tolerant execution engine MR3, and provides additional features that can be implemented only at the layer of execution engine. Here is a list of such features.  
  
1. You can run Apache Hive directly on Kubernetes (including AWS EKS), by creating and deleting Kubernetes pods. Compaction and distcp jobs (which  
are originally MapReduce jobs) are also executed directly on Kubernetes. Hive on MR3 on Kubernetes + S3 is a good working combination.  
  
2. You can run Apache Hive without upgrading Hadoop. You can also run  
Apache Hive in standalone mode (similarly to Spark standalone mode) without requiring resource managers like Yarn and Kubernetes. Overall it's very easy to install and set up Hive on MR3.  
  
3. Unlike in Apache Hive, an instance of DAGAppMaster can manage many  
concurrent DAGs. A single high-capacity DAGAppMaster (e.g., with 200+GB of memory) can handle over a hundred concurrent DAGs without needing to be restarted.  
  
4. Similarly to LLAP daemons, a worker can execute many concurrent tasks.  
These workers are shared across DAGs, so one usually creates large workers  
(e.g., with 100+GB of memory) that run like daemons.  
  
5. Hive on MR3 automatically achieves the speed of LLAP without requiring  
any further configuration. On TPC-DS workloads, Hive on MR3 is actually  
faster than Hive-LLAP. From our latest benchmarking based on 10TB TPC-DS, Hive on MR3 runs faster than Trino 453.  
  
6. Apache Hive will start to support Java 17 from its 4.1.0 release, but  
Hive on MR3 already supports Java 17.  
  
7. Hive on MR3 supports remote shuffle service. Currently we support Apache Celeborn 0.5.1 with fault tolerance. If you would like to run Hive on  
public clouds with a dedicated shuffle service, Hive on MR3 is a ready solution.  
  
If interested, please check out the quick start guide:  
  
[https://mr3docs.datamonad.com/docs/quick](https://mr3docs.datamonad.com/docs/quick)/  
  
Thanks,",2,0,1731586608.0,bigdata
1gq9eil,"Seeking Advice on Choosing a Big Data Database for High-Volume Data, Fast Search, and Cost-Effective Deployment","Hey everyone,

I'm looking for advice on selecting a big data database for two main use cases:

1. High-Volume Data Storage and Processing: We need to handle tens of thousands of writes per second, storing raw data efficiently for later processing.
2. Log Storage and Fast Search: The database should manage high log volumes and enable fast searches across many columns, with quick query response times.

We're currently using HBase but are exploring alternatives like ScyllaDB, Cassandra, ClickHouse, MongoDB, and Loki (just for logging purposes). Cost-effective deployment is a priority, and we prefer deploying on Kubernetes.

Key Requirements:

* Support for tens of thousands of writes per second.
* Efficient data storage for processing.
* Fast search capabilities across numerous columns.
* Cost-effective deployment, preferably on Kubernetes.

Questions:

1. What are your experiences with these databases for similar use cases?
2. Do you happen to know if there are other databases we should consider?
3. Do you happen to have any specific tips for optimizing these databases to meet our needs?
4. Which options are the most cost-effective for Kubernetes deployment?",3,1,1731491442.0,bigdata
1gpj7tz,So I Have A Data Product... Now What?,,2,0,1731413258.0,bigdata
1gpl5a6,Possible options to speed-up ElasticSearch performance,"The problem came up during a discussion with a friend. The situation is that they have data in ElasticSearch, in the order of 1-2TB. It is being accessed by a web-application to run searches.

The main problem they are facing is query time. It is around 5-7 seconds under light load, and 30-40 seconds under heavy load (250-350 parallel requests).

Second issue is the cost. It is currently hosted by manager ElasticSeatch, two nodes with 64GB RAM and 8 cores each, and was told that the cost around $3,500 a month. They want to reduce the cost as well.

For the first issue, the path they are exploring is to add caching (Redis) between the web application and ElasticSearch. 

But in addition to this, what other possible tools, approaches or options can be explored to achieve better performance, and if possible, reduce cost?

UPDATE:
* Caching was tested and has given good results.
* Automated refresh internal was disabled, now indexes will be refreshed only after new data insertion. It was quite aggressive.
* Shards are balanced.
* I have updated the information about the nodes as well. There are two nodes (not 1 as I initially wrote).",1,2,1731419412.0,bigdata
1gnbfea,"Solidus Hub: Alignment In AI, Data Analysis & Social Mining","Bringing together AI, Data Analysis, and Social Mining is a notable feature due to a recent partnership between Solidus and DAO Labs. We all agree that Social Mining focuses on analyzing social media posts, comments, and other online interactions to understand public opinion, sentiment, and behavior, but having a key feature of fair rewards draws the attention of content creators, it shows an aspect of individual **data ownership.**

# Solidus Hub

Solidus Hub is a specialized platform for community-driven content and engagement centered around AI and blockchain. The partnership with DAOLabs brings in an initiative that empowers community members to earn rewards in **$AITECH tokens** for creating, sharing, and engaging with content related to Solidus AI Tech. 

The combination of both projects utilizes **""Social Mining"" SaaS**, which incentivizes users to generate quality content and engage in tasks such as social media sharing and content creation. 

**Let's continue to discussion in the comment section should you need a link that addresses all your concerns!**",0,4,1731162979.0,bigdata
1gmtzix,A New Adventure Solidus Hub,"I was also excited to see Solidus AI Tech on the DAO Labs platform, which I have been involved in for 3 years by examining all the advantages of the social mining system. Solidus HUB will be a new adventure for me ",8,3,1731102284.0,bigdata
1gmnkfq,How to show your Tableau analysis in PowerPoint ,"https://preview.redd.it/w8um99cxmpzd1.png?width=1241&format=png&auto=webp&s=abd1b8c5f3bc59d4bea7016e3c47245e52c1dbc3

Here's how:

1. Create a free account at [Rollstack.com](http://Rollstack.com)
2. Add Tableau as a data source
3. Add your PowerPoint as a destination
4. Map your visuals from Tableau to PowerPoint
5. Adjust formatting as needed
6. Create a schedule for recurring reports and distribute via email

Try for free at [Rollstack.com](http://Rollstack.com) ",3,1,1731085845.0,bigdata
1gmpoxf,Where can I pull historical stock data for free or a low cost?,"I want to be able to pull pricing data for the past 10-20+ years on any stock or index in order to better understand how a stock behaves. 

I saw that Yahoo now charges you and you can only pull data that goes back so many years. Is there anywhere that I can get this data for free or for a low cost?",2,3,1731091176.0,bigdata
1glongi,ETL Revolution,"Hi everyone! I’m the Co-Founder & CEO at a startup aimed at transforming data pipeline creation through AI-driven simplicity and automation. If you're interested in learning more, feel free to check out our website and support the project. Your feedback would mean a lot—thanks! [databridge.site](http://databridge.site/)",0,2,1730980351.0,bigdata
1gkchnw,All About Parquet Part 10 - Performance Tuning and Best Practices with Parquet,,2,0,1730828576.0,bigdata
1gjkzae,All About Parquet Part 08 - Reading and Writing Parquet Files in Python,,3,0,1730743913.0,bigdata
1gjkyw9,All About Parquet Part 09 - Parquet in Data Lake Architectures,,1,0,1730743887.0,bigdata
1gioicp,GPUs Enhancing Technology and Sustainability with Solidus AI Tech,"GPUs (Graphics Processing Units) are chips specialized in creating images quickly, their demand has increased in enterprises, governments and gaming due to their ability to handle complex tasks. 

r/solidusaitech  is a company that offers energy efficient GPUs, with the use of advanced cooling technology, they reduce the environmental impact, being ideal for green data centers.

Solidus AI Tech improves technological efficiency while driving sustainable practices.",15,6,1730645323.0,bigdata
1gin0ki,I wanna start my big data first project m thinking of products analysis but i donnu where to start and what to start with i found no tutos i have installed hadoop that s all i did anyone can help please,,1,2,1730641055.0,bigdata
1gh9n2c,Looking for guidance on how i can start on the field of Bigdata and where I can begin with?,Lemme know about any books which would be helpful for me to progress in understanding the field.  ,4,3,1730478843.0,bigdata
1ggycmg,Active Graphs: A New Approach to Contextual Data Management and Real-Time Insights,"Hey r/bigdata,

I wanted to share something I’ve been working on that could shift how we think about data management and analysis. I call it Active Graphs—a framework designed to organize data not as static tables or isolated points, but as dynamic, context-aware relationships. I’m hoping to get some feedback from the community here and open a discussion on its potential.

What Are Active Graphs?

Active Graphs represent a shift in data structure: each data point becomes a “node” that inherently understands its context within a broader ecosystem, linking dynamically to other nodes based on predefined relationships. Imagine a data model that’s not just about storing information but actively interpreting its connections and evolving as new data comes in.

Key Features:

	•	Dynamic, Real-Time Relationships: Relationships aren’t rigidly defined; they adapt as new data is added, allowing for a constantly evolving network of information.
	•	Contextual Intelligence: Data isn’t just stored; it understands its relevance within the network, making complex queries simpler and more intuitive.
	•	Built for Multi-Domain Data: Active Graphs allow cross-domain insights without re-indexing or reconfiguration, ideal for industries with highly interconnected data needs—think finance, healthcare, and legal.

How Active Graphs Could Be a Game-Changer

Let’s take healthcare as an example. With Active Graphs, patient data isn’t just recorded—it’s actively mapped against diagnoses, treatments, and outcomes. You could run a query like “Show all admitted patients with Pneumonia and their most recent treatments,” and Active Graphs would deliver real-time insights based on all relevant data points. No custom code, no complex reconfiguration—just actionable insights.

Or in finance, imagine a trading bot that can adapt its strategy based on real-time contextual updates. Each trade and indicator would be dynamically linked to broader contexts (like day, week, and market sentiment), helping it make informed, split-second decisions without needing to retrain on historical data.

Why This Matters

Traditional databases and even graph databases are powerful, but they’re often limited by static relationships and rigid schemas. Active Graphs breaks from that by making data flexible, relational, and inherently context-aware—and it’s ready for integration in real-world applications.

TL;DR: Active Graphs turns data into a self-organizing, interconnected network that adapts in real-time, offering new possibilities for industries that rely on complex, evolving datasets. I’d love to hear your thoughts on this approach and how you think it might apply in your field.

Disclaimer: Active Graphs and its associated concepts are part of an ongoing patent development process. All rights reserved.",3,2,1730438629.0,bigdata
1ggekwc,All About Parquet Part 07 — Metadata in Parquet | Improving Data Efficiency,,6,0,1730381465.0,bigdata
1ggfguk,"Calling Data Engineers and Architects with hands-on experience in Real-Time and Near Real-Time Streaming Data solutions!
","Hi,

If you’re skilled in **streaming data** – from ingesting and routing to managing and setting real-time alerts – we want to hear from you! We’re seeking experienced professionals to provide feedback on a new product in development.

During the session, we’ll discuss your experience with streaming data and gather valuable insights on our latest design flow.

By participating, you’ll help shape the future of streaming data experiences!

**Study Details:**

* Qualified participants will be paid
* Time Commitment: Approximately 90 minutes.
* Format: Remote online session.

If you’re interested, please complete this short screener to see if you qualify:

https://www.userinterviews.com/projects/O-tG9o1DSA/apply.

Looking forward to hearing from you!

**Best,**  
**Yamit Provizor**  
**UX Researcher, Microsoft – Fabric**",2,0,1730383884.0,bigdata
1gfonrf,The Power Combo of AI Agents and the Modular Data Stack: AI that Reasons,,2,1,1730300606.0,bigdata
1gf6zqv,"Dremio 25.2 Release: Built-in on prem catalog, Polaris/Unity connectors, dark mode",,2,0,1730239813.0,bigdata
1gf0mtn,All About Parquet Part 05 — Compression Techniques in Parquet,,2,0,1730223861.0,bigdata
1gepdji,Beginner’s Guide to Spark UI: How to Monitor and Analyze Spark Jobs,"I am sharing my article on Medium that introduces Spark UI for beginners.

It covers the essential features of Spark UI, showing how to track job progress, troubleshoot issues, and optimize performance. 

From understanding job stages and tasks to exploring DAG visualizations and SQL query details, the article provides a walkthrough designed for beginners. 

Please provide feedback and share with your network if you find it useful.

[Beginner’s Guide to Spark UI: How to Monitor and Analyze Spark Jobs](https://medium.com/@suffyan.asad1/beginners-guide-to-spark-ui-how-to-monitor-and-analyze-spark-jobs-b2ada58a85f7)",3,3,1730188947.0,bigdata
1geo9qo,Unfolding the Role of Black Box and Explainable AI in Data Science,"USDSI® can be the key differentiator that stands you out from the herd and propel your career forward. [black box ai](https://www.usdsi.org/data-science-insights/unfolding-the-role-of-black-box-and-explainable-ai-in-data-science)

https://preview.redd.it/4nh8h8td4nxd1.jpg?width=1922&format=pjpg&auto=webp&s=6a77e81bbf25d35e8cd21972f6820113d3128455

",1,0,1730183697.0,bigdata
1ge2okb,All About Parquet Part 04 — Schema Evolution in Parquet,,1,0,1730123834.0,bigdata
1ge2ktf,CAN DATA SCIENCE COMMAND THE FUTURE OF BUSINESSES IN 2025?,"Foster huge growth with top skills in **data visualization,** [data mining, and machine learning](https://www.usdsi.org/data-science-insights/resources/can-data-science-command-the-future-of-businesses-in-2025) today. Look at the interesting trends and future that data science holds.

https://preview.redd.it/791mf2gl5ixd1.jpg?width=1921&format=pjpg&auto=webp&s=254ee52dcdfb761eda8bc3a6c3162249251977b3

",0,0,1730123560.0,bigdata
1gcdb39,HOW TO GAIN KNOWLEDGE IN DATA SCIENCE | INFOGRAPHIC,"Data science is an interdisciplinary field and to succeed in your data science career path, you must have a strong knowledge in the foundational subjects and core disciplines of data science which are Mathematics and statistics, computer science, and domain or industry knowledge.

The knowledge of programming language, mathematical concepts like probability distribution, linear algebra, and business acumen will help you understand the business problem efficiently and develop accurate data science models.

Explore the core data science subjects that you must master before starting your career in data science and learn about specialized data science components like data analysis, data visualization, data engineering, and more in this detailed [infographic](https://www.usdsi.org/data-science-insights/how-to-gain-knowledge-in-data-science).

https://preview.redd.it/f9e7l7u251xd1.jpg?width=1200&format=pjpg&auto=webp&s=1e81077f56cd6de3e14fe8d147c3d6ab7c78fd0d

",0,0,1729917568.0,bigdata
1gbuywq,"All About Parquet Part 03 — Parquet File Structure | Pages, Row Groups, and Columns",,3,0,1729864892.0,bigdata
1gbpdlh,Folks who do data modeling: what is your biggest pain in the a**??,"What is your most challenging and time consuming task?  
Is it getting business requirements, aligning on naming convention, fixing broken pipelines?

We want to build internal tools to automate some of the tasks thanks to AI and wish to understand what to focus on.",5,1,1729844569.0,bigdata
1gbvjq3,Looking for database + analytics solution to analyze 3D printed data,"Hello, I am looking for a software which can injest data from a 3D printer and provide a analytics sandbox where that data can be analyzed / dashboards can be built. The type of data ranges from PLC data (export JSON), log files (text), csv files, to images. I am looking at solutions such as Cloudera (seems expensive) or SPLUNK. Does anybody have any other advise for such a flexible software solution that is also affordable? Thanks!",1,2,1729866467.0,bigdata
1gbs2er,"How to become famous in data analytics without needing to film a Youtube video every week or building an open source library that you have to maintain. Come up with your own 'number', 'coefficient', or 'theorem'.",,0,0,1729855966.0,bigdata
1gb616a,All About Parquet Part 02 - Parquet's Columnar Storage Model,,3,1,1729785406.0,bigdata
1gb1fhp,The Data Product Marketplace: A Single Interface for Business,,2,0,1729772880.0,bigdata
1gb2b34,Partecipate to a research,"I developed this questionnaire for my PhD. It analyses the influence of the human factor in Big Data Analytics. To answer you need to work in the field of data analytics. We need to collect a large number of answers for the analysis, if you want to help us it will only take 10 minutes of your time. At the end of the questionnaire (if you have entered your email) you will receive the average of the answers so far to compare with the averages of the other answers.

[https://docs.google.com/forms/d/e/1FAIpQLSeIrT1\_ERSIcBMYOt8GcDoAKG3cHJ5b3q9W-SBQDmTbzisXBA/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSeIrT1_ERSIcBMYOt8GcDoAKG3cHJ5b3q9W-SBQDmTbzisXBA/viewform?usp=sf_link) ",0,0,1729775567.0,bigdata
1gb1oja,Transform Your Accounts Payable &Receivable with Agentic AI,,1,0,1729773691.0,bigdata
1gb00fh,A BEGINNER'S ROADMAP TO WB SCRAPING IN PYTHON USING BEAUTIFULSOUP,"Looking to explore the world of web scraping? [Python's BeautifulSoup](https://www.usdsi.org/data-science-insights/beginners-guide-to-web-scraping-in-python-using-beautifulsoup) is your gateway! Learn how to transform unstructured web data into valuable insights in just a few steps.  

https://preview.redd.it/gx7h45upsowd1.jpg?width=1080&format=pjpg&auto=webp&s=acabd6471b11dba64b2eb6ebde2be31328f39a77

",0,1,1729768140.0,bigdata
1gafeb5,Blog: All About Parquet Part 01 - An Introduction (1/10),,3,0,1729702365.0,bigdata
1g9eype,"Notion Templates Every Data Scientist Needs for Success
",,0,0,1729593207.0,bigdata
1g9bas3,Data Science v/s Cloud Computing: An Overview,"**Want to know how data science and cloud computing are shaping the future of business?** Our new guide breaks down the key differences and shows you how these technologies work together to drive innovation.

USDSI® presents this unique guide on Data Science vs Cloud computing that discusses how each of these technologies contribute for organizations to making data-driven decisions. The guide also discusses several interesting stats and facts related to data science and cloud computing, for example, AWS is the biggest player in cloud computing with a 31% market share. Did you know it?

[Download](https://www.usdsi.org/data-science-insights/resources/data-science-vs-cloud-computing-an-overview) your copy now and explore more facts.

https://preview.redd.it/oyk0bbz209wd1.jpg?width=1921&format=pjpg&auto=webp&s=5ccede7f54b63a2863512fbb066275dceffab2c3

",2,0,1729576909.0,bigdata
1g7ol3l,Need help! How to upload json files on databricks,"I'm given a project on detecting fake reviews on yelp, for this I need to use databricks and apache spark. Here, I have the dataset  downloaded in zip folder which have json files in it. As I'm completely new to use databricks, I don't know how to upload this zip file on databricks. Please need help!",1,2,1729390275.0,bigdata
1g75l6o,"This article provides a practical guideline for unit and integration testing in Apache Flink. Using a financial fraud detection application as an example, we demonstrate how to write effective tests to ensure the correctness of your Flink jobs.",,2,0,1729331948.0,bigdata
1g73bvi,Top 3 Tips Marketing Teams Need to Know About Data Science In,"https://reddit.com/link/1g73bvi/video/0c153gz5wnvd1/player

Data science is changing the game for marketers everywhere. Get ready to supercharge your strategies with data science insights for 2024. In our latest video, you will discover the top three tips every marketing team needs to know about data science. Learn how AI is reshaping marketing tactics, why data democratization is on the rise, and the crucial role of data in delivering personalized customer experiences across channels. Ready to level up? Enroll in USDSI®'s data science certifications today and unlock endless possibilities!",2,0,1729321353.0,bigdata
1g67wsy,Data Lakehouse Roundup #1 - News and Insights on the Lakehouse,,1,0,1729218563.0,bigdata
1g5qbu6,Mind-Blowing Facts About Big Data You Can't Afford to Miss!,,4,0,1729170103.0,bigdata
1g5pwr2,"Data Engineers, Here’s How LLMs Can Make Your Lives Easier",,0,0,1729168785.0,bigdata
1g5lhz0,Functional World #12 | How to handle things in your project without DevOps around?,"This time during Functional World event, we're stepping a bit outside of functional programming while still keeping developers' needs front and center! The idea for this session actually came from our own team at Scalac, and we thought it was worth sharing with a wider audience :) We hope you'll find it valuable too, especially since more and more projects these days don't have enough dedicated DevOps support.

Check out more details about the event here: [https://www.meetup.com/functionalworld/events/304040031/?eventOrigin=group\_upcoming\_events](https://www.meetup.com/functionalworld/events/304040031/?eventOrigin=group_upcoming_events)",1,0,1729150701.0,bigdata
1g4uwhy,How Data Illuminates the Darkest Corners of Consumer Anxiety,"In a world where consumer fears dictate brand success, #data is the key to understanding the hidden drivers behind those anxieties. Equip yourself with a Data Science Certification to master the art of decoding consumer behavior and shaping the future.

https://preview.redd.it/bnjl207423vd1.jpg?width=1080&format=pjpg&auto=webp&s=03aa9a76237b9e0dc7d881194f05408371dabd2a

",2,1,1729069085.0,bigdata
1g4wgg9,Iceberg Table Maintenance: 4 Best Practices,,1,1,1729075733.0,bigdata
1g4gats,How to go about testing a new Hadoop cluster,,2,0,1729020940.0,bigdata
1g42oqh,Data-Driven Recruitment: Using Workwolf to Reduce Bias and Increase Efficiency,"https://reddit.com/link/1g42oqh/video/5vhltn6ynvud1/player

Dive into the future of hiring with our latest insights on data-driven recruitment trends! Explore how federated learning is enabling collaborative model training, while explainable AI ensures transparent and justifiable hiring decisions. ",0,0,1728979633.0,bigdata
1g3fa0g,Don’t Trust Decentralisation Yet? Game Theory Might Change Your Stance,,3,0,1728911116.0,bigdata
1g3gbl3,Done with trendytech big data course (now pls help ),"Hi guys I have done with this course it's seems to be good for me but I want to know is there any other thing which is required for DE 


I learn big data , Hadoop, mapreduce ,Hive pyspark , batch processing and stream processing , azure data engineering, azure data bricks , delta lake ,data lakes , azure synapse lake ,azure Dara factory , system design , AWS S3 Athena ,Kafka ,airflow 

Anything other required?

Also  If you guys intrested you can ping me on telegram I can help you 

Id :- @Develop_developerss ",2,0,1728914063.0,bigdata
1g1tgvb,Fresher training,"I've been enrolled to databricks (stream training) I know that databricks falls under big data. Other than that, I have no knowledge in it and have doubts on the scopes of the course. Does this course has a better opportunity for me in future? I was wishing to get enrolled in java but that didn't happen..I'm planning to jump after 2 years. Will this course help me to land in a better job? ",1,1,1728711740.0,bigdata
1g1hxe1,Increase speed of data manipulation ,"Hi there, I joined a company as Data Analyst and I received around 200gb of data in CSV file for analysis. And we are not allowed to install python, anaconda or any other software. When I upload a data to our internal software it takes around 5-6 hours. And I was trying to increase the speed of the process. What you guys can suggest? Any native Windows software solution or maybe changing hdd to latest ssd can help to increase the data manipulation process? And installed ram is 20gb.",3,6,1728674843.0,bigdata
1g1bwyd,Tutorial de redes KAN en español,[https://www.youtube.com/watch?v=Jb9wMCPUlnc](https://www.youtube.com/watch?v=Jb9wMCPUlnc),0,1,1728659057.0,bigdata
1g14h5a,DATA SCIENCE VS BUSIENESS INTELLIGENCE  VS BIG DATA,"Unravel the complexities surrounding data science, business intelligence, and big data to uncover their interconnected nature. Explore how these disciplines complement each other to transform raw data into actionable insights. 

https://preview.redd.it/ypv1psvb03ud1.jpg?width=1080&format=pjpg&auto=webp&s=d24a5f2ed90b04e3bea7d3eeba79d6e400a0b66c

",0,1,1728632638.0,bigdata
1g0mfyf,Bronze/Silver/Gold and Dremio’s Reflections,,3,0,1728576658.0,bigdata
1g0qidc,Ready to Get sheet Done ?,"[Automate data extraction in your browser. No code, no limits, no headaches.](https://reddit.com/link/1g0qidc/video/cga6j4p49ztd1/player)

Hey Folks!

We are two co-founders based in sunny Barcelona who just launched Get Sheet Done.

Get Sheet Done is a Chrome extension that enables you to scrape any website. There is no coding needed; just navigate to the website of your choosing and start building your automation. It's easy to use, affordable, and fast.

It's free for up to 1,000 records/month. Our limited launch offer is 50% off on our monthly plan for life.

You can check it out here: [https://gsd.social/rd](https://gsd.social/rd)

P.S. We plan to add more features in the future, such as integrations, data manipulation, and assistive AI. If you want to chat further, come say hi on our Discord server here: [https://getsheetdone.io/community](https://getsheetdone.io/community)

Cheers!",1,2,1728587230.0,bigdata
1g060xz,Distributed databases that handle both OLAP and OLTP workloads efficiently,"In my conversation with Adam Szymański from [Oxla](https://www.oxla.com/) on our podcast, [Cloud Frontier](https://youtu.be/oc5SBnP-ILw) by simplyblock. He had this to say: ""If you work with a typical OLAP database like Snowflake, you cannot use it efficiently in serving traffic because of long response times. Oxla can do both OLAP and OLTP, allowing for faster, more versatile use cases and simplifying the data stack"".

For those managing hybrid workloads, how do you handle the complexity of maintaining separate OLAP and OLTP databases? Would a unified approach like Oxla’s reduce your infrastructure overhead?",1,1,1728518641.0,bigdata
1fzsrvi,NVIDIA Developer Day for Healthcare and Life Sciences,"*We would like to invite you to attend the first-ever NVIDIA Developer Day focused on healthcare and life science.*

*Developers, data scientists, machine learning, AI, and infrastructure engineers working across the healthcare and life science sector are welcome to attend this free event, run by NVIDIA, with a separate track for infrastructure engineers being presented by Run:ai, Weights & Biases, and Scan Computers.*

*This is an invite-only event, tailored to your needs. Therefore, we are seeking your input on what sessions solution experts in healthcare and life sciences should run to give you maximum benefit from the day.*

*Please fill out this form to indicate your intent to attend and specify which sessions you are particularly interested in -* [https://events.bizzabo.com/NVIDIAdeveloperday](https://events.bizzabo.com/NVIDIAdeveloperday)

[*ai@scan.co.uk*](mailto:ai@scan.co.uk)

*Processing img nruvgsp0rqtd1...*

",0,0,1728484224.0,bigdata
1fywhy0,Road map for BigData Engineer ,How to get started? ,2,4,1728383154.0,bigdata
1fytzoe,Building a Robust Data Observability Framework to Ensure Data Quality and Integrity,,3,0,1728371623.0,bigdata
1fyrsto,A Closer Look at the Average Data Scientist's Salary,"The field of data science is consistently ranked among the top three most desirable job options. The compensation of data scientists is significantly greater than the normal wage scale. As of 2024, the Bureau of Labor Statistics (BLS) of the United States of America reported that the median data scientist salary in the world was $ 115,240. During the same period, the Bureau of Labor Statistics (BLS) estimated that the median annual pay for all workers was $57,928.

# Unveiling the Mystery of Average Data Scientist Salary

Are you curious about the amount of money that data scientists make in terms of their salary? 

You have arrived at the ideal location if you are thinking about pursuing a career in data science or if you are interested in learning more about the possible earnings in this profession. Within the scope of this blog, we will explore the data scientist salaries. This will include the data scientist's salary in the United States as well as the data scientist's salary in other countries across the world.

# Breaking Down the Numbers

In the modern data-driven world, there is a significant demand for data scientists. To assist firms in making decisions that are based on accurate information, these specialists play a significant role because of their capabilities to analyze and comprehend complicated data. 

As a consequence of this, pay for data scientists is quite competitive. According to the surveys, data scientists’ salary in the United States may anticipate earning a base pay of $125,645 per year on average. The wage trends of data scientists may vary greatly around the world, but they are competitive due to the high demand for talent at all times.

# Why Experience Is Crucial?

As is the case in any other industry, the amount of experience a data scientist has is a crucial factor in establishing their pay rate. 

● Data scientists in the US who are just starting and have no experience may anticipate earning around $98,600.   
● On the other hand, mid-level professionals who have one to three years of expertise can command salaries of $1,10,956.   
● Data Scientists with 3 to 5 years of experience earn about $1,21,773, whereas one with an experience of 5 to 7 years earns about $1,34,614.   
● On the other hand, senior data scientists who have more than seven years of experience might make upwards of $1,53,383, which is a reflection of the great value that is placed on experienced experts in data scientist professions. 

# Location As a Crucial Factor

As a data scientist, the location of your workplace can also have a big influence on the amount of money you make. As a result of the great demand for tech expertise in these places, tech giants in San Francisco, Seattle, and New York generally offer higher wages to data scientists. 

Data scientist jobs in rural locations or smaller towns could have slightly lower incomes than their counterparts in larger cities. In the process of comparing the various income offers in various areas, it is vital to take into consideration the cost of living.

# The Influence of Industry

The sector in which you are employed might also affect the amount of money you can make as a data scientist. Data scientists often receive greater compensation from companies operating in finance, healthcare, and technology when compared to companies operating in other industries. This is because these sectors largely rely on data analytics to drive business choices and maintain their competitiveness in the market. It contributes to the increasingly competitive wage scales for data scientists that are observed all over the world.

# Perks of Being Data Scientists

A competitive base income is typically offered to data scientists, and in addition to that, they frequently receive a variety of bonuses and benefits that further boost their entire compensation package. 

These additional incentives are frequently utilized by employers to entice and keep the best data science talent in a very competitive work market.

# Attempting to Negotiate Your Pay

When it comes to negotiating your wage as a data scientist, it is necessary to gather information and come prepared with the necessary information. You should try to establish a baseline for negotiations by gaining an understanding of the average compensation of a data scientist in the United States and throughout the world. 

During wage conversations, it is important to highlight your unique abilities and accomplishments, and you should not be hesitant to argue for better pay or more perks if you think that you contribute value to the firm.

# Final Thoughts

The salary of data scientists might vary based on several parameters, such as employment history, geographic region, and the sector in which they work. The typical salary that data scientists may anticipate earning is competitive, and they also receive extra bonuses and advantages, which is one of the reasons why many people are interested in pursuing a career in data science. As the need for [data science jobs](https://www.usdsi.org/data-science-insights/unlocking-careers-for-the-future) continues to increase, the opportunities for professions that are both profitable and satisfying in this sector continue to be high.",0,0,1728362336.0,bigdata
1fy7aui,The Skill-Set to Master Your Data PM Role | A Practicing Data PM's Guide,,3,0,1728306809.0,bigdata
1fy9c5n,"I made Faker.js wrapper in 3 hours to generate test data, do you think it is useful?","A few months ago I was working on a database migration and I used this python library to generate test datasets.

I used these datasets to populate a test database to query and see if my migration package generated the json I expected. 

The code was done with purely nested for loops in python, but it occurred to me that a friendly UI might be useful for future cases, so in one afternoon I made this with the js library's counterpart in next.js

I tried to do a product hunt release but it didn't attract much interest 😂

What do you think?

Link: https://www.data-generator.xyz/

",1,5,1728312208.0,bigdata
1fxnksn,Do data visualisation in natural languages ,"Datahorse simplifies the process of creating visualizations like scatter plots, histograms, and heatmaps through natural language commands. 

Whether you're new to data science or an experienced analyst, it allows for easy and intuitive data visualization.

https://github.com/DeDolphins/DataHorse",17,0,1728240191.0,bigdata
1fx2sk2,"Blog: Ultimate Directory of Apache Iceberg Resources (Tutorials, Education, etc.)",,7,0,1728169536.0,bigdata
1fx8uuj,A tool to simplify data pipeline orchestration,"Hello - are there any tools or platforms out there that simplify managing pipeline orchestration - scheduling, monitoring, error handling, and automated scaling, all in one central dashboard? It would abstract all this management over a pipeline that comprises of several steps and tech - e.g. Kafka for ingestion, Spark for processing, and HDFS/S3 for storage. Do you see a need for it?",1,3,1728190113.0,bigdata
1fwidyy,Big data Hadoop and Spark Analytics Projects (End to End) ,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",9,0,1728102568.0,bigdata
1fvytga,Top Data Science Trends reshaping the industry in 2025,"Data science has been a revolutionizing factor for several companies across all the industries and it will do so in the coming years as well. By leveraging data-driven decision-making and predictive models’ organizations have been able to achieve high level of productivity, efficient business operations, and enhanced consumer experience.

The great thing about the modern interconnected world is the ever-increasing amount of data which is expected to grow by 180 zettabytes by 2025 (as predicted by IDC). This means more opportunities for organizations to innovate and elevate their businesses.

For all the data science enthusiasts, USDSI® brings a comprehensive guide on various trends that are shaping the [future of data science](https://www.usdsi.org/data-science-certifications). This extensive resource will definitely influence your understanding of data science technologies and your career in it. So, [download your copy now](https://www.usdsi.org/data-science-insights/resources/top-data-science-trends-reshaping-the-industry-in-2025).",2,1,1728047561.0,bigdata
1fvxuow,"🚀 Top AI Search and Developer Tools 🤖
",,2,1,1728044621.0,bigdata
1fvf04d,"Tired of waiting 2-4 weeks for business reports? Use Rollstack for automated report generation from your BI Tools like Tableau, Looker, Metabase, and even Google Sheets. Get the reports you need now with Rollstack. Try for free or book a live demo at Rollstack.com. ",,4,3,1727981121.0,bigdata
1fv6iiy,Being good at data engineering is WAY more than being a Spark or SQL wizard. ,It’s more on communication with downstream users and address their pain points. ,7,3,1727958527.0,bigdata
1fuvn8b,OSA Con (The Open Source Analytics Conference) - Free and online Nov 19-21,"Full discloser: I am from Altinity, one of the sponsors and organizers of OSA Con, a non-vendor conference dedicated to open-source analytics.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Many devs haven’t heard about OSA Con, so I am posting it here since some of you may be interested. I highlighted a few cool talks below, but check out the program for the full list of talks.

* **Building your AI Data Hub with PyAirbyte and Iceberg** (Michel Tricot, Airbyte)
* **pg\_duckdb: adding analytics to your application database** (Jordan Tigani, DuckDB)
* **Open Source Analytic Databases - Past, Present, and Future** (Robert Hodges, Altinity)
* **Leveraging Data Streaming Platform for Analytics and GenAI** (Jun Rao, Confluent)
* **Presto Native Engine at Meta and IBM** (Aditi Pandti and Amit Dutta at Meta/IBM)
* **Vector search in Modern Databases** (Peter Zaitsev, Percona)
* **Observability for Large Language Models with Open Telemetry** (Guangya Liu and Nir Gazit)
* **Open Source Success: Learnings from 1 Billion Downloads** (Avi Press, Scarf)



Here is the website if you want to register and/or check out the full program: [osacon.io](http://osacon.io) ",3,1,1727916377.0,bigdata
1fup8i2,Milestone: 500.000 public bulk profiles available for instant analysis in the open access online R2 platform,,0,0,1727898483.0,bigdata
1ful8rs,Can Inheritance break Encapsulation while extending different common modules in pipeline?,[https://en.wikipedia.org/wiki/Inheritance\_(object-oriented\_programming)#Issues\_and\_alternatives](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)#Issues_and_alternatives),1,0,1727888535.0,bigdata
1ftvm9a,"""39 QBRs in 3 hours."" - Rollstack Customer","""39 QBRs in 3 hours."" - Rollstack Customer

Got a bunch of QBRs on your plate this week? If you use Tableau, Looker, Metabase, or Google Sheets for Analytics, you can use [Rollstack.com](http://Rollstack.com) to automate them. Try for free or book a live demo.",0,0,1727808176.0,bigdata
1fsuiup,What makes a dataset worth buying?,"Hello everyone!

I'm working at a startup and was asked to do research in what people find important before purchasing access to a (growing) dataset. Here's a list of what (I think) is important.

* Total number of rows
* Ways to access the data (export, API)
* Period of time for the data (in years)
* Reach (number of countries or industries, for example)
* Pricing (per website or number of requests)
* Data quality

Is this a good list? Anything missing?

Thanks in advance, everyone!",6,17,1727699960.0,bigdata
1fsugra,Solve Governance Debt with Data Products,,1,0,1727699787.0,bigdata
1fsp7g5,3 Best Ways to Merge Pandas DataFrames,"https://reddit.com/link/1fsp7g5/video/et2vi91r5wrd1/player

Want to seamlessly combine your data? Learn the top 3 ways to merge Pandas DataFrames. Whether it's concatenation, merging on columns, or joining on index labels, these techniques will streamline your data analysis.",0,1,1727678037.0,bigdata
1fs97es,Chew: a library to process various content types to plaintext with support for transcription,,2,0,1727629551.0,bigdata
1fs3x4y,My latest article on Medium: Scaling ClickHouse: Achieve Faster Queries using Distributed Tables,"I am sharing my latest Medium article that covers Distributed table engine and distributed tables in ClickHouse. It covers creation of distributed tables, data insertion, and query performance comparison.

Read here: https://medium.com/@suffyan.asad1/scaling-clickhouse-achieve-faster-queries-using-distributed-tables-1c966d98953b

ClickHouse is a fast, horizontally scalable data warehouse system, which has become popular due to its performance and ability to handle big data.",2,0,1727615177.0,bigdata
1fr6na0,UNLOCK THE POWER OF DATA SCIENCE IN THE 21ST CENTURY,"Discover how data science is revolutionizing [businesses in the 21st century](https://www.usdsi.org/data-science-insights/resources/the-21st-century-data-science-world-an-insight)! From evolving career paths to cutting-edge insights, mastering data science could be your gateway to growth and success.

https://preview.redd.it/fapm1swachrd1.jpg?width=1080&format=pjpg&auto=webp&s=56643e45674d342fb1ff159190f2cc7151ebb093

",0,0,1727498616.0,bigdata
1fr25hh,Need help on a project ,"I hope everyone in this forum is doing well. I am currently looking for two current or former data scientists to interview, preferably someone with less than 5 years of experience and another with more than 15 years. I would be just be asking questions about your career path, education and finances. I am free from today till Monday. If it helps someone decide on this, I would also be able to compensate for the time, about $40. The interview would be 45 mins tops with the max of 30 questions. Thanks yall, I would really appreciate it.",1,1,1727482906.0,bigdata
1fqx5fu,Trained a classification model in plain English using DataHorse,"🔥 Today, I quickly trained a classification model in English using Datahorse!

It was an amazing experience leveraging Datahorse to analyze the classic Iris dataset 🌸 through natural language commands. With just a few conversational prompts, I was able to train a model and even save it for testing—all without writing a single line of code!

What makes Datahorse stand out is its ability to show you the Python code behind the actions, making it not only user-friendly but also a great learning tool for those wanting to dive deeper into the technical side. 💻

If you're looking to simplify your data workflows, Datahorse is definitely worth exploring.

Have you tried any conversational AI tools for data analysis? Would love to hear your experiences! 💬

Check out DataHorse and give it a star if you like it to increase it's visibility and impact on our industry.

https://github.com/DeDolphins/DataHorse",0,0,1727468496.0,bigdata
1fqit37,TAKE THE ULTIMATE STEP IN DATA SCIENCE LEADERSHIP,"Elevate your career and become a [Data Science leader with CSDS™](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist). Demonstrate your technical knowledge and strategic mindset, and show the world your capability to drive business success.

https://preview.redd.it/6jojhq649brd1.jpg?width=1080&format=pjpg&auto=webp&s=9a617f1850c41ff2144a7f064a0a27eefe0797d0

",0,0,1727424903.0,bigdata
1fq9o69,Part 1: Comparing the pricing models of modern data warehouses,,3,4,1727392040.0,bigdata
1fpn91t,Deep dive into Statistical Analysis with DataHorse,"DataHorse is an open-source tool that simplifies data analysis by allowing users to perform statistical tests using natural language queries. This accessibility makes it ideal for beginners and non-technical users.

Key Features:
Conversational Queries: Users can ask questions in plain English, and DataHorse executes the relevant statistical tests.

Educational Value: Each query generates Python code, helping users learn programming and customize their analyses.

Common Statistical Tests Supported: Includes t-tests, ANOVA, and regression analysis for assessing treatment effectiveness and variable relationships.


Why It Matters

In today’s data-driven world, being able to analyze and interpret data is crucial for informed decision-making. DataHorse aims to empower individuals and organizations to engage with their data without the typical barriers of complexity.

If you're interested in learning more, check out my latest blog post where I dive deeper into how DataHorse can transform your approach to data analysis: 

Blog: https://datahorse.ai/Blogs/Statstical-Analysis.html

Star us on GitHub: https://github.com/DeDolphins/DataHorse

I’d love to hear your thoughts and any feedback you might have!


",2,0,1727322110.0,bigdata
1fpow18,How to Build Impactful Data Visualizations with Pandas and Matplotlib? | Infographic,"Do you want to create smart and impactful data visualizations? Unleash the best amalgam of pandas and Matplotlib for orchestrating data-wrangling tools to succeed!

https://preview.redd.it/4fzexh4j93rd1.jpg?width=2000&format=pjpg&auto=webp&s=a57aeac4b500d0179a12c0a2bee4290f6e2c879a

",1,0,1727328194.0,bigdata
1fp949y,Virtualization + Lakehouse + Mesh = Data at Scale,,0,0,1727282752.0,bigdata
1fognr3,Airbyte 1.0 released,,26,6,1727195215.0,bigdata
1fnj76j,"The Analytics Engineering Flywheel, Shifting Left, & More With Madison Schott ",,3,0,1727094513.0,bigdata
1fnhjmy,HOW TO BUILD IMPACTFUL DATA VISUALIZATIONS WITH PANDAS AND MATPLOTLIB?,"*Do you want to create smart and impactful* ***data visualizations***? Unleash the best amalgam of [pandas and Matplotlib](https://www.usdsi.org/data-science-insights/how-to-build-impactful-data-visualizations-with-pandas-and-matplotlib) for orchestrating **data-wrangling** tools to succeed!

https://preview.redd.it/6l2gqgldhjqd1.jpg?width=2000&format=pjpg&auto=webp&s=0e77dbc10d015aee7d131fe742f8192f211cf4a7

",0,0,1727088697.0,bigdata
1fnc7c7,Privacy-focused architecture to enable personalized experience (e.g. dynamic CTAs) using Redis and RudderStack Data Apps,,1,1,1727065336.0,bigdata
1fn2acl,"My Medium article - Handling Data Skew in Apache Spark: Techniques, Tips and Tricks to Improve Performance","I want to present my Medium article titled Handling Data Skew in Apache Spark: Techniques, Tips and Tricks to Improve Performance.

Link: https://medium.com/@suffyan.asad1/handling-data-skew-in-apache-spark-techniques-tips-and-tricks-to-improve-performance-e2934b00b021

In this article, I try to cover detecting and fixing data skew in Apache Spark, alongwith code examples. It has been written for beginners of Spark. Please review and provide feedback, and please share in your network.
",1,3,1727035583.0,bigdata
1fmxwr9,Survey on data formats [responses welcome],"The following survey aims to gather empirical data to better understand the expectations of data format users concerning comparing them.  
It should take no more than 10 minutes:  
[https://forms.gle/K9AR6gbyjCNCk4FL6](https://forms.gle/K9AR6gbyjCNCk4FL6)  
Your response would be greatly appreciated!",1,0,1727024182.0,bigdata
1fmp8gl,Best BigData tool,"I'm wondering what's the best BigData tool on demand to learn, I put my eyes on pyspark but I'm not sure if it's the right one, based on what I read pyspark is really good for streaming, and Hadoop really good when dealing with giant data but it seems it's outdated for 2024, so I'm so confuse!!",2,4,1726997016.0,bigdata
1fmph3v,Advice on how to find a software engineer to co-found a big data health company,"I am a non-technical founder looking for a software engineer to co-found an analytics platform similar to [amplitude.com](http://amplitude.com) and [cbinsights.com](http://cbinsights.com), but I have no idea on where to find someone who would want to lead a startup in that way.

  
Please advise what would interest a SE in a bootstrapped business.

  
Thanks!",0,4,1726998066.0,bigdata
1fm15d3,A Beginner's Roadmap to Python web scraping with BeautifulSoup,"Looking to explore the world of web scraping? [Python's BeautifulSoup](https://www.usdsi.org/data-science-insights/beginners-guide-to-web-scraping-in-python-using-beautifulsoup) is your gateway! Learn how to transform unstructured web data into valuable insights in just a few steps.

https://preview.redd.it/6q7dd6r6h5qd1.jpg?width=1080&format=pjpg&auto=webp&s=37a05b43e9767dc8708c4ae05a3ac3f7ead5f64a

",0,0,1726919129.0,bigdata
1fli1s5,"Imagine waking up on October 1st, and all of your QBRs were exported and in a file ready to go. Pinch yourself. It’s not a dream. It’s Rollstack. Rollstack maps your reports from your BI and analytics tools to PowerPoint, Google Slides, Word, and Docs. Schedule a discovery call or try for free today",,0,0,1726854529.0,bigdata
1fl8oo3,BECOME THE ULTIMATE DATA SCIENCE LEADER,"[Data Science leaders](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist) bridge the gap between technology and business strategy. Elevate your career by mastering both domains and becoming an invaluable asset to your organization. 

https://preview.redd.it/vai0d6gmxxpd1.jpg?width=1080&format=pjpg&auto=webp&s=2b6b53c0b3c454a2214172343e7f2c7cf122c996

",0,0,1726827813.0,bigdata
1fkug3e,Looking for a BIG DATA alternative for Reporting tool,"We have IBM Cognos in the company (it's an old company) and we have a lots of reports schedueled. Probably the reports are running all the time because of queue (175 reports run in parallel, but looks like not enough).

Data in Cognos is refreshed every three hours (I guess Cognos is connected to some Oracle server/datawarehouse).

Each time I want to build a custom report (basically pulling columns), it will never run in time and I have to wait many many hours or even next day. I will press run, and it will take so long.

-Is  there a modern solution/big data solution (although Cognos holds ERP and CRM data of a big company)?  
-Perfect solution would be all reports could be pulled instantly at anytime with no delay and all schedueled reports would come without any delay or long queues.

Please advice, I will talk to the IT team (who are all old people).",1,3,1726778218.0,bigdata
1fjp1i3,Cluster selection in Databricks is overkill for most jobs. Anyone else think it could be simplified?,"One thing that slows me down in Databricks is cluster selection. I get that there are tons of configuration options, but honestly, for a lot of my work, I don’t need all those choices. I just want to run my notebook and not think about whether I’m over-provisioning resources or under-provisioning and causing the job to fail.

I think it’d be really useful if Databricks had some kind of default “Smart Cluster” setting that automatically chose the best cluster based on the workload. It could take the guesswork out of the process for people like me who don’t have the time (or expertise) to optimize cluster settings for every job.

I’m sure advanced users would still want to configure things manually, but for most of us, this could be a big time-saver. Anyone else find the current setup a bit overwhelming?",2,1,1726653407.0,bigdata
1fjow2j,Anyone else wish you could switch roles on the fly in Databricks?,"**I wish Databricks had an easy way to switch roles while running queries**

I’ve been using Databricks for a while now, and one thing that I feel is missing is a quick way to toggle between different access roles when working with sensitive data. In some industries like healthcare and finance, the data access policies can be really strict, and sometimes I have to switch between querying production data and something like clinical data. It would be amazing if there was a built-in feature where you could just toggle between roles (like data analyst, admin, etc.) \*right at execution time\* without needing to leave the notebook.

This would make life so much easier—no more worrying about whether you’re accidentally accessing the wrong dataset for your role. It could dynamically adjust what you’re allowed to query based on your current role, which would also help reduce the chances of non-compliance or unauthorized access. Has anyone else dealt with this kind of issue? Would love to know how you're handling it.",2,0,1726652717.0,bigdata
1fjjpx6,Future Of Data Science: 10 Predictions You Should Know,"Data Science will keep evolving in 2023 and beyond. Here are the 10 predictions of Data Science.

https://preview.redd.it/lugybpdtohpd1.jpg?width=1920&format=pjpg&auto=webp&s=0c12cbe7abed6dd4c03974c41064d2096b47f199

",0,0,1726631154.0,bigdata
1fjiwbc,Want to enter Big data and AI field ,"For context I am someone with Adhd dont kmow how I am gonna be able to thrive here. 
Wanted to know is there a way to acquire certifications or credibility in this field for a total newbie without having to get a conventional degree?",0,1,1726628508.0,bigdata
1fizekl,DevOps for Developers - challenges?,"Hi everyone!

I want to talk about lack of DevOps expertise inside the organizations. Not every company can or should have a full time DevOps Engineer. Let’s say we want to train Developers to handle DevOps tasks. With the disclaimer that DevOps is the approach and not a job position :D

1/ What are the most common cases that you need DevOps for, but developers are handling it?  
2/ What kind of DevOps challenges do you have in your projects?  
3/ What DevOps problems are slowing you down?  
4/ Is there any subject you want to know from scratch or upgrade your existing knowledge - with DevOps mindeset/toolset?

Thanks!",2,0,1726580055.0,bigdata
1fi47ij,Upscaling Marketing Analytics: A CDO’s Guide to Building Data-Driven Domains,,5,0,1726492173.0,bigdata
1fi0s36,"CDC to Iceberg: 4 Major Challenges, and How We Solved Them",,2,0,1726481016.0,bigdata
1fh9zfd,Tutorial: Hands-On intro with Apache Iceberg on Your Laptop,,3,0,1726398236.0,bigdata
1feksrh,9 social media insights from my recent global hack-a-thon,"My [dbt™ Data Modeling Challenge - Social Media Edition](https://www.paradime.io/dbt-data-modeling-challenge) just wrapped up!   
  
Submissions are in, and judges are reviewing insights from data participants worldwide.   
  
Winners will be announced tomorrow, so stay tuned!   
  
This unique challenge, had participants dive into social media data, turning raw information into valuable insights.   
  
Here's a glimpse of some fascinating insights participants uncovered... 

https://preview.redd.it/n7qs3fv0t8od1.png?width=1620&format=png&auto=webp&s=b2ebf9123d6c0107399f2b30913b814b5e145dfc

https://preview.redd.it/1lt8ihv0t8od1.png?width=1156&format=png&auto=webp&s=b7ce937291edf282faa6efdaa602e42a2e4b26a4

https://preview.redd.it/wolyb6w0t8od1.png?width=1157&format=png&auto=webp&s=91e25e79b2350cf16966822dc67cb2a557676933

https://preview.redd.it/ilt8nfv0t8od1.png?width=1647&format=png&auto=webp&s=9a2e29fe07b2cc621505ad895a24bafb1a7578c9

https://preview.redd.it/fe5gtev0t8od1.png?width=1184&format=png&auto=webp&s=d9127523c39ac7e62f1878d26d0b23f20e3e6ca0

https://preview.redd.it/qo9z3fv0t8od1.png?width=1639&format=png&auto=webp&s=d8a49062fe904317946aa01fd38781976a0d133b

https://preview.redd.it/hy3ogiv0t8od1.png?width=932&format=png&auto=webp&s=4beb32f475dd17b560157e3e37efe367e96b7b6e

https://preview.redd.it/amjy77w0t8od1.png?width=1634&format=png&auto=webp&s=5056220c577ecb1331c366af216b7273cc85205d

https://preview.redd.it/v8i4afv0t8od1.png?width=1162&format=png&auto=webp&s=ae7b7cf2b38e61e125746b0861a60ec5f8abe412

",6,0,1726087833.0,bigdata
1fe9s5e,Top Enterprise Data Catalog Tools for Effective Data Management,,3,3,1726060324.0,bigdata
1feap55,Trying to understand the internal architecture of a fictitious massive database. [Salesforce related],"Hey Humes, I'm currently trying to understand the internal optimization strategy for querying a database like Salesforce may use to handle all its users data. I'm studying for a data architect exam and I'm reading into an area I have no background business of looking into, but its super interesting.

So far I know that Salesforce splits its tables for its ""objects"" into two categories.

Standard and Custom

I was looking into it, as on the surface, at least logically, it feels like abstracting the data just leads to more steps computationally. I learned that wide tables impact performance negatively but, if we have a table 3,000 columns wide, splitting into two tables 1,500 columns wide each, would still require processing 3,000 columns (if we wanted to query them all) but with the added step of switching tables. To my limited understanding this means ""requires more computational power"". However, I began reading into cost-based optimization and pattern database heuristics. It seems that there some unique problems at scale that make it a little more complicated.

I'd like to be able to get a complete picture of how a complex database like that works, however I'm not really sure where I would go for more information. I can somewhat use ChatGPT, but I feel I'm getting a bit too granular to be accurate now and I need a real book or something along those lines. (Really seems like its sending me into the weeds now.

Cheers",1,0,1726062846.0,bigdata
1fdgytj,Operationalizing Data Product Delivery in the Data Ecosystem,,3,0,1725972309.0,bigdata
1fdmtun,International School on Open Science Cloud: best showcase tech?,,1,0,1725987475.0,bigdata
1fdhbag,Big Data Spreadsheet Showdown: Gigasheet vs. Row Zero,,2,0,1725973300.0,bigdata
1fdg57d,Scrapear Datos Inmobiliarios de Idealista en Python,"Octoparse ofrece una guía detallada sobre cómo extraer datos de Idealista mediante web scraping. Explica los pasos clave para configurar un proyecto de scraping, incluyendo la selección de elementos de la página, la extracción de información relevante como precios, ubicaciones y características de propiedades, y consejos para automatizar el proceso de forma eficiente, todo mientras se respetan las normativas legales y éticas.  


>Ref: [Cómo Raspar Datos Inmobiliarios de Idealista en Python](https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping)",0,0,1725969746.0,bigdata
1fder78,Help ,"
I’m working at a company that provides data services to other businesses. We need a robust solution to help create and manage databases for our clients, integrate data via APIs, and visualize it in Power BI.

Here are some specific questions I have:

1. **Which database would you recommend for creating and managing databases for our clients?** We’re looking for a scalable and efficient solution that can meet various data needs and sizes.
2. **Where is the best place to store these databases in the cloud?** We're looking for a reliable solution with good scalability and security options.
3. **What’s the best way to integrate data with APIs?** We need a solution that allows efficient and direct integration between our databases and third-party APIs.",1,2,1725964781.0,bigdata
1fd35a9,Handling Large Datasets More Easily with Datahorse,"A few days ago, I was dealing with a massive dataset—millions of rows. Normally, I’d use Pandas for data filtering, but I wanted to try something new. That’s when I decided to use Datahorse.

I started by asking it to filter users from the United States: ""Show me users from the United States over the age of 30."" Instantly, it filtered the dataset for me. Then, I asked it to ""Create a bar chart of revenue by country,"" and it visualized the data without me writing any code.

But what really stood out was that Datahorse provided the Python code behind each action. So, while it saved me time on the initial exploration, I could still review the code and modify it if needed for more in-depth analysis. Has anyone else found Datahorse useful for handling large datasets?
",3,0,1725922807.0,bigdata
1fd5oxm,"Felt now integrates with Databricks for instant maps and performant data dashboards, with real-time data updates. Read about how it works in our latest blog post!",,1,0,1725930095.0,bigdata
1fcge50,Big data courses ,"Hi guys If you want to big data engineer course of famous tutor pls ping me  on telegram Id:- @Robinhood_01_bot


You won't regret 😅",0,0,1725853418.0,bigdata
1fb74v4,Are there any apps that pharmaceutical companies use ?,"I am a Software Engineering student,  Interested to  see how and what type of patient data is valuable, for companies to enhance healthcare/treatments.",1,1,1725716349.0,bigdata
1faw3pi,The datification,"I'm new to the world of data. I was recently amazed by a concept called ""datification"", which according to The Big Data World: Benefits, Threats and Ethical Challenges (Da Bormida, 2021), is a technological tendency that converts our interactions in daily life into just data, ""where devices to capture, collect, store and process data are becoming ever-cheaper and faster, whilst the computational power is continuously increasing"". Indirectly promoting workflows that lead to the disuse of Big Data, violating certain privacy laws and ethical mandates.

  
Da Bormida, M. (2021). The Big Data World: Benefits, Threats and Ethical Challenges. En *Advances in research ethics and integrity* (pp. 71-91). [https://doi.org/10.1108/s2398-601820210000008007](https://doi.org/10.1108/s2398-601820210000008007)",6,2,1725674261.0,bigdata
1fb1kvc,The Data Revolution 2025: Emerging Technologies Reshaping our World,"Stay ahead of the booming data revolution 2025 as this read unravels its core components and future advancements. Evolve with the best certifications today!

https://preview.redd.it/f63pyhrwbcnd1.jpg?width=1920&format=pjpg&auto=webp&s=c34d78210ea02e44717dfcf105f9c50f22bacc8b

",0,0,1725694563.0,bigdata
1famlat,Using geospatial workloads within SnowflakeDB? Felt is a modern & cloud-native GIS platform & we just announced support for native connectivity to the Snowflake database!,"At [Felt](https://felt.com/?utm_source=community&utm_medium=social&utm_campaign=mql_community), we made a really cool cloud-native, modern & performant GIS platform that makes mapping and collaboration with your team really easy. We super recently released a version of the software that introduces native connectivity with SnowflakeDB, bringing you your Snowflake datasets to Felt. [So, here's how you do it!](https://felt.com/blog/map-with-snowflake?utm_source=community&utm_medium=social&utm_campaign=mql_community) 

I work here at the company as a developer advocate. If you have any questions, please comment below or DM and I can help! :-) ",2,0,1725648079.0,bigdata
1fa9klc,"Invitation to compliance webinar(GDPR, HIPAA) and Python ELT zero to hero workshops","Hey folks,

dlt cofounder here.

**Previously**: We recently ran our first 4 hour workshop ""Python ELT zero to hero"" on a first cohort of 600 data folks. Overall, both us and the community were happy with the outcomes. The cohort is now working on their homeworks for certification. You can watch it here: [https://www.youtube.com/playlist?list=PLoHF48qMMG\_SO7s-R7P4uHwEZT\_l5bufP](https://www.youtube.com/playlist?list=PLoHF48qMMG_SO7s-R7P4uHwEZT_l5bufP) We are applying the feedback from the first run, and will do another one this month in US timezone. If you are interested, sign up here: [https://dlthub.com/events](https://dlthub.com/events)

**Next**: Besides ELT, we heard from a large chunk of our community that **you hate governance** but it's an obstacle to data usage so you want to learn how to do it right. Well, it's no rocket/data science, so we arranged to have a professional lawyer/data protection officer give a webinar for data engineers, to help them achieve compliance. Specifically, we will do one run for GDPR and one for HIPAA. There will be space for Q&A and if you need further consulting from the lawyer, she comes highly recommended by other data teams.

If you are interested, sign up here: [https://dlthub.com/events](https://dlthub.com/events) Of course, there will also be a completion certificate that you can present your current or future employer.

This learning content is free :)

Do you have other learning interests? I would love to hear about it. Please let me know and I will do my best to make them happen.",2,0,1725608571.0,bigdata
1fac2ga,The Dawn of Generative AI While Addressing Data Security Threats,"Discover the dual-edged nature of Generative AI in our latest video. From revolutionary uses like drug creation and art development to the dark side of deepfakes and misinformation, learn how these advancements pose significant security threats. Discover how businesses can protect themselves with cutting-edge strategies. Equip yourself with the skills needed to tackle data security challenges. Enrol in data science certifications from USDSI® today and stay ahead of emerging threats! Don't forget to like, subscribe, and share this video to stay updated on the latest in tech and data security.

https://reddit.com/link/1fac2ga/video/uj0a51ig46nd1/player

",1,0,1725619438.0,bigdata
1f9ouvb,i need help in mapper.py code it was giving json decoder error,"here the link to how data set looks:[ link](https://drive.google.com/file/d/1OdAIdUVKWQb1ndU_m_x6iWuAUiTS6ksG/view)

brief description about dataset:  
\[  
{""city"": ""Mumbai"", ""store\_id"": ""ST270102"", ""categories"": \[...\], ""sales\_data"": {...}}

{""city"": ""Delhi"", ""store\_id"": ""ST072751"", ""categories"": \[...\], ""sales\_data"": {...}}

...

\]

mapper.py:

    #!/usr/bin/env python3
    import sys
    import json
    
    for line in sys.stdin:
        line = line.strip()
        if line == '[' or line == ']':
            continue
        store = json.loads(line)
        city = store[""city""]
        sales_data = store.get(""sales_data"", {})
        net_result = 0
    
        for category in store[""categories""]:
            if category in sales_data and ""revenue"" in sales_data[category] and ""cogs"" in sales_data[category]:
                revenue = sales_data[category][""revenue""]
                cogs = sales_data[category][""cogs""]
                net_result += (revenue - cogs)
                
        if net_result > 0:
            print(city, ""profit"")
        elif net_result < 0:
            print(city, ""loss"")
    
    
    

  
error:

https://preview.redd.it/42cu13vjb0nd1.png?width=685&format=png&auto=webp&s=ec34fef36a44fe3c3b33314830226be3c43ea0ec

",2,0,1725549187.0,bigdata
1f8wov8,"Huge dataset, need help with analysis","I have a dataset that’s about 100gb (in csv format). After cutting and merging some other data, I end with about 90gb (again in csv). 
I tried converting to parquet but was getting so many issues I dropped it. 
Currently I am working with the csv and trying to implement DASK and pandas for efficiency of handling the data with dask but then statistical analysis with pandas. 
This is what ChatGPT has told me to do (yes maybe not the best but I am not good and coding so have needed a lot of help). When I try to run this on my uni’s HPC (using 4 nodes with 90gb memory per) it’s still getting killed because too much memory. 
Any suggestions? Is going back to parquet more efficient? My main task it just simple regression analysis ",3,18,1725464879.0,bigdata
1f8tkj2,Is parquet not suitable for IOT integration?,In a design i chose parquet format for iot time series stream ingestion (no other info on column count). I was told its not correct. But i checked online on AI and performance/storage benchmark and parquet is suitable. Just wanted to know if there are any practical limitations causing this feedback. Appreciate any inputs pls.,1,4,1725457107.0,bigdata
1f8npls,Free RSS feed for tousands of jobs in AI/ML/Data Science every day 👀,,2,0,1725436791.0,bigdata
1f8p6tb,HOWTO: Write to Delta Lake from Flink SQL,,1,0,1725443157.0,bigdata
1f8hc3g,Working with a modest JSONL file anyone has asuggestion?,"I am currently working with a relatively large dataset stored in a JSONL file, approximately 49GB in size. My objective is to identify and extract all the keys (columns) from this dataset so that I can categorize and analyze the data more effectively.

I attempted to accomplish this using the following DuckDB command sequence in a Google Colab environment:

duckdb /content/off.db <<EOF

-- Create a sample table with a subset of the data

CREATE TABLE sample\_data AS

SELECT \* FROM read\_ndjson('cccc.jsonl', ignore\_errors=True) LIMIT 1;

-- Extract column names

PRAGMA table\_info('sample\_data');

EOF

However, this approach only gives me the keys for the initial records, which might not cover all the possible keys in the entire dataset. Given the size and potential complexity of the JSONL file, I am concerned that this method may not reveal all keys present across different records.

I tried loading the csv file to Pandas but it is taking 10s of hours, is it a right options? DuckDB  at least seemed much much faster.

Could you please advise on how to:

Extract all unique keys present in the entire JSONL dataset?

Efficiently search through all keys, considering the size of the file?

I would greatly appreciate your guidance on the best approach to achieve this using DuckDB or any other recommended tool.

Thank you for your time and assistance.",1,1,1725414156.0,bigdata
1f6vbmc,Event Stream explained to 5yo,,4,0,1725242404.0,bigdata
1f72y7h,TRENDYTRCH BIG DATA COUSE," 

Hi guys if you want big data course or any help .. pls ping me on telegram 



In these course you will learn hadoop,hive ,mapredue,spark(steam and batch ) ,azure ,adls ,adf, synapse, databeticks,system design,delta live table , AWS Athena , s3 Kafka airflow and projects etc etc 

If you want pls ping me on telegram

My telegram id is :-
 @TheGoat_010",0,1,1725271002.0,bigdata
1f6eq9z,Supercharge Your Snowflake Monitoring: Automated Alerts for Warehouse Changes!,"Supercharge Your Snowflake Monitoring: Automated Alerts for Warehouse Changes!

  
[https://www.linkedin.com/posts/arunjangili\_supercharge-your-snowflake-monitoring-activity-7236004083386433536-Ikum?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/arunjangili_supercharge-your-snowflake-monitoring-activity-7236004083386433536-Ikum?utm_source=share&utm_medium=member_desktop)",1,0,1725198112.0,bigdata
1f6azei,How to implement business intelligence at an enterprise organisation?,"1. Understand the Company’s Needs:

	•	Begin by researching the company’s current challenges, goals, and industry trends. Understand their pain points, such as inefficient processes, lack of data-driven decision-making, or missed opportunities. Tailor your approach to show how Business Intelligence (BI) can address these specific needs.

2. Highlight the Benefits of BI:

	•	Present the advantages of BI, such as improved decision-making, enhanced efficiency, and real-time insights. Emphasize how BI can help the company stay competitive by leveraging data to predict trends, optimize operations, and drive strategic decisions. Provide examples of successful BI implementations in similar industries to build credibility.

3. Demonstrate Quick Wins:

	•	Offer to run a small pilot project or proof of concept to demonstrate the immediate benefits of BI. For instance, create a simple dashboard that visualizes key performance indicators (KPIs) relevant to the company. This tangible demonstration will help stakeholders see the value of BI firsthand, making them more likely to support a full-scale implementation.

4. Address Concerns and Misconceptions:

	•	Be prepared to address common concerns, such as costs, complexity, and data security. Explain that modern BI tools are scalable and can be customized to fit the company’s budget and technical capabilities. Highlight your company’s Privacy-First Policy to ensure data security and compliance with regulations.

5. Involve Key Stakeholders:

	•	Engage decision-makers early in the process, including department heads, IT teams, and executives. Tailor your messaging to each stakeholder’s priorities—show the CFO how BI can reduce costs, demonstrate to the COO how it can streamline operations, and convince the CEO how it aligns with strategic goals. Collaborative discussions will help gain buy-in from all levels of the organization.

https://aleddotechnologies.ae
",1,0,1725185139.0,bigdata
1f6awki,How to convince a company to use business intelligence ,"1. Understand the Company’s Needs:

	•	Begin by researching the company’s current challenges, goals, and industry trends. Understand their pain points, such as inefficient processes, lack of data-driven decision-making, or missed opportunities. Tailor your approach to show how Business Intelligence (BI) can address these specific needs.

2. Highlight the Benefits of BI:

	•	Present the advantages of BI, such as improved decision-making, enhanced efficiency, and real-time insights. Emphasize how BI can help the company stay competitive by leveraging data to predict trends, optimize operations, and drive strategic decisions. Provide examples of successful BI implementations in similar industries to build credibility.

3. Demonstrate Quick Wins:

	•	Offer to run a small pilot project or proof of concept to demonstrate the immediate benefits of BI. For instance, create a simple dashboard that visualizes key performance indicators (KPIs) relevant to the company. This tangible demonstration will help stakeholders see the value of BI firsthand, making them more likely to support a full-scale implementation.

4. Address Concerns and Misconceptions:

	•	Be prepared to address common concerns, such as costs, complexity, and data security. Explain that modern BI tools are scalable and can be customized to fit the company’s budget and technical capabilities. Highlight your company’s Privacy-First Policy to ensure data security and compliance with regulations.

5. Involve Key Stakeholders:

	•	Engage decision-makers early in the process, including department heads, IT teams, and executives. Tailor your messaging to each stakeholder’s priorities—show the CFO how BI can reduce costs, demonstrate to the COO how it can streamline operations, and convince the CEO how it aligns with strategic goals. Collaborative discussions will help gain buy-in from all levels of the organization.

If you are looking on how to implement BI at your company, contact - https://aleddotechnologies.ae",1,1,1725184833.0,bigdata
1f6er5o,AI is Taking Over: What You Need to Know Before It's Too Late!,"AI is Taking Over: What You Need to Know Before It's Too Late!

  


[https://medium.com/@ridian.dany/embrace-tomorrow-how-ai-will-revolutionize-every-aspect-of-our-lives-3dd0530361bc](https://medium.com/@ridian.dany/embrace-tomorrow-how-ai-will-revolutionize-every-aspect-of-our-lives-3dd0530361bc)

",0,0,1725198184.0,bigdata
1f50ue3,"Open source python library that allows you to chat, modify, visualise your data","Today, I used this open source python library called DataHorse to analyze Amazon dataset using plain English. No need for complicated tools—DataHorse simplified data manipulation, visualization, and building machine learning models. 

Here's how it improved our workflow and made data analysis easier for everyone on the team. 

Try it out: https://colab.research.google.com/drive/192jcjxIM5dZAiv7HrU87xLgDZlH4CF3v?usp=sharing

GitHub: https://github.com/DeDolphins/DataHorsed",26,3,1725038560.0,bigdata
1f4pu6a,HOW TO BUILD YOUR ORGANIZATION DATA MATURE?,"Is your organization ready to transition from basic data use to complete data transformation? Explore the 4 stages of data maturity and the key elements that drive growth. Start your journey with USDSI® Certification.

https://reddit.com/link/1f4pu6a/video/egpl4eotdrld1/player

",0,0,1725005119.0,bigdata
1f4hse4,Looking for researchers and members of AI development teams to participate in a user study in support of my research ,"We are looking for researchers and members of AI development teams who are at least 18 years old with 2+ years in the software development field to take an anonymous survey in support of my research at the University of Maine. This may take 20-30 minutes and will survey your viewpoints on the challenges posed by the future development of AI systems in your industry. If you would like to participate, please read the following recruitment page before continuing to the survey. Upon completion of the survey, you can be entered in a raffle for a $25 amazon gift card.

https://docs.google.com/document/d/1Jsry_aQXIkz5ImF-Xq_QZtYRKX3YsY1_AJwVTSA9fsA/edit",2,3,1724976997.0,bigdata
1f49fqb,Data sets for all S&P 500 companies and their individual finacial ratios for the years of 2020-2023,"Not sure if I am in the right place but I’m hoping someone can lead me in the right direction atleast. 

I am a masters student looking to do a research paper on how data science can be used to find undervalued stocks. 

The specific ratios I am looking for is 
P/E Ratio
P/B Ratio
PEG ratio
Dividend yield
Debt to equity
Return on assets
Return on equity
EPS
EV/EBITDA
Free cash flow 

Would also be nice to know the stock price and ticker symbol 

An example
AAPL 2020
PRICE: X
P/E Ratio: x
P/B Ratio: X
PEG ratio: x
Dividend yield: x
Debt to equity: x
Return on assets: x
Return on equity: x
EPS: x 
EV/EBITDA: x
Free cash flow: x

Then the next year after:

AAPL 2021
PRICE: X
P/E Ratio: x
P/B Ratio: X
PEG ratio: x
Dividend yield: x
Debt to equity: x
Return on assets: x
Return on equity: x
EPS: x 
EV/EBITDA: x
Free cash flow: x

Then 2022 and so on till the year 2023.

I am not a cider but I have tried extensively to make a program using Chatgpt and Gemini to scrape the data from multiple sources….I was able to get a list of everything that I was looking for, For the year 2024 using Yfinance on python but was not able to get the historical data using yfinance. I have tried my hand at trying to scrape the data from EDGAR as well but as I said I am not a coder and could not figure it out. Would be willing to pay 10-50$ for the dataset from a website too but could not find one that was easy to use/had all the info I was looking for. (I did find one I believe but they wanted $1800 for it) willing to get on a phone call or discord call if that helps. ",3,4,1724955384.0,bigdata
1f406ac,DATA SCIENCE AND ARTIFICIAL INTELLIGENCE- FUTURE CATALYST IN ACTION | INFOGRAPHIC,"[Data science and artificial intelligence](https://www.usdsi.org/data-science-insights/data-science-and-artificial-intelligence-future-catalyst-in-action) are viewed as the best duo working to excel in the business landscape. With digitization and technology advancements taking rapid strides; it is widely evident that the industry workforce evolves with these changes.

With hyper-automation, cognitive abilities, and ethical considerations guiding the [data science industry](https://www.usdsi.org/) far and wide. It is expected that these smart tech additions assist in managing data explosion, advanced analytics, and enhancing domain expertise. Understanding the core convergence, challenges, and opportunities that this congruence brings to the table is inevitable for every data science enthusiast.

If you wish to build a thriving career in data science with futuristic skillsets on display; it is the time to invest in one of the [best data science certifications](https://www.usdsi.org/data-science-certifications); that empower you with core AI nuances as well. The generative AI market size is expanding at an astounding rate. This will give way to even smarter advances in data science technology and ways to counter the staggering data volume worldwide.

This is why, global industry recruiters are looking forward to appointing a skilled certified workforce that can guarantee enhanced business growth and multiplied career advancements as well. Start exploring the best credentialing options to get closer to a successful career trajectory in data science today!

https://preview.redd.it/91nzvo5ualld1.jpg?width=1200&format=pjpg&auto=webp&s=ca367da9e3353652327638019f9648dcb07baff4

",0,1,1724931486.0,bigdata
1f3yi4k,"Pharmacy Management Software Development: Costs, Process & Features Guide",,1,1,1724925347.0,bigdata
1f3imvs,Analyze Big Social Media Data: $6000 Challenge (12 Days Left!),"Hey all! There's still time to jump into our Social Media Data Modeling Challenge (Think hack-a-thon) and compete for $6000 in prizes! Don't worry about being late to the party – most participants are just getting started, so you've got plenty of time to craft a winning submission! Even with just a few hours of focused work, you could create a competitive entry!

**What's the Challenge?**

Your mission, should you choose to accept it, is to analyze real social media data, uncover fascinating insights, and showcase your SQL, dbt™, and data analytics skills. This challenge is open to all experience levels, from seasoned data pros to eager beginners.

Some exciting topics you could explore include:

* Tracking COVID-19 sentiment changes on Reddit
* Analyzing Donald Trump's popularity trends on Twitter/Reddit
* Identifying and explaining who the biggest YouTube creators are
* Measuring the impact of NFL Superbowl commercials on social media
* Uncovering trending topics and popular websites on Hacker News

But don't let these limit you – the possibilities for discovery are endless!

**What You'll Get**

Participants will receive:

* Free access to professional data tools (Paradime, MotherDuck, Hex)
* Hands-on experience with large, relevant datasets (great for your portfolio)
* Opportunity to learn from and connect with other data professionals
* A shot at winning: $3000 (1st), $2000 (2nd), or $1000 (3rd)

**How to Join**

To ensure high-quality participation (and keep my compute costs in check 😅), here are the requirements:

* You must be a current or former data professional
* Solo participation only
* Hands-on experience with SQL, dbt™, and Git
* Provide a work email (if employed) and one valid social media profile (LinkedIn, Twitter, etc.) during registration

Ready to dive in? [**Register here**](https://www.paradime.io/dbt-data-modeling-challenge) and start your data adventure today! With 12 days left, you've got more than enough time to make your mark. Good luck!",1,0,1724872363.0,bigdata
1f35t0u,Storing and Analyzing 160B Quotes in ClickHouse,,1,0,1724837216.0,bigdata
1f1w6ii,Coordinate Reference System for NREL Wind Resource Database,"I'm working with geospatial windspeed data from the [NREL Wind Resource Database](https://wrdb.nrel.gov/data-viewer), but it's not clear what coordinate reference system is being used. I found on their [GitHub](https://github.com/NREL/hsds-examples/blob/master/datasets/wtk-us.md) that they use a \`\`modified Lambert-conic"" system, but none of the various Lambert-conic EPSGs or PROJ strings I've found online seem to be correct.

Does anyone know how I can find out what's the exact CRS they used? Thanks :)",2,1,1724698614.0,bigdata
1f1uhrk,Final year project idea suggestion ,"I am a final-year computer science student interested in real-time data streaming in the big data domain. 

Could you suggest a use cases along with relevant datasets that would be suitable for a final-year project?",1,3,1724694626.0,bigdata
1f1fbb5,"FREE AI WEBINAR: 'How to build an AI layer on your Snowflake data to query your database - Webinar by deepset.ai' [Aug 29, 8 am PST]",,1,1,1724645152.0,bigdata
1f054em,Essential AI Engineer Skills and Tools you Should Master,,2,0,1724505619.0,bigdata
1f020t6,TRANSFORM YOUR CAREER PATH WITH USDSI®'S DATA SCIENCE CERTIFICATION PROGRAM,"Take your [data science career](https://www.usdsi.org/data-science-certifications) to the next level with USDSI’s industry relevant certification program. Whether you're a students, professionals, and career switchers, our program offers practical skills and knowledge with minimal time commitment.

https://preview.redd.it/58ua16i88lkd1.jpg?width=1080&format=pjpg&auto=webp&s=6cbf8e082e57c867436ded8d300729fa7f8e23ee

",0,0,1724494754.0,bigdata
1ezh70t,My Medium article on ClickHouse,"My Medium article on ClickHouse

I recently published an article on Medium (around a month ago) about ClickHouse. 

ClickHouse is an SQL compliant, extremely fast, and horizontally scalable data warehouse and analytics platform, which has recently gained popularity mainly due to its performance.

I have tried writing it for beginners to provide enough information to start working with ClickHouse, to build a basic understanding of its capabilities, and also to provide enough information to decide whether ClickHouse is the right tool for the task at hand.

Read here: https://medium.com/@suffyan.asad1/beginners-guide-to-clickhouse-introduction-features-and-getting-started-55315107399a

It also contains a section about other useful articles and links about how ClickHouse is used in various systems by others, and also serves as a collection of beyond the basics.  

Please read and provide feedback, it'd be very helpful for me to improve my writing and utility of my articles. Additionally, I write mainly about Apache Spark and other data engineering topics. ",0,0,1724430688.0,bigdata
1eyr5wz,Google Sheets Integration is Live!,,1,0,1724352270.0,bigdata
1eyc6iq,How State-Level Data Reveals Hidden Asbestos Risks in Talc Products: What the Numbers Tell Us,,3,1,1724307145.0,bigdata
1ex866n,Sourcetable - Free bulk-CSV analysis tool (feedback plz!),,3,1,1724190784.0,bigdata
1ewwier,Evolving the Data Lake: From CSV/JSON to Parquet to Apache Iceberg,,3,0,1724162858.0,bigdata
1ewso6h,The Future of Healthcare: Nationwide Digital Health Records Programme," As we progress further into the digital age, the need for streamlined and accessible health information becomes increasingly critical. The Nationwide Digital Health Records Programme aims to enhance healthcare delivery by establishing a unified system that allows for better data management, patient care, and informed decision-making.  

Imagine a world where your medical history, test results, and treatment plans are all available at the touch of a button, no matter where you are! This initiative not only promises to reduce administrative burdens but also ensures that healthcare providers have real-time access to vital patient data.  

However, with such a monumental shift towards digital records, we must also address concerns regarding data privacy, security, and equitable access to technology. What do you think about this move towards a nationwide digital health record system? Are there any potential challenges or benefits that you foresee in this transformation? Let's discuss! 
https://7med.co.uk/nationwide-digital-health-records-programme/",2,1,1724151841.0,bigdata
1ewwjuy,8 Tools For Ingesting Data Into Apache Iceberg,,1,1,1724162972.0,bigdata
1ewpywu,BOOST YOUR BUSINESS WITH AI & DATA LITERACY,"In today's [data-driven world](https://www.usdsi.org/data-science-insights/ai-and-data-literacy-future-proof-combination-to-strengthen-business), businesses must prioritize data literacy to harness the full potential of AI. Learn how upskilling your workforce can transform data into actionable insights, driving innovation and growth. 

https://preview.redd.it/pw5i4gi30sjd1.jpg?width=1080&format=pjpg&auto=webp&s=4579f0e3f4f07acb082fd5e555609cec553493b8

",0,1,1724140916.0,bigdata
1ewlffi,How hard is to start a career in Big Data with just a BS in Marketing?,"I just got my B.S. in Marketing and was wondering if you need more of a Data Analytics degree. If I can get an entry-level position in big data and marketing, what should it be?",0,1,1724123974.0,bigdata
1euev53,DRIVEN TOMORROW WITH USDSI® DATA SCIENCE CERTIFICATION,"Shape your destiny in data science with [USDSI® Certifications](https://www.usdsi.org/data-science-certifications). Whether you're an enthusiast or a seasoned analyst, our programs empower you for future challenges. Join USDSI® on the journey to professional success.

https://preview.redd.it/jbhsspm4f7jd1.jpg?width=736&format=pjpg&auto=webp&s=15874b5bd4d17c432d9c55dd52ebe1c73f6bb44e

",1,0,1723891720.0,bigdata
1eu9mco,How to skip header rows from a table in Hive? (Hands On),,1,0,1723870223.0,bigdata
1etib1e,TOP 15 Data Science Advantages for Business,"Data science is undoubtedly the biggest transformation factor for businesses across all industries.

Data science has numerous benefits across all industries. While educational institutions are using data science to personalize their educational content, find our student dropouts, and enhance their administration, the [healthcare industry is using data science](https://www.usdsi.org/data-science-insights/revolutionizing-the-healthcare-industry-with-the-help-of-data-science) to treat patients in a more personalized way by analyzing huge amounts of health data.

https://preview.redd.it/1iaeyzh76zid1.jpg?width=1155&format=pjpg&auto=webp&s=b3a5508b83e2aa910ed135e2b229e902904296ed

**This is just an example.**

Data science has wide applications in all industries, from finance to retail, to manufacturing. [USDSI®](https://www.usdsi.org/) brings a comprehensive guide discussing its advantages in different sectors.

We highlight how it can be effectively used to detect frauds in financial sectors, how data science helps to analyze vast amounts of data and assist with anomaly detection to detect cyber threats easily. Not just that, learn how using data science, organizations can incorporate a culture of [data-driven decision-making](https://www.usdsi.org/data-science-insights/resources/master-data-driven-decision-making-in-2024) that will ultimately lead to boosting their businesses and enhancing their customer service.

[Download](https://www.usdsi.org/data-science-insights/resources/top-15-data-science-advantages-for-business) this guide now and learn how you can implement data science to boost your business.",0,0,1723791883.0,bigdata
1etl1pt,TOP 11 PROGRAMMING LANGUAGES FOR DATA SCIENTISTS’ INSTANT RESUME BOOST,"Understanding a [programming language for data science](https://www.usdsi.org/data-science-insights/top-11-programming-languages-for-data-scientists-instant-resume-boost) is of utmost importance today than ever before. No data science task is complete without the expert leveraging of top-notch programming languages. As the world grows with whopping data generation rates; it is imperative to understand the way programming and data science communicate to bring out the most targeted insights for business growth.

This read shall assist you with the most comprehensive and contemporary programming languages and allow you a quick sneak into them. Mastering these core nuances that guide the [data science industry](https://www.usdsi.org/) is indispensable as you build your career as a data scientist. Make it a priority to enroll with the most trusted and seasoned players when it comes to the globally renowned [best data science certifications](https://www.usdsi.org/data-science-certifications). You must grow your data science niche with sheer skill and futuristic talent on offer.

Not only that; you will be offered a higher salary, a meatier data science role, and an industry career progression like none other; when you get certified with the global leaders in credentialing. If you are someone who wishes to understand the inside out of the programming languages and envision yourself earning top-notch roles with your dream industry recruiters- Start Right Here!

https://preview.redd.it/qyr2zobx30jd1.jpg?width=2000&format=pjpg&auto=webp&s=2ca516908ef93a60de838b7ccf7b10fd21f2a3f2

",0,0,1723803212.0,bigdata
1es5sd3,Rollstack Connects Dashboards to PowerPoint,"This is a super common issue in reporting. The data people use dashboards, but monthly and quarterly reports are still done in PowerPoint. Rollstack connects your dashboards to PowerPoint and Google Slides for automated report generation. No more screenshots! Just thought it was pretty helpful, and wanted to share. ",3,0,1723652163.0,bigdata
1ervtos,BIG DATA ANALYTICS MYTH V/S REALITY,"In the age of data-driven decisions, understanding the true capabilities of big data is crucial. Bust the myths that obscure the value of [big data analytics](https://www.usdsi.org/data-science-insights/resources/exclusive-big-data-analytics-myths-debunked) and gain behind-the-scenes knowledge from leading experts.

https://preview.redd.it/29sj5lb55lid1.jpg?width=1080&format=pjpg&auto=webp&s=d1a2ed6672df35625aee195233f609c0b972e7e8

",1,0,1723622020.0,bigdata
1erag9d,Real-time Computation of Option Greeks Using Pathway and Databento,"I am excited to share this tutorial that demonstrates how to compute Option Greeks in real-time. Option Greeks are essential tools in financial risk management, measuring an option’s price sensitivity.

Using Pathway, a real-time data processing framework, this tutorial computes Option Greeks based on Databento’s market data. The values are continuously updated in real-time with data provided by Databento.

In our latest article, you’ll learn how to compute these Option Greeks using Databento’s market data and keep them updated in real-time.

Learn more about the project here: [https://pathway.com/developers/templates/option-greeks](https://pathway.com/developers/templates/option-greeks)

GitHub: [https://github.com/pathwaycom/pathway/tree/main/examples/projects/option-greeks](https://github.com/pathwaycom/pathway/tree/main/examples/projects/option-greeks)",5,2,1723562051.0,bigdata
1erbhy3,User Management in ClickHouse® Databases: The Unabridged Edition,"August 21 @ 8:00 am – 9:00 am PDT

User management is a key problem in any **#analytic** application. Fortunately, **#ClickHouse** has a rich set of features for **#authentication** and **#authorization**. We’re going to tell you about all of them. We’ll start with the model: users, profiles, roles, quotas, and row policies. Then we’ll show you implementation choices from **#XML** files to **#SQL** commands to external identity providers like **#LDAP**. Finally, we’ll talk about features on the horizon to improve ClickHouse security. There will be a sample code plus plenty of time for questions.

Join us to learn how to manage your users simply and effectively.",1,1,1723564501.0,bigdata
1eqpnd3,Fan of LLMs+RAG? Put any URL after md.chunkit.dev/ to turn it into markdown chunks,"For example, https://md.chunkit.dev/https://en.wikipedia.org/wiki/Chunking_(psychology)",3,1,1723497491.0,bigdata
1ep4d2m,Big Data ,,1,3,1723326668.0,bigdata
1eoxq9c,Chew: a library to process various content types to plaintext,,1,0,1723308885.0,bigdata
1eo4oay,Best Practices to Manage Databricks Clusters at Scale to Lower Costs,,0,0,1723222046.0,bigdata
1eo0dy2,Request for guide for Big data in a vm,"Hey,

I am an beginner in Big data, and is considering to install the necessary software like hardoop and spark.

Many senior members suggested I use Vm for it. 

Can anyone suggest which Linux version I should download for it along with any thing I need to look out for while setting it up for big data",1,2,1723211865.0,bigdata
1entnzw,7 Popular Data Science Components To Master in 2024,"Before starting a career in data science, it is important to understand what it constitutes of. Explore different components of data science that you must master in 2024. 

https://preview.redd.it/090a1pxhclhd1.jpg?width=1084&format=pjpg&auto=webp&s=9a4873e2efcc7bf4165851edd7c6f1b579ae7534

",1,0,1723188642.0,bigdata
1en9f0k,How do companies that deal with a large amount of excel spreatsheet data from various clients that have different standards for their data? Do they keep them as spreadsheets? Do they convert them into SQL databases or NoSQL databases?,,3,6,1723132067.0,bigdata
1en70j5,Migration Guide for Apache Iceberg Lakehouses,,2,1,1723126294.0,bigdata
1en1yhu,7 Popular Data Science Components To Master in 2024,"Before starting a career in data science, it is important to understand what it constitutes of. Explore different [components of data science](https://www.usdsi.org/data-science-insights/7-popular-data-science-components-to-master-in-2024) that you must master in 2024.

https://preview.redd.it/rduli32jxehd1.jpg?width=1084&format=pjpg&auto=webp&s=73f090ae8d0b238e594449cdb67d8f5bcc5c7353

",3,0,1723110973.0,bigdata
1en1z73,Impact of Data Science in Robotics,"[Data Science and Robotics](https://www.usdsi.org/data-science-insights/impact-of-data-science-in-robotics) are the cross-disciplines of similar fields of study – science, statistics, computer technology, and engineering.

https://preview.redd.it/1gp50tlsxehd1.jpg?width=1920&format=pjpg&auto=webp&s=b3a1b7e57212d1353d1e58c1929b3391e481cb06

",1,0,1723111047.0,bigdata
1emmvxs,"6-Week Social Media Data Challenge: Tackle large Social media datasets, win up to $3000!","I've just launched an exciting 6-week challenge focused on analyzing large-scale social media data. It's a great opportunity to apply your big data skills and potentially win big!

What's involved:

- Work with real, large-scale social media datasets

- Use professional tools: Paradime (SQL/dbt™), MotherDuck (data warehouse), Hex (visualization)

- Chance to win: $3000 (1st), $2000 (2nd), $1000 (3rd) in Amazon gift cards

My partners and I have invested in creating a valuable learning experience with industry-standard tools. You'll get hands-on practice with real-world big data and professional technologies. Rest assured, your work remains your own - we won't be using your code, selling your information, or contacting you without consent. This competition is all about giving you a chance to apply and showcase your big data skills in a real-world context.

Concerned about time? No worries, the challenge submissions aren't due until September 9th. Even 5 hours of your time could put you in the running, but feel free to dive deeper!

Check out our [explainer video](https://youtu.be/5RPUghtyCfM?si=KNZMnjj6R18WzWQP) for more details.

Interested? Register here: [https://www.paradime.io/dbt-data-modeling-challenge](https://www.paradime.io/dbt-data-modeling-challenge)",9,1,1723063538.0,bigdata
1eln16x,Vm failed connection in hadoop,I ran “start-all.sh” command after making sure it wasn’t running and when i try running “hdfs dfs -ls /“ for testing if hdfs is working that error shows up “ls: call from localhost.localdomain/127.0.0.1 to localhost:9000 failed on connection” how can i fix it,2,1,1722963588.0,bigdata
1elw52m,10 Reasons Why You Should Own a Great Dane,,0,5,1722985618.0,bigdata
1elb45c,"Real Time Data Project That Teaches Streaming, Data Governance, Data Quality and Data Modelling
","[https://www.linkedin.com/posts/arunjangili\_datagovernance-dataquality-datamodelling-activity-7226473747531120640-\_Gn3?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/arunjangili_datagovernance-dataquality-datamodelling-activity-7226473747531120640-_Gn3?utm_source=share&utm_medium=member_desktop)

Practice above project and master All Data Governance, Quality, Modelling and Streaming",1,0,1722927457.0,bigdata
1elata9,BEST DATA SCIENCE CERTIFICATIONS IN 2024,"Data science has become the hottest career opportunity of today’s time. It is essentially indispensable for empowering yourself with the most trusted [data science certifications](https://www.usdsi.org/data-science-certifications). 

https://preview.redd.it/j49ywh2bozgd1.jpg?width=1504&format=pjpg&auto=webp&s=09f8465db80a3a445a3e6e12d7b8d7f77fd96fb2

",0,0,1722926283.0,bigdata
1eklq15,6 HOTTEST DATA ANALYTICS TRENDS TO PREPARE AHEAD OF 2025,"It is your time to gain insightful training in the world of data science with the best worldwide. USDSI® presents a holistic read that gathers maximum information and guidance on the most futuristic trends and technologies that are stipulated to guide the data world. Predict the future of data analytics with exceptional skills in data unification in the cloud, the rise of small data, the evolutionary role of data products, and beyond. this could be your beginning to grab the top-notch career possibilities with both hands and elevate your career in data science as a Pro!

https://reddit.com/link/1eklq15/video/v558k9lf2ugd1/player

",0,1,1722858403.0,bigdata
1ej193j,WHY CHOOSE USDSI® FOR YOUR DATA SCIENCE JOURNEY?,"Explore the unique advantages of the USDSI® Data Science Program. Equip yourself with real-world skills and expertise to stay ahead in the data-driven world.

https://preview.redd.it/dw9qqaryqfgd1.jpg?width=1080&format=pjpg&auto=webp&s=231422b19502dd9a2c538cf49d0caac8c01c382e

",0,0,1722685021.0,bigdata
1ei7id7,Announcing the Release of Apache Flink 1.20,,1,0,1722597648.0,bigdata
1eh8c2v,Created Job that sends Report without integrity checks,"So, im an intern at this bank in the BI/Insights department. I recently created a Talend job that queries data from our data warehouse from some tables every first day of the month at 5:00 am, generates an excel report and sends it to the relevant business users. Today's the first time it ever run officially outside testing conditions and the results are rather shameful.

The first excel sheet hasn't been populated by any data, except formulas and zeros... it was dependent on data from a different sheet, which was blank. This was because that latest data wasn't yet loaded into the warehouse tables i was querying from, as my report requires latest info as at the last day of the month.

I think i need to relearn BI/Bigdata principles, especially regarding data governance and integrity checks. Any help and suggestions would be appreciated.",2,2,1722488219.0,bigdata
1egnrun,Using Pathway for Delta Lake ETL and Spark Analytics,"In the era of big data, efficient data preparation and analytics are essential for deriving actionable insights. This tutorial demonstrates using Pathway for the ETL process, Delta Lake for efficient data storage, and Apache Spark for data analytics. This approach is highly relevant for data engineers looking to integrate data from various new sources and efficiently process it within the Spark ecosystem.  
  
**Comprehensive guide with code:** [https://pathway.com/developers/templates/delta\_lake\_etl](https://pathway.com/developers/templates/delta_lake_etl)

**Why This Approach Works:**

* **Versatile Data Integration:** Pathway’s Airbyte connector allows you to ingest data from any data system, be it GitHub or Salesforce, and store it in Delta Lake.
* **Seamless Pipeline Integration:** Expand your data pipeline effortlessly by adding new data sources without significantly changing them.
* **Optimized Data Storage:** Querying over data organized in Delta Lake is faster, enabling efficient data processing with Spark. Delta Lake’s scalable metadata handling and time travel support make it easy to access and query previous versions of data.

**Using Pathway for Delta ETL simplifies these tasks significantly:**

* **Extract:** Use Airbyte to gather data from sources like GitHub, configuring it to specify exactly what data you need, such as commit history from a repository.
* **Transform:** Pathway helps remove sensitive information and prepare data for analysis. Additionally, you can add useful information, such as the username of the person who made changes and the time of the changes.
* **Load:** The cleaned data is then saved into Delta Lake, which can be stored on your local system or in the cloud (e.g., S3) for efficient storage and analysis with Spark. 

Would love to hear your experiences with these tools in your big data workflows!",10,0,1722434053.0,bigdata
1eh0i0f,Data extraction- Historical Cost data,"Hello guys! not sure if this is the right spot to post. I have to extract historical cost data from a large pdf over 900 pages. it seems simple but i need to maintain the CSI CSI MasterFormat division structure to ensure compatibility with our existing data tables. This is the specific data in question. [RSMeans Building Construction Cost Data 2014 : Free Download, Borrow, and Streaming : Internet Archive](https://archive.org/details/RSMeansBuildingConstructionCostData2014/page/n29/mode/2up)",2,0,1722465093.0,bigdata
1egbf2z,Modern Data Quality Summit 2024,"The world is experiencing a data revolution, led by AI. However, only 48% of AI projects reach production, taking an average of 8.2 months. This shows the need for AI-readiness and quality data. At the Modern Data Quality Summit 2024, we offer insights into best practices, innovative solutions, and strategic frameworks to prepare your data for AI and ensure successful implementation.

Here’s a sneak peek of what we have in store for you:

* Data quality optimization for real-time and multi-structured AI applications
* Approaching data quality as a product for enhanced business focus
* Implementing proactive data observability for superior quality control
* Building a data-driven culture that prioritizes quality and drives success

Register Now - [https://moderndataqualitysummit.com/](https://moderndataqualitysummit.com/)",5,1,1722391187.0,bigdata
1egiwp2,IS Generative AI BENEFICIAL FOR A DATA ENGINEER?,"Accelerate your [data engineering journey with Generative AI](https://www.usdsi.org/data-science-insights/the-data-science-method-and-generative-ai) ! Learn how this cutting-edge technology streamlines SQL and python code generation, debugging, and optimization, enabling data engineers to work smarter.

https://preview.redd.it/jvs9rvr6qtfd1.jpg?width=1080&format=pjpg&auto=webp&s=43500c5ab5b574e3bf3e4f7679a380accacdadfc

",0,0,1722418441.0,bigdata
1efsltk,How does Data Science revolutionize the education sector?,"Data science is rapidly transforming the education landscape. By analyzing vast amounts of student data, educators can gain profound insights into learning patterns, challenges, and strengths. This enables personalized learning experiences tailored to individual needs, early identification of struggling students, and optimized resource allocation.

Predictive analytics, a powerful tool within data science, allows institutions to forecast student outcomes, enabling proactive interventions to improve academic performance and prevent dropouts. Furthermore, data-driven insights inform curriculum development, teacher training, and policy decisions, ensuring education aligns with the evolving needs of students and society.

Currently, the adoption of [data science in the education industry](https://www.usdsi.org/about-united-states-data-science-institute) is at the infant stage, however, it is growing rapidly. It is evident from the fact that the global education and learning analytics market is expected to reach $90.4 billion by 2030 (*source: Data Bridge*)

However, the ethical use of data is paramount. Protecting student privacy and ensuring data security are critical considerations. Additionally, educators and administrators require ongoing training to effectively leverage data-driven insights.

By embracing data science, educational institutions can create more equitable, efficient, and effective learning environments. The potential to enhance student outcomes and drive educational innovation is immense.

[**Download**](https://www.usdsi.org/data-science-insights/resources/how-does-data-science-revolutionize-the-education-sector) your copy of [USDSI’s](https://www.usdsi.org/) comprehensive guide on ‘how data science is revolutionizing the education sector’, and gain valuable insights on data science for the education sector.",1,0,1722344115.0,bigdata
1eeugh2,How To Make a Solid Portfolio for An Aspiring Data Analyst,"Check out our detailed infographic guide on [data analyst portfolios](https://www.usdsi.org/data-science-insights/how-to-make-a-solid-portfolio-for-an-aspiring-data-analyst) and understand their importance in today’s competitive world. Also, learn how to build an attractive one. 

https://preview.redd.it/10kanysw5ffd1.png?width=1080&format=png&auto=webp&s=578fef51dee81d2f0208b4f4cf3f29a9d1413d8b

",3,0,1722242103.0,bigdata
1ed9vcf,"Free ebook for Bigdata Interview Preparation Guide (1000+ questions with answers) Programming, Scenario-Based, Fundamentals, Performance Tunning",,0,0,1722062004.0,bigdata
1edg5bn,TRANSFORM YOUR CAREER AND ELEVATE YOURSELF TO DATA SCIENCE LEADER,"Elevate your career and become a [data science leader with CSDS™](https://www.usdsi.org/data-science-certifications/certified-senior-data-scientist). Demonstrate your technical knowledge and strategic mindset, and show the world your capability to drive business success. 

https://preview.redd.it/zdyhki1ca2fd1.jpg?width=1080&format=pjpg&auto=webp&s=516e16cbd742511321e4912a18df2ab76d0bd5b0

",0,0,1722086189.0,bigdata
1ec4sbk,"mods are asleep, post big data",,39,2,1721938500.0,bigdata
1ecjmny,Help with Data Catalog application architecture,"Hello guys,

I have a project in which I have to collect aggregate data for each customer from one big table. In banking an example could be, a customer having an id, purchase\_amount, money\_conversion\_amount columns and in table it is stored as  
id,    purch.,   mon.,   date  
100,   85,      200,     2024-07-26  
100,   12,       0,        2024-07-25  
101,   34,      10,       2024-07-26  
100,   11,      56,       2024-07-24  
101,  10,       0,        2024-07-25

so aggregate data for each use stored in one big table  
My project aims to have one more aggregate table having this columns:  
id, purchases\_sum\_last1day, purchases\_sum\_last3day, purchases\_sum\_1month, money\_conversion\_amount\_sum\_last1day .....  
aggregate functions are sum, min, max and avg  
Data is stored on data lake (hdfs) and we are using spark as well.  
Right now I have a working application but I am not happy with the performance, it reads a config file and generated a very long sql query and executes it with spark.  
I would like to get ideas about how efficiently I can handle the project (like having metadata table or using streaming somehow).",1,15,1721985233.0,bigdata
1eb04y4,Apache Fury 0.6.0 Released: 6x serialization faster and 1/2 payload smaller than protobuf serialization,[https://fury.apache.org/blog/fury\_0\_6\_0\_release](https://fury.apache.org/blog/fury_0_6_0_release),4,3,1721823836.0,bigdata
1eazix6,Sending Data to Apache Iceberg from Apache Kafka with Apache Flink,,2,0,1721821944.0,bigdata
1eb60t4,ChatGPT for data science 📊,,0,0,1721839126.0,bigdata
1eaim7a,Introducing Airbyte Refreshes: Reimport Historical Data with Zero Downtime,,6,0,1721767594.0,bigdata
1ea9gw6,Handling Out-of-Order Event Streams: Ensuring Accurate Data Processing and Calculating Time Deltas with Grouping by Topic,"Imagine you’re eagerly waiting for your Uber, Ola, or Lyft to arrive. You see the driver’s car icon moving on the app’s map, approaching your location. Suddenly, the icon jumps back a few streets before continuing on the correct path. This confusing movement happens because of out-of-order data.

In ride-hailing or similar IoT systems, cars send their location updates continuously to keep everyone informed. Ideally, these updates should arrive in the order they were sent. However, sometimes things go wrong. For instance, a location update showing the driver at point Y might reach the app before an earlier update showing the driver at point X. This mix-up in order causes the app to show incorrect information briefly, making it seem like the driver is moving in a strange way.  
This can further cause several problems like wrong location display, unreliable ETA of cab arrival, bad route suggestions, etc.

**How can you address out-of-order data?** There are various ways to address this, such as:

* **Timestamps and Watermarks**: Adding timestamps to each location update and using watermarks to reorder them correctly before processing.
* **Bitemporal Modeling**: This technique tracks an event along two timelines—when it occurred and when it was recorded in the database. This allows you to identify and correct any delays in data recording.
* **Support for Data Backfilling**: Your system should support corrections to past data entries, ensuring that you can update the database with the most accurate information even after the initial recording.
* **Smart Data Processing Logic**: Employ machine learning to process and correct data in real-time as it streams into your system, ensuring that any anomalies or out-of-order data are addressed immediately.

**Resource: Hands-on Tutorial on Managing Out-of-Order Data:** In this resource, you will explore a powerful and straightforward method to handle out-of-order events using Pathway. Pathway, with its unified real-time data processing engine and support for these advanced features, can help you build a robust system that flags or even corrects out-of-order data before it causes problems. Link to the code and more resources: [https://pathway.com/developers/templates/event\_stream\_processing\_time\_between\_occurrences](https://pathway.com/developers/templates/event_stream_processing_time_between_occurrences)

**Steps Overview:**

* **Synchronize Input Data**: Use **Debezium**, a tool that captures changes from a database and streams them into your application via Kafka/Pathway.
* **Reorder Events**: Use **Pathway** to sort events based on their timestamps for each topic. A topic is a category or feed name to which records are stored and published in systems like Kafka.
* **Calculate Time Differences**: Determine the time elapsed between consecutive events of the same topic to gain insights into event patterns.
* **Store Results**: Save the processed data to a **PostgreSQL** database using Pathway.

This will help you sort events and calculate the time differences between consecutive events. This helps in accurately sequencing events and understanding the time elapsed between them, which can be crucial for various applications.

Credits: Referred to resources by Przemyslaw Uznanski and Adrian Kosowski from Pathway, and Hubert Dulay (StarTree) and Ralph Debusmann (Migros), co-authors of the O’Reilly Streaming Databases 2024 book.

Hope this helps!",2,0,1721745524.0,bigdata
1e9xok6,What skills to learn for Big Data Specialization?,"I am an upcoming third year student in Computer Engineering Program. In our first two years in college we were taught Object-Oriented Programming, Data Structures and Algorithms, and Operating Systems. The language we used are Python and C++. What skills should I learn to pursue a specialization in Big Data?",2,0,1721704811.0,bigdata
1e9zcl8,Create Hive Table (Hands On) with all Complex Datatype,,0,0,1721710428.0,bigdata
1e9dtsr,TOP 3 TIPS MARKETING TEAMS NEED TO KNOW ABOUT DATA SCIENCE IN 2024,"Ready to take your marketing efforts to the next level? Discover the top three data science insights for 2024 and learn how to harness the power of AI, democratize data access, and create personalized customer experiences.

https://reddit.com/link/1e9dtsr/video/llx1z96uj2ed1/player

",0,0,1721653539.0,bigdata
1e99667,DATA SCIENCE CERTIFICATION,"Shape your destiny in [data science with USDSI® Certifications](https://www.usdsi.org/data-science-certifications). Whether you're an enthusiast or a seasoned analyst, our programs empower you for future challenges.

https://preview.redd.it/3q70ocp171ed1.jpg?width=1080&format=pjpg&auto=webp&s=1e1540f169fd78d1a7cf21c468e2663065963808

",0,0,1721637131.0,bigdata
1e8x001,Best translation service from english to arabic for less than 100$ or free,,1,0,1721596976.0,bigdata
1e7329a,DATA SCIENCE & MACHINE LEARNING THE FUTURE OF ROUTE PLANNING IN LOGISTICS,"The logistics industry is embracing [data science and machine learning](https://www.usdsi.org/data-science-insights/when-data-science-meets-route-planning-in-logistics) to revolutionize route planning. Discover how these technologies predict traffic, suggest alternative routes, and enhance delivery efficiency. 

https://preview.redd.it/7u9p6bk73hdd1.jpg?width=650&format=pjpg&auto=webp&s=21ec405f2e258b21a3edbd3ea992eca08f6decc0

",3,0,1721393715.0,bigdata
1e6vlyw,Sending Data file to Kafka Topic,,2,1,1721365004.0,bigdata
1e62j81,Apache Druid for Data Engineers (Hands-On) ,,4,0,1721276174.0,bigdata
1e5olw7,Want to be A Data Analyst ,"""I want to learn data analytics from the beginning. Can anyone provide me with a roadmap, resources, and a good learning path?""",5,2,1721238700.0,bigdata
1e5r1zu,"AI, Big Data Analytics, and the Modern Data Stack ","While AI continues to captivate executive attention—and rightfully so—it's essential to underscore the profound impact of robust automation and self-serve analytics. Before diving into the complexities of AI, it's critical to establish a solid foundation with proven tools and practices:

✨ Data Modeling: Utilize tools like dbt and Tableau Prep for self-serve data modeling that empowers teams to manage and transform data efficiently.

🔀 ETL/ELT Processes: Implement solutions like Fivetran or Airflow to streamline your data integration, ensuring a seamless data flow across your systems.

📊 Data Visualization: Leverage platforms like Tableau, Looker, Metabase, and Power BI to transform raw data into actionable insights through compelling visual narratives.

🤖 Report Automation: Generate your reports [Rollstack](http://www.rollstack.com). Facilitating automated reporting frees up your team's time to focus on high-impact work.

🛠️ Implement Data Best Practices: Adopt practices like version control, CI/CD, and unit testing to maintain code quality and ensure reliability in your data operations.

Prioritizing building a dependable data foundation is what enables your team to harness the power of AI; without this foundation, the output of your AI will always be a step behind.",2,0,1721244637.0,bigdata
1e5qwba,ETL speeds of raw source data into postgresql,"I'm doing ETL work through python into postgresql. just trying to get an idea of if my processes are fast enough or need to look at ways to do better to keep up with my peers.

mostly dealing with csv files, the occasional xls/xlsx. Bringing in hourly and 5 minute interval data for a couple hundred thousand things. Once datafiles are cached on a drive, it's ETL'd through python, date validated into datetime, floats, int, strings, sanity checking, transforming the data into a postgres record.

My minimum bar is loading 30k records per minute into postgresql, files with only a handful of data points and easy, or only a few transformations, I bounce around a 1million per minute.",0,0,1721244238.0,bigdata
1e5adfi,Data Architecture Complexity ,,1,0,1721194394.0,bigdata
1e59q3g,5 COMPONENTS OF POWER BI,"Data science teams can solve problems with more accuracy and precision than ever before, especially when combined with soft skills in creativity & communication.

[Data science teams can solve problems with more accuracy and precision than ever before, especially when combined with soft skills in creativity & communication.](https://reddit.com/link/1e59q3g/video/qm0d1hblf0dd1/player)

",1,0,1721192067.0,bigdata
1e4p858,Data Analytics: Future Roadmap & Trends for 2024,"The ""Data Analytics Roadmap 2024: A Comprehensive Guide to Data-driven Success"" outlines a strategic plan for implementing data analytics initiatives to drive innovation, enhance decision-making, and gain a competitive edge. This roadmap includes key components such as data strategy, infrastructure, analysis techniques, and visualization, providing a framework for businesses to collect, analyze, and interpret data effectively. Implementation steps involve defining goals, assessing current infrastructure, developing a data strategy, acquiring and preparing data, analyzing and interpreting data, and visualizing results. The roadmap offers benefits like improved decision-making, enhanced efficiency, and better customer experiences, but also highlights challenges including data quality, governance, and privacy. Analytics reports and case studies demonstrate real-world applications and success stories, while future trends such as AI integration, augmented analytics, and evolving data privacy regulations are anticipated to shape the landscape. The [Skills Data Analytics website](https://skills-datanalytics.com/) is recommended for those seeking to enhance their skills through courses, tutorials, and certifications in data analytics.",2,0,1721137992.0,bigdata
1e4pxd4,Resumable Full Refresh Data Syncs: Building resilient systems for syncing data,,1,0,1721139771.0,bigdata
1e26bgm,Mastering the Maze: How AI Transforms Lead Scoring with Unprecedented Data Analysis,,1,0,1720862750.0,bigdata
1e1b9et,Animals and Plant DB,"Hello guy we need all mostly known animals(including everything fishes, animals, birds) and plants to our new project. Is there free API's to get them?",2,1,1720767810.0,bigdata
1e0nyxs,Attribution modeling techniques: How Do you Select the right one?,"👋🏽 Hello everyone,

I'm currently learning all about attribution modeling techniques and have explored rule-based (first click, last click, exponential, uniform), statistical-based (Simple Frequency, Association, Term Frequency), and algorithmic-based methods (like Naive Bayes).

However, I'm struggling to understand how data scientists decide which modeling technique to use for their attribution projects, especially since ML and statistical models often compute different attribution scores compared to rule-based approaches.

I've created a short video demonstrating rule-based attribution techniques using Teradata Vantage’s free coding environment, and a sample dataset. For part 2, I plan to cover statistical and ML attribution modeling using the same data and include advice on choosing the right modeling technique.

I would love your insights on how you select your attribution modeling techniques. Any advice or guidelines would be greatly appreciated!

Here is the video I just created: [https://youtu.be/m1dkFxQiTNo?si=dfH5hljiPA0Bd7IK](https://youtu.be/m1dkFxQiTNo?si=dfH5hljiPA0Bd7IK)",3,0,1720701580.0,bigdata
1e0r3nv,Experiencia en con academia MundosE,Hola! Estoy pensando en inscribirme en MundosE para hacer la diplomatura en DevOps pero no encuentro muchas reviews al respecto. Alguno que pueda contar su experiencia?,1,8,1720710213.0,bigdata
1dztn6j,What if there is a good open-source alternative to Snowflake?,"Hi Data Engineers,

We're curious about your thoughts on **Snowflake** and the idea of an **open-source alternative**. Developing such a solution would require significant resources, but there might be an existing in-house project somewhere that could be open-sourced, who knows.

Could you spare a few minutes to fill out a short 10-question survey and share your experiences and insights about Snowflake? As a thank you, we have a few **$50 Amazon gift cards** that we will randomly share with those who complete the survey.

[Link to survey](https://docs.google.com/forms/d/e/1FAIpQLSd1IO83bBHIzc5WnSp_-GaryzeTD6r1C-aU8oupwvYIFKRepQ/viewform)

Thanks in advance",4,6,1720611691.0,bigdata
1dz6uev,Bufstream: Kafka at 10x lower cost,,0,0,1720542711.0,bigdata
1dxn5qi,Chunkit: Convert URLs into LLM-friendly markdown chunks for your RAG projects,,3,3,1720376619.0,bigdata
1dv7lfg,Best Alternative to zoominfo? We found Techsalerator but want to benchmark ,,1,1,1720101539.0,bigdata
1dud6ts,Need help about getting the users list from Cloudera data platform,I'm looking for anyone if they have experience working with cloud era data platform. I just want to know how can we get a list of users and the permissions they have who are using our analytical Cloudera data platform.,1,0,1720009892.0,bigdata
1dr4qkn,"Here is my playlist I use to keep motivated when I’m coding and studying. Feel free to share your music suggestions that can fit the playlist. Thank you !
",,0,0,1719642011.0,bigdata
1domnrj,June 27th Data Meetups,"JUNE 27TH DATA MEETUPS

- Talking about “Open Source and the Lakehouse” at the Cloud Data Driven Meetup

- Talking about “What is the Semantic Layer” at the Tampa Bay Data Engineers Group.",0,0,1719365615.0,bigdata
1dob519,Pornhub,,0,2,1719335305.0,bigdata
1do0688,The US crude oil export by countries by years,Has crude oil export become a new driver for the US economy?,3,2,1719300381.0,bigdata
1dno1kg,Financial careers heavy on data science? Scope in India?,"Hi folks. 
So recently, a frnd who is preparing for data science career let me know that India has plenty financial analyst opportunities that pay well. 
I am wondering what is the reality of that niche and how to go abt it-

To my limited knowledge I have gathered that:-

1) you don't need an mba for that. But a CMA or CFA would help
2) Importantly, you need to know SQL/ powerbi/ python( a bit of coding?) / tableau or related data heavy skills. Data analytics certifications also?

I was planning to go for a CFA anyways
I am willing to get certifications in above mentioned skills and deep dive into data science.

Problem is I am not a techie. So I was wondering what r financial careers that are data analysing inclined? And what can I do to crack into them having a non tech background. 

What is there scope in India?

Ps. Before anyone suggests posting this on financial subs. I have. I want to know the tech/data science angle to this. Since the friend who suggested this path have been preparing for that career. I have assumed it is related to this. Correct me if I am wrong tho.  ",1,2,1719262228.0,bigdata
1dmv7r0,Advice that I seek in my 20s as a data science kiddo,"short intro

Hello everyone, I moved to Canada 11 months ago. I did my bachelor’s in cse engg and specialization in AI and Data Science.
To put everything straight, I would rate myself as 5/10 for everything I learnt till now. I can do technical stuff but I am not sure thats my area of expertise. I want to get into techno managerial work. Something like consulting! I am not sure but I am sure that my work needs to be in data science and artificial intelligence 


What do i need?
I TOOK A MANAGEMENT DEGREE, inspite of my tech background. It is not like I dislike this program, However, I concern that this is not competitive enough for me. I am graduating by Dec 2024. 

Hypothetically lets say I am ready to prepare from sept 2024 - dec 2024. Consider my background knowledge in data science and research. What should I do? How should I start with? Please consider yourself in my shoes and tell me what should i do to secure a good job? 
( I humbly request you not to give me advice like, start from scratch, start from basics and do projects, network. I can do these things but I need a definite pathway) 

My rating would be as follows
Python 5/10
R 4/10
Sql 6/10
ML 6/10
Analytics (data processing, data management and data cleaning) 6/10
Data visualization 7/10
Storytelling 8/10

",0,0,1719173904.0,bigdata
1dlop60,Big data Hadoop and Spark Analytics Projects (End to End),"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.",13,1,1719035354.0,bigdata
1dk8o4f,"Data processing modes: Streaming, Batch, Request-Response",[https://nussknacker.io/blog/data-processing-modes-streaming-batch-request-response/](https://nussknacker.io/blog/data-processing-modes-streaming-batch-request-response/),2,0,1718880647.0,bigdata
1djt59w,Vodacom fires hundreds of workers in crime crackdown ,,1,1,1718828858.0,bigdata
1djpe77,Libraries for large-scale vector similarity search,"Hi, so I'm working on a project in which I want to calculate the cosine similarity between a query vector and corresponding document vectors ( around a billion of them ) and then threshold them to get the most relevant documents. (Something similar to the retrieval phase of RAG.) The number of relevant documents isn't bounded so kNN isn't very relevant other than for initial pruning. Here, the speed is of the essence so the scale is a problem (as with most big data applications). I initially looked into FAISS and ScANN but are there any other libraries that I can look at that would be faster than these? Also, should I instead turn to some other programming language (or a dbms like postgres) altogether to get the additional boost in performance? (PS: I'm supposed to deploy the system on gcp. ) ",1,2,1718819493.0,bigdata
1dj1p7q,Big data vs cybersecurity,"Hello guys , i finished my preparatory cycle in CS and i have a confuse in continuing my studies in cybersecurity or big data 
Too many people’s tell me big data = mathematics and I’m not good at mathematics i struggled with it a lot of times 
But i love an i’m very good at computer network which is an important part of cybersecurity 
please i wanna know the opinion of specialist person in data and cybersecurity 
",8,14,1718745197.0,bigdata
1dj6j7n,Best Big Data Courses on Udemy for Beginners to Advanced -,,1,1,1718758458.0,bigdata
1dhw6ni,"Best End-to-End Open Source MLOps: Platforms, Frameworks and Tools",,5,1,1718625131.0,bigdata
1dh6ey3,Seeking Feedback on ETL and Data Warehousing Architecture with Multi-Source Systems,"In my project, which is based on ETL and Data Warehousing, we have two different source systems: a MySQL database in AWS and a SQL Server database in Azure. We need to use Microsoft Fabric for development. I want to understand if the architecture concepts are correct. I have just six months of experience in ETL and Data Warehousing.As per my understanding, we have a bronze layer to dump data from source systems into S3, Blob, or Fabric Lakehouse as files, a silver layer for transformations and maintaining history, and a gold layer for reporting with business logic. However, in my current project, they've decided to maintain SCD (Slowly Changing Dimension) types in the bronze layer itself using some configuration files like source, start run timestamp, and end run timestamp. They haven't informed us about what we're going to do in the silver layer. They are planning to populate the bronze layer by running DML via Data Pipeline in Fabric and load the results each time for incremental loads and a single time for historical loads. They’re not planning to dump the data and create a silver layer on top of that. Is this the right approach?

And I think it's very short time project is that a reason to do like this?",1,0,1718540512.0,bigdata
1dgc2ti,Getting started with stream processing,,1,0,1718434576.0,bigdata
1dg6rtx,Best Big Data Books for Beginners to Advanced to Read,,0,1,1718415188.0,bigdata
1dfsddo,The New Wave of Composable Data Systems and LLM Interfaces,"Hi all,

We recently published an article on the evolution of composable data systems and the integration of Large Language Models (LLMs) at WrenAI (https://github.com/Canner/WrenAI).

This article explores the shift towards open standards like Apache Iceberg and Arrow, and modular execution engines such as Velox and DuckDB.

[Structure of a composable data system \(https:\/\/voltrondata.com\/codex\/a-new-frontier#structure-of-a-composable-data-system\)](https://preview.redd.it/b0uf61wrsj6d1.png?width=1246&format=png&auto=webp&s=84c3a6ecc2ae478c792285ee2366ca19e9b38454)

It also introduces our WrenAI project, which uses a semantic engine to enhance context-aware interactions between LLMs and data systems.

[The WrenAI project in the composable data system](https://preview.redd.it/5b66x8ztsj6d1.png?width=1283&format=png&auto=webp&s=92ba17d5a5ce6fdeda0c76834e0e778a53bb499f)

Read more here if you're interested: [https://blog.getwren.ai/the-new-wave-of-composable-data-systems-and-the-interface-to-llm-agents-ec8f0a2e7141](https://blog.getwren.ai/the-new-wave-of-composable-data-systems-and-the-interface-to-llm-agents-ec8f0a2e7141)

>Disclosure: I'm a member of WrenAI",5,1,1718375849.0,bigdata
1dfqw88,Top AI Conferences & Expos Worldwide,,2,0,1718371841.0,bigdata
1df9l4h,"Like clockwork, every three months... /s ",,5,1,1718312387.0,bigdata
1dfir0r,Bigfile Tablespace Defaults in Oracle Database 23ai,,1,0,1718340391.0,bigdata
1df2uwv,Gretel Navigator is Now Generally Available ,,1,0,1718295344.0,bigdata
1deshk0,"
¿Preparado para olvidarte de la extracción manual de datos?",,1,1,1718260787.0,bigdata
1dea65f,How Apache Iceberg is Built for Open Optimized Performance,,2,1,1718207973.0,bigdata
1dea3vy,Ihre Expertise ist gefragt - Umfrage zu ETL und Data Warehouses,"Hallo zusammen,

im Rahmen meiner Bachelorarbeit am Department Informatik der Hochschule für Angewandte Wissenschaften Hamburg (HAW Hamburg) führe ich eine Umfrage zum Thema **""Qualitative Analyse und Vergleich von ETL-Prozessen und Tools für Data Warehousing im Kontext moderner Technologien 2024: Theoretische Grundlagen, praktische Umsetzung und Expertenmeinungen""** durch.

Eure Teilnahme ist wichtig, um wertvolle Erkenntnisse für meine Abschlussarbeit zu gewinnen.

Als Fachleute und Praktiker im Bereich Data Warehousing und ETL-Prozesse lade ich euch herzlich ein, an einer kurzen Umfrage teilzunehmen. Eure langjährige Erfahrung und euer Fachwissen sind von unschätzbarem Wert, um die Herausforderungen und Trends in diesem Bereich besser zu verstehen. Mit eurer Unterstützung können wir wertvolle Erkenntnisse gewinnen.

Die Umfrage dauert ca. 10-15 Minuten.

**Hier geht's zur Umfrage:** [https://campus.lamapoll.de/Bewertung-und-Nutzung-von-ETL-Tools](https://campus.lamapoll.de/Bewertung-und-Nutzung-von-ETL-Tools)

Ich versichere euch, dass alle Angaben vertraulich und anonym behandelt werden. Bei Interesse könnt ihr gerne die Ergebnisse der Umfrage erhalten.

**Vielen Dank im Voraus für eure Unterstützung! Eure Rückmeldung ist für mich von großer Bedeutung.**",0,0,1718207808.0,bigdata
1de2kzh,Top 10 Artificial Intelligence APIs for Developers,,2,0,1718184957.0,bigdata
1ddyrhl,"A Novel Fault-Tolerant, Scalable, and Secure NoSQL Distributed Database Architecture for Big Data","In my PhD thesis, I have designed a novel distributed database architecture named ""Parallel Committees.""This architecture addresses some of the same challenges as NoSQL databases, particularly in terms of scalability and security, but it also aims to provide stronger consistency.

The thesis explores the limitations of classic consensus mechanisms such as Paxos, Raft, or PBFT, which, despite offering strong and strict consistency, suffer from low scalability due to their high time and message complexity. As a result, many systems adopt eventual consistency to achieve higher performance, though at the cost of strong consistency.  
In contrast, the Parallel Committees architecture employs classic fault-tolerant consensus mechanisms to ensure strong consistency while achieving very high transactional throughput, even in large-scale networks. This architecture offers an alternative to the trade-offs typically seen in NoSQL databases.

**Additionally, my dissertation includes comparisons between the Parallel Committees architecture and various distributed databases and data replication systems, including Apache Cassandra, Amazon DynamoDB, Google Bigtable, Google Spanner, and ScyllaDB.**

Potential applications and use cases:

* The “Parallel Committees” distributed database architecture, known for its scalability, fault tolerance, and innovative sharding techniques, is suitable for a variety of applications:
* Financial Services: Ensures reliability, security, and efficiency in managing financial transactions and data integrity.
* E-commerce Platforms: Facilitates seamless transaction processing, inventory, and customer data management.
* IoT (Internet of Things): Efficiently handles large-scale, dynamic IoT data streams, ensuring reliability and security.
* Real-time Analytics: Meets the demands of real-time data processing and analysis, aiding in actionable insights.
* Healthcare Systems: Enhances reliability, security, and efficiency in managing healthcare data and transactions.
* Gaming Industry: Supports effective handling of player engagements, transactions, and data within online gaming platforms.
* Social Media Platforms: Manages user-generated content, interactions, and real-time updates efficiently.
* Supply Chain Management (SCM): Addresses the challenges of complex and dynamic supply chain networks efficiently.

I have prepared a video presentation outlining the proposed distributed database architecture, which you can access via the following YouTube link:

[https://www.youtube.com/watch?v=EhBHfQILX1o](https://www.youtube.com/watch?v=EhBHfQILX1o)

A narrated PowerPoint presentation is also available on ResearchGate at the following link:

[https://www.researchgate.net/publication/381187113\_Narrated\_PowerPoint\_presentation\_of\_the\_PhD\_thesis](https://www.researchgate.net/publication/381187113_Narrated_PowerPoint_presentation_of_the_PhD_thesis)

My dissertation can be accessed on Researchgate via the following link: [Ph.D. Dissertation](https://www.researchgate.net/publication/379148513_Novel_Fault-Tolerant_Self-Configurable_Scalable_Secure_Decentralized_and_High-Performance_Distributed_Database_Replication_Architecture_Using_Innovative_Sharding_to_Enable_the_Use_of_BFT_Consensus_Mec)

If needed, I can provide more detailed explanations of the problem and the proposed solution.

I would greatly appreciate feedback and comments on the distributed database architecture proposed in my PhD dissertation. Your insights and opinions are invaluable, so please feel free to share them without hesitation.",4,0,1718168895.0,bigdata
1ddfvmb,Solving the GenAI data quality problem with synthetics,,1,0,1718117086.0,bigdata
1db110d,To stay relaxed and focused while coding/working,"Here's Ambient, chill & downtempo trip, a carefully curated playlist regularly updated with chill and mellow electronica, downtempo, deep, hypnotic and atmospheric electronic music. The ideal backdrop for concentration and relaxation. Perfect for staying focused during my coding sessions. Hope this can help you too :) 

[https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=ZjANX6QhQ-e3rCa-gswFUQ](https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=ZjANX6QhQ-e3rCa-gswFUQ) 

H-Music ",0,0,1717846933.0,bigdata
1daqld0,Raw Datasets/Sources on Criminal Sentencing in the USA?,"So obviously there’s a lot out there with aggregate and precategorized stats from the FBI but I think it would be interesting to see some of the underlying data. The most important features would be: 

1. Name of the court
2. Specific charges the person was convicted of
3. The scentence administered by the judge

Anything else is just a bonus to have. I do not have access to any paid legal database software and this is just a hobby project because I find the subject matter interesting. Any tips are greatly appreciated!
",2,2,1717807703.0,bigdata
1daegeh,Full job data downloads now available @ jobdata API 🔥,,1,0,1717776315.0,bigdata
1d9uo8q,Summarizing Recent Wins for Apache Iceberg Table Format,,0,1,1717711824.0,bigdata
1d9unwd,Summarizing Recent Wins for Apache Iceberg Table Format,,0,0,1717711802.0,bigdata
1d9eldn,Data Lake(house)s research,"Hi! My name is Alina and I'm a product marketing manager at [Qbeast](https://qbeast.io/).

We're trying to get a better understanding of the challenges people face when it comes to managing their data, whether in data lakes or data lakehouses. We'd love to hear about your experience with data storage approaches.

If you could take a few minutes to fill out [this](https://forms.gle/DJ5N3zcfWLxYUJmF8) survey, we'd be really grateful. Link to the survey: [https://forms.gle/DJ5N3zcfWLxYUJmF8](https://forms.gle/DJ5N3zcfWLxYUJmF8)

And if you have more to share about lake(house)s, I'd be happy to chat with you. Thanks so much!",1,1,1717668259.0,bigdata
1d9dpo2,🤖 AI Automation with Multi-Agent Collaboration,,1,0,1717664391.0,bigdata
1d8ijzt,AI-Fueled Enterprise Data Management: The Rise Of Oracle Database 23ai,,1,0,1717566224.0,bigdata
1d7zsbk,"Open Source Table Format + Open Source Catalog = No Vendor Lock-in (Nessie, Polaris, Gravitino)",,0,0,1717514435.0,bigdata
1d71ref,A simple API to gather insights into the hiring market and access millions of job posts in JSON format,,4,0,1717412775.0,bigdata
1d6fu70,Here’s a playlist I use to keep inspired when I’m coding/developing/studying. Post yours as well if you also have one!,,1,0,1717342185.0,bigdata
1d5b7ng,Seeking Advice for AWS Data Engineer Exam Preparation,"Hello everyone,

I'm planning to take the AWS Data Engineer certification exam soon, and I would love to hear your advice and tips on how to prepare effectively.

For those who have taken the exam:

1. What study materials did you find most helpful?
2. Are there any particular topics or areas I should focus on more?
3. How did you structure your study schedule?
4. Were there any practice exams or resources that closely matched the actual exam?

Any insights or recommendations would be greatly appreciated. Thanks in advance!",3,3,1717203887.0,bigdata
1d4ru2q,You Won't Believe These 3 Undervalued AI Stocks That Could Make You Rich!,,0,0,1717148972.0,bigdata
1d430cd,How did American Airlines slash their big data costs by 23%?,"[How did American Airlines slash their big data costs by 23%?](https://www.reddit.com/user/global_dots/comments/1cabs0d/did_you_know_that_american_airlines_slashed_their/)

🎥 In our webinar ""Cut Big Data Costs by 23%: 7 Key Practices,"" we took a deep dive into the best practices for reducing costs effectively.

Watch the [full webinar for free](https://www.globaldots.com/lp/cut-big-data-costs-webinar/) to learn how you could:

💰 Cut costs: Learn from the successes of major corporations and see how

straightforward adjustments can lead to significant financial savings.

⏱️ Streamline operations: Explore how to make your data operations leaner and more efficient.

📈 Enhance performance: Boost your systems' efficiency without compromising on quality or output.

# bigdata #databricks #cloudinnovation",0,0,1717072600.0,bigdata
1d3zksw,Bigdata conference in the world ?,"I was looking at the bigdata conferences that takes place in the year and was wondering if  had better feedback than others, I went to the Bigdata europe conference last year and it was very nice, much better than the devox conference that took place in london in 2022.  
I then come across that one [https://www.globalbigdataconference.com/training-details.html](https://www.globalbigdataconference.com/training-details.html) but couldn't tell the quality of it.

I know bigdata is a vast term now but i'm looking for something heavely data relatad (not web) with some non cloud part as well.",1,6,1717059679.0,bigdata
1d3cn87,HeavyIQ: Understanding 220M Flights with AI,,11,0,1716990507.0,bigdata
1d39qi7,Blazingly-fast serialization framework for bigdata transfer: Apache Fury 0.5.1 released,,3,0,1716981526.0,bigdata
1d2x5t8,Artificial Intelligence in Welltory Health App,,2,0,1716936947.0,bigdata
1d2iwaz,Ingesting big data from Spark into feast feature store,"I am currently building a big data pipeline for an MLOps project, the pipeline is intended for batch processing.

This is the current setup:

* I am storing my raw structured data in Hive.
* Spark jobs ingest raw data and process it.
* I am intending on using feast and Apache Cassandra as an offline store.

My problem is passing processed data from spark to feast and then storing it in the offline store, I want to do it in a manner that is scalable and conveys to the requirements for a big data system.

I think intermediary data persistence is needed for passing data but I have no idea how to do it in a big data context.

Please any suggestions or resources that may help are appreciated.",1,0,1716901246.0,bigdata
1d2aq0s,GPT-4o: Learn how to Implement a RAG on the new model,,1,0,1716869374.0,bigdata
1d14cnj,Here’s a playlist I use to keep inspired when I’m coding/developing/studying. Post yours as well if you also have one!,,1,0,1716738762.0,bigdata
1d0d3ka,Researchers found that accelerometer data from smartphones can reveal people's location passwords body features age gender level of intoxication driving style and be used to reconstruct words spoken next to the device.,,16,5,1716646890.0,bigdata
1czpsy8,"dbt alternatives: dbt-core alternatives, dbt Cloud alternatives, and Graphical ETL tools",[https://datacoves.com/post/dbt-alternatives](https://datacoves.com/post/dbt-alternatives),3,0,1716570518.0,bigdata
1czod6c,Generate Differentially Private Synthetic Text for Fine-tuning AI Models,,1,0,1716566864.0,bigdata
1czhclp,How AI will solve XR’s biggest problem,,1,0,1716545620.0,bigdata
1cye508,RDS to S3 Data Transfer options ,"Moving data from AWS RDS to S3 to later be used by Databricks and eventually Tableau.

What is the best way to transfer this data to s3?
1. AWS DMS 
2. AWS Glue
3. Create job in Databricks to connect to RDS, retrieve data and store in S3.

",3,8,1716419250.0,bigdata
1cy425q,"Run SQL Queries Locally on your CSV, JSON, XLS and Parquet files with Ease",,3,2,1716394222.0,bigdata
1cy521z,Vector Search - HNSW Explained,,2,0,1716396649.0,bigdata
1cxt4go,🤖 PaliGemma – Google's Open Vision Language Model,,2,0,1716357196.0,bigdata
1cwik75,Uber Migrates 1 Trillion Records from DynamoDB to LedgerStore to Save $6 Million Annually,,9,0,1716219532.0,bigdata
1cwoav5,Data Lake: what is the best approach,"Hi everyone, 

  
I've been learning a little bit about Data lakes recently and I'm currently using apache Iceberg in AWS (Athena), I have a few questions that I couldn't find the answers on youtube, I have a kafka producer that send real-time logs regarding my employees activites, I want to perform some hourly analysis over it and I got kind of confused, sending data from Kafka to S3 and using it as external table is very powerfull and I would love to use it, however to keep my apache iceberg table updated, I'll need to perform an insert into from select every hour and I'm wondering if I'm doing it wrong, I'm willing to create some lambda function that runs everyhour, execute the insert into command, and maybe empty the hourly normal table (The one I use to feed my iceberg datalake), does this sounds logical to you guys? 

  
Appreciate the feedback!",1,1,1716233863.0,bigdata
1cw9dt3,What are the trends in big data analytics?,,0,0,1716188541.0,bigdata
1cvvetb,Big data (ideally free) sources for car ownership?,"I'm trying to find owners of a few certain vehicles to make offers for them.

Are there any state or data brokers that have model of vehicle, owner name/email/phone where I can filter by vehicle model?",0,0,1716144857.0,bigdata
1cvp81r,How to Leverage Privacy-Enhancing Technologies for Data Protection and Privacy - Guide,"The guide below provides definition, objectives, and examples of  privacy-enhancing technologies (PET) like anonymization, encryption, consent management, data minimization, synthetic data, differential privacy, etc. as well as relationship between data protection and data privacy and its practical applications across healthcare, finance, messaging, IoT/smart devices: [How to Leverage PET for Data Protection and Privacy](https://pvml.com/how-to-leverage-pet-for-data-protection-and-privacy/)",1,0,1716128037.0,bigdata
1cuwknv,SDSM 2024 : Suicide Detection on Social Media @ IEEE BigData 2024,,2,0,1716036447.0,bigdata
1cul3ql,"Data, AI and Robotics: How e-commerce brands can stay ahead",,0,0,1715993402.0,bigdata
1cth2u3,Best Big Data Courses on Udemy for Beginners to Advanced -,,2,0,1715877051.0,bigdata
1csdc6k,The roadmap for becoming a Data Engineer ,,2,1,1715750909.0,bigdata
1cruw4i,"New #Altintiy #Webinar Petabyte-Scale Data in Real-Time: #ClickHouse, S3 Object Storage, and #Data Lakes ",,1,0,1715700377.0,bigdata
1crknxk,How to create HIVE Table with multi character delimiter? (Hands On) ,,0,0,1715664812.0,bigdata
1cq165l,Get data as csv from a very large MySQL dump file,"have a MySQL dump file as .sql format. Its size is around 100GB. There are just two tables in int. I have to extract data from this file using Python or Bash. The issue is the insert statement contains all data and that line is too lengthy. Hence, normal practice cause Memory issue as that line (i.e., all data) is load in loop also.

Is there any efficient way or tool to get data as CSV?

Just a little explanation. Following line contains actual data and it is of very large size.",4,4,1715494104.0,bigdata
1cpiv3r,"Anyone knows where can I find current and historical actual / recorded weather data parameters like wind speed, temperature, humidity recorded at Airports or any public institutions. ","I'm building a wind resource analysis tool for an assignment and need historical actual / recorded weather data parameters like wind speed, temperature, humidity recorded at Airports or any public institutions in India.

It would be great if anyone can share a link to open source data like this. I found historical data from NASA's POWER LARC and windy to be reliable but these are satellite parameter data and I need actual / recorded data points.",2,1,1715438089.0,bigdata
1cpgga6,AI Cheatsheet: AI Software Developer agents,,1,1,1715430654.0,bigdata
1cpgbon,🤖Beat Proprietary LLMs With Smaller Open Source Models,,1,0,1715430226.0,bigdata
1cozepu,"Parallel-Committees"": A Novelle Secure and High-Performance Distributed Database Architecture","In my PhD thesis, I proposed a novel fault-tolerant, self-configurable, scalable, secure, decentralized, and high-performance distributed database replication architecture, named “Parallel Committees”.

I utilized an innovative sharding technique to enable the use of Byzantine Fault Tolerance (BFT) consensus mechanisms in very large-scale networks.

With this innovative full sharding approach supporting both processing sharding and storage sharding, as more processors and replicas join the network, the system computing power and storage capacity increase unlimitedly, while a classic BFT consensus is utilized.

My approach also allows an unlimited number of clients to join the system simultaneously without reducing system performance and transactional throughput.

I introduced several innovative techniques: for distributing nodes between shards, processing transactions across shards, improving security and scalability of the system, proactively circulating committee members, and forming new committees automatically.

I introduced an innovative and novel approach to distributing nodes between shards, using a public key generation process, called “KeyChallenge”, that simultaneously mitigates Sybil attacks and serves as a proof-of-work. The “KeyChallenge” idea is published in the peer-reviewed conference proceedings of ACM ICCTA 2024, Vienna, Austria.

In this regard, I proved that it is not straightforward for an attacker to generate a public key so that all characters of the key match the ranges set by the system.I explained how to automatically form new committees based on the rate of candidate processor nodes.

The purpose of this technique is to optimally use all network capacity so that inactive surplus processors in the queue of a committee that were not active are employed in the new committee and play an effective role in increasing the throughput and the efficiency of the system.

This technique leads to the maximum utilization of processor nodes and the capacity of computation and storage of the network to increase both processing sharding and storage sharding as much as possible.

In the proposed architecture, members of each committee are proactively and alternately replaced with backup processors. This technique of proactively circulating committee members has three main results:

* (a) preventing a committee from being occupied by a group of processor nodes for a long time period, in particular, Byzantine and faulty processors,
* (b) preventing committees from growing too much, which could lead to scalability issues and latency in processing the clients’ requests,
* (c) due to the proactive circulation of committee members, over a given time-frame, there exists a probability that several faulty nodes are excluded from the committee and placed in the committee queue. Consequently, during this time-frame, the faulty nodes in the committee queue do not impact the consensus process.

This procedure can improve and enhance the fault tolerance threshold of the consensus mechanism.I also elucidated strategies to thwart the malicious action of “Key-Withholding”, where previously generated public keys are prevented from future shard access. The approach involves periodically altering the acceptable ranges for each character of the public key. The proposed architecture effectively reduces the number of undesirable cross-shard transactions that are more complex and costly to process than intra-shard transactions.

I compared the proposed idea with other sharding-based data replication systems and mentioned the main differences, which are detailed in Section 4.7 of my dissertation.

The proposed architecture not only opens the door to a new world for further research in this field but also represents a significant step forward in enhancing distributed databases and data replication systems.

The proposed idea has been published in the peer-reviewed conference proceedings of IEEE BCCA 2023.

Additionally, I provided an explanation for the decision not to employ a blockchain structure in the proposed architecture, an issue that is discussed in great detail in Chapter 5 of my dissertation.

The complete version of my dissertation is accessible via the following link: [https://www.researchgate.net/publication/379148513\_Novel\_Fault-Tolerant\_Self-Configurable\_Scalable\_Secure\_Decentralized\_and\_High-Performance\_Distributed\_Database\_Replication\_Architecture\_Using\_Innovative\_Sharding\_to\_Enable\_the\_Use\_of\_BFT\_Consensus\_Mec](https://www.researchgate.net/publication/379148513_Novel_Fault-Tolerant_Self-Configurable_Scalable_Secure_Decentralized_and_High-Performance_Distributed_Database_Replication_Architecture_Using_Innovative_Sharding_to_Enable_the_Use_of_BFT_Consensus_Mec)

I compared my proposed database architecture with various distributed databases and data replication systems in Section 4.7 of my dissertation. This comparison included Apache Cassandra, Amazon DynamoDB, Google Bigtable, Google Spanner, and ScyllaDB. I strongly recommend reviewing that section for better clarity and understanding.

The main problem is as follows:  
  
Classic consensus mechanisms such as Paxos or PBFT provide strong and strict consistency in distributed databases. However, due to their low scalability, they are not commonly used. Instead, methods such as eventual consistency are employed, which, while not providing strong consistency, offer much higher performance compared to classic consensus mechanisms. The primary reason for the low scalability of classic consensus mechanisms is their high time complexity and message complexity.  
  
I recommend watching the following video explaining this matter:  
[https://www.college-de-france.fr/fr/agenda/colloque/taking-stock-of-distributed-computing/living-without-consensus](https://www.college-de-france.fr/fr/agenda/colloque/taking-stock-of-distributed-computing/living-without-consensus)  
  
My proposed architecture enables the use of classic consensus mechanisms such as Paxos, PBFT, etc., in very large and high-scale networks, while providing very high transactional throughput. This ensures both strict consistency and high performance in a highly scalable network. This is achievable through an innovative approach of parallelization and sharding in my proposed architecture.  
  
If needed, I can provide more detailed explanations of the problem and the proposed solution.

I would greatly appreciate feedback and comments on the distributed database architecture proposed in my PhD dissertation. Your insights and opinions are invaluable, so please feel free to share them without hesitation.",0,1,1715373189.0,bigdata
1coo3kw,How to use Dremio’s Reflections to Reduce Your Snowflake Costs Within 60 minutes.,,1,0,1715343252.0,bigdata
1co88wg,Where Can we buy B2B Data ? We found Techsalerator to be the best so far but are looking for more.,,3,9,1715289349.0,bigdata
1coadrc,Ayuda de asesoramiento | Counseling Help,"I'm about to finish high school, specializing in 'Personal and Professional Computing', and I need opinions from knowledgeable people to argue in favor of and defend my project, as my teacher is about to dismiss it as impractical and 'unfeasible for us.' But we have a lot of faith in it.

My project is called 'E.C.D.U.I.T.', which stands for 'Quantitative Study of Useful Data in the Textile Industry.' It will involve analyzing massive amounts of data (using Hadoop clustering, utilizing home computers to demonstrate that we did it ourselves, etc.) that can provide useful information to textile companies. The objective is to raise awareness among small companies in our region about this technology in order to improve their competitiveness. As you can see, the project aims to apply everything learned in these 7 years of study in a final integrative project.

The needs/problems that the project aims to solve, and which the teacher believes we will not be able to solve, are:

Main Problem: Insufficient capacity of regional textile companies to compete in a highly competitive and dynamic national environment. 

Deficiency in technological innovation and digitization of internal operational processes within the organization. 

Lack of focus on financial characteristics within companies. 

Inability of regional companies to recognize customer needs, combined with resistance to change.

&#x200B;",1,0,1715294909.0,bigdata
1cm4cuy,Unlock Your Potential: Join Our Free Python Course - Getting Started with Python using Databricks,,1,0,1715059735.0,bigdata
1clzrm6,OS framework + catalog project looking to get more feedback from PySpark users,"Hey all we just [open sourced a whole system](https://blog.dagworks.io/p/hamilton-ui-streamlining-metadata?r=17gs0j&utm_campaign=post&utm_medium=web) we've been developing for a while that ties together a few things for python code (see [README](https://github.com/DAGWorks-Inc/hamilton/tree/main/ui), [quick youtube feature walkthrough](https://www.youtube.com/watch?v=0VIVSeN7Ij8)). 

1. Execution + metadata capture, e.g. automatic code profiling
2. Data/artifact observability, e.g. summary statistics over dataframes, pydantic objects, etc...
3. Lineage & provenance of data, e.g. quickly see what is upstream & downstream of code/data.
4. Asset/transform catalog, e.g. search & find if feature transforms/metrics/datasets/models exist and where they’re used.

Some screenshots:

[Lineage & code - one view of it](https://preview.redd.it/3i10g6shmwyc1.png?width=1852&format=png&auto=webp&s=8c7cdbcb3529e84849b5d0ed35f09e0d95944e8f)

[Catalog view and pointers to versions and executions](https://preview.redd.it/wibi3q4jowyc1.png?width=1841&format=png&auto=webp&s=2c93eb1b17db218e52822d17d4bb340acd1299cf)

[Execution profiling of functions and comparing with another run.](https://preview.redd.it/q31blhnnmwyc1.png?width=1823&format=png&auto=webp&s=d1129443c81aa2d5ea640215c69ecb7fcf9ca399)

[Data comparison view of outputs comparing two runs](https://preview.redd.it/d0rra3zomwyc1.png?width=1852&format=png&auto=webp&s=1adb0607cb48b923928f170de2cd50c357cc9f08)

To use the above, you need to use [Hamilton](https://github.com/DAGWorks-Inc/hamilton) (which is a light lift to move to, see [this blog post on using it for PySpark](https://blog.dagworks.io/p/expressing-pyspark-transformations)). So why am I telling you all this? Well for PySpark, you can't get some of the above insights that easily, well because it's PySpark, e.g. execution time for your code, & profiling data without redoing computation. So, I'm looking find some PySpark users that would be interested in code that's more manageable that can also integrate with a cool UI in exchange for testing out a couple of features.   
  
E.g. exposing query plans and knowing exactly which place in the code caused it to blow up.   
E.g. linking with the Spark History Server to get execution information so you can more logically tie together code and then what spark actually did.  
E.g. build a better data profiling integration  
etc.

Thanks all!",1,0,1715044908.0,bigdata
1cll8dn,Apache Fury 0.5.0 released,"We're excited to announce the release of Fury v0.5.0. This release incorporates a myriad of improvements, bug fixes, and new features across multiple languages including Java, Golang, Python and JavaScript. It further refines Fury's performance, compatibility, and developer experience. 

Fury can be used to accelerate the data transfer efficiency in big data distributed frameworks such as flink/spark.

See more in release notes: [https://github.com/apache/incubator-fury/releases/tag/v0.5.0](https://github.com/apache/incubator-fury/releases/tag/v0.5.0)",3,2,1715007758.0,bigdata
1ck5sv3,From ETL and ELT to Reverse ETL,,4,1,1714844189.0,bigdata
1cj7s25,cassandra snapshot,"HI all  
i was working on Cassandra db and i am using nodetool snapshot command to take snapshot of my database i want to know that does cassandra provide incremental snapshot or not. ( i have read the documentation and they wrote about incremental backup but not abot the incremental snapshot)  
would u please guide me .  
thank you !",0,0,1714740296.0,bigdata
1cgxboy,Effective Strategies for Search Engine Optimization (SEO),"Search Engine Optimization (SEO) plays a critical role in helping your website rank higher in search engine results pages (SERPs) and drive organic traffic. In this post, we'll explore some effective strategies to optimize your website for better visibility and relevance in search engine results.

**1. Keyword Research and Optimization**: Start by conducting thorough keyword research to identify relevant keywords and phrases that your target audience is searching for. Use tools like Google Keyword Planner or SEMrush to discover high-volume and low-competition keywords. Incorporate these keywords naturally into your website's content, including titles, headings, meta descriptions, and body text.

**2. High-Quality Content Creation**: Content is king in the world of SEO. Create high-quality, relevant, and engaging content that addresses the needs and interests of your target audience. Aim to provide value and answer users' queries with comprehensive and informative content. Regularly update your website with fresh content to keep both users and search engines engaged.

**3. On-Page Optimization**: Optimize your website's on-page elements to improve its search engine visibility. This includes optimizing title tags, meta descriptions, heading tags (H1, H2, H3), URL structure, and image alt attributes. Ensure that your website is user-friendly and easy to navigate, with clear and descriptive internal linking.

**4. Mobile Optimization**: With the increasing prevalence of mobile devices, it's essential to optimize your website for mobile users. Ensure that your website is responsive and mobile-friendly, with fast loading times and intuitive navigation. Google prioritizes mobile-friendly websites in its search results, so optimizing for mobile is crucial for SEO success.

**5. Technical SEO**: Pay attention to technical aspects of SEO, such as website speed, crawlability, indexing, and site architecture. Fix any technical issues that may be impacting your website's performance in search results. Use tools like Google Search Console to identify and resolve technical SEO issues.

**6. Link Building**: Build quality backlinks from reputable and relevant websites to improve your website's authority and credibility in the eyes of search engines. Focus on acquiring natural and organic backlinks through content marketing, guest blogging, influencer outreach, and social media engagement.

At Windsor.ai, we understand the importance of effective SEO strategies in driving organic traffic and improving online visibility. Our platform offers advanced analytics and attribution tools that can help you track and analyze the performance of your SEO efforts, allowing you to make data-driven decisions and optimize your SEO strategy for better results.

What other effective SEO strategies have you found useful? Share your insights in the comments!",1,3,1714495063.0,bigdata
1cfo60j,Survey on the Role of Artificial Intelligence and Big Data in Enhancing Cancer Treatment,"Hello everyone, I am currently doing my dissertation paper on Big Data and AI. Right here is a questionnaire that I prepared for my primary research. 

Anyone who answers my questions will remain anonymous.

1.	**Background Information**
	
•	What is your professional background? (Options: Healthcare, IT, Data Science, Education, Other)
	
•	How familiar are you with AI and big data applications in healthcare? (Scale: Not familiar - Extremely familiar)

2.	**Perceptions of AI and Big Data in Healthcare**
	
•	In your opinion, what are the most promising applications of AI and big data in healthcare?
	
•	How do you think AI and big data can improve cancer tumor detection and treatment?

3.	**Challenges and Barriers**
	
•	What do you see as the biggest challenges or barriers to implementing AI and big data solutions in healthcare settings?
	
•	How concerned are you about privacy and security issues related to using AI and big data in healthcare? (Scale: Not concerned - Extremely concerned)

4.	**Effectiveness and Outcomes**
	
•	Can you provide examples (if any) from your experience or knowledge where AI and big data have significantly improved healthcare outcomes?
	
•	How effective do you believe AI is in personalizing cancer treatment compared to traditional methods?

5.	**Future Trends**
	
•	What future developments in AI and big data do you anticipate will have the most impact on healthcare in the next 5-10 years?
	
•	What role do you think cloud computing will play in the future of AI and big data in healthcare?

6.	**Personal Insights**
	
•	What advice would you give to healthcare organizations looking to integrate AI and big data into their operations?
	
•	What skills do you think are essential for professionals working at the intersection of AI, big data, and healthcare?

7.	**Open-Ended Response**
	
•	Is there anything else you would like to add about the role of AI and big data in healthcare that has not been covered in this questionnaire?


Thank you for your time!",1,0,1714357980.0,bigdata
1cf6wjl,I recorded a Python PySpark Big Data Course and uploaded it on YouTube,"Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!



[https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=9&t=1s](https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=9&t=1s)",7,0,1714312350.0,bigdata
1ce8sln,20 Popular Open Source AI Developer Tools,,2,0,1714203283.0,bigdata
1ce8s0v,We're inviting you to experience the future of data analytics,,1,0,1714203213.0,bigdata
1cc6jyp,Open Source SQL Databases - OLTP and OLAP Options,"Are you leveraging open source SQL databases in your projects?

Check out the article here to see the options out there: [https://www.datacoves.com/post/open-source-databases](https://www.datacoves.com/post/open-source-databases)

**Why consider Open Source SQL Databases?** 🌐

* **Cost-Effectiveness:** Dramatically reduce your system's total cost of ownership.
* **Flexibility and Customization:** Tailor database software to meet your specific requirements.
* **Robust Community Support:** Benefit from rapid updates and a wealth of community-driven enhancements.

Share your experiences or ask questions about integrating these technologies into your tech stack.",0,0,1713985352.0,bigdata
1cc5j2k,Google Search Parameters (2024 Guide),,1,0,1713982893.0,bigdata
1cba77w,WAL is a broken strategy?,"Hi,

I'm studying a bit on big data systems.

I've bounced into [this](https://valyala.medium.com/wal-usage-looks-broken-in-modern-time-series-databases-b62a627ab704) article, from 2019, which explains WAL is a broken strategy and actually inefficient - Written by VictoriaMetrics founder. In short: He says: Flush every second in SSTable format (of your choice), and do the background compaction to slowly build it up to descent size block. He says there are two systems out there using this strategy: VM and ClickHouse.

  
Would love to hear some expert Big Data take on this.

",8,0,1713892626.0,bigdata
1cavms9,Big data Hadoop and Spark Analytics Projects (End to End),"Hi Guys,   

I hope you are well.   

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.** 

***Apache Spark Analytics Projects:*** 

&#x200B;

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)   
 
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)   
 
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)   
 
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)   
 
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)   
 
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)   
 
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)   
 
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)   
 

***Bigdata Hadoop Projects:*** 

&#x200B;

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)   
 
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)   
 
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)   
 
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)   
 
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)   
 

I hope you'll enjoy these tutorials.   ",29,0,1713846196.0,bigdata
1cb6im7,Strategies for Handling Missing Values in Data Analysis,"As data scientists and data analysts delve into the intricate world of data, they often encounter a common challenge: filling over gaps. The identified information can be lost due to several reasons, for instance human error, breakdown of sensors as well as lack of collection of data. Getting the missing values problem right is critical because if they are not handled correctly, they can be very detrimental to the functioning of machine learning models and statistical estimation. [Click here to read more >>](https://www.dasca.org/world-of-data-science/article/strategies-for-handling-missing-values-in-data-analysis)",1,0,1713883681.0,bigdata
1cb21dd,How can I share BigQuery reports with non-technical folks?,"**Want to easily share BigQuery insights with your external clients, partners, or vendors?**

If complex BI tools or clunky CSV exports are your current solutions, it’s time for an upgrade! [Softr](https://www.softr.io/blog/softr-bigquery-integration) now integrates with BigQuery, allowing you to easily connect to your BigQuery database to create dedicated dashboards and reports— without coding or complex analytics tools.

Here’s what you can do:

&#x200B;

* Data portals: Create intuitive, customized dashboards directly within Softr. No need for third parties and non-technical team members to master complex analytics software.
* Secure access control: Fine-tune permissions to determine exactly what data each external user can see.

Transform the way you share your BigQuery insights.",1,0,1713871207.0,bigdata
1cavor7,Strategies for Handling Missing Values in Data Analysis,"As data scientists and data analysts delve into the intricate world of data, they often encounter a common challenge: filling over gaps. The identified information can be lost due to several reasons, for instance human error, breakdown of sensors as well as lack of collection of data. Getting the missing values problem right is critical because if they are not handled correctly, they can be very detrimental to the functioning of machine learning models and statistical estimation. This article covers some data scientists skills and methodologies that are a must for effectively managing missing data. [Click here to read more >>](https://www.dasca.org/world-of-data-science/article/strategies-for-handling-missing-values-in-data-analysis) ",3,0,1713846387.0,bigdata
1ca72q6,Data Integration Unlocked: From Silos to Strategy for Competitive Success,,2,0,1713780821.0,bigdata
1caaeeg,ClickHouse Performance Master Class – Tools and Techniques to Speed up any ClickHouse App Webinar,"[ClickHouse Performance Master Class – Tools and Techniques to Speed up any ClickHouse App](https://hubs.la/Q02t2dtG0)  
We’ll discuss tools to evaluate performance including ClickHouse system tables and EXPLAIN. We’ll demonstrate how to evaluate and improve performance for common query use cases ranging from MergeTree data on block storage to Parquet files in data lakes. Join our webinar to become a master at diagnosing query bottlenecks and curing them quickly. [https://hubs.la/Q02t2dtG0](https://hubs.la/Q02t2dtG0) ",1,0,1713791476.0,bigdata
1c9s5pp,Graph Database,"Apache Software Foundation PROJECT SPOTLIGHT: APACHE AGE [https://news.apache.org/foundation/entry/asf-project-spotlight-apache-age](https://news.apache.org/foundation/entry/asf-project-spotlight-apache-age)

[https://age.apache.org/](https://age.apache.org/)

[https://github.com/apache/age](https://github.com/apache/age)",2,1,1713732172.0,bigdata
1c9qogd,Seeking Data Sets of 2023 Headlines from Major Publications,,1,0,1713728514.0,bigdata
1c9e53h,The Future of Healthcare: How AI is Revolutionizing Medical Diagnostics,"Hey everyone, stumbled upon this fascinating article discussing the urgent need for AI integration in healthcare diagnostics. In today's rapidly evolving world, it's crucial for the healthcare sector to adapt, and this piece dives deep into why AI is the way forward.  
  
Check it out: [The Integration of AI in Healthcare: Enhancing Diagnostic Accuracy and Patient Outcomes](https://7med.co.uk/ai-in-healthcare-enhancing-diagnostics-and-patient-outcomes/)  
  
From highlighting the burden of diagnostic errors to exploring the promise of AI in addressing these challenges, this article offers a comprehensive overview. It delves into real-world examples, showcasing how AI is already making a tangible difference in patient outcomes.  
  
What's particularly intriguing is the discussion on upcoming innovations in AI and the skills healthcare professionals need to develop to thrive in this AI-integrated environment.  
  
Definitely worth a read for anyone interested in the intersection of technology and healthcare! Let's spark some discussions on how AI is shaping the future of medicine.",3,0,1713693168.0,bigdata
1c8lig4,Reporting system for microservices,"Hi,
we are trying to implement a reporting system for our microservices: our goal is to build a business intelligence service that correlates data between multiple services.

Right now, for legacy services, there is an ETL service that reads data (sql queries) from source databases and then stores it in a data warehouse where data is enriched and prepared for the end user.

For microservices, and in general for everything that is not legacy, we want to avoid this approach because multiple kinds of databases are involved (es: postgresql and mongodb) and our ETL service need to read an high amount of data, including things that has not been changed, every day (very slow and inefficient).

Because people of ""data team"" (the one who manage ETL jobs and business intelligence stuff) are not the same of dev team, every time a dev team decides to change something (e.g: schema, database engine, etc), our ETL service stops working, and this requires a lot of over coordination and sharing of low level implementation details.

We want to obtain the same level of backwards compatibility between changes and abstraction used for service-to-service interaction (REST API) but for data, delegating the dev team to maintain that layer of backwards compatibility (contract with data team), also because direct access to source databases and implementation details is an anti-pattern for microservices.


A first test was made using debezium to stream changes from sources database to kafka and then s3 (using iceberg as table format) in a kind of data lake, while using trino as query engine.
This approach seems to be very experimental and difficult to maintain/operate (e.g. what happens with a huge amount of inserted/updated data!?).
In addition to that, it is not clear how to maintain the ""data backwards compatibility/abstraction layer"": one possible way could be to delegate it to dev teams allowing them to create views on ""data lake"".

Any ideas/suggestions?
",3,2,1713604007.0,bigdata
1c8a74y,Exploring Ethics in Data Analysis,"In the world of digital marketing, data analysis plays a crucial role in strategic decision-making. However, it's important to address the issue of ethics when collecting, analyzing, and using data for marketing campaigns. In this post, we'll explore the ethical considerations that should be taken into account in data analysis.

**User Privacy Protection**: It's essential to respect user privacy when collecting data for marketing campaigns. Make sure to obtain proper consent before collecting personal data and to comply with all data protection laws and regulations, such as GDPR in Europe or CCPA in California.

**Transparency and Consent**: Users should be fully informed about how their data is being collected, stored, and used. Providing transparency about data collection practices and obtaining explicit consent from users before using their data for marketing purposes is important.

**Avoiding Discrimination and Bias**: When analyzing data for marketing campaigns, it's crucial to avoid discrimination and unfair bias. Ensure that your data analysis practices do not contribute to discrimination or exclusion of certain user groups.

**Data Security**: Protect your users' data through appropriate security measures to prevent unauthorized access or disclosure of confidential information. Implement robust security practices to ensure the integrity and confidentiality of your users' data.

**Accountability and Transparency**: Be transparent about your data analysis practices and take responsibility for any misuse or misunderstanding of the data. Adopt a proactive approach to address any ethical concerns that may arise and work collaboratively with stakeholders to ensure the integrity of your data analysis practices.

At Windsor.ai, we value the importance of ethics in data analysis and are committed to upholding the highest ethical standards in all our operations. Our platform offers advanced analytics and attribution tools that help brands make informed decisions ethically and responsibly.

What other ethical considerations do you think are important in data analysis for marketing campaigns? Share your ideas in the comments!",1,0,1713566432.0,bigdata
1c81qv4,adapt() gives error while using Normalization Layer in Sequential Models?,"While using Normalization layer in Sequential Model, while adapt(), I am getting Unbound Error:

normalizer = Normalization()

normalizer.adapt(X\_train)

    ---------------------------------------------------------------------------
    UnboundLocalError                         Traceback (most recent call last)
    Cell In[198], line 2
          1 normalizer = Normalization()
    ----> 2 normalizer.adapt(X_train)
    
    File /usr/local/lib/python3.10/site-packages/keras/src/layers/preprocessing/normalization.py:228, in Normalization.adapt(self, data)
        225     input_shape = tuple(data.element_spec.shape)
        227 if not self.built:
    --> 228     self.build(input_shape)
        229 else:
        230     for d in self._keep_axis:
    
    UnboundLocalError: local variable 'input_shape' referenced before assignment",2,2,1713545455.0,bigdata
1c7ptz4,The Role of Smart Maritime IoT Solutions in Enhancing Maritime Safety,,2,0,1713507541.0,bigdata
1c7qhtq,Best Big Data Courses on Udemy for Beginners to Advanced -,,1,0,1713510149.0,bigdata
1c7kbga,Exploring Content Personalization in Marketing Strategies,"Content personalization has become a cornerstone tool in digital marketing strategies, allowing brands to create unique and relevant experiences for their audiences. In this post, we'll explore the importance and benefits of content personalization in marketing.

**Importance of Content Personalization:**

1. **Enhances User Experience**: Content personalization enables brands to tailor the message and offer according to each user's specific preferences and needs, enhancing the user experience and increasing the likelihood of conversion.
2. **Increases Relevance**: By offering personalized content, brands can increase the relevance of their messages and promotions, resulting in higher engagement and loyalty from the audience.
3. **Builds Trust and Loyalty**: When users receive content that aligns with their interests and needs, they are more likely to trust the brand and develop a long-term relationship, which can lead to higher loyalty and retention.

**Benefits of Content Personalization:**

1. **Higher Engagement**: Content personalization can significantly increase audience engagement, as users are more inclined to interact with content that is relevant and specific to them.
2. **Improves Conversion Rates**: By offering personalized content, brands can increase conversion rates by targeting users who are more likely to be interested in their products or services.
3. **Optimizes Return on Investment**: Content personalization allows brands to optimize return on investment by focusing their resources on more receptive audiences and more profitable market segments.

At Windsor.ai, we understand the importance of content personalization in marketing strategies. Our platform offers advanced analytics and attribution tools that help brands better understand their audience and deliver highly personalized and relevant content.

What other advantages do you think content personalization offers in marketing? Share your ideas in the comments!",1,0,1713489359.0,bigdata
1c6q763,Inteligencia Artificial en el Mundo Empresarial [Tecnología E3],,1,0,1713400955.0,bigdata
1c6egmh,Building Customizable Database Software and Apps with Blaze No-Code Platform,"A cloud database is a collection of data, or information, that is specially organized for rapid search, retrieval, and management all via the internet. The guide below shows how with Blaze no-code platfrom, you can house your database with no code and store your data in one centralized place so you can easily access and update your data: [Online Database - Blaze.Tech](https://www.blaze.tech/online-database)",1,0,1713371690.0,bigdata
1c6b5so,"Flink SQL—Misconfiguration, Misunderstanding, and Mishaps",,1,0,1713363722.0,bigdata
1c5kewy,Best Big Data Books for Beginners to Advanced to Read,,0,0,1713284866.0,bigdata
1c5c49x,QCon London: Lessons Learned From Building LinkedIn’s AI/ML Data Platform,,2,0,1713260871.0,bigdata
1c5g5sa,Color Psychology in Data: The Role of Color in Data Visualization,,1,0,1713274235.0,bigdata
1c3lha0,Help me pick a laptop for Data engineering/Big data work,"I am planning to buy a laptop and confused which one to pick. Considering high performance,  budget under 40k. Thanks in advance! ",1,20,1713070833.0,bigdata
1c39g6g,How can I derive associations between player positions? ,"So I have a csv containing football data about goals where each goal has a scorer, GCA1(the player that gave assist), GCA2(the player that gave the pass to the assister) 

I want to discover patterns of player positions that lead to a goal AKA buildups to a goal

Example: RB passed to a CAM which assisted a goal scored by a ST, or CB passed to a RW which assisted a goal scored by a LW

I want to find the most frequent buildups, think of it as finding frequent itemsets for a supermarket to derive discount decisions. Except my goal is to know which buildups are most common and make up coaching plans to better strengthen the relationship between the players in those buildups

I was thinking of using APRIORI algorithm or FP-Growth, I tried CHATGPT but it didn't help me that much (I'm getting only one association between FW players and no one, or sort of saying forward players scoring solo, which is definitely not logical based on my dataset) and gemini is the most awful AI out there. Seriously my grandma can do better, I gave it a prompt and rephrased it 3 times and it still gave me 'Rephrase your prompt and try again' 

So does anyone know a way I can do this, or if there is a way to do it better. I'm still a junior data scientist so I'm still learning and I would gladly appreciate any feedback or advice. 
",1,1,1713035184.0,bigdata
1c31h0j,"🌐 Meta releases OpenEQA, open-source dataset",,1,0,1713013389.0,bigdata
1c1fma9,"Example Data Pipeline with Prefect, Delta Lake, and Dask","I’m an OSS developer (primarily working on Dask) and lately I’ve been talking to users about how they’re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:  
   
\- \*\***Run locally (optionally)**\*\*. Should be easy to try out locally and easily scalable.    
\- \*\***Scalable to cloud**\*\*. I didn’t want to think hard about cloud deployment.    
\- \*\***Python forward**\*\*. I wanted to use tools familiar to Python users, not an ETL expert.    
The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.  
   
I really like the outcome, but wanted to get more balanced feedback since lately I’ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I’ve had include:    
\- \*\***Prefect vs. Airflow vs. Dagster?**\*\* For the users I’ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.    
\- \*\***DeltaLake or something else?**\*\* To be honest I mostly see vanilla Parquet in the wild, but I’ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).    
   
Anyway, if people have a chance to read things over and give feedback I’d welcome constructive critique.  
Blog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html  
Code: https://github.com/coiled/etl-tpch",2,0,1712843555.0,bigdata
1c1ce56,IoT-Powered Smart Warehouse Management: A Detailed Guide,,2,0,1712833882.0,bigdata
1c1bwpo,Migration from MongoDB to PostgreSQL,,0,0,1712832141.0,bigdata
1c0x1vv,Complete Survey on Database Tech Education for a Chance to Win a $100 Amazon Gift Card!,"**$100 Amazon gift card** opportunity for participating in a 10-minute survey. We're inviting students from universities across the globe to participate in a brief survey conducted by Valley Consulting Group at UC Berkeley, in collaboration with **Oracle Corporation**.Your valuable perspectives will contribute to understanding database technology instruction in higher education globally. As a token of our appreciation, participants who complete the survey will be entered into a drawing for a chance to win a $100 Amazon gift card!

[https://berkeley.qualtrics.com/jfe/form/SV\_6wWbyuBP7pBJUmG](https://berkeley.qualtrics.com/jfe/form/SV_6wWbyuBP7pBJUmG) ",1,0,1712784291.0,bigdata
1bzs1es,🔄 Migration from MongoDB to PostgreSQL,,2,0,1712668901.0,bigdata
1bzw7ot,Blog: Dremio’s Commitment to being the Ideal Platform for Apache Iceberg Data Lakehouses,,1,0,1712679531.0,bigdata
1bziskn,A Questionnaire on Big Data and Digital Governance,"Dear Folks!

I am a PhD Research Scholar at Central University of Punjab. I am seeking your expert opinion on some questions. Here is the attached link to the questionnaire. This will take approximately 10-20 minutes to complete. Your input would be greatly appreciated.

Thanks for your kind cooperation.

Link: [https://forms.gle/FVEdAQoUkrzqd1JbA](https://forms.gle/FVEdAQoUkrzqd1JbA)",2,0,1712635166.0,bigdata
1bzguyb,Companies to apply for ,Suggest companies that have a stable data team in Egypt and employ junior/fresh regularly or remote companies that provide Internship in data science/engineering ,1,1,1712629340.0,bigdata
1byvl44,Bringing Home Your Very First Data Product,,2,0,1712576048.0,bigdata
1bxofsq,Choose Your Lakehouse Adventure,"Experience how easy it is to take data from your source data systems, ingest them into Apache Iceberg and serve a BI dashboard from the confines of your laptop with these tutorials.",0,0,1712443253.0,bigdata
1bwifgj,The Art of Data Wrangling in 2024: Techniques and Trends,,1,0,1712324710.0,bigdata
1bwbs2e,Smart Factory Case Study,,2,0,1712301036.0,bigdata
1bvlxa5,Trigger CPI Endpoint from SAP BTP cloud Application - ERP Q&A,,1,0,1712233857.0,bigdata
1bv99j8,Understanding Conversion Attribution in Marketing," Conversion attribution is a crucial concept in marketing analytics that helps businesses understand the impact of their marketing efforts and allocate resources effectively. In simple terms, it refers to the process of assigning credit to different marketing channels or touchpoints for driving conversions or sales.

There are several different models of attribution, each with its own strengths and weaknesses. Here are some common attribution models and how they affect marketing decisions:

1. **Last Click Attribution**: This model assigns all the credit for a conversion to the last touchpoint a customer interacted with before converting. While this model is simple and easy to implement, it often overvalues direct response channels like paid search and undervalues other channels that contribute to the customer journey.
2. **First Click Attribution**: In contrast to last click attribution, this model gives all the credit to the first touchpoint a customer interacted with. While this model can help businesses understand the initial customer acquisition journey, it may overlook the influence of later touchpoints that drive conversions.
3. **Linear Attribution**: This model evenly distributes credit across all touchpoints in the customer journey. While it provides a more balanced view of the customer journey, it may not accurately reflect the true impact of each touchpoint on conversion.
4. **Time Decay Attribution**: This model gives more credit to touchpoints that occur closer to the time of conversion, while giving less credit to touchpoints that occur earlier in the customer journey. While this model recognizes the importance of recency in influencing conversion, it may undervalue touchpoints that contribute to brand awareness and consideration earlier in the journey.
5. **Position-Based Attribution**: Also known as U-shaped attribution, this model gives more credit to the first and last touchpoints in the customer journey, while giving less credit to touchpoints in the middle. This model acknowledges the importance of both initial touchpoints that introduce customers to a brand and final touchpoints that drive conversion.

By understanding the strengths and weaknesses of different attribution models, businesses can make more informed decisions about how to allocate their marketing budgets and optimize their campaigns for maximum impact. While no single attribution model is perfect, using a combination of models or customizing attribution models to fit your business goals can provide a more accurate picture of the customer journey and drive better results.

At Windsor.ai, we're committed to helping businesses navigate the complex world of marketing analytics and attribution. Our platform offers comprehensive tools and solutions to track, analyze, and optimize marketing performance across channels. Whether you're looking to understand the impact of your Facebook Ads, Google Ads, or email campaigns, Windsor.ai has you covered.

Have questions about conversion attribution or marketing analytics? Feel free to ask in the comments below!",1,0,1712190450.0,bigdata
1buve6y,The DynamoDB Book - Basic Package Link in comments,,1,1,1712157915.0,bigdata
1bucvdg,Tips for Creating Effective Data Visualizations in Power BI," Visualizing data effectively is crucial for understanding and communicating important insights in data analysis. Whether you're a beginner or an experienced analyst, mastering the art of data visualization in Power BI can greatly enhance your ability to extract meaningful insights from your data. Here are some key tips to help you create impactful visualizations:

1. **Simplicity is key**: Keep your visualizations simple and easy to understand. Avoid cluttering your charts with unnecessary elements that may distract viewers from the main message of your data. Remember, less is often more when it comes to data visualization.
2. **Choose the right chart type**: Selecting the appropriate chart type is essential for effectively conveying your message. Consider factors such as the type of data you're working with, the relationships you want to highlight, and the audience you're addressing. Power BI offers a wide range of chart types, including bar charts, line charts, pie charts, and more, so choose the one that best suits your data and your goals.
3. **Use colors effectively**: Colors can be a powerful tool for highlighting important information and drawing attention to key insights in your visualizations. However, it's important to use them judiciously to avoid overwhelming your audience. Stick to a consistent color palette, and use contrasting colors to differentiate between different data categories. Avoid using too many colors in a single chart, as this can make it difficult to interpret.
4. **Add context**: Providing context is essential for helping viewers interpret your visualizations accurately. Add labels, titles, and descriptions to your charts to explain what they're showing and why it's important. Consider including annotations or callouts to draw attention to specific data points or trends. Adding context helps ensure that your visualizations are meaningful and actionable for your audience.
5. **Embrace interactivity**: Power BI offers powerful interactive features that allow viewers to explore and interact with your data dynamically. Take advantage of these features to create engaging and immersive visualizations that encourage exploration and discovery. Add slicers, filters, and drill-down capabilities to enable viewers to dive deeper into the data and uncover insights on their own.

By following these tips, you can create visually stunning and informative visualizations that effectively communicate your data insights. And with Windsor.ai, you can take your data visualization to the next level by seamlessly connecting multiple data sources, such as Facebook Ads, TikTok, Shopify, and more, directly to Power BI. With Windsor.ai, you can unlock the full potential of your data and gain a comprehensive view of your business.

If you would like to save even more time creating your reports, you can check our free templates available for Power BI: [https://windsor.ai/powerbi-templates/](https://windsor.ai/powerbi-templates/)

What other tips do you have for creating impactful data visualizations in Power BI? Share your ideas in the comments below!",1,0,1712099137.0,bigdata
1btxzmi,Invitation: Technical Theme for 17 April: 745-9am PST. 'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. ',[ Removed by Reddit on account of violating the [content policy](/help/contentpolicy). ],2,1,1712062426.0,bigdata
1btxq84,Metrics-Focused Data Strategy with Model-First Data Products,"**Purpose-driven data, metric dependency trees, product prototypes, and more!** 

**Concept**: Quenching the perpetual business curiosity  
**Tech**: Collection of Model-First Data Products  
💡 The Metric Dependency Tree  
💠 The Model-First Data Product(s)  
Step 1: Retrieving Requirements  
Step 2: Modeling the Product Prototype  
Step 3: Validating the Prototype  
Step 4: Materialising Product from Prototype   

**Read more:** [https://moderndata101.substack.com/p/model-first-data-products](https://moderndata101.substack.com/p/model-first-data-products) ",2,0,1712061636.0,bigdata
1bty0qz,Technical Theme for 17 April: 745-9am PST. 'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. ',"You are cordially invited to join us for an Invite- Only, zoom session. Limited to CISO's, CIO's, CTO's and Cloud SME's. (No Sales Executives please).

Technical Theme for 17 April: 745-9am PST.
'Cyber teams leading with 100% Private LLM's: A cyber/ CISO perspective on Large Language Models. '

Presenter: Tim Rohrbaugh: (former CISO of Jet Blue)
thanks
DLH",1,0,1712062516.0,bigdata
1browv0,Apache Hive 4.0 has been released ,"Hi Guys,

Apache Hive 4.0 has been released . It's a really cool project , do check it out.

https://github.com/apache/hive

https://hive.apache.org/general/downloads/

https://hive.apache.org/",12,5,1711823908.0,bigdata
1bre8uq,🚀 Valkey: The Open Source Alternative to Redis,,2,0,1711792752.0,bigdata
1bqhmvs,Smart Supply Chains: Driving eCommerce Success with IoT,,2,0,1711690727.0,bigdata
1bqeh61,"Academic Survey, asks about the Challenges of Big Data Security","[https://forms.office.com/r/3qC141C9Xd](https://forms.office.com/r/3qC141C9Xd)  
This survey asks for your opinion on how concerning (On a scale from 1-5) are a number of specified challenges in Big Data security. It also asks for what you believe to be challenges to it in open ended questions that allow for long form answers. I'll admit that this survey is not at a high level as some of the other posts on this Sub, but I am just a student and this is my first foray into Big Data in a serious manner.   
This survey takes just under 5 minutes to complete, and you are the exact demographic that I would love to hear from. Thank you in advance.",2,0,1711680499.0,bigdata
1bpx91c,TUTORIAL: From Postgres to Dashboards with Dremio and Apache Iceberg,,2,0,1711636887.0,bigdata
1bpxkae,Apache Ranger UserSync Configuration HELP!!,"I am trying to configure Apache ranger usersync with unix ! and Iam stuck at this point !:

After i execute this : sudo JAVA\_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ ./setup.sh

Then this error pops up:

teka@t3:/usr/local/ranger-usersync$ sudo JAVA\_HOME=/usr/lib/jvm/java-8-openjdk-arm64 ./setup.sh

\[sudo\] password for teka:

INFO: moving \[/etc/ranger/usersync/conf/java\_home.sh\] to \[/etc/ranger/usersync/conf/.java\_home.sh.28032024144333\] .......

Direct Key not found:SYNC\_GROUP\_USER\_MAP\_SYNC\_ENABLED

Direct Key not found:hadoop\_conf

Direct Key not found:ranger\_base\_dir

Direct Key not found:USERSYNC\_PID\_DIR\_PATH

Direct Key not found:rangerUsersync\_password

Exception in thread ""main"" java.lang.NoClassDefFoundError: com/ctc/wstx/io/InputBootstrapper

at org.apache.ranger.credentialapi.CredentialReader.getDecryptedString(CredentialReader.java:39)

at org.apache.ranger.credentialapi.buildks.createCredential(buildks.java:87)

at org.apache.ranger.credentialapi.buildks.main(buildks.java:41)

Caused by: java.lang.ClassNotFoundException: com.ctc.wstx.io.InputBootstrapper

at java.net.URLClassLoader.findClass(URLClassLoader.java:387)

at java.lang.ClassLoader.loadClass(ClassLoader.java:418)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)

at java.lang.ClassLoader.loadClass(ClassLoader.java:351)

... 3 more

ERROR: Unable update the JCKSFile(/etc/ranger/usersync/conf/rangerusersync.jceks) for aliasName (usersync.ssl.key.password)

Can any one help me with that ?

Tools Iam using:

Host Device: MacBook m1

Guest Device: Ubuntu 20.04 LTS

Apache Ranger: 2.4 (Build from source code)",1,0,1711637718.0,bigdata
1bp82iq,Seeking Opninions: What's the Equivalent of my Degree Internationally?,"Hi everyone!

3 years ago i completed my bachelor 's degree in what would be translated to information science. However after some research it seems more like what other countries would call computer science.  


I have been wondering what to call my degree when applying for international jobs.   
I was therefore hoping someone here could give me a pointer of what my degree equates to, and how it stacks up against similar bachelor´s degrees around the world?  


 Here are a few examples of the courses I've taken during my degree: 

* Programming (Basic and advanced Python)
* Machine Leaning
* Artificial Intelligence 
* Data Management
* System Developement
* Knowledge graphs
* Knowledge representation and Reasoning
* Human-Computer Interaction  


Thanks in advance for any opinions!",1,1,1711561621.0,bigdata
1bo7tih,The Critical Role of Data Science in the Climate Crisis Battle,,3,0,1711459458.0,bigdata
1bo2dyi,🤖 GitHub’s AI tool can fix code vulnerabilities,,0,0,1711440150.0,bigdata
1bnnncv,Seeking Guidance on Advancing Skills," **I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field?**  ",2,0,1711397579.0,bigdata
1bnkjnw,Is GraphX still SOTA?,"My team is looking to update and improve our monthly TB scale graph pipeline that currently uses Spark DFs. While there has been recent development of Graph Databases, graph data processing seems like a quieter space. Is GraphX still state of the art or are there newer large scale tools that are better? ",1,0,1711390325.0,bigdata
1bmi11h,Price/performance for on-prem cluster in 2024?,"For reasons which I'd like to leave out of scope of this thread, I've the opportunity to spec up some on-prem Big Data (HDFS, Ranger, Spark, Hive, Zeppelin etc) clusters where the exact workloads aren't known in advance, just that we want to get the maximum performance for common uses cases for the amount that we're charging.

Have there been any studies with 2020s systems that might shed light on what would perform best for most typical use cases, out of e.g. clusters of 6x $20,000 machines, vs 12x $10,000 machines, vs 24x $5,000 machines, vs 60x $2,000 machines? (assume electric/cooling bills are baked into the price already).

My gut instinct is that the 60-node cluster would probably win, but I've zero evidence to back that up and it doesn't seem to be what any of the big players do.

",2,4,1711277630.0,bigdata
1bmhg1y,Using LangChain to teach an LLM to write like you,,2,0,1711275268.0,bigdata
1blk9u7,Unlocking the Potential of ML Models with High-Quality Data through sCompute," 

Hello BigData enthusiasts!

I wanted to share an article that I believe could spark an interesting discussion among us, especially those who are into the intersection of big data and machine learning.

The article introduces us to sCompute, a platform that emphasizes the importance of high-quality data for building effective machine learning models. For those who have been involved in big data analytics, you know how the quality of data can make or break our models.

Here's a quick overview of what sCompute brings to the table:

* **Enhanced Data Quality**: sCompute has developed a system to ensure that the data fed into ML models is clean, relevant, and of high quality.
* **Efficient Data Preparation**: The platform provides tools to streamline the often-tedious process of data preparation, making it easier for ML practitioners to get their datasets ready for analysis.
* **Scalability**: sCompute seems to have tackled the issue of scalability, helping data scientists to handle larger datasets more effectively.

The implications for big data analytics are significant. By improving data quality, we can potentially achieve more accurate insights, better predictive models, and more effective decision-making processes.

I'm curious to hear your thoughts on this. How do you currently handle data quality issues in your ML projects? Are there any platforms or methods you swear by to ensure the data you're working with is top-notch?

Here's the [link to the article](https://ankbig.hashnode.dev/build-ml-models-on-the-highest-quality-data-meet-scompute) for those interested in a deeper dive.

Looking forward to reading about your experiences and insights on this topic!",1,0,1711170253.0,bigdata
1blitfw,Need help :Pricing Inquiry for Data Cleaning and Analysis Service with Databricks and PySpark Expertise," 

Hello,

I'm currently exploring options for professional data cleaning and analysis services, particularly those utilizing Databricks and PySpark expertise. I have a dataset that requires thorough cleaning to address inconsistencies and erroneous data, followed by in-depth analysis to extract valuable insights for my business.

Here's a breakdown of the tasks I'm looking to outsource:

1. **Initial Evaluation:** Assessing my dataset to identify data quality issues.
2. **Data Cleaning:** Applying advanced data cleaning techniques to rectify inconsistencies and erroneous data.
3. **Databricks Analysis:** Utilizing Databricks for large-scale data analysis, optimizing processing performance.
4. **PySpark Development:** Writing PySpark scripts for efficient processing and analysis of distributed data.
5. **Reporting and Insights:** Generating detailed reports and providing insights based on the analysis performed.
6. **Continuous Optimization:** Recommending strategies for ongoing improvement of data quality and analysis processes.

I understand that the cost of such services can vary depending on factors such as the complexity of the dataset, the volume of data, and the specific requirements of the analysis. However, I would appreciate any ballpark estimates or insights from forum members who have experience with similar projects.

Additionally, if you have recommendations for reputable service providers or consultants specializing in data cleaning and analysis with Databricks and PySpark, please feel free to share them.

Thank you in advance for your assistance!",1,2,1711165371.0,bigdata
1bl8rh9,Apache AGE: Graph Database Magic in PostgreSQL,"Hey r/BigData!

Ever mixed graph databases with SQL? Apache AGE lets you do just that in PostgreSQL, opening new doors for analyzing complex data.

**Why Apache AGE?**

* Integrates graph and SQL for deep data insights.
* Ideal for navigating complex data connections.
* Open-source with a supportive community.

**Let’s Talk:**

* How would you use AGE to model intricate networks or data relationships?
* Share any cool queries or insights you’ve discovered.
* Tips for optimizing AGE? Let's exchange ideas!

**Dive In:** Curious about Apache AGE? Check it out on GitHub or their docs. Whether you're exploring new data solutions or enhancing current projects, AGE has something to offer.

For a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).

Excited to see your thoughts and how you're leveraging Apache AGE in your data adventures!",4,0,1711138092.0,bigdata
1bkzmla,Kafbat UI for Apache Kafka v1.0 is out!,,4,0,1711115188.0,bigdata
1bkg4mv,"Need Guidance, 4th Semester Data Science Student","Hey everyone,

&#x200B;

I'm currently in my 4th semester of data science, and while I've covered a fair bit of ground in terms of programming languages like C++ and Python (with a focus on numpy, pandas, and basic machine learning), I'm finding myself hitting a roadblock when it comes to diving deeper into big data concepts.

&#x200B;

In my current semester, I'm taking a course on the fundamentals of Big Data. Unfortunately, the faculty at my university isn't providing the level of instruction I need to fully grasp the concepts. We're tackling algorithms like LSH, PageRank, and delving into Hadoop (primarily mapreduce for now), but I'm struggling to translate this knowledge into practical coding skills. For instance, I'm having difficulty writing code for mappers and reducers in Hadoop, and I feel lost when it comes to utilizing clusters and master-slave nodes effectively.

&#x200B;

To add to the challenge, we've been tasked with building a search engine using mapreduce in Hadoop, which requires understanding concepts like IDF, TF, and more – all of which we're expected to learn on our own within a tight deadline of 10 days.

&#x200B;

I'm reaching out to seek guidance on how to navigate this situation. How can I set myself on a path to learn big data in a more effective manner, considering my time constraints? My goal is to be able to land an internship or entry-level position in the data science market within the next 6-12 months.

&#x200B;

Additionally, any tips on approaching this specific assignment would be immensely helpful. How should I go about tackling the task of building a search engine within the given timeframe, given my current level of understanding and the resources available?

&#x200B;

Any guidance, advice, or resources you can offer would be greatly appreciated. Thank you in advance for your help!",3,0,1711051877.0,bigdata
1bk13nm,Need help on a data problem,"Hi, I currently new in this field and want to ask for some advice on this problem.

Given N items (N \~ 10\^8), each item has a list of unique items that is ""related"" to it. The average size of the ""related"" list of an item is about 5000. The problem is, each time, a list of items is given with size \~ 10\^3 items, we have to return the number of unique items in the concatenated list of all the ""related"" items of at least 1 item in the given list.

* Input: Each line is the item id and its ""related"" items. So the input matrix is around 10\^8 \* 10\^3.
* Output:
   * When given a list of X (X \~ 10\^3) items, we have to concatenate the lists of ""related"" items of X items, and return the number of unique items.
   * For each query, the inference time is <= 1s.

Example:

Input:

1 2 3 4

2 1 3 5

3 1 2

4 2 5

5 1 4

So the item 1 is related to 2, 3, 4. item 2 is related to 1, 3, 5. item 3 is related to 1, 2 and so on.  
If the query is (1, 4), then the answer is 4. (the list is (2, 3, 4, 5) = (2, 3, 4) + (2, 5)).

Requirements:

* Exact solution with inference time <= 1s
* Cannot use cloud computing (must run with my own hardwares)

Priority (top to bottom is most prioritized to least)

* Inference time
* Use the least memory
* Simplicity
* Scalability...

What might be the most probable solutions for this? Thanks in advance.",2,1,1711007070.0,bigdata
1bk3d8k,‘Sankeying’ with Plotly,,1,0,1711016883.0,bigdata
1bjtgtw,Making Data Easy and Open with Alex Merced,,1,0,1710981411.0,bigdata
1bjbnwe,"What are the potential implications of Big Data analytics on ethical business practice, with a focus on assumed data discrimination and bias in relation to data processing, within the UK financial sector? (18+, ideally FinTech & AI experts/enthuiasts)",,1,0,1710935793.0,bigdata
1bin1en,"Dask Demo Day: Dask on Databricks, scale embedding pipelines, and Prefect on the cloud","I wanted to share the talks from last month’s Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing.

Last month’s talks:

* One trillion row challenge
* Deploy Dask on Databricks with dask-databricks
* Deploy Prefect workflows on the cloud with Coiled
* Scale embedding pipelines (LlamaIndex + Dask)
* Use AWS Cost Explorer to see the cost of public IPv4 addresses

Recording on YouTube: [https://www.youtube.com/watch?v=07e1JL83ur8](https://www.youtube.com/watch?v=07e1JL83ur8)

Join the next one this Thursday, March 21st, 11am ET [https://github.com/dask/community/issues/307](https://github.com/dask/community/issues/307#issue-1587938315)",5,1,1710862314.0,bigdata
1bifuf0,Best Big Data Courses on Udemy for Beginners to Advanced -,,2,0,1710839083.0,bigdata
1bifpr5,Quickwit 0.8: Searching into Petabytes of logs on S3,,2,0,1710838530.0,bigdata
1bifbqw,Hive Shell Issues,"Whenever i try to run a query, it provides me the output, but along with it, so many junk text come up and fill the screen that it’s difficult to find where’s the output.

Even while starting hive, i get these series of error messages, and it opens finally after some wait. Please help me out with this!

Video: https://drive.google.com/file/d/10uKD6iZbEWUG9epxPKns0Q5zkOtCldqV/view?usp=drivesdk",1,0,1710836701.0,bigdata
1bhkk2a,[D] Blog on Spark Caching,"Hello everyone! I've recently begun writing blogs on Medium. I'd love to hear your suggestions on how I can enhance my content. Your input is greatly appreciated!

[https://medium.com/@algorhythm2411/caching-in-spark-what-how-why-f412aac3acf5](https://medium.com/@algorhythm2411/caching-in-spark-what-how-why-f412aac3acf5)

&#x200B;",1,0,1710745008.0,bigdata
1bhg429,Best Big Data Courses on Udemy for Beginners to Advanced -,,0,0,1710729231.0,bigdata
1bgmall,Help for uni project,"Hi guys, I am a university student enrolled in Accounting program that will set me up for CPA when I graduate. I am part of a group project for Information Systems course of my program. Our group has to write a report on an emerging accounting technology and out of available topics the group chose **Big Data & Accounting Analytics**. 

There needs to be several sections in the report- an introduction of the technology, its current uses, advantages & disadvantages, potential future uses and a **case study** describing **a real-life**  
 **example of how Big Data & Accounting Analytics is being used at an organization** (in approx. 1000 words).   
 I need some **help for the case study**. I want to build a good project, so I thought why not try reddit for help. Any advice from people who really are working in Big Data & Analytics can help the report be more nuanced. Any suggestions on how to approach it? Where to research? Even better if you use it in your organization, then can you provide some information as to **how you use it?** **Why you use it?** and **what benefits you derived** from its introduction?",2,0,1710640841.0,bigdata
1bfa89a,Postgres is eating the database world,,20,3,1710496305.0,bigdata
1bff2zm,Need help! location data rearranging,"I am looking to arrange the top dataset in the order like the bottom one. Stock location course should always be in alphabetical order. The first row should always be accessed from the highest stock location position. When there is a switch in stock location course, it should look at the last stock location position of the previous line's stock location course. If the number is above 73, it should select the highest number from the next stock location course and order from high to low. If the number is below 73, it should select the lowest number from the next stock location course and order from low to high. I want to achieve the fastest and easiest walking route this way. stock location level is irrelevant as this only points to the hight a location is stored.



Does anyone have tips on how to fix this? ChatGPT is not helpful unfortunately

https://preview.redd.it/v19ug0owbioc1.jpg?width=784&format=pjpg&auto=webp&s=4e50887c7356afb950b5d51b2af9f57fc66346db

",1,0,1710512495.0,bigdata
1be2fah,Getting started with big data,"Hi folks, I'm with a small but growing company. Our data sets are growing quickly and need to be moved out of the operational data stores ( mainly MySQL) but remain accessible for historical analysis.

I've been researching big data strategies and have found the number of available tools and technologies to be overwhelming. Given that incremental learning can be costly in terms of time and effort due to the sheer volume of data, I'm wondering where best to begin.

As I said, I need to offload historical data from the operational database, but still be able to access it. There's no immediate need for real-time queries, but it's quite possible that there will be in the very near future. Just moving it from one relational store to another (been there, done that) only puts off solving the problem.

So I need to move it somewhere but where? We are in an AWS environment, so is it S3? Hadoop? NoSQL? Kafka? ...? And presumably this choice will affect the decision of what tools to use to access it for historical views within the application. And I can't start moving the data until I also have a way to access it.

Would be wide open to an answer being to read this book or take this course. It's just hard to know which given that everyone seems to be trying to peddle their particular solution.

Thoughts anyone? Thanks!",7,8,1710363045.0,bigdata
1be0lz6,Data skewness issue while extracting data from rdbms,"Hi guys,
I am facing data skewness issue while reading data from rdbms into a dataframe using spark in emr serverless. I tried to apply salting technique while reading data using spark because the saltkey( trunc (rdbms_random.value*10)). The salt key logic I am using is generating different values in different executer. I am looking for a solution who handled extracting rdbms skewness issue with partition column.

Thanks",3,0,1710358711.0,bigdata
1bcwghr,26 AI Reports from Famous Brands,,2,0,1710247746.0,bigdata
1bctdhw,10 Top Big Data Analytics Companies Serving the Healthcare Industry?,"*Top big data analytics services companies have been meticulously selected based on a proprietary company competitiveness analysis, taking into consideration their portfolio strength and company strength. These companies are instrumental in meeting the growing demand for big data analytics in healthcare market. We analyzed a comprehensive list of companies offering big data analytics services to manage and analyze large volumes of unstructured healthcare data.* \[https://www.rootsanalysis.com/key-insights/top-big-data-analytics-companies.html\] ",1,0,1710236944.0,bigdata
1b9mgmo,Need Help: Optimizing MySQL for 100 Concurrent Users,"I can't get concurrent users to increase no matter the server's CPU power.

Hello, I'm working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user 'generates reports' of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users' report generation tests using the PostMan application. **Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.**

When multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application's ability to manage concurrent users effectively. For example, a single user might generate a report for one month's worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, **when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.**

I am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I'm currently not considering this option.

Is there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?

# Additional Information:

Backend: Dotnet Core 6 Web API (MVC)

# Database:

MySql Community Server (free version)  
**table 48, data length 3,368,960,000, indexes 81,920**  
But in my calculation, I mostly only need to query from 2 big tables:

# 1st table information:

Every 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range—using both start and finish times—alongside an integer field representing a list of user IDs.  
For example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.

|Type|
|:-|
|bigint(20) unassigned Auto Increment|
|int(11)|
|int(11)|
|timestamp \[current\_timestamp()\]|
|timestamp NULL|
|double(10,2) NULL|
|int(11) \[1\]|
|int(11) \[1\]|
|int(11) NULL|

# 2nd table information:

The second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.  
Queries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.

|Type|
|:-|
|int(11) Auto Increment|
|date|
|int(10)|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|time|
|int(10)|
|timestamp \[current\_timestamp()\]|
|timestamp \[current\_timestamp()\]|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(200)|
|varchar(100)|
|int(10)|
|int(10)|
|varchar(200) NULL|
|int(100)|
|varchar(100) NULL|

# Test Results (Dedicated Bare Metal Servers):

SystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88 threads | 448GB DDR4 RAM  
Single User Report Generation time: 3mins (for 1 week's data)  
20 Concurrent Users Report Generation time: 25 min (for 1 week's data) and 2 users report generation were unsuccessful.  
**Maximum concurrent users it can handle: 40**",1,3,1709899986.0,bigdata
1b9vq4h,"YOUR MIC IS 99% PROBABLY OPEN IN THE BACKGROUND, CHECK YOUR SETTINGS NOW, FOR ANDROID ""AUTHORIZATION MANAGER""->""MIC""/""CAMERA""->SELECT #2 ""ALWAYS ASK"" GIVING PERMISSION ON YOUR MIC IS A CONSENT, SO DO NOT GIVE THIS CONSENT UNLESS STRICTLY NECESSARY"," YOUR MIC IS 99% PROBABLY OPEN IN THE BACKGROUND, CHECK YOUR SETTINGS NOW, FOR ANDROID ""AUTHORIZATION MANAGER""->""MIC""/""CAMERA""->SELECT #2 ""ALWAYS ASK"" GIVING PERMISSION ON YOUR MIC IS A CONSENT, SO DO NOT GIVE THIS CONSENT UNLESS STRICTLY NECESSARY",0,0,1709923472.0,bigdata
1b8y6iq,Kafka ETL: Processing event streams in Python.,,4,0,1709827098.0,bigdata
1b8xa7z,Would a fuzzy search database for structured data be interesting?,"We needed a solution where we can search our database with fuzzy matching and couldn't find any that offered highly customizable matching, we needed phonetic, geospatial and similarity matching. So we decided to build it ourselves all based on AWS serverless services. The solution turned out great and fulfilled our needs. I am now wondering if there other options out there that we just didn't find before we built this? and if not, would such a solution be interesting for others?",2,1,1709824941.0,bigdata
1b8ilsc,The Apache Iceberg Lakehouse: The Great Data Equalizer,"As we stand at this inflection point, the significance of open-source technologies like Apache Iceberg cannot be overstated. They are not merely tools or platforms; they are the harbingers of a new era in data management, where flexibility, collaboration, and innovation take precedence over walled gardens and restrictive practices. The narrative of Snowflake and Databricks, their responses to Apache Iceberg, and the rise of Dremio's Lakehouse Platform all underscore a fundamental truth: in the world of data, openness is not just a feature—it's the future.",5,2,1709777755.0,bigdata
1b88srg,LinkedIn Open Sources OpenHouse Data Lakehouse Control Plane,,6,0,1709753451.0,bigdata
1b69xik,"What's ""Modern"" in the Modern Data Stack","**Industry scenarios in 2024, community reviews, platform standards, and more!**     


𝐖𝐡𝐚𝐭 𝐭𝐡𝐢𝐬 𝐩𝐢𝐞𝐜𝐞 𝐞𝐧𝐭𝐚𝐢𝐥𝐬:   

* Current State of the Modern Data Stack 
* Getting into the Specifics:  
\- The Structure  
\- Elements of the Stack  
\- Before & After at a Glance     
 

𝐑𝐞𝐚𝐝 𝐭𝐡𝐞 𝐜𝐨𝐦𝐩𝐥𝐞𝐭𝐞 𝐚𝐫𝐭𝐢𝐜𝐥𝐞 𝐡𝐞𝐫𝐞: [https://moderndata101.substack.com/p/whats-modern-in-the-modern-data-stack](https://moderndata101.substack.com/p/whats-modern-in-the-modern-data-stack) ",2,8,1709555951.0,bigdata
1b65aqg,GreenGauge Analytics: Competitive Edge for Eco-Friendly D2C E-Commerce Businesses," Hi everyone,

I’ve been working on something that’s really important to me, and I think it could be valuable to you too. It’s called GreenGauge Analytics. The idea came from my own struggles and desires to make my e-commerce operations more sustainable without just shooting in the dark. I wanted to make decisions based on what consumers really care about when it comes to sustainability, not just what we think they do.

So, I started building a platform to provide actionable insights into consumer sentiment on sustainability and the latest trends that are shaping the e-commerce landscape. But here’s the thing - it’s not just about what I believe is needed. This is about creating something that truly serves our community of eco-conscious brands and consumers.

I’m reaching out because I need your help. Before we go full steam ahead, I want to make sure we’re on the right track. If you have a moment, I’d really appreciate it if you could sign up to stay in the loop and maybe share your thoughts through a short survey. Your feedback would mean the world to me and really help shape this into something that can genuinely support businesses like yours in making a positive impact.

You can sign up [here](https://123carmartin321.wixsite.com/greengaugeanalytics). Thank you so much for your time and for considering being a part of this journey. Let’s make sustainability at the core of e-commerce together.

Looking forward to hearing from you,

GreenGauge Analytics",1,0,1709538484.0,bigdata
1b5g3s4,Physical Data,"Hello, I’m a physic student. I am studying the biomechanics of high jump and long jump.  So I wanted to ask where on the Internet I can find tables of physical data (instantaneous speed, altitude, time) related to performances of different athletes, if you can Marco Tamberi.
Thank you. I tried to ask in other communities.",1,0,1709469071.0,bigdata
1b4r767,10 Reasons to Make Apache Iceberg and Dremio Part of Your Data Lakehouse Strategy,,0,0,1709394341.0,bigdata
1b4g55j,Data Architecture Complexity,,1,0,1709355878.0,bigdata
1b3xej2,IOMETE released the most generous free Data Lakehouse platform,"Hello Spark Community!

We're launching, the IOMETE Community Edition on AWS, and looking for insightful testers like you. This is your golden ticket to experience our scalable data lakehouse platform, designed to transform terabytes to petabytes of data, absolutely free. You'll be amazed by what you can achieve with our platform.

We're excited to see how users experiment with the platform by Leveraging Apache Iceberg and Spark for a managed data lakehouse that grows with your data—from terabytes to petabytes—without any vendor lock-in. Enjoy complete control over your data stored in S3 in parquet format, and pay only for the AWS resources you use. Whether you're using Spot or Reserved Instances, IOMETE ensures an affordable path compared to other vendors. Ready to transform your data management strategy?

IOMETE offers several Apache Spark features including:

1. A user-friendly interface and integrated notebook service for data processing and analysis.
2. Comprehensive monitoring and debugging capabilities for Spark jobs.
3. Automatic scaling of Spark clusters based on demand.
4. Capabilities to process real-time data streams from various sources.
5. A platform for training and deploying machine learning models for tasks like predictive analytics, fraud detection, and customer segmentation.

These features are designed to help you focus on your data analytics workloads by taking care of the infrastructure and management tasks associated with running Spark - [https://2ly.link/1wFi0](https://2ly.link/1wFi0)

Intrigued? A short video awaits you to guide you through the details and the wonders that IOMETE Community Edition promises - [https://2ly.link/1wFi3](https://2ly.link/1wFi3)

If you have any questions regarding installation and usage, join our dedicated Discord community, and let's shape the future of data management together - [https://2ly.link/1wFi1](https://2ly.link/1wFi1)

As of now, the IOMETE Free Community Version can only be deployed on AWS. Please let us know where you would like to deploy the platform so we can prioritize it - [https://2ly.link/1wFi2](https://2ly.link/1wFi2) . We will let you know when your preferred deployment option becomes available.",1,0,1709307934.0,bigdata
1b3x4ww,A Deep Dive into the Concept and World of Apache Iceberg Catalogs,,1,0,1709307279.0,bigdata
1b3lq4l,Zapier dataset,"I have a new startup company that is using Zapier and i am searching for other small business owners and startup clients 

I came across this post on https://www.usesignhouse.com/blog/zapier-stats which breaks down the top industries that use Zapier and it lead me here

I will like to ask if you can share the dataset you used for the analysis or if anyone can point me in the right direction so i can get the list and distribution of the various types of companies that use Zapier so i can target similar companies for my marketing.

I am looking for datasets in a csv format i can further analyze industries or companies using data analytics to find a good niche that is underserved but needs Zapier automations so i can find clients. 

Any help would be appreciated.",1,0,1709268370.0,bigdata
1b34mcm,🤖 Forecast Evaluation for Data Scientists,,1,0,1709224517.0,bigdata
1b34j7a,Confused about 2024's ESG reporting? You're not alone - Discover this snackable content to help your company handle ESG regulations with ease.,,0,0,1709224297.0,bigdata
1b2tbkk,Unlock the Full Potential of Azure for Data Engineering and Analytics with Our Comprehensive Video Guide," 

Hey Azure enthusiasts and data wizards! 🚀

We've put together an **in-depth video series** designed to take your Azure Data Engineering and Analytics skills to the next level. Whether you're just starting out or looking to deepen your expertise, our playlist covers everything from **real-time analytics** to **data wrangling**, and more, using Azure's powerful suite of services.

**Here's a sneak peek of what you'll find:**

1. **Twitter Sentiment Analysis with Azure Synapse Analytics** \- Dive into real-time sentiment analysis and build end-to-end big data pipelines.
2. **Real-time Vehicle Telemetry Processing** \- Learn how to handle real-time vehicle data with Azure Stream Analytics and Event Hub.
3. **Fraudulent Call Detection** \- Discover how to detect fraudulent calls in real-time using Azure Stream Analytics.
4. **Weather Forecasting with Azure IoT Hub** \- Explore how to forecast weather using sensor data from Azure IoT Hub and Machine Learning Studio.
5. **Web Scraping with Azure Synapse** \- Get hands-on with web scraping using Azure Synapse, Python, and Spark Pool.
6. ... and much more across 20+ videos covering Azure Databricks, Azure Data Factory, and other Azure services.

**Why check out our playlist?**

* **Varied Topics**: From analytics to processing, explore Azure's capabilities through practical examples.
* **Skill Levels**: Content tailored for both beginners and experienced professionals.
* **Community Support**: Join our growing community, share your progress, and get support from fellow Azure learners.

Dive in now and start transforming data into actionable insights with Azure! Check out our playlist

[https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0](https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0)

**What's your biggest challenge with Azure or data engineering/analytics?** Let's discuss in the comments below!",1,0,1709186910.0,bigdata
1i51oay,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",12,14,1737302445.0,analytics
1dj1a5b,Looking for community feedback,"Hey r/analytics community,

As this group continues to grow I want to make sure majority are finding it useful.

I'm looking for your ideas of where we can improve this group and what do you love about it, leave your comments below.",15,18,1718744143.0,analytics
1i7km3s,How long do you think the bad job market will last?,"I got laid off in early 2023. Market was bad in 2023, bad last year, and seems to be better this year but still bad. I’m so done with it. Took my first offer end of 2023 and have massive regret. Been wanting to switch jobs ever since but its been very difficult. ",56,36,1737576581.0,analytics
1i7u4sb,how to transition in healthcare analytics role without healthcare experience?,"I’m looking into jobs within healthcare analytics, mostly data analyst title but I don’t have any prior experience in the healthcare industry. I have degree in mathematics and 3+ years of experience as a data analyst in the marketing and eCommerce space. The challenge is that most healthcare analytics jobs posting that I came across, requires prior experience with healthcare data and knowledge of healthcare-specific work. They all specify 2+ or 3+ years of healthcare experience. Since my education background is also unrelated to healthcare, just like my experience, I am finding it difficult to get any interviews. 

To those of you in the healthcare analytics field any tips for someone with 3+ years of experience in different domain and none healthcare experience, how to stand out? Is showing some good health data personal projects helps? Any other tip? I am open for entry-level roles too but I haven't came across to any jobs yet that does not specify requirement of having healthcare experience. ",9,5,1737602456.0,analytics
1i7tfv1,Does Google analyze their own website using Google Analytics?,It feels like a bit of an analytics inception moment—tracking the tracker! ,6,4,1737600343.0,analytics
1i7puds,How do you collect data?,"Good afternoon, I am an aspiring Data Analyst and am a junior going for my Bachelor Science in Information System. After lurking on this sub for a while I’m always reading, my personal project collects data of road traffic, health care visit, etc. I understand the important of collecting data, or manually collecting it. 

But my question is how are you guys finding all this information. The only projects I’ve done under my belt have come from data collected at work.

How are you collecting sales data, road data, local data, etc… continuously and regularly. Is there something I’m missing? I swear this type of information  would be private, away from the public. How are you guys getting access or able to collect this data in a local scale?

",9,9,1737589957.0,analytics
1i7vl22,Comp TIA data cert-worth it?,"Hi all. I’m a total newbie and came across the CompTIA Data certification, and have a membership to LinkedIn that has training videos and test prep for this. It’s included in my membership so I’m taking the time to do it-May or may not take the exam. 

Also, the exam is recommended for ppl who have 18 months experience in data, which I found interesting. Might be completely over my head, but thought I’d ask here. 

TIA.",2,2,1737607246.0,analytics
1i7vk83,Learning steps - Excel? Data Analyst training,"Hi everyone,

I have been researching the data analyst career field for a little bit now and thinking about diving in. I love getting data and facts, and organizing things so it sounds kind of interesting to me. 

I'm looking into online courses, certificates and just wondering if I need to start with an Excel class? I've used Excel some but it's been a while. I wasn't sure if it was really needed, or just start with SQL, Python, R, Power BI, etc. 

Thanks in advance!",2,3,1737607167.0,analytics
1i7w14t,Looking for a Partner to Build an Analytics Startup,"
I’m a skilled financial and data analyst with experience working for Fortune 500 companies across various industries, including oil & gas and REITs. With an MBA and a Bachelor’s in Finance from the University of Texas at San Antonio , I specialize in Excel, Power BI, financial modeling, business intelligence, and data-driven decision-making.

I’m currently looking for a like-minded partner interested in launching an analytics startup focused on delivering data-driven solutions for small  to medium businesses. My strengths lie in financial analysis, data visualization, and automation using tools like Power BI, Power Automate, and Excel. I’m also well-versed in working with APIs and building scalable reporting solutions.

If you’re passionate about data, business strategy, and helping companies make better decisions through analytics—whether you have technical expertise, a business development mindset, or industry-specific experience—let’s connect and explore how we can build something impactful together.

Let’s discuss ideas and collaborate to turn insights into actionable value.
Message me ! 

Thanks ",0,1,1737608837.0,analytics
1i7otvm,Create Looker Views with API,"Hi, has anyone had any experience creating Looker views (NOT Looker Studio) using the API, if possible at all?

Context is that I am building a dashboard with many charts. Most of these charts have the same setup (type, pivots, etc.) and are just plotting different measures. Looking for a way to automate this process using Python. 

Thanks in advance. ",2,1,1737587210.0,analytics
1i7fg2h,Tricks for tracking cohort retention user metrics?,"I'm interested in knowing how you manage and track cohort retention users efficiently. I'm using PostHog, I'd love to hear how you set specific metrics, where in the product lifecycle, etc.?",4,2,1737564188.0,analytics
1i7l3jw,Data & AI @ London Olympia Feb,Is anyone going to this event? I'm planning on going and it would be great to connect with some people in analytics. ,1,1,1737577787.0,analytics
1i7ccua,Please give me some suggestions,"I have just recently started exploring this Data Analytics field, currently going through the Google Data Analytics Program to grasp the basic knowledge before i can dive into deeper concepts.

But the problem is, i am already at my last year in my university so i need to look for jobs too, and i heard building projects is very important in this aspect.

So, my question is... should i start making projects now through the help of YouTube or other resources, even if i have very basic knowledge of Data Analytics? 
Is it possible to learn about the different concepts through building projects simultaneously? Or should i concentrate on learning this field thoroughly before diving into the projects?",3,18,1737556228.0,analytics
1i7f24n,Free masters in DA through work,"If you could get a free masters (through work) in data analytics would you do so? Or is it more beneficial for future employers to see your work experience in analytics? 

The job postings I’m seeing don’t all necessarily require a masters but my bachelors isn’t in the same field (social sciences) so I can’t break that barrier very easy. But at the same time if I don’t have experience to add to my resume, is a masters going to even be worth it? (I can’t transition to a career in data analytics without giving up the option to get my masters degree, it’s an endless loop). 

So I either stay in this career and get my masters degree then pivot out eventually, or keep trying for a job without a masters and instead get experience to move up in the field organically. 

Also, I know this is a heavy saturated field and it’s hard to get a job. But it’s also hard to get a job in my current field so I’m just trading something I do for something I enjoy more. I don’t think it’s easy to find a job for any field right now but ymmv. ",2,3,1737563233.0,analytics
1i6ybro,Need Advice on a Data Analyst Offer as a New Grad,"Hello everyone,

I recently received an offer for a Data Analyst position at a company as a new grad with a Bachelor's in Statistics, and I’d love to hear your thoughts on my situation.

The role is completely new at the company, they have never worked with data before, aside from some basic Power BI visualizations using SQL. My concern is whether I, as a new grad, will be capable of kickstarting their data-driven decision-making without a senior analyst or a team of analysts to guide me. I’m worried about not having anyone to fall back on when facing challenges. 

Additionally, the job description includes process automation using Power Automate. They want to streamline their order-handling process, such as ensuring the right person gets an email based on the ordered product, sending automated confirmation emails to customers when an order is placed or shipped, and generating alerts when inventory is running low. To me, this sounds more like an IT or operations role rather than a typical data analyst position, and I’m unsure whether I have the skills to fulfill these expectations. Or is it ok for a data analyst to take care of such tasks as well?

Currently, I’m enrolled in a Master’s program in Statistics, but I feel like I might learn more by gaining real-world experience as a Data Analyst rather than just working with perfect datasets in R all day.

What do you guys think? Should I take the job and learn on the go, or would it be too risky to hop out the master's for a fresh started data analyst role at a company? I have a feeling one would learn and become a better analyst if I got a job where there is already a data driven culture to sort of guide me. Any advice would be greatly appreciated!",20,26,1737506626.0,analytics
1i6v6dp,"What counts as a ""project"" for cutting your teeth on data analytics?","I keep seeing people recommending to do one instead of a cert (I'm doing google data analytics right now and it sucks so far, philosophy mumbo jumbo about thinking, mostly), but I'm just not sure what would meaningfully count as a project idea. 

Are the projects just.... rearranged spreadsheets? what are we even looking to do?",10,11,1737498208.0,analytics
1i6wytt,Full time course,"Hey all
I just completed a professional course in Accountancy and wish to switch to data analytics. Initially I was of the view that the same can be learned by me online. I started with learning the various tools, however I now am inquisitive about full time courses in universities on stats and DA combined. The problem is I think most require one to be a grad in tech (engineer or the likes I guess). Maybe I'm getting it wrong, in case anyone has any idea with this regards, it'd help me.",0,5,1737502897.0,analytics
1i629u6,2024 End of Year Salary Sharing thread,"I haven't seen anything posted here for 2024 EOY. Please let me know if there actually has been.  
Please only post salaries/offers if you're including hard numbers, but feel free to use a throwaway account if you're concerned about anonymity. You can also generalize some of your answers (e.g. ""Large biotech company""), or add fields if you feel something is particularly relevant.

**Title:**

**Tenure length:**

**Location:**

**$Remote:**

**Salary:**

**Company/Industry:**

**Education:**

**Prior Experience:**

**$Internship**

**$Coop**

**Relocation/Signing Bonus:**

**Stock and/or recurring bonuses:**

**Tech Stack Used:**

**Total comp:**",106,104,1737410100.0,analytics
1i6bgpa,"I aspire to be a data analyst, was starting with python a good idea?","Title basically, I started learning python with interest of becoming a data analyst and now with a bit more research on what that entail, I realize that I may should have started with something else like math or sql, should I just stop python now and take it later or the order I learn the skills doesn't matter? ",24,35,1737437002.0,analytics
1i6vt9z,Types of Analyst roles,"I have been working as a data analyst/project manager but would like to move towards a bit more technical role. I was looking around the job market, but it can be overwhelming to decide what direction to take.

I know that I hate the project management part of my job so I'd rather focus on the ""problem-solving on my own"" aspect.

Do most people have a goal and work towards a specific industry? For example, if I have a security interest are there very specific niche jobs that I can develop into from being a data analyst? To be fair I even struggle to find relevant jobs to my experience because the word analyst is so overused on the job market. How did you find a field that fit your interest?",1,3,1737499828.0,analytics
1i6v0fz,"Recommendations for Practicing Interview Skills (Podcasts, Websites, etc.)?","Hi everyone,

I’m preparing for interviews for data science/analyst roles and would love to hear your recommendations for resources to improve my skills. Specifically, I’m looking for:

\- Tips on how to introduce myself effectively.

\- Ways to refine my wording and expressions for clarity and confidence.

Do you know of any great podcasts, websites, or other resources that could help? I’d really appreciate your suggestions!

Thanks in advance!  
",1,2,1737497797.0,analytics
1i6mdw7,"Building a Minimalistic BI Stack with PostgreSQL, FDW, and Superset – Looking for Feedback!","Hey everyone! I’m in the process of setting up a lightweight BI (Business Intelligence) architecture and would love to hear your thoughts before I go too far down the rabbit hole. Here’s the broad outline:

# My Setup

1. **Dedicated PostgreSQL “BI” Server**
   * I have a separate Postgres instance just for analytics (i.e., not the production DB). I really don’t want Superset or heavy queries hitting prod directly.
2. **Foreign Data Wrapper (FDW)**
   * I’m using FDW on the BI Postgres to pull data from the production DB (and potentially from other sources in the future). This way, I can query external tables without actually copying them over or building huge ETL pipelines right now.
3. **Materialized Views**
   * On the BI Postgres, I’m creating materialized views to store pre-aggregated data. The idea is to speed up reporting queries and reduce the load on both the BI and production DB. I’ll refresh these on a schedule (using either pg\_cron or a simple script).
4. **Apache Superset for Visuals**
   * I’ll point Superset at the BI Postgres so I can build dashboards from the materialized views. I’m trying to keep everything cost-effective and as simple as possible.

# Why I’m Doing This

* **FinOps & Simplicity**: I want to avoid a giant stack or expensive cloud services for moderate data volumes. We’re already comfortable with Postgres, so it made sense to leverage it fully.
* **Avoid Overloading Production**: I really don’t want to connect Superset directly to the production DB. I’m hoping FDW plus a separate BI Postgres instance is a safe approach.

# What I’m Unsure About

1. **FDW Performance**: Have you seen any issues with slow queries when using FDW for relatively large tables? I’ve read about “query pushdown” being pretty good, but I’m worried about network overhead or unexpected performance hits.
2. **Materialized View Refresh**: Right now, I’m planning on refreshing them daily (or maybe hourly for certain critical tables). Any advice on how to handle partial vs. full refreshes?
3. **Scaling**: Is this going to become a nightmare if data volumes suddenly jump? I realize I might need to move to something more robust if we get hammered with queries. But how do you all handle that growth path?
4. **Security & Access Controls**: Are there any best practices I should consider for user roles, especially to ensure people only see the data they’re allowed to see in Superset?

# Questions for the Community

* **Is this approach workable in the long run, or is it an early-stage hack?**
* **Any big gotchas with Postgres FDW or materialized views I should watch for?**
* **Would you recommend a different setup if you were building from scratch?**
* **Any cool tips for automating view refreshes (e.g., triggers, using pg\_cron, or external schedulers)?**

I’d really appreciate your feedback! If anyone has done something similar (or sees a looming pitfall), please let me know. My goal is to keep the architecture straightforward and cost-friendly, but also robust enough that I’m not drowning in queries or stale data.

Thanks in advance!",2,1,1737476591.0,analytics
1i61xtb,What are the best entry level jobs that help you get into data analytics? ,"I've noticed a lot of people here are saying that a job in data analytics isn't really an entry level position, and you usually have to have worked in another job for a while before moving to data analytics 

In that case, what are good entry level jobs to go for that help you transition into data analytics? What are the jobs that you should go for right out of college (AKA entry level jobs) that will help you move into data analytics? 

For reference, I got a degree in economics, and I'm looking for my first post college job. ",61,21,1737409265.0,analytics
1i6qdao,Will learning AI and machine learning help me become a data analyst? ,"Hi all, as the title says, will learning artificial intelligence and machine learning models help me become a data analyst? I just graduated with a masters in IS and concentrated in data analytics. I see available entry level positions and internships mainly focused on these two and I wasn't sure if these would fit me and my end goal. I know SQL and Python but haven't learned R yet and I know data visualization tools like Tableau and honestly I think I'm more comfortable with SQL rather than the other languages that I feel like ML and AI focuses more on. I've been trying to find other entry positions for data analytics but no luck yet.

Also to add, my bachelor's is in a completely different field so I don't have any related work experience with manipulating and handling data much.

I was just wondering if my degree is more business IT and analytical rather than data science in general? Thanks for any feedback.",0,6,1737486337.0,analytics
1i6kngc,would these job duties help me land a role in data analytics?,"hey, i would rly appreciate if someone could look over these job duties and let me know if they think this could be helpful in landing a data analytics job, this would be for a procurement and supply chain leadership development program for a packaging company, i want to work in data analytics but i dont think this role utilizes SQL

duties: 
-material pricing automation- this would include pricing standardization and automation, not sure what tool is used 

-spend cube optimization- ai tool is used for this, will be trained in using it and debugging it, either they don’t know what it is yet or i forgot what it is (not sure), this would include going to different packaging sites to better understand the ordering process and figuring out how to keep the data standardized 

-general data capturing and visualization for other tasks, would work with the procurement team, buyers, executive team and the IT team to do this 

",0,3,1737472096.0,analytics
1i6k3zr,Support/Guidance for my master thesis in Learning Analytics ,"Hello,

I’m a masters student and working as a working student in a product company. The company has an e -learning platform with a good number of active learners. I plan to do my master thesis from there as my manager agreed to give me data. But the issue is no one will be there to support me in office as well from university(because my professors are not experienced with data analytics, product based companies). 

As of now, I just have an idea to do an analysis of how I can retain learners using interventions in the learning platform. It would turn out well if I could get guidance from someone who has experience in Learning Analytics or something similar. Any tips or experience is appreciated. On the side, I’m also approaching professors from other universities for a collaboration. I study in Germany by the way. Thanks in advance! ",0,2,1737470661.0,analytics
1i6d48f,"How can you build insights across Google Analytics, HubSpot, Salesforce, Stripe, and Snowflake - without hopping multiple tools?","I work in SaaS and consistently need to connect data points from various stages of the sales funnel, from site visitors and lead generation to sales opportunities, revenue, and product usage.   
  
Typically, I have used SQL querying tools, Spreadsheets, BI Tools for Dashboards, Notebooks for exploration and a lot more. Is there a way I can sync data from these tools, explore when I need to and present dashboards and work on all this collaboratively with my team (a lot of ad-hoc Qs keep coming up from different stakeholders).   
  
This process is static and cumbersome to replicate regularly.   
  
I'm also building something myself to solve for this and would like to know what methods are effective for integrating and analyzing these diverse data streams more dynamically. ",3,9,1737443533.0,analytics
1i5xnnu,jobs in US government,"Would it be wise to not apply for any analytics-related jobs in the US govt (agriculture, homeland security, etc.) for a while given the change of administrations? Not a political post - just a question as I'm looking to transition to a different role. ",12,14,1737398961.0,analytics
1i6ivt9,Guidance! ,"I'm a B.A. graduate in English Literature, and I was preparing for government jobs. Due to the unpredictable nature of govt jobs I want to have a backup plan and I want to pursue Data Analyst or Data Scientist courses but I'm not aware of the difference between the two and whether am I eligible for this courses or not? Whether it's a worthy or not in today's context? 

If yes, then kindly guide me as I don't have any knowledge regarding this...like from where should I learn, which courses or skills I should learn etc. From the scratch..

Thank you in advance ",0,4,1737467099.0,analytics
1i6373y,Credit Risk Analyst,"Hello all,

Is a corporate credit risk analyst position a good starting point or a good bridge to a data analyst career? I have a masters/bachelors in management and economics, and only 2 YoE. Won’t be using python or SQL, but would be taking courses on the side (currently doing the google data analyst course).",4,2,1737412392.0,analytics
1i6fwyq,Career this year ,I am planning to start to learn data analytics. Is it good time to start this year ? If I finish data analytics Google certificates this year and create some projects will I be able to get job in this year? What is your opinion should I start to learn or should I should I focus on something else?,0,5,1737456346.0,analytics
1i5lfe5,what is the difference of a financial analyst and a data analyst? ,could anyone tell me the difference 😅,13,16,1737360491.0,analytics
1i5tp08,Variable Compensation Chart ,"Hi, 

I've been tasked with outlining a variable salary compensation plan for two of our most senior employees.   
What I have right now are, if 100% of KPI targets met in Q1 and Q2 = $5000 bi-annual bonus pay. What I do not know how to do is formulate the accelerator/decelerator from that.

Does anyone know of a template or formula that can help with this? ",2,6,1737389389.0,analytics
1i65qcx,Management Information systems or business analytics ? Help me choose a degree ,"Which degree is the most employable ? 
Work life balance
Salaries 
",0,15,1737419111.0,analytics
1i5u9a1,Need help transitioning from FA to BA,"Hi, New Grad here. Has anyone made the transition to Financial analyst to Business Analyst? In my role i’ll be using mainly Excel. If I want to make the switch should I pursue MSBA after a couple years working? Should I Just go straight to MSBA instead of even working the FA role? I feel lost.",0,6,1737390786.0,analytics
1i5kuu1,Analyst but not a Data Analyst,"Hi everyone, I’m an Analyst at a film/tv studio in CA. However, as far as being an analyst goes, I mainly use the following in my line of work; Excel, Box, PSL (film/tv specific type of Quickbooks but each file is one production instead of a whole business). 

My role really requires me to comb through 80k+ lines of data for inconsistencies in Free Form coding to maximize money back on qualified spending. 

But that’s it. This portion accounts for 50% of the job and the rest of it is communicating with other departments for backup and additional information. 

I want to aim towards working as a Data Analyst, whether it be for my studio or another. 

About me: BS in Business Admin, worked 10+ in accounting / finance ranging from accounting manager, senior project accountant, business analyst, and analyst. Currently 2 years into my specific role. 
I have used Tableau, Excel, SAP, Smartsheet, Workday, Microsoft Dynamics 365: M4, M5, F&O, Oracle Netsuite

I have free access to certification via Coursera, LinkedIn, QA, and Pluralsight. 

I was hoping to get feedback on how I can make this transition? I’m comparing straight up getting an MBA in Analytics, a CalTech Certification in Data Analytics, and simply getting certifications from the above sites I have access to for free (I can obtain certifications for free since my company pays for them)

What is the best move here? Where am I going wrong? Willing to learn in-depth and take as long as I need. Thank you! ",2,5,1737357963.0,analytics
1i5bgi4,College major question,Does it really matter what your college major is in to break into analytics? Currently pursing a BS in Business Admin with a concentration in business analytics and a applied mathematics minor (I know its a mouthful). I heard the data analytics program at my school is really watered down and a lot of people are having to learn all the different languages on their own time anyways.,8,16,1737326840.0,analytics
1i5flri,Analytics Major,"I am currently a junior in high school and I’m achieving my Associates Degree in the broad subject of business. I need to decide what major to do when I go into college. First of all I’m looking for a job that I enjoy with also making good money. I am a huge extrovert and love talking to people, I have a huge interest in sports and money. Is analytics the best major for business do you think? What are the pros and cons of analytics major. I was also looking at the option to double major in finance and analytics would this be smart?",3,7,1737339069.0,analytics
1i4nlb2,"What master's should I pursue: CS, Math, Stats, Economics, or MBA? ","Data Analyst with 3 years of experience and currently working as one. 

I have a Bachelor's in mathematics from Cal State Northridge. 

I'm looking to go for a master's in any field that will help look more impressive for promotions and to increase my knowledge further with data. I also would like to look more appealing(apart from any work experience and projects) for future prospective jobs. ",29,32,1737252428.0,analytics
1i4tkr1,How do you guys create data presentations after analysis?,"I have to frequently create data presentations to explain my findings. I've tried a few tools and a I have to do lot of things are pretty manually. So, just wanted to know if you guys have any tricks or do you also manually insert data in powerpoint charts or add screenshots. 

And how do you decide on the story? Isn't this is hard process, to use right brain after all that left.

Would love to know your workflow or any tools you guys use?",8,21,1737273371.0,analytics
1i4aavf,"Without a degree, now planning to shift into Data Analyst ","So initially i did my Bachelors but due to one zero credit subject which i failed held me back and i didn't get my degree, due to family pressure and finances I Had to return my own country where I got a job as supervisor in a company, and soon promoted to assistant manager on the side I did SEO and other analytical stuff which i was always interested about.

Now im planning to take a jump in my career im 26 and i don't want to be late on the boat, Im thinking of going through basic SQL, fundamentals, Power Bi, Tableu, thinking of doing some projects to add to my portfolio, thinking of also doing few months apprenticeship in data analyst meanwhile thinking of networking in Linkedin and finally applying for a data analyst job to get my career to begin, am i missing anything ? Do let me know ? Thanks in advance",24,45,1737215271.0,analytics
1i4tpjq,Need recommendation and sources ,"Hello guys I'm a finance graduate and also want to breat into data analytics with finance. Although I have a basic knowledge of python. I don't know which courses should I do...
Please help",1,9,1737273939.0,analytics
1i4skez,"Business Analysts in Aus - what course did you take, what would you do differently?","I am a project manager keen to learn BA skill. Can you become a BA without formal qualifications?  If you need qualifications to work, what courses would you recommend? If you were to start over - what would you change about the way you came to become a BA.",1,3,1737269094.0,analytics
1i4h8fn,"Want to transition into more into a data management titled role, what resources or training could help transition into this role with only document management experience",c,2,5,1737233921.0,analytics
1i42obl,Data analysis tools,"Please bear with me I have a question, I'm an IT BA and recently I'm considering moving to another company, but I realized a lot of job postings require certain DA tools knowledge like power BI, Tableau...etc. And I was thinking I could start working on them to learn and stuff but my issue is that my day to day job doesn't include working with data in anyform, since I do systems and software business analysis I don't seem to find a purpose in learning these tools. I mainly work with requirements and technical specifications and visualization of the projects scope, I do have to track all that and keep backlogs of everything but again I don't think DA tools would help with any of that.

But I still feel like I wanna aquire this skillset just in case.

My question is, is there a way to incorporate these tools to fit within my work scope, or should I consider learning other tools? Or should I just learn them for the heck of it!

Guide me please 🥺 

",11,7,1737186662.0,analytics
1i3w394,is it worth it?,,2,3,1737162899.0,analytics
1i3pen0,Am I a decent candidate for a analytics job? ,"Recent graduate with biomedical informatics degree, relevant courses to Data Analytics taken - Biomed Data Analytics l and ll. also did a capstone project where took a bunch of data from an excel and used python to clean, analyze, and put it through machine learning to predict an outcome. Although my major isn’t something specific like data science, i’m wondering if i should waste my time applying for data analyst positions, or any other “analyst” positions with what I have. Thank you ",9,15,1737144387.0,analytics
1i3tdpa,Data headcount vs company size,,3,5,1737155057.0,analytics
1i3tyno,Is this considered Data Analytics?,"Hi! I have a degree in Economics with a major in Applied Statistics and am interested in pursuing a career as a data analyst, with the eventual goal of transitioning into data science.

I recently started a new role where my tasks include creating report charts (using Q for significant testing and then visualizing the data in PowerPoint), cleaning datasets, and performing QA to ensure accuracy in reports.

However, I don’t currently use SQL or Python in my job. Given these responsibilities, do you think this experience will help me progress toward a career in data science, or would it be better to explore other opportunities?",0,25,1737156699.0,analytics
1i3ve37,QlikSense Developer [$45 per hour] Needed in SF Bay Area,"Hi Folks, 

I am looking for a mid-level QlikSense Developer \[$45 per hour\] to work for an AI startup.

This is a hybrid role - at times you have to come in person. 

If interested please get in touch with me [Chris@Analyze.Agency](mailto:Chris@Analyze.Agency)",0,13,1737160790.0,analytics
1i2wy2q,Mid-level and a bit stuck,"I’m a mid-level data analyst with 6 years experience and a SQL, PowerBI, PowerQuery, Excel stack. 

I recently quit my job because of the workload (they had me doing 10-14 hours a day, insane) and now I feel kinda stuck in terms of where to go. 

I’ve been applying to mid-level positions but it feels like my tech stack isn’t enough anymore. Lots os positions include Python, R, database management, etc. I feel like I need to expand my stack but I’m a bit lost as to what I should focus on. 

In your experience, what are some areas which have good demand for  mid-level professionals?",36,29,1737055375.0,analytics
1i2wbgh,Google Data Analytics worth it?,"Hi, is the above really worth it? I'm currently studying L4 Data Analytics via work but the material is much better I think on Coursera (trialling the 7 day free version). 

Is the cert still worth it? YouTube tells me one thing but I wanted thoughts from real people in the field.

Thanks ",35,51,1737053794.0,analytics
1i356je,Desperate for Advice ,"Started uni in 2021 as a Finance major --> 

**1st year**: Didn't do much.

**2nd year**: Joined 2 finance clubs my 2nd year thinking I was going to pursue Investment Banking. Realized that a 90+ hour work week was not for me, so I decided not to pursue that path.

**3rd year**: Took a data analytics class and intro comp sci class during the Fall semester. Turns out I enjoyed technical work more than making stock pitches so decided to add a Business Analytics major and a Comp Sci minor. Took 3 business analytics classes and 1 cs class during the Winter semester and realized most BA courses were poorly structured and often unavailable due to lack of professors. Figured taking Comp Sci classes would be a better investment of my time and money.

**4th year**: Took 3 cs classes last semester (F2024) and currently taking 2 this semester. 

Currently have a 3.75 GPA and will be graduating next year in may (W2026)

Have used Python (Pandas, NumPy, Scikit-learn), SQL, PowerBI, R, Java, Excel, PowerPoint for class assignments/projects (nothing too complicated).

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Due to family constraints, I was not able to work during my time at uni and, as a result, have no internship experience. Things have changed this year though so I am now able to pursue internships. Since I am at the end of my 4th year, I have F2025, W2026, and possibly S2026 to gain internship experience.

So I guess I'm looking for advice on how to move forward towards getting an internship within data analytics. I know a solid portfolio is super important, so that should be 1st step, but I don't know what projects to do and where to even begin. Ideally I'd want to leverage my finance background to land a data analyst/business intelligence role within the finance sector so any ideas for projects would be very appreciated. Also what skills should I learn/refine, what books/resources should I be reading/using to put myself in a better position to land an internship. Honestly just looking for general tips on the steps I need to take atm. 

Thank you :)

",7,8,1737077814.0,analytics
1i2tt60,Do personal projects help in getting a data analyst role?,"I’m currently working as a data analyst which only requires some basic Excel and Power BI. I want to break into a role that works with SQL and Python as well (i’ve been self-learning about them). I’ve only been invited to job interviews of the roles that only require Excel & Power BI, which are what i’ve been doing in my current role. ",25,38,1737047470.0,analytics
1i3bqsj,Any websites where I can download books for analytics ,My uni requires me to buy books its kinda expensive is there any place or source to get these for free ,1,6,1737101800.0,analytics
1i380hi,"Degree in English with no previous data anlytics experience, starting learning statsitics and planning to move to tools, any recommendations for building up a resume?",entry job,1,12,1737086841.0,analytics
1i2xvyt,Chances of getting a job with a cs degree and projects ,I live in Orlando and am open to in office (but it’s not exactly a tech hub so remote would be preferable). Moving is not really an option due to marriage/kids/house. I’m 2 classes away from graduating and want to know if I should even bother or just change careers with how depressing the CS and all related career forums have been. Am I cooked? Does the CS degree hold any weight? I thought this was an entry level field but others say no so then what is? I think my personal goal is at most a year of job searching. Is this realistic in this job market? ,7,19,1737057763.0,analytics
1i2z986,PlumbingJobs.com ~ Sharing the analytics of my job board site (summary of how it's going after the third month),"On October 12th 2024, I launched PlumbingJobs.com, and this is my first update (January 2025) in what I hope will be a long journey.

To stay accountable and track progress, I’ll be sharing monthly updates about the site's stats, achievements, challenges, and my plans moving forward. While these posts are mostly to document the journey, I hope they’ll also be helpful to others, especially members of r/analytics who might be interested in learning the web analytics of a job board website.

If this post isn’t a good fit for this subreddit, I’m happy to remove it or move updates elsewhere.

The goal for PlumbingJobs.com is clear: to become the #1 job board for plumber jobs, featuring hand-picked opportunities the plumbing industry.

Let’s dive right in:

# Statistics update ~ 4th Quarter of 2024

|\-|October|November|December|
|:-|:-|:-|:-|
|Jobs Posted:|2|16|43|
|Paid Post:|0|2|2|
|Free Post:|0|1|2|
|Visitors:|72|138|1,164|
|Avg. Time Per Visit:|1 min. 24 sec|2 min. 15 sec|3 min. 41 sec|
|Pageviews:|196|308|2,590|
|Avg. Actions:|1.1|2.3|2.3|
|Bounce Rate:|87%|73%|40%|

I'm not a very technical guy and I don't know how to code. So the best way for me was learning to build it using Wordpress through YouTube. Also, I believe in the power of a great domain name, and the stats from the first three months have only reinforced that belief:

* **49.2% of traffic** comes directly from users typing the URL into their browsers.
* **48% of traffic** is from search engines like Google and Bing.
* The remaining **1.8%** comes from social media and other backlinks.

# Tech-Stack

Wordpress - Website + CMS

Gravity Forms - Form + PayPal payments integration

GeneratePress - WP Them

WP Grid Builder - For Grids & Cards

Clicky - Traffic Analytics

ChatGPT - Rewriting the job ad

Make - automating the flow

# Pricing Tiers and Early Wins

I offer three pricing tiers for job listings:

* **Free Listing**: Basic exposure for job openings.
* **Silver Listing** ($45): Greater visibility and placement on the site.
* **Gold Listing** ($95): Premium visibility and enhanced promotion.

To my surprise, my very first sale in October was a **Gold Listing**! That initial $95 sale was the motivation I needed to keep building. Later that month, I sold a **Silver Listing**, bringing my total revenue for October to $140. The same revenue was generated in December 2024, showing consistent early interest.

# Steps Taken in December

To boost SEO and add value to the site, I created a Plumbing Directory, featuring:

* Plumbing companies across the U.S.
* Their stories, contact information, logos, addresses, business hours, and more.

This directory serves as free marketing for these businesses and increases the likelihood they’ll discover my site and support it by posting job openings.

# Plans Moving Forward

1. **Social Media Marketing**: I plan to automate posts using AI to expand reach and drive more traffic to the site.
2. **Consistency in Job Postings**: I’m committed to posting 2–3 plumbing jobs daily to keep the site fresh and useful for plumbers seeking work.

Looking forward to grow this niche job board slowly but surely this 2025. If you have any questions, concerns, come across glitches - feel free to reach out, happy to chat.

Thank you all again, and see you in a month.  
[Romel@plumbingjobs.com](mailto:Romel@plumbingjobs.com)",4,2,1737061300.0,analytics
1i2vuna,Legit Contractor Companies?,"I get contacted by recruiters all the time and many of them seem like a scam. They barely speak English or they send a poorly worded email that goes right to spam. I was considering doing contract work, but it is difficult to determine which companies are legit. Does anyone have some suggestions for Companies that hire analytics, data science, and data architects that are legitimate and actually have contracts? ",5,3,1737052595.0,analytics
1i2y2j0,Aws vs Google data certificates ,Most posts I see are saying that the Google cert is pretty worthless. What about an AWS cert for an entry level candidate in business or data analytics? Maybe the data engineer cert or solutions architect? Or are the days of certs behind us in this job market? ,2,3,1737058223.0,analytics
1i32xb0,Perplexity with complex data aggregation,"I’m working on a project that uses AI to calculate complex statistics that don’t exist anywhere online—percentages and insights that typically take hours to aggregate manually. For example:

* **“What percentage of unicorn startups from 2024 had at least one remote cofounder?”**
* **“What percentage of developed countries with republican leaders have governments that cover 100% of healthcare costs for their citizens?”**

The tool generates these stats automatically, combining information from various sources using a controllable reasoning process.

My question to you is:

* Would this be useful for your daily work? Are there situations where you've been frustrated by perplexity failing to get a statistic you were looking for?
* Would you pay a premium for access to this kind of data, and why?

I’d love to hear your thoughts and ideas! Feel free to ask questions if anything sounds unclear.",1,4,1737071205.0,analytics
1i26ypb,Should I learn Python or SQL as a complete beginner to become Data Analyst?,"Basically the title, some are suggesting to begin with Python and some say SQL.

Can I/Should I learn both simultaneously?

P.S. I do not have any coding experience.",101,85,1736972998.0,analytics
1i2tkkl,Rotman MMA vs McGill MMA,"Hi,

  
So I've recently been given an offer for both McGill MMA and Rotman MMA programs. I was wondering what the pros and cons are for both and if anyone has any tips on which program I should choose to complete my graduate studies.

",2,3,1737046864.0,analytics
1i2vw45,Best place to find an Oracle Analytics consultant,"The businesses main application runs on an Oracle database.  We have Oracle Analytics v5.9.  I'm looking for an expert that could build us an executive dashboard in OA to show previous day, current day, YOY, etc and build us some automated reports.  

Where is the place to find a consultant for this?",1,4,1737052703.0,analytics
1i2p8lx,Generalization of Newton's method,"Hello, all,

This may be a stupid or naive question but here goes: I know the univariate version of Newton's method from having studied numerical analysis in grad school.  I am currently taking the Andrew Ng machine learning course and am about to learn the gradient steepest descent method and its application in ML.  (Learned about gradients and their properties in Calc 3 in college.)  Can someone explain to me why you would use the steepest descent method vs. the generalized multivariate Newton's method in optimization problems?  What am I missing?

Thanks,

K.S.",2,4,1737035145.0,analytics
1i2kpg6,Where can I find data or market research on the games Silver Spenders play?,"Hello everyone,

I’m looking for information or insights about the ""Silver Spenders,"" a term used to describe people aged 50 and above.

What types of games are they playing, and where can I find or conduct market research on this demographic?

Thanks a lot!",1,2,1737017320.0,analytics
1i2nnud,had a technical interview 2 days ago and having a panic attack because I haven't heard back,"I don't know why I'm having a panic attack because I think did really fucking bad in the interview, I got so nervous that I had to look up the syntax for the group by function in pandas, so why would I expect anything besides a rejection anyway

they started by asking me some theory stuff (discuss the differences between sets, lists, dicts, what's a tuple, etc) which I did really well on because of my math background. that sort of stuff is my strongest area, I can remember theory much more easily than I can remember precise syntax. then we did some pandas shit and I completely froze up for a second, had to google group by and something else, but I told them that I was like really panicking in the moment and freezing up. I was able to do some of the other stuff they asked for, transform a column and turn it into a new column, I optimized the work with a lambda function. I don't fucking know. then some more theory stuff, what's an array in numpy? which I sort of answered, it's a multidimensional vector or tensor, I also said I was pretty sure every element had to be of the same type, but I wasn't able to speak to the more technical components since I don't directly work with numpy often 

then there was a sql question, I did ok on the first question though it took a bit of prompting, second question I didn't understand it was something about primary keys and regular keys and I was like yeah I completely forgot what a regular key is, then the third question was to write a query which was easy

I told them at the end I don't think I did well. one of the interviewers said I did better than I think and the other said I was in ""the top percentile,"" I really don't know what the hell that's supposed to mean in context

now it's been two days and I haven't heard anything, I'm so fucking over this I;ve been looking for eight + months for a job and ive done so many interviews and nobody will fucking hire me and id on't know what to do because I can't get EXPERIENCE if nobody fucking HIRES ME",0,25,1737029954.0,analytics
1i286j9,How to drive business outcomes with data and AI products (price optimization),"We must not forget that our job is to create value with our data initiatives. So, here is an example of how to drive business outcome.

CASE STUDY: Machine learning for price optimization in grocery retail (perishable and non-perishable products).

BUSINESS SCENARIO: A grocery retailer that sells both perishable and non-perishable products experiences inventory waste and loss of revenue. The retailer lacks dynamic pricing model that adjusts to real-time inventory and market conditions.

Consequently, they experience the following.

1) Perishable items often expire unsold leading to waste.

2) Non-perishable items are often over-discounted. This reduces profit margins unnecessarily.

METHOD: Historical data was collected for perishable and non-perishable items depicting shelf life, competitor pricing trends, seasonal demand variations, weather, holidays, including customer purchasing behavior (frequency, preferences and price sensitivity etc.).

Data was cleaned to remove inconsistencies, and machine learning models were deployed owning to their ability to handle large datasets. Linear regression or gradient boosting algorithm was employed to predict demand elasticity for each item. This is to identify how sensitive demand is to price changes across both categories. The models were trained, evaluated and validated to ensure accuracy.

INFERENCE: For perishable items, the model generated real-time pricing adjustments based on remaining shelf life to increase discounts as expiry dates approach to boost sales and minimize waste.

For non-perishable items, the model optimized prices based on competitor trends and historical sales data. For instance, prices were adjusted during peak demand periods (e.g. holidays) to maximize profitability.

For cross-category optimization, Apriori algorithm was able to identify complementary products (e.g. milk and cereal) for discount opportunities and bundles to increase basket size to optimize margins across both categories. These models were continuously fed new data and insights to improve its accuracy.

CONCLUSION: Companies in the grocery retail industry can reduce waste from perishables through dynamic discounts. Also, they can improve profit margins on non-perishables through targeted price adjustments. With this, grocery retailers can remain competitive while maximizing profitability and sustainability.

DM me to join the 1% of club of business savvy data professionals who are becoming leaders in the data space. I will send you to a learning resource that will turn you into a strategic business partner.

Wishing you Goodluck in your career.",6,8,1736976163.0,analytics
1i27wsy,Which database certifications should I get for working with databases?,"I am really doubtful since there are a lot and I see the database administator role in many of them. Could you please give me a hand out?

",5,5,1736975470.0,analytics
1i25z4m,Looking for Yearly Sales Dataset for Myntra,"I’m looking for a dataset that contains yearly sales figures for Myntra, preferably covering the past few years. 

I’m working on a project that analyzes trends in the Indian e-commerce sector, and this data would be incredibly helpful. If anyone has access to such datasets or knows where I can find them (free sources preferred), I’d really appreciate your help.

",2,2,1736970414.0,analytics
1i2aa2t,JD skills,"I came across this, “Build logistics plan and own logistical coordination for high complexity learning experiences including physical and digital requirements. Manage administrative data and processes for high complexity programs, including LMS entry, learner communications, enrollments, and assignments. Update master training calendar as needed based on program scheduling Monitor performance and effectiveness and perform quality assurance testing on high complexity in-person and digital content, identify and prioritize areas of improvement; design curriculum solutions to improve learner impact Facilitate various topics for high complex in-person and virtual content delivered to leadership and teammate audiences Other duties as assigned”

What do I need to learn to satisfactorily perform these tasks? ",0,3,1736981694.0,analytics
1i196lv,Frustrated as a Data Analyst: Are we just storytellers?,"I’ve worked in five different roles in the data field, and across most companies, I’ve noticed a common trend: data analysts are primarily tasked with producing dashboards or generating figures based on very specific business requests. However, when it comes to tackling broader, more open-ended questions, things seem to get more challenging—especially in companies where Python isn’t part of the toolkit.

In my current company, for example, we’re expected to find new insights regularly, but everything is done using SQL and Tableau. While these tools are fine for certain tasks, doing deeper data exploration with them can feel tedious and limiting. We’re also not encouraged to use statistical knowledge at all, since no one on the team, including our boss, has a statistical background. It feels like there’s no understanding or value placed on applying more advanced techniques. We just need to have exceptional data storytelling skills + put up some nice figures which confirm already known intuitions.

Honestly, I’m feeling a bit frustrated. I can’t help but wonder if this is common across the field or if it’s just the nature of certain industries or companies. Would things be different in a more tech-focused company or in a dedicated data science role?

What’s your experience with this? Is this a frequent issue in your work as well, or does it vary depending on the company or team? I’d love to hear your thoughts.",178,97,1736870187.0,analytics
1i1lu49,Where is the DS career headed?,"Just saw the Rogan / Zuck podcast on how AI is changing most tech careers. I’m just now transitioning in a DS career, getting well versed with the ML algorithms and Gen AI concepts. For the more experienced folks in the field, how is the DS career specifically going to change in the coming years? How can we try to stay on top of all the changes coming in? 

PS: This might be more of a question for the r/datascience sub, but unable to post question there. ",21,38,1736903448.0,analytics
1i1klda,YouTube channels for background noise?,"So for IT it's easy to throw on any tech youtubers video for ambient noise relevant to the field and occasionally pick up some useful information. I understand it's easier to make content for IT, but  I'm wondering if there's anything similar for analytics that isn't just a python tutorial or a how to on landing your first job.

Thanks for any suggestions.

Also, if there's a better place to post this I'd be glad to move it there",7,6,1736899832.0,analytics
1i1j7vr,Predictive Analytics Cert?,I'm curious if I should get a certificate in Predictive Analytics. No one on my team or in my organization currently offers reporting like this and I would like to start. I manage a small team of analysts specializing in financial and operational reporting and analytics. We do most of our analytics in Tableau & excel but I'm trying to think ahead and there are plenty of use cases for predictive analytics. Any suggestions on who to get certified through? Has it been useful/successful at your organization? Thanks in advance!,7,6,1736895947.0,analytics
1i1vjy0,Title: Should I Take an EPR Support internship While Aspiring to Be a Data Analyst?,"Hi everyone, I'm a bsc Computer Science graduate  in 2024. I want to become a Data Analyst. Despite applying to many roles, I haven't landed an opportunity in the IT field yet. I've received a 3-month offer for an EPR support internship and the company is related with marine industry. Should I take it or focus solely on upskilling and searching for data analytics roles?

They are giving me stipend of 10k per month for this internship.
",0,9,1736941190.0,analytics
1i1s73x,What is the best practice for number of events added to an ecommerce website?,,1,3,1736925795.0,analytics
1i1njjf,How can I create a function using values from two different data sources in Looker Studio?,"In my report, Data Source A is giving me the fields A, B and C. 

Data Source B is giving me the fields D, E and F. 

There's a formula behind each of these fields.

I want to create an additional field which would be pretty much (A-B) / D, but that is not possible because they come from two different data sources. 

If I select them and try to choose ""Blend data"", the option is greyed out saying ""You can't blend with an already blended chart"". So I'm currently lost if there's anyway to display this information to my client without manually calculating this.

Alternatively, is there any any to just use the fields as values, instead of replicating the massive formula that's behind each one of them?",2,3,1736908679.0,analytics
1i1efq3,New grad jobs,"Is January a bad time to look for jobs? Recently graduated in December but the issue I’m having is that there’s not that many jobs to begin with. LinkedIn is only showing about 20 - 30 jobs. Most of them are for senior roles too.

I’m not sure if I’m competitive enough for this job market tbh. I only have 1 internship utilizing sql, excel, and some data visualizations. The rest of my resume is some other unrelated job and a couple of projects on tableau public. ",6,11,1736883447.0,analytics
1i18boz,How do people progress from an Academic environment to real world? ,"I recently graduated from an MS in Business Analytics program and had classes in Data Analytics, Stats, Machine Learning, R and Python. The courses covered things but some things were pretty basic. Like we covered SQL but we did not do queries involving multiple joins or CTEs or complex stuff. Rather simple individual queries on a chosen dataset, things like that. It feels like we did learn but did not go too far or deep like people do in industry or real jobs. We did not work with things like Qlik or do ETL. For Excel/Sheets, we had no class and just did some basics, while I have seen some jobs require proficiency. All in all, I feel like classes and class projects might not be enough. Or is this enough to get started? Because I have seen data roles are individual contributor roles where you are kind of on your own. How can an entry level person manage this straight out of college? Is it possible? What did people with experience do or what did your journey look like? ",14,23,1736867902.0,analytics
1i19n03,Can I use Leadsnavi as a lightweight alternative to GA for web analytics?,"I have both GA and Leadsnavi on one of my client’s websites. We are using GA for analytics and Leadsnavi for identity resolution and lead generation. The web pages have gotten a little slow and I’m considering switching to a much lightweight analytics tool. I have tried MS clarity but there is not much difference there either.

Leadsnavi has analytics too but I’ve never used it for that, we just use it for identity resolution. I’m considering doing away with both GA and MS clarity and let Leadsnavi handle the analytics too.

Will it be enough or do I need to continue looking for alternative analytics tools?

Note: It was the client’s idea to add Leandsnavi for identity resolution and lead generation, my role is to set up the infrastructure, he uses the tools himself, that’s why I want to know if Leasnavi is good for analytics from a business point of view.

",11,2,1736871377.0,analytics
1i1osow,New to SEO: I need some help with deciphering my GSC graph!,"Does anyone have any experience with your page showing a sudden decrease in clicks and impressions? I optimised the blog on 26th December, which showed an increase for 1-2 days, but then hit a 0 on the third/fourth day. Position wise is one now.",1,1,1736912731.0,analytics
1i1akf5,Need help deciding which route to take for transition into DA,"Hi everyone ! 

I bet this is a pretty much always asked question and sorry for asking it again but i would like some answers specific to my situation. 

First lemme say i live in France for some context, so things are a bit different here. 

I have 2 masters in engineering, one in Material Science and the other in Space Systems, from 2 highly recognized schools (+ i did my final year at Imperial College in the UK). 

I have worked 2 years as an R&D engineer in microelectronics, doing 40% of theorical physics and the other basically doing the job of a data analyst. The firm i was in had no data person whatsoever so i kinda became it and built a whole application in VBA to extract, transform, load, analyse and dashboard data coming from our devices tests. Did some python and Power BI dashboarding while i was there. 

 I am saying all this because i keep reading posts where ppl say that a degree is the most imporrtant thing in the field and a bootcamp in case you have the diploma will help but not as much. 

So i have a degree, in a related field, but we kinda did everything you do as a DA (or even DS). A lot of proba, stats, machine learning, math, python and such...

I quit my job a few months ago now and i'm lost between doing a bootcamp (and pay 5k+ for it) to learn more DA skills and have the certification or going the self taught route and build a learning path to be as close as the bootcamp's one, using DataCamp or Maven analytics resources.

On the one hand, self-teaching would save me a lot of money, and there’s a ton of free or affordable resources out there. On the other hand, bootcamps offer access to career coaching and industry networks, which could be invaluable for landing a job. A structured curriculum might also keep me on track and ensure I don’t miss any key concepts, plus they often provide real-world projects that would help me build a portfolio.

So i woul really need your advice here and what you think would be the best choice considering my background and situation. 

  
TL;DR: I’m an engineer with two master’s degrees and two years of data analysis related experience trying to decide between an expensive data science bootcamp and self-teaching. Looking for advice on which route might be better for breaking into data analytics

Thanks a lot ! ",2,8,1736873736.0,analytics
1i19sg5,Drone Data Analysis Projects,I recently got a DJI Tello drone and I am very passionate about drones. Would analyzing battery performance over time or doing flight data analysis be interesting projects? I was thinking I could use the SDK to get data from the drone and put it in the database. Then from there use SQL and maybe Looker Studio to manipulate the data and create a dashboard or some visualizations. Could these be interesting passions projects? Any recommendations or has anyone done something similar? ,2,2,1736871766.0,analytics
1i0l0i5,Projects that got you A job,"If you don’t mind sharing, what project got you an entry level job? 

Background: I want to transition from teaching. I have a degree in math and computer science. I have completed Google Data Analytics on coursera. I currently have 2 personal projects completed. One is analyzing my finances using python to automate things. The other is analyzing student tests performance with excel. 

I want my 3rd project to be more business facing and impressive. Ive looked on Kaggle for data sets but the data seems basic. Like i can find average, increasing or decreasing trends, max and min but if i was a hiring manager i would not be that impressed. 


Tldr: 
I finished learning the basics and have 2 simple projects. I want to work on a project that would impress people but i am having a hard time finding interesting data sets. What project impressed your hiring manager enough to get you your first job? 

Thanks!",77,33,1736793429.0,analytics
1i0v2kv,How Big is Your Team?,"I’ve worked in analytics for a few years, starting off as a Sales Operations Analyst to now working as a Business Intelligence Analyst for a Fortune 50 company. 

Throughout the duration of my career, I’ve mostly worked on a team where I’m the only analyst and the only one responsible for data related projects and reporting. From the rhetoric I’ve seen on Reddit and having conversations with other analysts, there doesn’t seem to be many fully developed analytical teams within companies. 

Is this true for most businesses? Do most companies generally keep a small analytic team if not solely relying on one person? ",17,24,1736819726.0,analytics
1i15z8z,Need interview preparation sources,"For analyst interviews, can you suggest some good sources where I can practice questions on SQL, Python, Pandas, etc.?",2,9,1736861104.0,analytics
1i0u7q9,Analytics communication and writing style,"I've long struggled with writing style. I'm usually either too verbose or too concise. Rarely find a spot in the middle.

I've found some success with writing anything in a work/analytics doc as TLDRs, separating into bullet points, adding a table, markdown, having a consistent format like 2 sentence insight + chart + chart link, appendix section, using a general template for specific repeated projects. But in Slack or more detailed analysis docs, it's harder. The audience can be a large range, technical and nontechnical, executive and non-exec, sometimes only 3-5 people, sometimes 30-50.

Writing takes me a disproportionate amount of time. I'll spend 20 mins tweaking a Slack message and still edit it a min or two after I send it because I forgot this and that or I edited so much that I left an extra word in.
 
What to do? Any useful analytics writing guides or something that you can recommend?

I wish there was a leetcode for writing. I know I can use ChatGPT for suggestions but this is so niche and I want to be able to do this going forward with more care in the moment.",6,10,1736817199.0,analytics
1i1dxku,Is 74k too low for new grad?,"I got an offer from a company that I've been interning for 2 years. The offer requires me to move to a State that I don't really like. The job is quite boring, but the pro is that I get to work remotely. Everyone at the company is quite chill and nice. The job is not too stressful and the company really values wlb. They also offer tuition reimbursement

The only thing I didn't feel happy about was the pay and the fact that I have to move to a different state. I don't know why I have to move, if they let me work remotely. I've been applying to other jobs and in the interview process with couple companies. Any advice what I should do moving forward?

I know the job market has been really difficult, so I'm grateful for my offer but I still want to know if there's anything else I can do.",0,38,1736882181.0,analytics
1i0oyno,Can I work in analytics with Master’s in Computer Science and Bachelor’s in Business? ,"Hey yall. I’m a 23 year-old with bachelor’s degree in Business Management. I recently got accepted to masters in CS in the UK( I actually applied to Data but they rejected it and offered me CS instead). I am quite interested in working in IT business analytics or data analytics. The thing is I don’t know if I can start working in those fields if I get masters degree in CS. I’m scared that it can cause me problems because it is completely different from my bachelor’s major. Can anyone give me advice? If I study CS what are my career prospects in business related field? Will it affect my career negatively?
Is it better to wait for the next year and apply to Data analytics at other universities? ",6,19,1736803168.0,analytics
1i0jsim,MIS/CIS or Data Science Degree,Hello everyone! I am currently finishing up my general studies in a community college and plan on transferring soon but not sure what to go for. I was planning on going for a data science major but started learning more about MIS/CIS degrees. I have to say I really like the versatility option of that but I most likely would still like to look for a job in the data science field when done. Would it be a waste to go for MIS/CIS degree? Is it a wiser choice since It would give me more options when i'm done? Another thing is I'm not sure if the data science program is just a cash grab from the school since it is fairly new. Anything helps!,3,17,1736790483.0,analytics
1i0mmj3,I want to enroll in Analytics,"Hello! I have an undergrad in clinical psych in India and want to enroll in georgia tech masters in analytics, online or offline both should be fine, what should I do to be proficient for this course and what requirements will I need to fulfill and would I be eligible? Would someone like me get in? I have decent knowledge of statistics and have dabbled into SPSS and a little bit into R. And what prospects would I be looking at after the completion of the course? Any advice would be very much appreciated. ",2,3,1736797392.0,analytics
1i0l1yo,Need Resource for Speeding Up Power BI Data Refresh from SharePoint Excel Files,"Hello,

I'm a Jr. Analyst working with Power BI and SharePoint, and my manager tasked me with fixing slow data refresh times for one of our reports. Currently, we're connecting to Excel files stored in a SharePoint folder using a basic web connection, which seems to be the bottleneck.

My manager requires that the refreshes be done in Power BI Desktop, and the reports to then be uploaded to the Power BI Service. Additionally, we rely on Power Automate flows to handle some parts of our workflow. These flows open up websites, download data, and upload it to SharePoint. However, this setup requires our laptops to remain running for the process to work, which isn't ideal for a nightly refresh scenario.

My manager suggested exploring the SharePoint API as a potential solution to improve refresh performance. They don't mind how it's done, as long as the data refresh speeds up significantly.

I've already looked at a few tutorials and articles, but I'm hoping someone with experience can point me to the best resources (videos, articles, or guides) to optimize this process. I'd appreciate help in cutting through bad advice and finding the most effective solution.

Thanks!",1,2,1736793531.0,analytics
1i0ku21,Few questions ,"1.Is there any data analytics course or tutorial that is worth learning and that is free ?
2. Google data analysis course- how long does it take to finish the course- any experience?",0,8,1736793002.0,analytics
1i0jxzk,Transitioning from Accounting Specialist ,"Hello there,
I am transitioning from Accounting Specialist to Data Analyst. I have more than 8 years of experience in accounting and Bookkeeping. I am freelancer and have Top Rated Plus badge on Upwork. My hourly rate on Upwork is $8-10. I just got IBM Data Science Professional Certified from Coursera.
My question is:
-To be more competitive, what should I learn more and from where I should learn it? (I got Coursera certification through financial aid. So I have budget constraint.)
-What should I expect in my earning when I enter the market as financial data analyst?",0,2,1736790859.0,analytics
1i0059n,Entry salary expectations?,"I know there’s been a few post regarding the same topic but everyone’s qualifications are different. I am entering my last semester and am graduating with a bachelor’s degree in business analytics. I’ve done 2 summer internships (about 7-8 months total) during my time in college. Some qualifications I have are mastering excel (who hasn’t at this point), good experience with power BI, JavaScript, python, tableau, and sql. So with that being said what are realistic salary expectations I can have for entry level jobs giving my qualifications?",20,54,1736724334.0,analytics
1i0euyy,Quick question for everyone,How do you make sure your content stands out in a crowded niche?,0,11,1736777638.0,analytics
1hzx2fe,Any good resources for case study? ,"Any thing that helps regarding a case study or a case interview - be it tutorials, example questions, tips, websites, YouTube channels etc. 

Thank you! ",3,1,1736716218.0,analytics
1hzxb4s,Considering business analytics post grad - Biz Econ Major,"I am a recent UCI grad with a Business Economics degree. I recently got laid off at my job as a business development associate. I am looking to move to a more analytical role and am trying to learn about data and business analytics. I have many questions on the topic and would appreciate any advice. 

1. Is a BA in Business Econ enough to start applying to business analyst roles  
2. I am currently taking a Google certificate for data analytics, but am finding that it's not teaching me the tools I need like SQL and Tableau. What recommendations are there for these certificates.   
3. I am considering applying to a master's in business analytics, but as the applications are due relatively soon I am unsure if this is a smart decision to rush through these right now. I would also like to consider a flex MBA/ MSBA as I would like to be working full-time   
4. As I am currently looking for new jobs, what is the best way to prioritize my time, and the best roles I could apply to? (can provide more info if necessary based on previous experience in more management roles)  
5. I am also considering project management. However, I do like the idea of being able to guide business decisions for a company (specifically the games industry). What does the day-to-day as a business analyst look like? I do prefer the idea of traveling to conferences and meeting with other people, but I hate traditional sales. I love the idea of working on a project, but don't want to be sitting without interaction all day, and also find coding to not be my favorite thing in the world. I enjoy a PM position but I feel I would not be as impactful in the overall decision-making for a company.",1,3,1736716832.0,analytics
1hz9kpi,Just landed an internship interview at BMW! Any advice?,"Its in 2 days and I really want this internship, can you experts give me any advice?

Edit: its online btw ",45,23,1736639744.0,analytics
1hzb2mu,how to transition into healthcare analytics,"I graduated from Michigan this past May and started working at the Mayo Clinic as a clinical scientist. I seek to exit the lab and work more in Healthcare Analytics or anything adjacent to this position. Given my current position, can I get some pointers on how to be a good candidate? 

  
I am seeking to be proficient in Excel, SQL, among other things. Thanks!",6,8,1736644074.0,analytics
1hz53m9,"Is ssms, SSRS and powerbi out of demand? ","I'm in USA and have been working with the above 3 for 8 years in healthcare. I'm looking to make a move as the product will be sunsetting soon unfortunately in a year. I'm fully remote for 4 years and have been applying for the past 1 year. Haven't even received an interview with the techical person, which is very disappointing.

Should I pivot for remote opportunities to data engineering or maybe learn Tableau? 

 ",6,27,1736627510.0,analytics
1hyhkdj,Is College Still Worth It?,"Hello,

I am a Sophomore in College and was just wondering which majors are useful in the current market. I am currently a Data Science Major, and I like it for the most part, but the tech job market is super competitive right now. I want to eventually get a job in analytics or something in big data, however, I've heard so many horror stories that I'm worried about going on about college and not being able to make it out with a job. Please let me know.

Thank you.",39,57,1736549995.0,analytics
1hyy26u,Excluding internal traffic,"I setup internal traffic filtering by the following method.

Data collection & modification > Data Streams  > Configuration > Define internal traffic

Using the IP address is in range (CIDR notation): 5.565.3.226/337 (not a real IP)

I have confirmed with the IT department the IP and hence know it is correct.

I then activated the Data Filter.

However, I am still seeing hits from the very small town the business is based. Hence I am pretty sure those are internal hits.

Is there a ""better"", more reliable way of filtering out internal traffic? ",0,2,1736608759.0,analytics
1hz20tc,Are you looking to crack into data domain.,"Hello everyone are you looking for data domain career. Lot of people are struggling and have no proper guidance and resources.Almost from 4 years in data domain and i can say that with right skills set you will definitely crack the data domain whether you are looking for data analyst, business analyst, financial analyst, product analyst. Some of the skills are common in this but yes enough to crack. Feel free to guide you and help you if you want any guidance.",0,3,1736619342.0,analytics
1hyv7yo,Zuck,Did you guys hear that in meta ai will replace mid-level engineers sometime this year? How do you guys think it will impact analytics?,0,6,1736599850.0,analytics
1hyc3oc,"Is it true that the field of analytics is over saturated? If so, what are other options or roles for one’s interested in analytics?","
In one of my previous posts someone commented that analytics is over-saturated. If that’s the case, what are other roles someone who is interested in analytics can look into ? I’m an MIS major at my undergrad college and my coding skills or skills necessary for analytics are below the bar for a tech/analytics role and I was wondering if analytics is actually over-saturated what are other roles I can look into ?",1,51,1736535947.0,analytics
1hy3van,Switching to Marketing Analyst from PPC,"Hi all, I went through previous similar questions and got some answers, but I am starting a new thread due to my somewhat specific situation. I work at a big e-commerce fashion company as a PPC/media activation manager. I have solid experience in digital marketing, having worked in media agencies before this job.

I need the change and don't see a future in PPC and media, to be honest, I would like to switch to Marketing analyst. I don't have any formal education from that field in maths and tech, my degree is in marketing and public relations. However, I always loved numbers and stats and I believe I can interpret data to real business conclusions and, let's say, real-life use.

My main question is - Do you think that's the right direction and more importantly how I should do it? The reason I think this is very doable is that my current company encourages career change and I could proactively reach to the Analytics team to have me in mind for that role when they need it (ofc when I am skilled enough) What do you think would be the best way education-wise (courses, specific tools, maybe go back to some math first) and how I could leverage a lot of marketing and business data I have access to to make some projects for myself and gain experience? Thank you!

",7,7,1736514039.0,analytics
1hxyy3j,Required work logging/tracking,"Role: data analyst (sql reporting, projects, validation, research, etc)

Recently required to start logging everything to the half hour. Example of one day below:

Meetings: 2.5 hours 
Project A: 3 hours 
Emails/issues: 1 hour 
Project B: 1.5 hours 


Does this seem a bit excessive? I get the need for resource planning but I haven’t heard of other teams doing this outside of an agile environment. Looking for ways to make this more efficient if I’m doing it everyday.

Any thoughts, experiences, or advice welcome.",10,13,1736493236.0,analytics
1hy4jbu,Google Apprenticeship 2025 - Status Discussion Sub,,0,3,1736516176.0,analytics
1hxktf2,Freelance/Practice Data Projects,"Between jobs now and trying to keep my data skills sharp. I have tried working on a few sports and gaming data projects, but wondering if there is anything out there that has real life examples with prompts that can help me keep improving and keep my skill level up. Trying especially to stay up on excel and SQL as well as learn about PowerBI. Thanks for any help. ",10,2,1736450564.0,analytics
1hxd98p,Is it possible to transition to this career?,"I graduated with a degree in Computer Science back in 2023. I have not found a job related to my degree. My internship was only a position as a QA Analyst which mostly involved testing software.

The problem is I'm not really passionate about CS. I have tried working on side projects but quickly lose interest/motivation in completing them. I have not really tried to find a job in CS hence why I have not held a position related to it since graduating. The job market for CS new grads is also really difficult where I live right now (not saying data analyst is any easier, I don't know).

Data Analyst has been something I've been interested in and I'm not sure how I can get my foot out the door. What should I do before applying for entry level positions to increase my chances? How long of a commitment do I need before I have decent chances at landing an entry level position?

I know the obvious answer is to go back to school and get a degree for it, but that isn't something I can do.",22,29,1736430803.0,analytics
1hwntvr,Resources to Learn APIs,"Hello Everyone,
I’ve been working as a data analyst for a little over a year now and have never needed to know how to use APIs until now. Does anyone have experience learning how? Any recommendations?",60,28,1736352858.0,analytics
1hwydwt,is it overkill to be asked to provide 5 different references 3 mgrs 2 coworkers for a senior/mgr role at a non profit?,No judgements or assumptions pls kindly stick to answering the question I just want to know if this is common. Thank you!,6,25,1736378948.0,analytics
1hx1nqx,Live Gaussian Filter Line (Zero Lag/Loss Average) for Time Series Data,"I have spent the past few months developing a formula (using python and linear regression models) for my time series data to generate a ""live"" Gaussian filter.  This way I can apply it to incoming data and have a smooth, zero lag, average for readability / further analysis. The best I have been able to accomplish so far is 94% correlation between my line and the original gaussian line...

I am looking for at least 96-98% for it to be useful in my case. There is still information to be extracted from the features I have derived for this calculation, since they are between 20-30% correlated with that last 6% error, but I am absolutely stumped and tired... 

Does anyone know where I can hire someone, who to hire, or where I could put out a prize, to come up with some kind of equation/function that is correlated with the error?",3,4,1736388019.0,analytics
1hwtr8j,Analytics Communities - Thoughts?,"When you look for subreddits, LinkedIn Groups, Slack channels, etc. that you engage with outside of work/on your free time at work - what are you looking for?

Hoping to help members, mods & just really the whole analyst community grow. Lots of appetite to help each other and build relevance in our beloved profession, I thought it would be interesting to see what people have to say on this.

Some options to get the conversation started:

* Career Advice
* Educational Resources (certs, upskilling, etc.)
* Networking
* Fun/Memes
* Vetting About Boss/Executive Stupid Requests
* Showcasing Work/Portfolio Building",2,16,1736367379.0,analytics
1hx1dl5,"Is this survey biased, or am I overthinking?","I'm reviewing survey data for one of our products. We have two versions: a digital version, and a printed version. Another team ran a survey because the print version is very, very expensive to publish, and we want to know if it's worth it. They gave me the results yesterday, and it seems there's a preference for print (82% want the print one to continue, 81% prefer print) -- until I asked **who** they surveyed. **They only emailed print users**!! Mind you, these people also receive the digital version (our lists double up), but isn't that iffy? Like, asking the people who use the thing if they want that thing to continue?? 

I think this is valuable data, but it needs more context (like offering a split survey wih one path for people who only use digital, and one for people who get both). Tell me why I'm wrong. ",1,0,1736387225.0,analytics
1hwefma,I’m a statistics student looking for a internship ,"What would you guys suggest I do to be ready or even a worthy candidate of finding a data science internship?

I am a junior at my college and I am 31 years old. 

Any suggestions would help. ",11,17,1736321480.0,analytics
1hwkhx6,Pima Native American diabetes dataset,"I have a question regarding this dataset because I have seen logistic regression models created from it with varying degrees of success.  Specifically, there are two fields that I think may be collinear but I am not sure.  One is [body] weight, and the other is BMI, which is a function of body weight and height.  I think it would make sense to trsnsform the BMI column so that it only contains height, because body weight is already represented in the data.  Thoughts?

Thanks, 

K. S.
",3,3,1736343713.0,analytics
1hw55l5,Thoughts on Thoughtspot?,"I have been using PowerBI and Looker with almost all of my clients. Just got a new client and they use Thoughtspot for analytics...anyone have any experience? Supposedly the big selling point is ""search-driven analytics"". I didn't really see a whole lot of that.",7,6,1736291967.0,analytics
1hw2jfo,"Free SQL, Tableau, Python certs?","Are there any free SQL, Tableau, Python certs I can get to better my resume for a data analytics job? ",6,7,1736285271.0,analytics
1hvbzxl,Got my first Analyst job!,"Just got my first job in analytics! Some background I have been doing a little bit of data analysis for a relative who owns a small business. Nothing crazy, not a lot of data but I used Excel, sql and powerBi to analyze it and to refine my skills. After getting my degree in CS I have been applying to jobs for 4-5 months and it finally happened! 

Keep pushing!
",354,80,1736202888.0,analytics
1hw3pq0,Career Path Advice Needed: Navigating Between Software Engineering and Data Science,"I have been working at my current company for three years. During my first year, I was involved in designing database schemas, building our database, and undertaking a data science project focused on building a customer retention model using data analysis and machine learning algorithms like scikit-learn. 

Eventually, I transitioned into a software engineering team and now work primarily as a software engineer, specializing in Python and SQL. I build applications using AWS Cloud Services such as Lambda, API Gateway, Step Functions, and DynamoDB etc. My role involves various responsibilities, including designing and implementing APIs, writing API and general documentation for our product. Being at a startup, I also lead many project areas and manage timelines for external integrations.

I’ve been a software engineer for 2 years now and have a solid foundation in Python and engineering because of this. However, I still have a strong interest in data science and analytics. I have a Master’s in Data Science as well as an MBA. After working for three years at the company that I’m currently at, I am in a complicated situation because while I have an interest in analytics, data science, and machine learning, I do not have a lot of work experience in these areas. I’m at the point where I need to think about the future and figure out what my next steps will be.

One thing I enjoy about being a software engineer is being able to build new things. Each sprint I get to work on something new and contribute to a part of a feature for our application. One looming problem is the fact it will be tough in the current job market to get a data engineer or ML role with little experience. I am at a crossroads trying to decide if I should continue on my current path in software engineering or dive deeper into data areas and leverage my education.

Any suggestions or experiences you could share that might help me navigate this decision would be much appreciated. ",3,3,1736288244.0,analytics
1hvy6g9,Advice for technical interview for Analyst position,"So, this is the first ever Analyst job I applied to. It's in London (if at all it matters). 

My previous education is not directly related to data analytics as I did masters in Psychological research, where I learnt all the technicalities of research and also spent a lot of time learning SPSS, R etc. So I can do data analysis.

I have had 2 interviews with this firm before - one with recruitment manager and the other with firm partners. In the Partners interview we talked a lot about research and different methodologies, since this job includes research as well (not just data analysis)

Now, I have final interview scheduled this Friday (10th Jan) and I am told that it is with team members and team lead. So I am assuming it will be a lot more technical. Can you all please help me with what kind of questions I should expect and how to effectively answer those? I really want this job, so I am prepping my best for it.

Thank you so much in advance!",4,11,1736274455.0,analytics
1hw74ry,GA4: How to see which pages are triggering a specific event?,"I know this is a very dumb question, but I am fairly new to analytics, so please forgive me. :/

  
I am working for a not for profit organization that has hundreds of pages on their website. Their website has a Donate button on the header which will be present on every single page you visit. Once you click in this button, a pop up appears, and the the URL will be kept the same, but ?form=donate-(countrynamehere) will be added. 

The pop up will collect all the necessary data required to complete the donation, and once you finish it, you get redirected to a page named ""Thank you for your donation!"". I am not sure about the URL for this page.

I can see that all the donations are marked as a purchase event on GA4. I can also see that this page named Thank you for your donation is the page that generated most revenue/purchases, but this is just the thank you page - I'd like to see what the users were actually reading that convinced them to click on the donation button and complete the purchase event?

  
All the help is deeply appreciated. :)",1,5,1736297454.0,analytics
1hvyd5s,Not another question about how to get started as an Analyst…,"Or even about what certifications would
help.

I’m already a working Senior level implementation analyst (Adobe Analytics specifically), but one that doesn’t utilize LinkedIn as someone likely should. 

I revamped it slightly when I was on the job hunt in 2022/2023 but have done zero with it since. My previous role wasn’t exactly focused on Analytics so I’m not too sure where to start. 

Anyone have any tips, suggestions or tips whatever I’d love to hear from you!",1,6,1736274916.0,analytics
1hvs49j,Remote jobs in europe,Is it possible to get a remote DA job without live in Europe. I live in Asia.  Is anyone working in remote? Any tips are highly appreciated. ,1,3,1736258658.0,analytics
1hvcrr6,Company Changing My Job,"Hello all,

I recently got hired as an analyst for a consulting company (I am the only DA at the company). My job description was oriented around data analytics and so far I've been doing that consistently for half a year. Lots of programming, good amount of statistics, and data wrangling. I've loved it thus far.

Now the company is restructuring and they are changing my job title to consultant. They want to standardize roles and job descriptions for everyone. My new description is way more general now and includes a lot of non-data related tasks. To be frank, my role has been dumbed down. I don't want to get into too many specifics, but it's now a lot of project management/report writing/admin assistant tasks which are things that I wanted to avoid when I first got hired. Data is still a part of the job, but it seems less so.

The transition hasn't been made yet, and there will be individual discussions about the restructuring. My question is how should I approach this? Am I being too pessimistic/cynical? Should I start looking for a new job despite being here for a short amount of time? Essentially I am asking what you guys would do? I have about 2 years worth of experience so I am still entry-level I suppose.",7,6,1736204903.0,analytics
1hv22cq,Almost-40 and want to switch careers to analytics,I work in Advertising Operations (campaign management) and desperately want to switch to a career where I’m working less with clients and more with data. Where do I begin? What qualifications/certifications do I need for DA? I plan on looking to make an internal switch to get my feet wet before trying to apply externally.,22,24,1736178573.0,analytics
1huzss4,Senior Web Analyst to Marketing Analytics Manager,"I have been working in web analytics for roughly 4-5 years and have worked my way into a senior web analyst role. I have yet to manage any employees but have managed tasks, projects and driven insights/results.

I am wondering how to break out of the analyst loop and into a manager role. Making the leap into management without management experience is a conundrum. I have a bachelors in communication and am self-taught with all technical knowledge.

How did you break into the marketing analytics manager role?",14,8,1736172419.0,analytics
1hv8bze,Data Analytics Externship red flag?,"So I recently joined an Externship to gain more practical experience but saw AI was a big part of it. 

“Over the next 12 weeks, you'll work on projects that will sharpen your skills in analyzing both quantitative and qualitative data, understanding customer sentiment, and leveraging AI to generate actionable insights.“

“Synthesize your insights into a final presentation, where you’ll use AI to deliver data-driven recommendations”

The term AI is used throughout the whole curriculum. Maybe it’s just me but it seems a bit lazy like there is a section called generate presentations and insights using AI.  Am I overthinking it? I know AI is the future but program feels superficial with so much AI being involved. Would appreciate insights. 

",3,9,1736193891.0,analytics
1hutndk,I built a tool to get quick insights before data visualization,"Hello everyone,

I have been working on an AI tool which generates good visualization and gives quick insights on your CSV file. 

Hi everyone,

As someone who frequently works with raw data, I’ve always found the initial steps of analysis (the cleaning, exploring, and understanding phase) time-consuming and often repetitive. I wanted something faster, simpler, and more intuitive, so I decided to build a tool to solve this problem.

Introducing **CSVIZ**, a lightweight app designed to help you get quick insights from your CSV files before diving into full-fledged data visualization.

# Key Features:

* Automatically highlights trends, outliers, and key statistics in seconds.
* Provides instant previews of possible visualizations based on your dataset.
* Simplifies data exploration, saving hours of manual effort.

Whether you're a data analyst, business user, or anyone who works with CSV files, CSVIZ helps you move from raw data to actionable insights without wasting time.

I’m currently launching the **beta version** and would love feedback from this community! If this sounds interesting, feel free to check it out or share your thoughts. Your input would mean the world to me.

Let’s make data exploration simpler for everyone! ",15,17,1736148453.0,analytics
1hurqt0,Data Governance - What do you do on your day to day? ,"My company is creating a new dept for data governance, but I don't know anyone that has experience in it, so does anyone here have some exp regarding this topic? How's your day to day? What type of projects you have? 

Thanks! ",22,4,1736140949.0,analytics
1hv8my7,Andrew Ng's Coursera courses,"Does anyone have any reaction, positive or negative, to Andrew Ng's beginner-level machine learning courses?  I am a little peeved that they are not included in the Coursera Plus subscription plan.

Thank you,

K. S.",1,2,1736194622.0,analytics
1humrno,Bachelor of Science in Biology looking to transition into data analyst role.,"Like the title says I have a bachelor's in biology with academic research experience. I've been pretty involved in data analytics in that realm during my undergrad but I would like to transition into data analytics from a business perspective. I have obvious experience in excel but I'm not an excel master. What certs are professionally recognized that would help me become relevant when applying for an entry level data analyst position and what other tips would analyst have for someone like me? My current role is in a quality assurance department in food manufacturing. I definitely analyze data (material breakage reports, inspections, testing results) but I wouldn't say that I have professional data analyst experience. TYIA for any advice and guidance.

Free courses would be great, but I could afford to purchase one at a time. ",7,9,1736125523.0,analytics
1huw4lc,Statistics student here! Advice suggestions everything is accepted thank you,"I would like to dedicate myself to RPA development, where can I find online practices or tutorials? What advice do you give me? I'm also interested in the area of ​​production forecasting, I think it's called that. What do you recommend and thank you!",1,5,1736159569.0,analytics
1huvrmx,Looking for a more efficient way of collecting commentary on financial movements?,"Hi All. 

I work in insurance and I am responsible for running a report on a weekly basis which shows all movements greater than 100k. I am then required to reach out to the respective adjusters and ask for their comment on the reason for the movement. Currently this is done by me sending them an email and populating their reply into an excel sheet. Once all narratives are in hand, I check for any conflict of interest claims and then circulate to a wider audience. Is there any way this could further be optimised using Microsoft suite? Preferably not sharepoint, but perhaps a PowerBI related tool or otherwise?",1,2,1736158001.0,analytics
1huqkyz,Advice on data warehouse options,,1,2,1736137034.0,analytics
1hu2j8q,Which certificate course is most recommended for securing a data analyst job?,"Which certificate course, covering everything from beginner to advanced data analysis concepts, provides hands-on projects and practical experience to best prepare for a data analyst role and increase the chances of securing a job in the field?",27,34,1736068212.0,analytics
1hu80ei,Data Adjacent roles and positions?,"Hello all!

While considering the oversaturation in the entry level data analytics job market, I was wondering what other titles, roles, or positions outside kf ""data analyst"" one can search or fine tune ones resume to to increase their chances of being hired after learning analytical skills and tools? 

For example, I heard accounting shares many traits and skillets with data analysis? Am I right in this assumption?

Thank you all!",8,22,1736086775.0,analytics
1huoahg,"Seeking literature(books, articles, theories, etc) related to a specific data topic ","I would put that topic in the title but truthfully I am not sure what to call it. Basically, my company went through a transformation years ago and they decided to make dashboards for everything and they never ever revisit what has been created unless someone yells loud enough. 

As a data analyst it’s a bit frustrating because the customers in the company seem to complain certain things are not even reflective of current needs and are outdated metrics. 

I want to convince my management(who have zero experience in data) that there should be some kind of operating model where we revisit key information and dashboards at specific intervals(could be yearly, quarterly, etc) and see if it still satisfies business requirements. It does not need to be done at a micro level either. 

I think I’d have a better case if there was actual industry standards on how data and reporting should be managed. Data governance is a bit wider of a scope than I had in mind. Are there any key terms I should be searching for that explains what I’m seeking?",1,4,1736129958.0,analytics
1huigky,Aspiring DA/DS w/social science degree and holistic health experience,"Hi, my domain experience is in holistic health, massage/spa services, human services jobs, and learning statistics and social research methods. Will study Six Sigma, and help desk (and maybe social media marketing).

Where can I work? ",1,2,1736114052.0,analytics
1huos2a,MS in Data Analytics,"Looking towards a masters in Data Analytics, what are the general pre-reqs for these programs? ",0,13,1736131423.0,analytics
1huijov,What can I do with a social science degree in DA?,"Hi, I have a background in holistic health, massage, human services (disabled community), and endurance athletics. 

What are possible domains for me?",0,10,1736114281.0,analytics
1htv0z9,Tracking Tool for Personal Development,Anyone have specific software or best practices to track accomplishments and/or work completed? EOY I am wanting to present a short summary for a promotion and I would want to track it the most efficient way. ,3,3,1736043207.0,analytics
1htizsw,Profile/Role transition from business Analytics,"
Hi All,

Need your advice at this crucial junction of my career.

I am a business analyst(34 M) with 8 years of experience in a US bank.
So i did my Masters in Computer Applications from top NIT in india. I studied computer science in my bachelors.

I worked on data and business analytics roles (campaign analytics domain) in banks and was unlucky in terms of organization, clients and quality of work.

My current skill involves SAS, SQL, Python, Excel, case studies,credit cards use cases.

I was good at coding & data structures during my college time.
Now somewhere i feel very underpaid if i compare myself to my batchmates who are software developers.

I am looking to learn something like machine learning, data science and move to a more niche role.
As i see people in data science, machine learning, data modelling or risk analytics teams earning well.

I need your advice where shall i take my career & what is the right domain for me from here where my experience can help in transition.

I know it is late after 8 years but I am someone who believes that you can start at anytime from zero.

Please help in comments or dm me your advice. Open for conversations and to answer any further question.
",4,2,1736010593.0,analytics
1hsqtue,Senior Analyst but only Excel & power bi? ,"can someone actually make it as a senior analyst with only those two tools? 

as a current junior analyst,  i find myself caught up answering business questions and building case studies but only using advanced excel and power bi dashboards and grabbing data from our SQL server

i know the ordinary 
“ analytics isn’t about what tools you use” 
but what is that really true or is it just some LinkedIn corny hype up posts ?

edit 1 : clarification
",63,40,1735922920.0,analytics
1hsrfpj,Unsure about analytics job market,"Hi all,

I'm 26, working remotely as a supply chain data analyst at a small company. My role is diverse—I manage the entire supply chain and create/mantain Power BI and SQL reports for other departments—but there’s limited room for growth.

My original plan was to use this job as a stepping stone into data analytics. However, seeing constant posts about layoffs and oversaturation in the field has made me question that path. I got this job about a year ago and when I was job hunting, the market was terrible. I thought the market would improve but it seems worse now. I'm also worried about AI automating analytics roles in the future. I value job security a lot. 

I’ve considered pivoting fully into supply chain since it feels more future-proof and secure—companies will always need people to manage supply chains. But those roles seem more stressful and less likely to offer remote work, which I value. Tech jobs just seem more ""cushy"" in comparison.

Am I being delusional about the tech job market? I'm unsure if I should focus on data analytics or start working on supply chain certifications instead?",57,25,1735924391.0,analytics
1htj0kv,Recommendations,Hello I am looking for recommendations for an online masters in data science or data analytics that offers courses synchronously. Basically I want to attend classes via zoom at a set time every week live. Most of the ones I have seen have been just recorded lectures but that is not what I am looking for. ,0,5,1736010646.0,analytics
1hsmjx2,Career Pivot,"I’ve been working in data analytics for almost eight years now. At the senior level and starting to see a lack of advancement opportunities. How have people made a career pivot away from analytics? Data engineering seems like the logical next step, but idk if the pay is all that different. Would software engineering be attainable, or is that so completely different that I’d be starting from scratch?",43,30,1735911202.0,analytics
1ht1hwc,SQL,I know this is the golden language to learn here and i know how to use it. But how is SQL used in day to day analytics? Is it integrated with something else or im just kind of confused as to how it all plays together. Thanks!,3,7,1735950006.0,analytics
1ht2m7f,Tips to be a Data Analyst,"Hello Everyone,

Newbie here I just started going in the world of analystics which is taking courses, watching tutorials, reading things that people post in this community and most especially doing a lot of research before I do a change of career but I have one question. 

Let's say in the near future I learned most of the basic stuff which is Excel,SQL, Tableu, Power Bi etc. gain some Industry certificates would it be a nice idea to do free lancing first before I do the leap of faith and go straight in job hunting and if I'm fortunate take the interviews. 

Because I read a post in this community one time that learning from courses and video tutorials in the internet is not enough to get a Entry-level data analyst job. I read that you should have atleast an experience to land a job.

Any tips and advices would be much appreciated. I want to learn more and gain understanding. Thank you in advance everyone and God bless 🙏❤️",1,22,1735953157.0,analytics
1hsna5x,"I tried and I failed, what's next?","Hi everyone,

I've been working in this field for two years now, and after all this time, I’ve come to realize that it might not be the right fit for me. I feel like I haven't really learned much, and I'm thinking of exploring something else. The thing is, I’ve never quite figured out what I truly enjoy doing in life, despite trying career coaching and aptitude tests.

I was wondering if anyone has found themselves in a similar situation and what steps they took to move forward. Any advice or personal experiences would be really appreciated.

Thanks!

",7,36,1735913422.0,analytics
1hssen9,Any sas platforms out there that connect to ga4 and automatically build reports ,"Hi there, 

I work for a small agency, and analytics and reporting aren’t our main focus but we’d like to have something to add as a value prop for our clients.

Currently that requires that someone, usually me, looks through all of the data and builds a lookerstudio report based on some interesting insights.

Are there any sas platforms out there that look at your data and automatically model it into interesting visualizations based on standout data? Something like a looker studio dashboard but where the reports are prebuilt or use ai to create powerful insights.

Thanks all!

",3,2,1735926796.0,analytics
1hsghuu,Struggling to Transition from Startup Analytics to mid size or bigger company Roles,"I’ve been working in marketing analytics for a few years, in a small ecommerce startup environment. My role involved working with tools like SQL, Power BI, Excel, google analytics. While I’ve gained valuable experience in many aspects on business apart from learning these tools, I’m finding it challenging to get analytics roles at mid size to larger companies. Larger I mean not even big tech companies, something like 500-600 people. I’ve been working on enhancing my portfolio with projects showcasing broader more advance skills and aligning them with industry standards. And have GitHub account to showcase that. Also, I have advanced degree in statistics and mathematics.

In about 50% of my interviews with hiring managers, some looked up ecomm startup I worked at and commented that it seems like a small business with limited requirements for analytics, some just got turned off by finding out smaller product range and not having larger data to manage I guess or who knows.  And these has made me wonder if my startup experience is being perceived as insufficient or if there’s something else I’m missing. I wonder If getting certified with some tools might help. Though I am extremely tired after finishing up my masters degree, I somehow thought degree plus some experience  will be enough to show my credibility to get entry to 3+ years experience jobs. 

I’d love to hear from those of you who’ve successfully made the transition or have insights into how I can improve my chances. Specifically:

Are there skills, certifications, or projects that helped you stand out?

What’s the best way to position working at a startup when applying to roles at mid to larger organizations? 

Thank you in advance for your advice and insights.",22,10,1735887155.0,analytics
1hshrvp,My website is being spammed by a Singapore IP address,"They rotate between 3 different IP addresses, daily, every few hours click on my page almost 100 times in 2 minutes. 

My analytics doesn’t mention it as a bot, or any device for that matter.

Is this something I should be worried about or move on? ",4,4,1735892562.0,analytics
1hs5fp8,Are any AI Analytics Tools Actually Good? ,"Like are you using analytics tools with built in AI, or just giving ChatGPT, MS CoPilot, or some other model access to your data? If you are using an AI is it sanctioned by your company? ",20,35,1735854397.0,analytics
1hsdh4k,Anyone here Export data from Epic Medical Software to Excel?,"Hi all.  I do revenue analysis for a small hospital.  We use Epic to keep track of all of our data.

I generally pull data from other data warehouses but I am helping out for someone who is on leave and I have a big (well tedious actually) problem with one particular section.

There are these work queues.  Each work queue has a number associated with it.  There are 70 of these work queues that I have to export.

The only way I can see the data is look at a list of work queues and click on the particular work queue, and then a list of a few hundred lines pops up.  From there I have to export it (no copy/paste), having to specify the name and location, along with a password (HIPAA thing), then open up the data, then add the WQ number to each, then copy and paste that to my main list, where I will do actual analytics.  Each export takes about 5 minutes (longer if the software crashes), and like i said, I have to do 70 of them.  I cannot take the tedium.

I can deal with it for this month, but I am going to have to do this for at least 6 more months and I am trying to save my sanity.  There has to be a better way but all I get is ""that's how we've always done it""

Is there a way to pull all of the work queues at the same time?

A shot in the dark I know, but if anyone knows what I am talking about please let me know, or at least point me in the right direction.  Thanks.",3,4,1735876499.0,analytics
1hror9q,Is it as bad as that guy said?,"So I saw that post recently where OP was a bit frustrated with the influx of new people trying to break into data analysis and not understanding what they are exactly getting into. Seemed like frustration with expectations of ease and salary as well as availability with them noting a declining job market. 

Should I be tuning this out and driving or should I heed the alarms and go back where I came from? 

I ask because I’ve just chosen to go down this past. I’ve done a lot of research and the job does genuinely sound like what I want to do. I’ve been researching different jobs for almost 2 years now and this is the first thing I said I really wanted to do from deep inside of me. I know it’s not just some ‘easy fun remote gig shortcut to 200k’ BUT aren’t jobs just hard in general? Not to say anything about data analytics but millions of jobs deal with overcoming new challenges, struggling to meet deadlines, and the alternatives are destroying your body doing manual labor or losing all opportunities to see family and maintain healthy relationships. 

I’ve been working in hospitality for going on 7 years now. I’ve come to realize I can feel my body being worn down, almost everyone I’ve met more senior struggles to be even a little happy. I haven’t gotten a major holiday off in maybe 3 years? I would do a lot to be able to spend Christmas with my family or go to Thanksgiving. 

My understanding is it will be a lot of hard work to even get an entry level job. My plan was simply to work hard everyday, try to get some certifications that show I am capable of learning and working hard and maybe eventually I will get an entry level position. I expect no tech salary and that isn’t even a long term goal. I don’t expect it to be easy though and I do expect it to still be a ‘job’, only so enjoyable. 

I’ve chosen this route because going back to school for a degree in it in person would be almost impossible working full time and getting an online degree even would be at least 3 years and tens of thousands of dollars. Not to mention I fail to meet GPA requirements simply because I was too immature to apply myself as a kid. I did well enough sleeping through most classes and just passing tests that I never learned how to learn, I was not an idiot in any way except the fact that I was too short sighted to begin building my future. 

I’ve now learned how to learn and filled with drive to build these skills. I’ve seen what life is like in service and it’s not what I want and I believe that hard work can eventually make something.

Am I just another hopeful imbecile wasting his time or is there truth that I can get an entry level job with hard work and multiple certifications? 

Hope this post is allowed by the rules! I’m not seeking career advice or assistance but I DO want to hear it from the community directly whether or not this is some bleak industry not even worth anyone’s time or if there is hope. 

Thank you!
-A hopeful person",32,75,1735804393.0,analytics
1hsjm4i,Is data analytics an entry level position?,"
I’ve heard professionals like the data janitor say it is",0,8,1735900390.0,analytics
1hrocwx,Best Resources For SQL Interview Questions,"Hi everyone!

Following the positive response to my previous post, *“*[Best Practical Way to Learn SQL](https://www.reddit.com/r/SQL/comments/1hqyvuh/best_practical_way_to_lean_sql/)*”*, I wanted to share the next step in your learning journey: practicing real interview questions.

In a typical Data Analyst or Analytics interview, you’ll encounter these stages:

# 1. Technical Screening

* Often a HackerRank or similar test designed to filter candidates based on technical proficiency.

# 2. Hiring Manager Round

* A discussion with the hiring manager focusing on your work experience, problem-solving skills, and understanding of business concepts.

# 3. Technical Rounds (1–2)

* **SQL-focused round**: This tests your hard skills, such as query writing and problem-solving with databases.
* **Product Case Study (varies by company)**: A more in-depth exploration of your analytical thinking and approach to product-related problems (more on this in a future post).

# 4. Cross-Team/Stakeholder Round

* A soft skills round assessing your ability to collaborate with other teams and communicate effectively.

Look at this pattern we realise that SQL is the cornerstone of success in most Data Analytics interviews. While other factors like communication and business understanding matter, as a hiring manager, I’ve often observed that candidates are underprepared for the SQL round.

Just like coding rounds are essential for Software Engineering roles, SQL proficiency is crucial for entry-level Data Analytics positions. When you lack prior experience, technical skills often carry the most weight.

I’ve said it before, and I’ll say it again: the **technical screening round can make or break your interview**.

# Recommended Resources for SQL Practice

Here are some resources I’ve personally used and recommend to ace SQL interview questions:

# 1. LeetCode (Database Section)

* Start with the free SQL questions — no need for a subscription initially.
* Explore the “SQL 50 Questions” list as a structured starting point.

# 2. DataLemur

* A dedicated platform for SQL interview questions.
* Use this after completing the free questions on LeetCode.
* Again no need to buy anything, the free section is good to begin with

# 3. StrataScratch

* Offers SQL questions alongside other analytics-related technical problems.

# 4. HackerRank (SQL Section)

* Not the most comprehensive but worth completing for its free and limited set of questions.

# Pro Tips for SQL Prep

1. **Practice Consistently**: Go through the resources 2–3 times to build speed and intuition.
2. **Bookmark Tough Questions**: Track questions that challenge you so you can revisit them during subsequent practice rounds.
3. **Master Problem-Solving**: Aim to develop an intuition for solving SQL problems, which only comes with repetition and persistence.



Feel free to drop any questions in the comments below — I’ll do my best to answer and help you succeed in your SQL and analytics career!

Happy learning and good luck! 🚀",18,3,1735802663.0,analytics
1hr493m,What are peoples' reasons for trying to break into analytics still?,"Each day I see numerous posts about people attempting to break into analytics with the most random backgrounds that make them less than ideal candidates. They likely face a massive uphill battle to break into an analytics related role.

Why does this keep happening?

Do people believe there's still a huge boom in the job market for analytics?

It just confuses me to be honest given how saturated the field is and bleak the job market is right now. You have an exponentially increasing supply of labor and decreasing demand for it.

Edit: it appears that a few people are getting upset and think that I am gatekeeping. All I am asking is what are poeples' motivations to try and enter this field. It seems like many people think we're in a 2021-22 situation where you can complete a bootcamp or masters with no relevant experience or domain knowledge and then have the opportunity to jump right into the industry with a hybrid/remote role as a data analyst/scientist, etc. I personally think people are getting influenced by trendy influencer/youtube videos and universities creating these programs.

Obviously people can do as they wish. I don't care, it's just a job. However, I worry that many of the people posting about how they want to break in don't understand the true nature of the general job market and the analytics industry in particular. No shit most industries are saturated right now, but analytics is clearly at a higher level due to the combination of hype, off-shoring and cooling of the overall job market.

I feel bad for the individuals who have decided to complete a bootcamp, a MS in analytics or just graduated with an irrelevant degree, and possess zero domain knowledge with few analytical skills but want to completely jump ship and break into analytics. They're going down a path that'll likely lead to hundreds maybe even over a 1000 applications with most being rejections and ultimately making a failed investment.

They can do what they want, however, I worry that many people think the barrier to entry is much lower than it truly is and are making poor decisions.",158,119,1735742762.0,analytics
1hrjqpf,Data Analyst vs Business Analyst,My current title is a DA but the duties I perform tend to align under a BA title which pays much higher in my area. What is a quick difference between the two (generally speaking) I could point out to management when negotiating my yearly raise? Should I request a title change?,16,18,1735786144.0,analytics
1hrmkff,Are Business Intelligence (BI) and Threat Intelligence linked with data analysis?,"Forgive me for the question as I have just learned about these roles that have piqued my interest since starting my data analysis learning journey. 

I'd love for anyone with experience in these roles to help me understand these and what sets them apart from data analysis itself, and (if at all possible) how can I build a history/portfolio to help me land these types of positions.

Happy 2025!",4,4,1735795525.0,analytics
1hqyv1u,Best Practical Way to Learn SQL,"I have seen multiple posts and youtube videos that complicate things when it comes to learning SQL. In my personal opinion watching countless courses does not get you anywhere. 

Here's what heled me when I was getting started. 

* Go to google and search Mode SQL Tutorial
* It is a free documentation of the SQL concepts that have been summarised in a practical manner
* I highly recommend going through them in order if you're a total newbie trying to learn SQL
* The best part? - You can practise the concepts right then and there in the free SQL editor and actually implement the concepts that you have just learned.   

Rinse and repeat for this until your conformatable with how to write SQL queries. 

P.S I am not affiliated with Mode in any manner its just a great resource that helped me when I was trying to get my first Data Analyst Job. 

What are your favorite resources?",95,19,1735719826.0,analytics
1hr4j9n,How to decide what field to work in with data?,"25yo.

I currently work in data analytics position for last 2 years, doing mainly updating code, some analysis, and basic visualizations like in Excel. 

I learned some basic data science the past few months and ran some small machine learning models, but didn't really enjoy it. My impression was ""doing this well would take a bit of work and time. I could be interested if I keep working at it, but I'm not sure"". I did some machine learning in college too which I didn't particularly like.

I also considered doing more visualizations/business analyst stuff. It's easier to do but if I go down data science, it'll pay more. 

  
How can I decide if I should do more like data/business analyst, or if I should go toward things like data science/engineer and start to move my career toward it. ",10,19,1735743656.0,analytics
1hqulws,"NBA employee - Paul George - criticizes NBA analytics for weakening the ""product""","Interesting perspective from PG on his podcast - he's basically saying analytics have sucked the soul out of NBA basketball by turning every team into robots chasing the same three shots: three-pointers, layups, or free throws. He points out that while players today might be more skilled than ever, they're all being forced to play this cookie-cutter style that's making games boring to watch, especially for old-school fans who miss the physical, gritty basketball of the past. The kicker is that even though we've got all this talent in the league, teams are so obsessed with playing the ""analytically correct"" way that we're not even getting to see players show off what they can really do on the court anymore.

FWIW: TIcket demand and fees for casting rights appear unaffected...",64,37,1735701721.0,analytics
1hrez89,Career switch to data analytics from Behavior Therapy?,"Hi, I have my undergrad in psychology with a minor in business/accounting. I landed in medical billing/ insurance claims analyst. I also did applied behavior analysis classes and training. I did that part time and worked remote mostly for medical billing. Now I would.like learn data analytics and focus on just that. Is that doable? How is the market for junior data analysts? I don't want to do another degree. I am open to affordable courses and training.",0,13,1735772175.0,analytics
1hqfi0m,Trouble Fining Entry Level Marketing Analytics/Database Marketing Jobs as an MBA Student,"I'm currently an MBA student and I want to have a job lined up by the time I graduate in May. I'm having trouble finding entry-level marketing analytics or database marketing roles. I'm starting to wonder if this type of job is too niche and if I should be looking for EITHER data analytics OR marketing roles.

I mostly check linkedin and google frequently. I always apply directly on the company's website when I can. I think I have a well-structured resume, as I've had many professionals critique it.

For more context, I have a bachelor's in economics, and my concentrations in my MBA are marketing and data analytics. Furthermore, I am learning Power BI and SQL right now and working on some data projects to add to my portfolio.

Please let me know if I'm doing something wrong in my job search and what I can do to improve!

Edit: I forgot to mention my experience. I currently have a marketing internship at a startup, I'm a statistics tutor, and I was a consulting intern for a mid-sized accounting firm. ",16,23,1735654430.0,analytics
1hq3126,Uninterested in being more technical; what to do next?,"Hi! I've been a data analyst for several years. Over the years, I've gathered a variety of skills, including the tech stack (SQL, Tableau, Python/Spark), PM (general and tools like Jira), and design (general and tools like Figma), and I've improved my stakeholder/project management skills. 

I'm not excited to dive deep into the technical work, hence ruling out data scientist/engineer careers. I don't feel motivated to learn more Power BI/DAX or continue to upskill in new tech stack, for example... and I don't see myself doing side projects outside of work. Because of this, I'm nervous about finding other data analyst positions in a difficult job market (e.g. in case of a layoff, etc.) considering how saturated & talented the market can be. I like mentoring others, teaching, and being creative about solutions to help the business. I've looked into some career fields that hit on these topics while maintaining the data background, but some seemed stressful, which isn't what I'm looking for either.

Has anyone been in a similar position where they were a data analyst but transitioned into a different position/career based on similar experience? Would love to hear any advice or hear about what you ended up doing!

\----

As another way of looking at this, I'm curious if I can still be successful as a data analyst without being more technical. What are areas I can focus in learning, etc.?

  
",40,34,1735607712.0,analytics
1hpvjny,How has your organization effectively managed data quality?,"Hi everyone, we all know that data quality is typically very bad which creates problems for analytics. My question is: what has your organization done to effectively combat poor data quality? What type of data governance protocols did you employ that was useful? How did you ensure that the same data quality issues didn't keep showing up in the future? Thanks for your insight! ",19,31,1735587711.0,analytics
1hq9ilf,31st December Meme,"Pov: 31st December

Me: I've got 1 day left to meet my project deadline

Data: I've got 1 day left to crash and lose all your progress",0,2,1735629510.0,analytics
1hpl9hx,Why operational analysts should sit in finance...one opinion,"Operational analysts should sit in Finance, not IT or scattered across business units. Here's why:

Key Benefits:

* They develop better financial awareness and can tie analysis directly to ROI/bottom line impact
* More consistent methods and data quality across the company when centralized in Finance
* Resources get allocated based on company priorities, not department politics
* Better career growth path with exposure to senior leadership
* Analysts get broader business context vs being stuck in one department's silo

Common Pushback:

* ""But they'll lose touch with operations!"" - Nope, just set up proper rotations and communication channels
* ""They need IT support!"" - Modern tools reduce this need, and you can still partner with IT while focusing on business outcomes
* ""Won't it slow down urgent requests?"" - Not with proper service agreements and priority frameworks

You don't want your analysts becoming Excel jockeys in IT or getting buried in one department's bubble. Finance gives them the perfect mix of business context and technical opportunity while keeping them focused on actual value creation.",17,17,1735557994.0,analytics
1hpklqi,How do you look for jobs ,"Back on the market and want to refresh my skills in Job hunting. I’m prepared to dedicate a lot of time to this as my contract for my job is ending in January. I check LinkedIn every morning, but I just see a bunch of promoted and reposted opportunities that seem stale. Indeed is a literal shit-hole (At least for my city: Chicago). Google jobs I feel meh about. 

Any tips for how to diligently job hunt and what has actually worked for you all.

My main goal along with networking is to be one of the first applicants for these jobs postings to give myself a good shot. 

Any tips or life hacks will be welcome!! 

Btw: I have analytics masters with nearly 5 years of work experience. + coding / readership internships before that. 2020 grad. Mostly worked in retail / cpg analytics roles along with a research role. ",13,9,1735555265.0,analytics
1hpqtz8,How do you keep track of reports/insights?,"Hey all,
I was wondering how other people in other companies keep track of reports or insights you made for different stakeholders.

Lets say that the marketing team wants to know how well a certain campaign did and you do an analysis on their ab test. Next year they want to do a similar test, how would they find it back, where is it stored?

I'm super curious as I'm thinking about a small SaaS solution to build for this. In our company we self host a small website where Jupyter notebooks could be hosted.",2,7,1735575690.0,analytics
1hpjz9e,38M - Inquiry about a master in business analytics,"Im 38, living in Madrid working for a large multinational consulting and tech firm in the finance area for the past 13 years, in particular FP&A and Deal Shaping and pricing. 

  
Feeling quite stuck in terms of career opportunities and wanted to pivot to business analytics and intelligence. I feel like my train has long left the station but still want to give it a shot, any recommendations for programs that offer part time studies so I can keep working?",4,7,1735552624.0,analytics
1hptn8m,Free Database Tool for Novice Personal Project,"I would like to create a small-ish database to analyze trading card game decklists, similar to limitlesstcg.com. I am a novice data builder, and would like to use a free tool to build the data sets. The basics are as such:
1. An event will have an event type, date, location, players who use a discrete decklist (the number of players and therefore lists will vary by event), and placement in the event
2. Track pairings of players; the scale would be small as most results I am tracking would be among the top 8-16 players
2. Each decklist will have a minimum of 1 leader, one starting location, and 50 cards, with an additional sideboard of 10 cards maximum
3. Cards that are included in the decklists can be in multiples, and have characteristics that I would like to analyze (cost, power, aspect/affiliation, etc)

With these data sets, I would like to analyze:
1. Popularity of a particular leader (and occasionally the starting location) over an adjustable period of time
2. The most frequently used cards with that leader/location pair <- this choice realistically eliminates the majority of cards that could be included in a deck, and in what quantities
3. Identify the most common place of the most frequently used cards, either in main deck or sideboard
4. Characteristics of the cards chosen for deck data sets

Assuming I have a decklist creator that generates a file in a specific format with names, quantities placement of the cards; would like to ingest the data from those files for the raw data. The data can be stored locally, but would lean toward something that could eventually be stored online and/or entered from multiple users.

Is there a recommendation you would make on a tool to create the database?

Thank you in advance!",1,2,1735582893.0,analytics
1hpg68n,Production Level Custom Analytics,"What is your go to analytics solution ready for production?

E.g. some tools I have used in the past:
- Apache Beam
- Custom Python based framework

Generally, not happy with either so want to explore options. ",4,3,1735536900.0,analytics
1hpj2wn,Where do i start?,Hey guys! Hope its okay to post this. I was wondering if you could give me advice on where to start or recommend any courses to take to start learning data analytics? Im a complete beginner but im very eager to learn and have lots of time on my hands! Id really appreciate suggestions ,0,6,1735548649.0,analytics
1hpbqkm,Advice on breaking in to the field,"Hi everyone,

I hope you’re all doing well! I have a quick question about breaking into the analytics field as a student. I’m majoring in business analytics, but my university program has focused more on data analysis. I’ve put together a resume that includes a project I completed for a class (I plan to add more as I finish them).

Since I’m in my final year, I’m trying to land a data-related role that I can use to bridge into an entry-level analyst position, as I know the market is tough even for entry-level roles. If I’m lucky enough, I’d love to land an entry-level analyst role directly.

I asked for feedback on my resume from the resume subreddit, specifically about whether I should focus on my projects since I don’t have direct work experience. Someone suggested that I “lie” about having logistics coordinator experience (even though I don’t have that experience) and also said my formatting needs work, which I’ll be fixing.

I’m not sure how to feel about the advice to “lie” on my resume. How can I convince an employer to give me a chance if I don’t have direct experience? Should I rely on my projects to show my skills, or is fabricating experience really necessary? Would love to hear the advice and feedback. 
Thank you!! 
",1,25,1735522697.0,analytics
1hpfqvu,How to Combine Files with Different Headers and Sheet Names Using Power ...,"In this video, I'll guide you through the process of combining multiple files where the column names and sheet names are not unique, using Power Query in Excel. This tutorial is essential for anyone handling diverse data sets from various sources, helping you merge them into a single, cohesive dataset for more efficient analysis.  
  
",0,2,1735535415.0,analytics
1hpejs2,Would you get into this field if you don't know what to do?,"I believe a strong strait that leads me to this career is my strong investigative skills, also I like the money potential and schedule and self employment opportunities. Would this be a regrettable career in 5 years time ?",0,22,1735531442.0,analytics
1hoq4ao,Probability calculation relevance,"Just asking, how much do you have to deal with probability calculation daily (as an analyst, hobbyist,...)

Any case you have to calculate a large set of linked events (and, or, conditional events,...), and need to find the probability of event X. If so, which tool do you use for that task ?",6,4,1735453963.0,analytics
1hohi5u,Funnel analytics ,"Hi all

Complete newbie to data / analytics. 

Can someone recommend me a way of finding out how far people get into my conversion funnel? So every page is tracked and I can see where they drop off between hitting the home page and  checking out?

If that’s  even possible?

Many thanks ",9,8,1735426119.0,analytics
1hoifpj,Data analysis or cybersecurity?,"Hello all!

I am considering starting a new career path after years of stagnant career growth within trust and safety and GenAI. 

I have done much research and I have come down to either data analytics or cybersecurity.

Cybersecurity because what had motivated me to follow tech in the first place years ago was protecting users from harm (internal content and external). But since then trust and safety has been all over the place from content moderation to customer support (and of course lots of layoffs ans outsourcing).

Data analysis because I have some familiarity with analysis concepts and tools, mostly excel, and I have found trends and insights through large datasets. I thought this would be a better choice since I already have some experience, and just need to aquire more technical skills and create a decent portfolio. 

But something about cybersecurity has always intrigued me, and feels like something more meaningful for me in the long run. I do understand that the general consensus is that cybersecurity is no entry level job and requires some time in IT helpdesk roles (which I'm fine with) before landing a threat intelligence role, ethical hacking, red teaming, or anything or the like. 

I would really appreciate some guidance here on which is a better career path for me. Again. I am not asking what exactly to do as I can always do more research in this subreddit. Just advice from a pro or two on whether one or the other is the right career path for me.

Thank you all and happy new year!",0,28,1735428784.0,analytics
1hnrmz0,What analytical and statistical methods do you use in your job regularly?,"What is your job/role, and what statistic and analytic methods/tools do you use? What are the critical lessons/skills/in-house-protocols needed for your specific role? 

I’ve heard a good amount of general advice, but I’ve been looking for a more tailored advice to explore different roles/fields and steps to take to be competent in different jobs. I won’t be able to be a top candidate for every path, so I want to see tangible steps to a variety of roles. I’d then choose from there and make a career/education roadmap from there.

Some background:
I’m a first-year MS Statistics student. I came from a finance background and I’m currently specializing in medical statistics, but I’ve (until now) planned my coursework to make me a generalizable analyst between fields/industries.

Discerning between:
- Federal govt. statistician 
- Hospital/Pharma statistics
- Business Analytics (seems like most here)

Programming background, in order of competency:
- R (my main language since undergrad) 
- SAS (graduate classes)
- Python (Self-taught. I thought it’s not too dissimilar from R. I also enrolled in classes next semester for machine learning and a general ‘apply Python to projects’ class)
- also SQL, Tableau/PowerBI, and Excel

General statistical topics I know to a decent degree: 
- Sigma-algebras (for understanding what my computer is doing)
- Bayesian methodology
- Regression (logistic, linear, negative binomial, MLE vs OLS)
- Data importing, cleaning, analysis, reporting
- Handling issues like confounding, reverse causality, multicollinearity, etc.


",80,56,1735340809.0,analytics
1ho46ez,What are the best techniques for Anomaly Detection in Complex Datasets?,"What are some of the latest techniques you use for detecting anomalies in complex datasets, and how do you apply them in real-world production systems?",3,5,1735386699.0,analytics
1ho4bf9,Upcoming Interview for Associate Analyst - Data Governance,"I have my interview scheduled soon for this position. This role is for undergraduate candidate from INDIA. I’ve been doing SQL interview questions on hacker rank and Leetcode but if someone has some insight on what types of questions will be asked, I would really appreciate it!

some point from Job description

* **Data Quality Management**: Conduct data profiling, analysis, and quality assessments using tools like Informatica, SQL, PL/SQL and MSFT Excel analytics. Implement data cleansing, linking strategies, and automate processes using AI/ML.
* **Stakeholder Collaboration**: Engage with Data Stewards and Business Stakeholders to address data quality issues, prioritize resolutions, and ensure compliance with data governance standards.
* **Documentation & Improvement**: Maintain data dictionaries, document business rules, perform root cause analyses, and drive continuous improvement through periodic assessments and gap analyses.",0,2,1735387287.0,analytics
1hoc29c,I built Marketing Analytics tools in 7 Days – What's next?,,0,13,1735411117.0,analytics
1hnnbcs,What's your approach to historical GA4 data migration to BigQuery?,"Working with GA4 data migration and curious how others handle historical data syncing to BigQuery.

What techniques have you found effective for:

* Managing multiple GA4 properties
* Ensuring data consistency during migration
* Handling large historical datasets
* Maintaining data quality

Would love to hear about your experiences and best practices.",7,1,1735329205.0,analytics
1hnghde,"Pathway from an IC to Analytics Manager, Advice?","I’m currently working in FP&A as a lead analyst, (F500, Aerospace, Canada) while I like finance I figure that my trajectory isn’t as high as people with a CPA and I strongly prefer data analytics. 

Most of my job is building dashboards for the finance department and providing analysis for senior managers and SLT, lots of business partnering (as well as forecasting and budgeting etc). My job revolves around SQL and excel/power query. I’m comfortable using python although my job has no real need for it.

Im not thinking of changing roles for the next little bit as I just got this gig but I would like to know what it takes to get to the next level on the hamster wheel at some point. 

Edit: I should note that I do have manager/leadership experience when I was in the army but I find civilian managerial/leadership qualities are a bit different. ",11,24,1735311091.0,analytics
1hn528u,R or Python,"I'm considering learning R or Python and was wondering which would be better for me. I'm on the younger side and not set on a single career path yet, but I'm currently leaning toward becoming a data analyst and I'm hoping specifically to become a data analyst in sports. I feel like one of these tools will be essential for whatever my future career ends up being. Any advice? R or Python? Pros and cons of both for my specific scenario?

Thanks in advance",37,60,1735267689.0,analytics
1hng7ke,Independent consultants and analytics agencies,"Hi guys, I want to ask independent consultants and analytics agencies, how you guys are getting clients.

How cold emailing and linkedin outreach worked for you?

What industries do you target?

Your monthly emails volume?

And the amount you guys are spending on cold outreach.

I'd love to hear from you.",3,6,1735310288.0,analytics
1hnikng,Uncertainty in Hypothesis Testing Method,"For my research on judicial fairness, I used a chi-square test to compare how different groups perceive fairness in the justice system. However, I realized that the chi-square test is typically used for categorical data, while some of my data are ordinal. SPSSAU didn't explain why it recommended the chi-square test. Should I be using a different statistical method for my data? How do I know if the hypothesis test I am using is appropriate?",2,2,1735316820.0,analytics
1hmvtdj,Psychology - Data Analytics,"I just recently found out that there is a POSSIBILITY of transitioning from a psych background and work in data analysis as there is lot of stats being thrown around in psych. Ngl i am very interested but seems like huge decision to make so i would like to get some proper input about this first. I have used SPSS and looking forward to enroll in an online course in SQL if possible. Really be honest, will this be worth it for me?? ",14,8,1735241152.0,analytics
1hnasjt,Pair Programming and Mentorship,"Can anyone help me with this concept, like how 'pair programming and mentorship' really works and how It can help freelancers.",1,2,1735289331.0,analytics
1hml273,Anyone else works as a tech analyst in a non-technical team?,"I think this is the secret to be an over performer. I work for one of the top tech companies in the world, and I am the only analytics professional in a non-technical/business team. 

Recently I created a Power BI dashboard that summarizes and shows my team’s products performance in a more structured way. I have gotten so many awards and recognition on this, even though to me it was a simple project. 

Anyone else with a similar experience? What other examples of projects you have done that have impressed your non-technical teammates? 
",64,29,1735205783.0,analytics
1hmrlyh,Quick question for current Data Analysts ,"As someone about to do some interviews for data analyst roles at start ups as I wrap up my masters program (can be for anyone though)….(context is the United States)

What does a day look like for you? 

What are your biggest challenges?

With tasks in SQL do you work with existing code?

How often are you using tools like python and SAS for analysis compared to data viz tools?

What kind of accountability is asked of you? (ex. monthly reports, yearly reports…)

any other insights would be nice ",18,31,1735229762.0,analytics
1hn76am,Building a Freelancing Platform Exclusively for Data Professionals – Need Your Input!,"Hi Everyone, 

I was wondering that there is no specific freelancing products which are solely cater to data professionals and platform like toptal, fiverr and upwork seems like too scatter and just competing with the third world countries 

I am building a freelancing platform for data professionals and want to solve some of the problems which I face 

1. Giving the rate price for beginners and intermediates like from 30 - 50 for beginners and for the intermediates 70 -100 and professionals can set their own rates depending on the experience

2. Have some rigorous vetting process so that I can ensure that the clients are getting the genuine freelancers not the mocked ones

3. Want to build some features where you can rate clients as well so that we can block them if they are not working as expected

  
I want this platform to truly serve the needs of the data community. **What features or processes do you think are critical to include to make this platform successful?**

Some specific questions I have for you:

* What frustrates you most about current freelancing platforms?
* What would attract you to join a platform dedicated solely to data professionals?
* How can we balance inclusivity for beginners while maintaining quality for clients?
* Are there any must-have features or tools you’d love to see on such a platform?

I’d love to hear your thoughts and feedback! If this is a problem you’ve faced, let me know how you’d like to see it solved.

Thanks in advance for your insights!",0,4,1735274817.0,analytics
1hmrg6b,I want to start a freelance career but I only use Excel… ,"I work in public health, managing studies and program evaluations at a university. I don’t have extensive quant skills but I learned how to run regression analysis in Stata during my MPH. I haven’t used R or even written script in years. I took a workshop on data visualization and got pretty good at it, exclusively in Excel. 

I am frequently approached by colleagues who want me to work with them/for their team. I also landed a side job conducting evaluations for an old university partner. This is my only freelance contract.

I really want to grow a freelance analytics business, but are my skills in Excel enough? For folks who’ve succeeded in building a freelance career, what other skills/tools would you suggest I focus on? Any advice is welcome!",10,22,1735229314.0,analytics
1hmu9t1,Which level of those analytical tools you needs to become succesful in your job?,"My question is how deep you needs to know tools such Excel, SQL, Phyton, BI/Tableau/Qlik for your current position?

Or any other tool that can improve your productivity a lot.

I assume Excel and SQL are a given and Phyton may improve your productivity by a lot paired with ChatGPT if you knows about automatization.

Whats the tool that became a game changer in your daily job?",3,15,1735236977.0,analytics
1hmzlv3,What business questions do you address in insurance in a marketing/customer focus department as a data/business analyst? ,"I would like to know what business questions/ analysis/ dashboards one do in a business performance & intelligence analyst role (inserted in the marketing department). This role is more customer data focused/ campaign data / etc 

Thanks!! ",1,3,1735251434.0,analytics
1hmklur,Help me approaching this case study which I'm doing as a part of getting into Sales Analytics,"So, I have beein working in Sales, which my role is to generate the revenue for the company. I have worked for almost 10 years and want to move into Analytics. To begin with, I have working on Sales Data Analysis and came across this project which I'm clueless about it on how to begin with?

>The case study outlines the below instruction.  
Analyse the historic sales performance of Umbrella Corp keeping in mind the various parameters at play viz. Region, Industry, Business and Client, across the 3 Partners. Create a narrative from the trends and insights you identify, highlighting the focus areas for the next 5 years.

>Your supervisor has provided suggestions on questions to be evaluated:

>•The current Region categorization is to be reassessed•

>There will be no change in the Industry prioritization

>•Investments the firm makes to drive growth should be split equally between the 3 alliances

>•Understanding performance by Clients, and understanding alliance performance relative to Firm performance will provide further insights

>

>Additionally, your supervisor has learnt that leadership feels large size deals will be the driver of growth going forward, which she is unsure of. She would like to understand this better and wants you to include this is your narrative

I just want to know the idea on how to begin with? what should be my approach and how can I make a breakthrough.

Data Set is given in the comment",7,7,1735203619.0,analytics
1hmw11f,Need ideas on how to start freelancing in the data analytics field. ,"Have been working for 2+ years in the data analytics field, have experience in snowflake, data visualization (power bi and excel) and power automate. Don't have a lot of hands on experience in scripting (Python Or R). If anyone has any freelance experience in the analytics field, would like to know where and how to start, whether I need to be an absolute master of the skills. Any kind of details is much appreciated.",0,3,1735241720.0,analytics
1hmmsbo,Can i get a Data science if i had completed Bcom,As the title suggest im a bcom computer applications graduate and im about to join in an institution which provides me with data science course. I know data science is a more technical job but im keen intrested in getting into IT and can learn coding as i have intrest in that too but not more very advanced level coding what would be my carrer options also ,0,4,1735213649.0,analytics
1hmhjno,Advice needed: Remote/Hybrid Courses/Diploma/MSc in Health Data Science or AI in Healthcare/Medicine Upskill,"
I am ICU Specialist in UK and have no coding background but happy to spend time in it to upskill. Of late, contemplating this as an additional knowledge to work in it as a part-time (maybe divulging into it completely at some point). 
(PS: My job pays good and not looking to match that with added skill)

Can you suggest some or comment on these:

1. Imperial College London – AI in Healthcare and Medicine (Online Short Course)
Duration: 8 weeks, part-time.

2. University of Oxford – Artificial Intelligence in Health and Care
Duration: 6 weeks, part-time.

3. Health Education England (HEE) – AI for Healthcare (Specialized Programs)
Duration: Varies (short courses and workshops).

4. King’s College London – Digital Health and Machine Learning in Healthcare
Duration: Flexible learning options (short courses or part of MSc programs).

5. FutureLearn – AI for Healthcare (Multiple Courses)
Duration: 2–8 weeks per course.

6. NVIDIA Deep Learning Institute (DLI)
Mode: Online, with certificates available.

7. University of Manchester – Health Data Science (Online MSc)
Duration: Part-time (2–5 years)",0,3,1735190127.0,analytics
1hm8rjb,Is google trends accurate?,"I know Google is a big “trusted” company but when I was comparing shows on the tool I have insider knowledge that one show is more popular than the other yet trends showed the other show had more searches…so I ask…

Is google trends accurate?",3,4,1735159920.0,analytics
1hlpr7n,Is it normal to constantly work past 5:30pm?,"Landed my first analytics job a few months ago and I’m having a really tough time not only getting stuff done, but understanding the business. There are many concepts I’m just not understanding and it’s affecting my work and it’s not fair to my coworkers as well because I try to do stuff, but I end up constantly asking them for help. When I do go to them, they’re so busy they either respond late or just tell me to study the topic myself. The problem is when I do, I still don’t understand some of the concepts. 

I’ve only been here for a couple of months so I don’t know if it’s something I’ll start to understand over time but I’m feeling very overwhelmed and am missing due dates on some projects. It’s gotten to the point where I’m trying to catch up way past 5:30pm on stuff and it’s stressing me out a ton. Any advice?",41,40,1735085251.0,analytics
1hlvahc,Do you have any colleagues who were hired based on nepotism and how did they work out?,"I've seen several people hired from the same university the manager was from, and they performed well. On the other hand I've seen a couple of hires that got their analytics position due to being related to a senior coworker. Not surprising they were not qualified. It feels bad to see all the people struggling to find analytics jobs right now while these nepotism hires get a free ride. What are your thoughts?",6,6,1735107855.0,analytics
1hliq2d,AI and Data Analysts layoffs ,"Hey everyone, has anyone noticed layoffs in data analyst roles due to AI advancements? Just curious if it's affecting the industry and how people are adapting. Drop your thoughts!",58,46,1735062706.0,analytics
1hlmneb,2025 forcast...,"After much debate and self reflection, as well as asking questions on this subreddit, I decided to begin my data analysis journey (starting with an IBM coursera cert and focusing on Excel and SQL first). 

From a novice to the more seasoned data analyst professionals, how does 2025 look like for new analysts? I have heard of the oversaturation and of course the tech layoffs (I hope to try to be an analyst in a different industry that isn't social media or tech), but I still wish to push forward, network, and hopefully find a mentor (feel free to message me!). Is data analysis still worth the uphill battle in your opinion?

Thank you! 

Merry Xmas and happy near year to all of you!",22,17,1735074767.0,analytics
1hm262r,Actual work happening in Data Science roles in India,"I'm working towards learning and building my Data Science portfolio. I want to know what kind of work actually happens in companies for Data Analyst and Data Scientist roles. I've completed a one year course from GL and now using udemy to brush up on my skills. However I find the course content to be very similar. I lot of posts also mention working on building models which are more or less limited to around 7-8 models universally used plus visualization which is also just tableau, power bi and couple of other tools. 
Is this actually the way jobs are in companies? Am I missing something specific (other than stakeholder management) regarding the job roles which have to be learnt if i have to excel in a data scientist role?",0,7,1735138749.0,analytics
1hl2hpf,Interested in Data Analytics -- What would you advise a total newbie?,"Hello everyone, as the title suggests, I am interested in becoming a Data Analyst. I'm currently a first year at SJSU. I do not have any coding experience, but I am actively doing more research about this field and searching for good skills/certifications to achieve. Please feel free to offer any advice you may have. 

\- What exactly does a Data Analyst do?

\- If you could go back and start it all over again, what would be the first 3 steps you would take?

\- What certifications do you recommend?

\- What type of coding should I start out with?

Thank you so much! :)",20,40,1735003283.0,analytics
1hkzt8p,NLP folks - What tools do you generally use? ,"Familiar with NLP basics but new to deep learning. Data science folks that work on NLP projects, what models do you generally use? RNN’s? LSTMs? If it’s not too much, could you go over what your NLP projects look like?",9,13,1734994880.0,analytics
1hl8l6f,Need help,Can anyone suggest me a good certification course in data science and analytics? (Beginner here!!) ,0,9,1735025746.0,analytics
1hkzphs,Platform for DS practice problems,"What’s a good platform to practice DS problems? Aiming for more so traditional data science roles with model development/NLP focus. Somewhere I can practice data science python problems (pandas, functions, loops etc). Ive used hacker rank for SQL questions and have loved doing those. I’ve never leet-coded but have heard those are more so focused on SWE roles. ",2,2,1734994587.0,analytics
1hl7rko,I built a tool to save hours on data visualization,"Hello everyone,

I have been working on an AI tool which generates good visualization and gives quick insights on your CSV file. This process happens in less than a minute thus saving the process of preparing your datasets and planning the visualizations. 

What do you think about such tool? Would you like to use it?",0,19,1735022312.0,analytics
1hkmzmf,I'm seeking guidance from anyone who is a Business Analyst or studying Business Analytics.,"

What is the scope of business analytics, and can you recommend the best online sources to learn this skill? I’m a business graduate; how much time will it take to become proficient in this field?

Given my lack of prior knowledge and technical expertise in Excel and other tools, should I pursue this skill?",5,14,1734957888.0,analytics
1hkl34z,Experience using Adobe Target,"Hey folks, my company is looking at integrating a platform for A/B testing, has anyone here used Adobe Target and if so could I get some info from you about what it was like to use and integrate?

The other option on the table is an in-house solution, but we're not obliged to use it. It's a kind of a ""how long is a piece of string"" question but if anyone has an experience of using an in-house product vs Adobe Target that would also be really helpful. 

Thanks",6,7,1734950101.0,analytics
1hkqkw4,"If you want data cleaning automated, would you prefer using SDK (python api) or web platform?","Hey folks,

Dev here

Working on data cleaning platform, automating data cleaning & mapping using LLMs

If you have this platform, would you prefer to have it as API (for example, python SDK) or web platform (when you can connect your db/upload csv, explore and iteratively process the data)?",0,11,1734969057.0,analytics
1hkgert,Can you help me planning my 6 months before the start of my MSBA degree at UW?,"I recently got admission from UW MSBA, and have 6 months before the start of the program. I want to know what should I prepare to maximize my productivity and skills within the 6 months before the start of the program.  
I have no coding experience and would like to know the details of where should I start.

Thank you so much!",6,13,1734929524.0,analytics
1hkkngg,FBCLID can't be decrypted right ?,"So i'm helping a startup capture conversion rates on their webpage. i've asked them to add utm params to their ads if they want those numbers. but my employer is telling me that i should use fbclid to decrypt campaign and ad info. he thinks facebook passes campaign info through the fbclid cookie. 

i'm of the understanding that fbclid is just a one way hash and acts as an index for facebook to identify clicks in their db. am i wrong

and if i'm not wrong how do i put this through to my guy",1,3,1734948112.0,analytics
1hk5d1m,Need Help Choosing a Data Analysis Course to Land My First Internship ,"I’m currently doing my master’s in economics and really want to break into data analysis. The thing is, I know there are a ton of courses out there, but I’m struggling to figure out which ones are actually worth it. I don’t just want to learn theory—I need something practical that’ll help me land my first internship and stand out in the hiring process.  

I’m looking for courses that:  
- Teach essential tools like Python, SQL, or R.  
- Include hands-on projects that I can add to my portfolio.  
- (Bonus) Offer certification that recruiters actually care about.  

If any of you have taken a course that genuinely helped you get your foot in the door or build your skills, please share! Whether it’s on Coursera, Udemy, or something more niche, I’d love to hear your recommendations.  

Thanks a ton in advance!  ",11,23,1734894753.0,analytics
1hjxy8n,How to deal with information Overload to stay focused for landing a data job in this market? ,"Hello, initially this was supposed to be a Mavens Analytics vs Data Camp vs edX question but then I decided to generalize it. My issue is I’ve been coming across so much resources that it’s so hard to pick one, then I just end up doing nothing. Or as soon as I start one thing, another thing pops up and always seems more in line with what I want to do. This has affected me in so many ways but I don’t want to bore y’all with the details. 

Lately I’ve been landing several interviews and made it to final rounds for a few, but they always go for the more senior person or for whatever reason just say they found someone else they preferred. Companies now are being very specific with what they want in a candidate. 

Note: I don’t want judgement. I want advice. I have 12yrs professional working experience. The most years I’ve spent were in home healthcare and in retail pharmacy. My last role was a BA/BSA which lasted a few months. I have a bachelors in health sciences, an MBA in MIS & currently in Georgia Tech’s MSc in Analytics. Don’t ask me why I’m pursuing another masters. And no I’m not just qualified for an entry level position like some of y’all have been saying in previous discussions. I’m aiming for Mid level roles MINIMUM where either my BSc, MBA or both can be utilized. Thank you!!!!",16,17,1734872069.0,analytics
1hkaccu,Customer profiling tips please,"Wanting to profile customers.
 The data I have right now is sales data, emails, addresses and UK housing transaction price data.

So far for each email I’ve calculated:
- number of purchases
- AOV
- months since last purchase
- last product purchased category 
- Item style purchased
- A list aggregate of all styles purchased
- Type of home they live in (flat, terraced etc)
- Last known sale price of that home with a linear regression on the county to then extrapolate its value in 2024 prices. **
- the 2024 house price / 4.5 to get a estimated household income

**When someone purchases multiple items, I take the last known items shipping address and then join that to my housing table for the housing info. 

 Spent a few hours on this but I am now at a loss on what to do to take this to next level. The idea would be to use this table for marketing efforts and to improve our email marketing campaigns. Any tips or publicly available datasets which could enchants my profiling would be appreciated.",1,4,1734909169.0,analytics
1hjncm2,Data Analysts: Do you use Linear Regression/other regression much in your work?,"Hey all,

  
Just looking for a sense of how often y'all are using any type of linear regression/other regressions in your work?

  
I ask because it is often cited as something important for Data Analysts to know about, but due to it being used predictively most often, it seems to be more in the real of Data Science? Given that this is often this separation between analysts/scientists... ",54,56,1734827990.0,analytics
1hk3tg1,"When do people typically take out loans? At what time of the month do they usually do so, and in which month are the most loans taken? ",I'm trying to determine the best timing for my affiliate marketing efforts by understanding when people are most in need of affordable small loans and cash advances.,0,7,1734890341.0,analytics
1hiygjk,"In one sentence, how do you describe your job to strangers?",You meet someone and they ask you what you do. What do you say?,25,66,1734742539.0,analytics
1hiuarm,Feeling burned out with data analytics,"As the title says I am feeling really burnt out within the field of data analytic. I have been working in the field for over 4 years now but it seems to have drained me that I don’t want to do it anymore. Please advise to other possible fields to get into, I am really looking for a career change without having to go back to school. 
I am well paid in my current role, in the lower 100s so I am looking for another high paying field as well. 
Any advice will be appreciated. 

Thanks ",45,62,1734730261.0,analytics
1hj796g,Rejections from BA roles. Need advice to get interviews.,"
Hi. I am a recent graduate with masters degree and decent professional experience in data analyst roles(approx 1 year) I also have good portfolio websites with data analytics projects. I have been applying to data analyst and business analyst positions. However I straight up get rejected from all the BA roles. I understand I don’t have academic background as in business analysis, but I want to understand what am I lacking here? What can I do to make my profile better suited for BA roles?  As for skills, I do have the technical skills, may be I lack business knowledge. But there must be a way to crack it. People currently working in the industry, help me out here - what would it take for a candidate like me to get into entry level BA role?
I can DM my resume if needed reference, thank you in advance:)",3,15,1734777121.0,analytics
1hi6b3m,Employer is paying for my Master’s Degree,"I’m a business major with a minor in business analytics and information systems. After a long and grueling job hunt, I landed a decent gig at a huge finance firm. Still wanting to pursue Data Analytics, what would be the best pick? I’m between Information Technology, Statistics, or just a regular MBA",93,53,1734651148.0,analytics
1him1cu,Career/Job Advice Question,"Hello,

I am posting this to get some perspective on possible avenues I can explore after graduation. I am currently in an MS in business analytics program and have an MS in clinical psychology with six years of clinical experience. I decided to pursue a business analytics degree because I am interested in how impactful data and metric-driven data are on individuals and companies. I am unsure how I could possibly incorporate my psychology experience in my future in business analytics. Do you have any recommendations on positions or avenues to explore?",3,13,1734708018.0,analytics
1hiami6,Pharmacy Data Analytics,Hey I'm senior about to graduate soon. I'm curious how to get into Health Data Analytics or Pharmacy. I'm about to take tableau desktop certification and thinking about doing Comptia cert as well with data analytics. Any suggestions is SalesForce or AWS cert is worth doing? or am I doing too much.,19,22,1734664709.0,analytics
1hip9ce,The data in this preview has been truncated.,"https://preview.redd.it/j6cdhz78i18e1.png?width=1919&format=png&auto=webp&s=ed337ab37aadf89809392d417ae715d216903fde

Hi guys, have been battling with this for a week now, please how can I resolve this issues?  
I tried with ChatGPT, everything it said I did, yet it won't work. Please help.",2,19,1734716646.0,analytics
1hhw27w,Does a data analyst need to know about prepared statements in SQL?,"I'm learning SQL as prep for my upcoming job (switching from SWE to DA).

I learned about how to pivot a table in MySQL. But it's sooooo clunky....

In order to pivot you need to use CASE WHEN statements. So I looked up if there's a different way. Turns out, there is!

By using prepared statements, GROUP\_CONCAT, CONCAT and a variable.

This makes me wonder, do data analysts need to know about prepared statements or did I go too far in my prep?

",33,33,1734624024.0,analytics
1hi0dtb,Highest Value Certification/EDU?,"I am looking for some help as my boss let me know they will pay for just about any additional education/certifications I am seeking, but they won’t pay for my masters. What should I look into?

Background: I am 2 years in as an Account Analyst in the Automotive Aftermarket industry. 

I learn better in person but open to online programs/courses if the value of it is worth it. 

I could use a refresher in BI but want to see what is out there to get the most bang for my (company’s) buck. ",11,8,1734635126.0,analytics
1hhzv1n,Course Recommendations?,"I’m wanting to keep my skills sharp while I’m job hunting because it’s taking forever. Unfortunately my current job doesn’t have any opportunities for me to use my data skills so I was thinking about taking some online courses. Anyone have recommendations? Udemy, Coursera, other? Or am I better off doing small projects and keeping them on GitHub? In particular I’d like to focus on SQL, Python, and PowerBI, but I’m not opposed to other suggestions!",7,10,1734633812.0,analytics
1hhyjbw,Master's in Data Science,"I recently obtained my BS in Electrical Engineering. I work in a manufacturing support role and am looking to further my education. I find myself liking using test data to analyze trends, see where we can improve in our process, and predict failures using existing data. I want to further my knowledge on methods I can use to help support the efficiency of production and testing of our product.

With this said, would y'all recommend me getting a Master's in Data Science? If not, is there something better that you might suggest I take a look at?",7,13,1734630429.0,analytics
1hhw29e,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",10,20,1734624027.0,analytics
1hib7s9,What should my job title be?,"**Here are my tasks:**

• Conceptualize, implement and build data models from ground up in both SQL & Power BI

• Automate various tasks via python

• Manage a team of outsourced software engineers to automate more tasks

• Process improvement projects

• Work with data engineering to have data show up in database / data lake ",1,8,1734666710.0,analytics
1hh56t7,Is it reasonable of my bosses to expect us to be data analyst and an economist? Unsure of what to learn anymore ,"For some context, my current team is very small and my daily work unfortunately involves churning adhoc data requests internal stakeholders than data projects. When i mean data projects, i refer to dashboards and playing around with data on a specific topic.

Lately, my bosses also expect us to do econometric modelling but they are not trained ij economics. I have undergraduate background in economics but I feel that this is always insufficient as many theoretical stuff are only taught in graduate school — as confirmed by my teammate who has graduate school knowledge in economics.

On a related note, my teammate also have extensive knowledge in programming and database including creating test suites, reading SQL scripts and API calling. All these were not part of my job scope and job description at all. Worst part is I have zero clue on how to begin them. 

So now I'm wondering, 
1. Is it reasonable for my bosses to expect us to do data projects, do research and/or econometrics project and do adhoc data requests with just the two of us? 
2. How can I improve my knowledge in econometrics (I use R) without graduate school? It's too expensive for me and my company cannot sponsor me. 
3. Should I be worried my teammate is clearly more qualified than me? The issue here is all these value-add they bring in were not what I was expected to do. Half the time i feel like an imposter with no clue on what's out there.
4. How can I improve my data analytics skills, e.g., using SQL in the real world, web scrapping, API etc? 
",41,38,1734538910.0,analytics
1hhjl86,Am I being asked to do too much?,"I’m sitting just a little over 60k (~60-61k) and I’m also at the lowest grade role that the company offers. Still an analyst, but there’s other analyst roles, admin, reporting, etc. that are higher grades.

I’m building out an org chart model. I need to be able to

- build an org chart from our server (doing through PowerBI, data from sql)

- compare that org chart to a should-be chart

- there’s multiple charts for different operating segments, and they all have different nuances and numbers

So…here’s a few “overrides” I’m doing to that allow us to make changes to our chart will still ingesting data directly from source:

- need “manual overrides” for employees whose managers are incorrect (fixing the source data is not an option because another incorrect record will show up before we can fix the first one…cycle repeats)

- need manual overrides for certain jobs, in the event a “standard job” shows up as multiple titles, codes, senior vs non-senior

- standard structures (things that don’t change like every operating division will have 1 accountant)

- changing structures (things that might be based on number of sales locations in a given area, ingested from another table)

- overrides (things that vary by region and don’t follow the “rules” outlined above)

Then I need to build out position_ids for these charts, which is complicated enough but I’m basically just planning to take “how many of this role are there” and index it the unique role so we can have (e.g.) salesperson-1, salesperson-2, and then I would need to index the actual chart and join the employees to the should-be chart

My actual big concern in all this is that it’s getting hairy. At first I thought it would be pretty simple conceptually, and it was, but now I’m adding a lot of levers to a tool that I may not even own long term. I’m thinking about how I might have to make a “data dictionary” to explain this thing and it just concerns me at the thought of someone else having to own this and figure out what the f*ck this thing is. It’s going to be used by non-data stakeholders so I need it to be at least moderately user friendly.

My other issues mainly have to do with the fact that it feels like I’m basically building out an application/software/tool. This isn’t just analysis, it’s not just pulling data, it’s not even some of the cool data modeling I did/have done as an intern. It’s messy, but I can’t figure out a more robust way to do this with all the “it should work this way but there might be some exceptions.” Does this at least sound like it matches my pay? I can’t tell if this is a lot to ask for me or not, I can see the whole thing in my head because I’m good at this stuff, but that makes it harder for me to gauge if this is something an entry level analyst with a good head on their shoulders might be able to do vs something that you might be pushing your luck for.

I know 60 is low, but the company isn’t reputed for their crazy salaries. 60 is low for my company though for an analyst role, but it’s the same “low” as another company’s low by relativity’. 60 is slightly above median for my pay grade. Anyways, So like I know I’m underpaid but I’m just struggling to gauge if this genuinely is a pretty cumbersome ask, or if it’s something you’d expect a fresher to be able to figure out by themselves. Frustrated because I feel like I’m setup to fail at something by being asked too much, I’m not allowed to consult with other analysts/data scientists because it’s confidential work.",3,22,1734578470.0,analytics
1hhq779,Is this a possible tracking issue?,"Greetings,

I have a big drop in my data, a frequency of a certain event that is triggered in distinct users... Now I notice a few weird things in the specific drop. 

* It's all browser data
* it's all Google Chrome
* it's all a specific browser version
* It's all from one city (Amsterdam)

What could this generally indicate and how would you generally perform follow up action to get a deeper idea of the cause? 

This data is in BigQuery. ",1,3,1734604831.0,analytics
1hhd1x5,how to do less,"How do you decide what NOT to do on your product roadmap?

I’ve been thinking a lot about how ambition can derail teams. Every new feature we add isn’t just more work—it’s more complexity: dependencies, testing, and the risk of things going sideways. Instead of delivering value, we end up managing chaos.

Take Google+ as an example. It tried to be Facebook, Twitter, and LinkedIn all at once. The result? A product that didn’t excel at anything and confused its users. Imagine if they’d focused on solving just *one* problem well—would the outcome have been different?

I’m curious how others here handle this. How do you make the tough calls to prioritize one big initiative over everything else? What’s your approach to saying “no” without killing momentum?",6,7,1734559302.0,analytics
1hhc9j6,What KPIs should I be measuring for a free-to-use productivity (time-keeping) mobile app? Is this a good retention rate?,"I launched an app this past year and it's sitting around 50% for 30 Day Retention and 29% for 90 Day. I'm seeing different benchmarks posted online, some say the average 30-day rate ranges from 27% to 43% and can be as high as 32% to 66% for high performing apps. Is this accurate? I'm seeing wide variation on the numbers listed (I know it's kind of dependent on industry) I think Todoist would be a good company to measure myself against but they don't publicly disclose those numbers. So it seems the app is doing well? But I am noticing user engagement is low, most users are only using the app a few times a month, on average <10 times / month.

Are there any other KPIs I should be paying attention to? I currently only have access to a report that only shows me who logged into the app in the past month and how many times (no timestamps or dates on their logins, just an aggregated number).

",4,4,1734557232.0,analytics
1hhluw4,"Hi, Can anyone suggest some good countries where analytics jobs are relevant, where market is good. I am thinking for Nov'25. I am thinking Newzealand. Any suggestions are welcome. 🙏 ","Rn I am a fresher getting difficult to find any relevant jobs, I am thinking to get some experience by end of 2025. And then go for masters outside for better exposure. Thanks ",0,9,1734586317.0,analytics
1hguv7e,what was your first data analyst career and how did you manage your fear and anxiety as entry position?,I'm just curious how life is going to be like as I go into data analyst jobs if I WFH or Hybrid. Did you manage your time well? Did you use mostly Zoom and Microsoft Teams as projects meetings? just curious as I graduate next year. Worried about laying off?,15,20,1734500186.0,analytics
1hh97dl,Transformation Step Involving Text Change - Best Practice?,"I have a dashboard, and the data is being extracted using a custom SQL query. There's a column that the business wants modified based on text, example:

Values of DBP-1, DBP-2, DBP-97, should all be trimmed to DBP. ATRP-2, ATRP-7 should all be trimmed to ATRP, etc.

My question is, what is the best practice for where this change should take place? Should I adjust the custom SQL to pull it in trimmed, or should I pull in the full data and make a calculated column in the dashboard to handle it?",1,3,1734549290.0,analytics
1hgzdyg,Requesting Laptop Recommendation for Data Analytics and Data Science (ocassional photo/video editing) folks.,"My budget is 1k to 2k USD. What's the best VALUE for money? 
I'm okay with both windows and mac (I'm leaning towards mac this time as they provide the best overall experience).

If I opt for mac should I choose MBA M3 15"" (16gb + 512gb) for 1300 USD or MBP M4 Pro 14"" (24gb + 512gb) for 1800 USD considering the additional benefits and longevity?

Your honest suggestions will be sincerely appreciated. 

Cheers guys.",4,34,1734520364.0,analytics
1hgcoit,"As an experienced data analyst, what are some of your best practices? ","Over the years of working in this field, what are some of the best practices (1) you think every data analyst should observe, and (2) you would have done in the beginning of your career in your first work (if you could go back in time)? ",111,39,1734448214.0,analytics
1hgy2wj,IB and PE role!?,"As a new investment analyst trainee with a role that combines investment banking and private equity, what daily tasks should I expect? 
Since I'm more interested in pursuing a career in investment banking, what specific responsibilities should I look for to ensure that I am aligned with the right domain?

Thanks 
",1,2,1734514365.0,analytics
1hgxy81,Help me pick my MSBA University,"Looking for help in deciding where to attend for a 16-month in-person MSBA program. I have been awarded merit scholarship at all of these schools. Please provide insight on any experiences or knowledge about these programs.

* Northeastern (D'Amore-McKim)
* Babson College (Olin School of Business)
* Brandeis University (International School of Business)
* Bentley University (McCallum School of Business)",0,6,1734513727.0,analytics
1hghjmu,ship faster = ship better,"Hey, I write a blog on product analytics (why number go up) and was curious to get feedback from some fellow analysts. Does this resonate with your experience? 

**the perfection illusion**

Have you fallen into analysis paralysis in hopes of finding the perfect answer? Endless dashboards, pristine PRDs, and perfectly aligned roadmaps can feel like progress but they’re often just distractions. You don’t learn about user pain by sitting in meetings or refining models. You only get there by shipping.

The longer you wait, the further you drift from reality.

**plans fail, products evolve**

No plan survives contact with the real world. Here’s the hard truth: No matter how much you analyze, you will never predict exactly what users want. Take Slack. It started as an internal communication tool for a game studio that failed. What they thought was the perfect plan for a game became irrelevant. By shipping fast and pivoting, they built a communication product millions now rely on.

Iteration always wins because user behavior is complex and assumptions break under real-world conditions.

**why shipping wins**

*Validate your assumptions*

Every product decision you make is a guess until users validate it. Shipping quickly gets those guesses into the wild and allows you to measure their impact. Analysis might help prioritize what to build, but only feedback tells you if it works.

Example: A team spends months improving a sophisticated search algorithm based on internal debates and assumptions. After launch they realize users don’t want improved search, they are looking for better content. If they had shipped improvement incrementally, they would may have seen this in their metrics sooner.

*Bet small to win big*

Shipping quickly isn’t about cutting corners; it’s about reducing risk. Smaller, faster releases help you make “small bets” instead of doubling down on a single, high-stakes feature. Small bets let you adapt to what works. Jeff Bezos calls this “two-way doors.” Small decisions can easily be reversed or improved. Ship them, learn, and iterate.

*Speed is good for morale*

Teams that ship quickly build momentum. They’re learning constantly, compounding improvements over time. When speed is prioritized, every small improvement adds up to better products and stronger teams. Teams chasing the perfect launch move slowly, get frustrated, and second-guess their (likely good) intuitions.

**how to ship faster**

1. Think small - Break large projects into atomic components that can validate hypotheses.

2. Stop chasing complexity - Prioritize simple projects that solve for a known pain point over complex projects that solve a suspected one.

3. Shipping as a metric - In the same vein of Elon's ""what did you get done this week"", anchor your team on readily measurable indicators of throughput and celebrate wins.

Shipping fast doesn’t mean cutting corners. It means getting real, messy data from the only people who matter: your users. You’ll never find the perfect product through analysis alone. You can only iterate your way there and speed is what makes iteration possible.

**tl;dr**

Stop overthinking. Start shipping. Iterate faster, learn faster, and you’ll build better products faster.",5,6,1734460916.0,analytics
1hgtqpf,Need Advice: Applying for a Business Analytics Internship Without Experience,"Hi everyone,

I’m starting my second year in a Master of Business Analytics program and planning to apply for a university placement next semester. The challenge is that I don’t have any experience in analytics yet—my bachelor’s degree is in a completely different field, and I’ve never worked in analytics before.

I know it’s okay to not have much experience as a student, but I expect the competition will be tough, and I really want to make my application stand out enough to land an interview. What do you recommend for someone in my position?

I’ve read that showcasing projects can help demonstrate your skills even without formal experience, and I have about a month until the application deadline. Are there any specific tips to do that?

Here are three sample placements I’m considering to give you a better idea of what’s expected:

# Placement 1

**Key Tasks**:

* Collect and organize data from various sources.
* Understand how data supports business functions like memberships, marketing, and partnerships.
* Perform ad hoc analysis and extract actionable insights.
* Present findings to internal stakeholders.

**Selection Criteria**:

* Passion for AFL and the sports industry.
* Strong analytical and data management skills.
* Intermediate to advanced Excel skills.
* Excellent communication and ability to present insights.
* Bonus: Experience with SQL, databases, Tableau, or Power BI (not essential).

# Placement 2

**Key Tasks**:

* Analyze and organize internal and external data.
* Learn how data is applied to business operations.
* Provide insights through reports and visualizations.
* Present findings to stakeholders as needed.

**Selection Criteria**:

* Strong analytical, data handling, and Excel skills.
* Clear communication and presentation abilities.
* Proactive attitude and eagerness to learn.
* Bonus: Familiarity with SQL, databases, and BI tools like Tableau or Power BI.

# Placement 3

**Key Tasks**:

* Collect and analyze data to generate actionable insights.
* Support business functions by applying data solutions.
* Create reports and dashboards for stakeholders.

**Selection Criteria**:

* Analytical mindset with data and Excel proficiency.
* Great communication and problem-solving skills.
* Proactive and self-motivated.
* Bonus: Experience with SQL, Tableau, Power BI, or databases.",0,1,1734496060.0,analytics
1hg3t42,DAE gets worried about the oversimplification of Data analysis?,"As the title says, lately I feel like becoming a data analyst is being treated as a ""get rich quick"" scheme, and honestly, it really concerns me. Let me explain why.

First of all, let me preface this by saying that I don’t think this is the hardest career to get into. Heck, it probably wouldn’t even crack the top 10 of hardest career paths,nor do I think it should. I genuinely believe everyone should be able to earn a decent, livable wage without having to study for 10+ years (Kudos to the ones who do tho).

That said, my main concern is how oversimplified data analysis is being portrayed. Everywhere I look, it feels like people are being told they can become a data analyst practically overnight. The number of certifications and bootcamps has exploded in the last years, and there’s no sign of it slowing down. Just Google “data analysis” right now, and I guarantee most of the top results will be courses promising to turn you into a data analyst in three months, one month, or even just a couple of weeks.

It honestly breaks my heart to see people signing up for these courses, because I really don’t think they’ll get what they need to actually become data analysts. Instead, they’ll probably just end up poorer and more frustrated. Heck, in a one-month certification, you might not even get a proper understanding of the difference between measures and calculated columns.

So, what do you folks think about this? I know we could just laugh it off, but I hate seeing people get scammed out of their money and watching my career path get devalued in the process.",31,45,1734413721.0,analytics
1hg2qou,Is a job in data science possible with a degree in Info Systems & Tech? Masters?,"Hi all. Hope all is well. I am in my 4th year with a degree in Information Systems & Technology with a concentration in Analytics. I am recently interested with pursuing a career in Data Science in the future but was not sure if that was even possible. After doing some research it seems math/stats, CS, or pure Data Science would be a better suited degree choice for an aspriation of a Data Scientist. The reason why I am not enrolled in a CS or more statistical/math based program is due to switching from another degree (a CS wouldve been impossible time & financial wise). So, I am graduating with my current and was also wondering what other jobs would possible aside from Analyst, Business Intelligence, and Cyber/Network Admin positions. I currently have experience working hands on with Data Analysis using Python, R, and SQL as well as a decent portfolio. However, im not sure if that would even cut it to being close enough to becoming a Data Scientist. ",6,10,1734409877.0,analytics
1hfxtgf,Best way to approach the job search for an entry level data position?,"Hi everyone,

  
Thanks for taking the time to check out my post here. 

  
Little bit about me:

  
Graduated from a target school for business and engineering in the NYC area with a bachelor's degree in Information Systems and management in May of 2023. My coursework consisted of a few Excel classes, an intro to Python class (which was taught extremely poorly), and a SQL class (also taught poorly with a professor we had to report for harassment among other things). 

  
The classes that were the most important to me were made obscenely difficult due to the poor communication, teaching environment, and my mental health. This is supposed to be one of the greatest schools in the area for this kind of thing, and I feel as though I've gotten absolutely no where in my degree. 

  
From December of 2022 to December of 2023, I applied to thousands of jobs, all well within my qualifications, and got no where. Spoke to recruiters, worked with my school, and really tried my best to land anything. 

  
I ended up getting a job as an IAE in sales for a VAR with an incredibly toxic work environment. I took this just to get my out of my retail job and was promised I'd make commission in 3 months, which turned into 6 months, and then a year. Started this place in January of 2024. I figured I'd try out sales to see if I like it, but I want out. 

  
My problem is my skills were never the best and now that it's been almost 2 years since I've graduated, I feel like I'm starting to miss the boat for this kind of thing. This has been absolute hell to sit and go through the application process knowing nothing will get returned. 

  
All I ask is the following:

What skills should I focus on sharpening to better myself as a candidate, and which courses would you recommend taking to sharpen said skills? 

  
What approach should I take to applying to jobs? I've tried recruiters, my school, LinkedIn, and looking for jobs on company websites. 



This is the most dehumanizing experience and I'm feeling defeated at this point. Then my parents get on my case as to why they helped pay for such a phenomenal degree and that I can't do something with it. 

Sorry for venting, this just sucks.

  
Thanks in advance.",12,15,1734394485.0,analytics
1hg58de,Analytics Career Progression,"Data science career progression, go for PGD Business Analytics (IIT B or IIITB), or learn on own

Hi. I am a working professional with 8+ years of experience in analytics (expertise in multiple tools). My_qualifications B.Tech from NIT. I am looking for a job switch as my growth and salary have stagnated, but honestly on various job boards, I have been unable to fetch even an interview. I am seeing lot of job profiles asking for ML (and some AI) and statistics. I have never had a certification in statistics. Higher management roles require MBA/ PGDM. Now I am exploring 3 options:

1. IIT B newly launched ePGD in AI and DS. It has got statistics as well in curriculum. It is an extensive 18 months course but the good part that it is online.

2. Similarly IIIT Bangalore with upGrad is offering Executive PGD in AI and DS. 12 month course, seems to have best reviews among other 12 month programs.

3. Learn stuff online on own and get certifications on Edx, coursera or Udemy. Tutort DS program seems legit too.

Now, first 2 options give me a degree (kind of certification) which I am reading online is not considered as an eligible degree by employers.
I can’t do MBA at this point of time, and I feel I won’t be able to do it later as well due family and MBA programs taking 2 years and so much money.

What is the best way to take my career forward?",3,1,1734419456.0,analytics
1hfkxaf,Mismatching numbers in different dashboards - how much time do you lose on this?  ,"In my company there's far too many dashboards, and one of the problems is that KPIs never match. I am wasting so much time every week on this, so just wondering if this is a common problem in analytics. How is it for you guys? ",46,35,1734361086.0,analytics
1hg6kb0,"Question for Data Analytics and Industry Professionals RE: ""The User Experience"" (From: a user)","So, serious question - do any of you work, or know anyone who works on UIs/Databases/User Experience designs specifically for one of the various streaming giants: Netflix, MAX, Prime, Disney+, Hulu?  


I apologize first-hand if this sounds overly-angry, but this problem has gotten EXPONENTIALLY WORSE over the past 7 years or so, in particular and will only be exacerbated by BULLSHIT AI TRENDS.   


Because I HAVE A NITPICK.   


Why do you make your service impossible to index properly to the average user? Occasionally, I may be able to sort a genre or sub-genre alphabetically, such as with MAX; however, the default sort is ALWAYS a smattered, mish-mash of ""here's shit you're likely to click on"" or ""more recent/promoted"" results...JUST LIKE THAT HORSE-SHIT THAT GOOGLE'S been pulling for the past five years.   
  
Look, I just want to know exactly WHAT IS THERE, and I DO NOT want it appearing in multiple categories thereby increasing the time it takes to scroll through each category, NOR do I want a NEVER ENDING category based on some bullshit, algorithm that tries to generate results on the fly via applied \[TAGS\], popularity, or god KNOWS however other ""hidden metrics,"" that we as users cannot see, nor control. (as is the current trend)  
  
From a design/user-experience standpoint, IT'S A GODDAMNED NIGHTMARE. IT'S FUCKING STUPID.    
  
I realize this may save the labor of having to curate and sanitize your records, BUT IT IS MASSIVELY UNFRIENDLY TO YOUR ENTIRE USERBASE.   
  
Look, here's what I'm talking about. I walk into a Library Today, or a Blockbuster Video a decade ago, titles aren't spontaneously being shuffled from one shelf to the other, aside from NEW RELEASE to OLD SHIT. The STATIC LOCATION of said item helps ME, the USER, keep track of what's available and what's no longer available. It makes BROWSINGS a MILLION TIMES EASIER, not to mention more satisfying.   
  
This is WHY people LOVED GOING TO THE STORE. The sort MADE SENSE, instead of whatever logarithmic HORSE SHIT you software developers are fucking with today.   
  
And it's not just limited to the Streaming services. This is what Amazon does, this is what STEAM does, hell - this is what FACEBOOK has been doing with your posts, instead of just giving a ""TRULY"" chronological timeline of your contacts.   
  
I   
DO  
NOT  
NEED/WANT    
YOU   
MAKING   
DECISIONS   
FOR   
ME   
  
FUCKING FIX IT. The amount of autonomy that's been STOLEN from users by the modern data informatics and analytics paradigm because of current data trends is FUCKING CRIMINAL.   
  
It intentionally obfuscates ""what is actually going on"" to ANYONE who doesn't have direct access to the individual data-records, and is ethically OPENLY HOSTILE TO THE END USER.   
  
In other words, YOU NEED TO MAKE IT POSSIBLE FOR ME (THE USER and YOUR CUSTOMER) TO DO A ""REAL"" not ""LOGARITHMIC"" search query of your records.   ",1,12,1734425589.0,analytics
1hg34fu,Data analytics,"Hey! I want to develop skills essential for data analytics, what skills I should start working on? Let me know best platform for that",1,15,1734411256.0,analytics
1hfwj6x,Examining the residuals when conducting a negative binomial regression,"Hey, what do I look for when looking at the residuals for a binomial regression? No matter what transformation I do, the residuals vs expected values are heteroscedastic. Is this supposed to be be this way for negative binomial regression?  What other assumptions of this model must I test?  I'm really struggling here so any help I much appreciated",1,4,1734390882.0,analytics
1hfl9il,What’s the first step in the thought process w.r.t analysis?,"Hello folks,
It’s been year since I translated to Data Science and working as a data scientist. This question is basically to learn how to go about dynamics of analysis. How do you all go about analysis? What’s the 1st step in the thought process of analysis to final model building you take? How do you decipher some aspects in the initial steps which translates to a model that effectively answers business questions and needs?

I still finding my feet in Data Science world. I switched my career from dentistry to Data Science. LOOKING FORWARD TO LEARNING! Thank you.",2,2,1734361997.0,analytics
1hf84z6,How can I effectively show employers my data projects?,"I'm currently working on a few personal data projects to show employers that I am actually capable of the skills which are listed on my resume (I'm a senior in university with no industry experience). I'm wondering if anyone has good advice on how to best display these projects so that employers don't just think I'm another person essentially copying and pasting code. 

  
One idea that I've had includes the links on my resume to some app or website like Kaggle or GitHub where I would have: a summary of the project (goals, outcomes etc.), the code that I've written, visualizations, a PowerPoint presentation, and step by step breakdowns of my thought processes throughout the project sprinkled intermittently. Essentially a more professional blog post of sorts. I think this way I could really touch on all of the phases of the project and properly display all of the work that went into it. The project would involve python (web scraping), SQL, and a visualization tool, and possibly some implementation of cloud services/databases. 

Does anyone have any recommendations? Is what I'm looking to do even necessary or effective? ",9,14,1734313405.0,analytics
1hfcm8k,Getting Masters in Information Systems and Analytics. Where Do I Go From There?,"So I'm currently enrolled in a master's program in information systems and business analytics and slated to graduate 2025. I'm also a senior financial analyst at a mid tier hospital system. I mainly do excel based data analysis, as requested by my supervisor, basically so that my work is easier to pass on if I leave (or get canned). For a few years, the forecast for the clinical side of the hospital system all hinged on my spreadsheets... we've since moved onto using a big name company cloud based application. As far as day-to-day goes I do forecasting, budgeting, and budget system admin for end users in smaller departments. I have a bachelors in finance, 7 years of experience, and make 95k a year.

In school, I have a decent handle on SQL and getting better with Python. Worked with powerbi, azure machine Tableau. And NLP, and tableau.

If I just applied to all IT and data jobs as much as possible, what type of jobs could I reasonably expect to get callbacks for that would pay more than what I make right now? I have enjoyed pretty much everything I have studied so far with cybersecurity being the least (but still fun). I ask this because I see the job market is tough right now and worry I won't be able to break in without taking more than just a haircut to my salary.",2,1,1734328620.0,analytics
1hfbdri,Australia - Post Grad options - DS/DA,"Hey all, got a few offers to study a Master of Data Science/Analytics online across a few universities here in Australia, but I’m having a bit of a hard time deciding on who to go with as I value the quality of education over the prestigiousness of the institute. So far, I have offers from UNSW (M Analytics) and UniSQ (M Data Science), whilst still waiting on a response from Griffith (M Data Science) and UNE (M Data Science). Can anyone speak of their experience with these degrees, or can comment on their learning experience at these institutes? Cheers!",2,1,1734324133.0,analytics
1henwfn,Data Teams Are a Mess – Thoughts?,"Do you guys ever feel that there’s a lack of structure when it comes to data analytics in companies? One of the biggest challenges I’ve faced is the absence of centralized documentation for all the analysis done—whether it’s SQL queries, Python scripts, or insights from dashboards. It often feels like every analysis exists in isolation, making it hard to revisit past work, collaborate effectively, or even learn from previous projects. This fragmentation not only wastes time but also limits the potential for teams to build on each other’s efforts. Thoughts?",80,29,1734249314.0,analytics
1hez1lc,Sales vs marketing vs analytics?,"
If you are comfortable sharing:
1. What industry and what background did you have?
2. Where were you happiest?
3. What was your pay in each and progression as you aged/advanced? 
4. Looking back, what do you wish you looked into or did differently?


Background- currently in med device. Got in with a great rotational program post grad and got experience in marketing, analytics, education, and now field sales. I’m struggling to find out next steps. Company outlook isn’t super positive, my current role is draining me, and I liked the aspects and lifestyle of marketing but like the idea of more reward in sales. I also see the ortho industry is dying a bit. ",12,21,1734287794.0,analytics
1hfcr9r,Do I need to get my BS/ Masters,"I am already a Pharmacy Data Analyst, and am trying to transition out of healthcare into a more finance related role or FAANG eventually. 

Majority of applications I apply for requires a BS/Master, but I don’t have it. Is it worth it to back to school for these pieces of paper? I have 3YOE already.",0,12,1734329137.0,analytics
1hf3ea6,Starting a new career with data analytics,"I just started going back to school for Computer Information Systems. My main focus is Data Analytics and Networking. Is Programming a good path with Data Analytics vs Computer Science? Quick background, I work in Healthcare and want to apply both discipline to become a Healthcare Information System/Data Analyst. Thank you Guys for any input",4,35,1734299539.0,analytics
1hf8lrk,☎️   Tracking calls after an ad click   ☎️,"UPDATE: This is not about working out stats, it is about assigning a Google Click ID to a person we have in our CRM. That way we know that person responded to a specific ad and we can attribute the conversion correctly.  
  
How can I track a call from a website if someone clicks on a Google Ad, ends up on the landing landing page, but gets distracted. They then disappear for a few days and call directly from the website?

The click would not be attributed to the call and subsequent conversion.

I am using the following platforms just in case that helps:

Google Tag Manager

GA4

High Level

Zapier",1,5,1734314864.0,analytics
1hf7yym,"Are There Any Worthwhile Certifications Concerning Novice Experience in Business Analytics/Data Analytics, Excluding the CBAP?","Hey all, I know this probably gets asked ad-nauseum here but I'm having a hard time getting a straight answer online. I'm just looking to see if there are any well-known certifications (as in, a recruiter for a Business Analyst or Data Analytics role would likely know what the certification is and it could give me a leg-up in the job market) for Business Analytics or Data Analytics. The only one that I could find like that is the CBAP, but I don't have the years of experience for it (3 yrs).

Any comments are appreciated! Thank you!",0,3,1734312863.0,analytics
1hfdice,How AI affect your workflow and actions,I am a data analyst in a gaming App. I just wonder how AI stuffs like chatGPT affect your workflow? eg: you will do some correlation analysis or reporting analysis just based on AI tools nowadays?,0,2,1734332328.0,analytics
1hek1ny,Is econometrics essential for business analysts? ,"I’m considering between two masters. One is informational technology. This includes a bit of everything regarding tech including analytics. The other master’s is strictly analytics which includes econometrics. It also includes prescriptive and predictive analytics (which actually is also offered in the informational technology master’s). 

They both share other classes like R, python, Tableau and such. Oh. And big data.

I am lost. 

",12,37,1734233832.0,analytics
1hegre2,makes decisions (not dashboards),"I started a Substack - whynumbergoup.  I'm only two posts in but would love to hear early feedback!

*It’s a common pitfall for data teams: leadership has a constant stream of questions, and you respond by building a 50-chart dashboard nobody checks. Product wants segmentation? You deliver a sophisticated clustering model that never sees production. Marketing wants acquisition tracking? You perfect a complex attribution model that fails as soon as a new channel appears.*

*Too often, we trade tangible impact for false sophistication. Sometimes these projects even earn praise, fueling a cycle of busywork. You’re working hard, getting recognized, but can’t tie your hours to real results.*

*In a data-saturated, attention-addled world, there’s no end of well-rationalized but low-impact analytics work. The key is to identify what moves the needle—the 20% of effort that yields 80% of the outcomes.*

*The solution? Break the loop. Focus on decisions. Your role is to help the organization make better, faster decisions. Everything else is secondary. If your dashboards, analyses, or models don’t directly inform a next step, pause and rethink.*

*You’ll never build a perfect product, but you can iterate toward one. Analytics truncates the distance between question and decision, letting you learn and improve with each cycle.*",11,7,1734222758.0,analytics
1herlzm,"Hi people, need some help with PayPal interview","Hi all, I'm having the third round of interview for PayPal for the role of Data Analyst - 2. It is in the collections strategy team and it is with a senior manager. In the last two rounds I was asked normal questions, cases, some SQL questions. I think this third round will be the final round. I wanted to know how the third round generally is, what can I expect from the interviewer. If anyone has gone through the process, please shower some knowledge, thanks!",0,7,1734265766.0,analytics
1hf2268,Should I study date analyst? ,"Hi everyone! Can someone tell me a bit about working in data analysis? I’m considering studying it but feeling unsure if it’s the right fit for me. I’m not very strong in math, so I’m worried about whether I’ll be able to handle the subject if I pursue it. Any advice or insights would be greatly appreciated—thanks in advance! #dateanalyst ",0,5,1734295927.0,analytics
1hef40h,Should I enroll myself in university again?,"I want to break into tech. I moved to USA last year and I have masters degree in information technology majoring in data analytics which is an international degree. I’ve worked in different industry and nothing related to my field. After relocation i lost my previous job and now im out of the work for a year. I’ve been applying to jobs in data analytics but had no luck whatsoever. I also tried to apply for different roles like IT tech support, project coordinator, etc. I was able to get 3 interviews out of those but none of them worked out. I’m confused about what do to now. Enrolling in a college would be a better option to get opportunities? 
Any suggestions is welcome 

Edit: I Graduated 4 years ago.",4,6,1734217763.0,analytics
1hdz2sz,Picked up a free Statistics book. Worth the time?,"I’m a college student with about 6-8 weeks off. I’m currently taking Google Data Analyst course and it suggest few books about big data etc. Obviously im gonna try to skim them and learn as much as I can but what about the textbook on STATISTICS: learning from Data? 
Really want to make sure I invest my time wisely, and my GE classes giving me an option to skip Statistics and take Econ for Data Science major. 
How much value can you really get out of studying a textbook on statistics?",28,12,1734166840.0,analytics
1hes3bj,Any recruiter who is willing to hire a data analyst for free please reach out me ,"I am looking for data analytics/ data science opportunities in the US. As the title says, I am willing to work for free. I have around 1 year of experience in data analytics and also a masters in CS. Please reach out to me if you have any relevant opportunities. I am open for internships also.",0,3,1734267602.0,analytics
1he0rap,supply chain to business analyst? how to make resume better w/o business analyst experience? ,"hello. i’m a senior in college, majoring in supply chain. I have experience analyzing supplier lead times, and performing cost and category analysis in excel from internship. i also have  tableau experience through a class project. I would like to be a business analyst but my major did not teach me the hard skills needed for it. SCM is an analytical field, which could maybe help me transition into the field. I would like to get a certificate (not sure which one) and create a portfolio that I can submit alongside my resume as a supplemental. do u think this would benefit in getting interview or making me a more competitive applicant or no? what are some ways i can improve my resume to increase chance of landing business analyst role? Ik there are supply chain analysts, but i still do not have enough technical experience and knowledge for these roles (most require Power BI and SQL, and sometimes coding languages). I’m looking at general business analyst roles and pricing analysts. let me know :) i know it’s unlikely I get a role like this anytime soon, but I wanted to ask ",3,6,1734174559.0,analytics
1hdldpd,Anyone know of a publicly available Grocery Dataset?,"Right now groceries are a hot topic item.  I also know that grocery prices vary depending on where you are located in the US, but figured there might be something out there that has that data to at least get some baseline change over the next few years.

  
I checked Kaggle, but all the datasets were old.

",21,9,1734121430.0,analytics
1hd6j5h,The guy who wanted to take his own life that posted in this sub,"Remember the guy threatened to off himself if he couldn’t get a job in analytics even if he is overqualified. Where is he now?

It’s been a month. Did somebody reported him to suicide prevention?

Even though you’re an asshole to everyone I hope you’re still alive somewhere.",76,21,1734072324.0,analytics
1hdp1pa,Meta Product Growth Analyst technical screening next week! Please help!!,"Hi, I have my technical screening (2nd round of 60mins video call) for Product Growth Analyst at Meta next week. The prep doc mentions it’ll contain SQL + a product sense question. Can somebody please help with what kind of product sense case study questions should I be expecting? Any prep materials or question banks would be super helpful? I’m kind of freaking out as I don’t have much experience with product case studies. I’d appreciate any help! TIA!!",2,4,1734131533.0,analytics
1hczj9b,Hit the 4 year mark as an analyst at my company. Looking for a reality / sanity check as to what is my actual role?,"Hey everyone,

As the title says, I’ve been a data / business analyst on the marketing team at my company for just over four years now and am considering looking for a new role, however the issue is I have no idea what I even truly can call what I do-- I do a bit of everything but feel I'm a ""master of none"" in the eyes of roles beyond what I'm currently in. Including my last position, I have about six years of experience doing various work with SQL and Tableau.

(for the record, I didn't run this through an AI program or anything, so if it reads weirdly, apologies!)

Since starting, I have done (and continue to do) a variety of things:

* Tableau
   * The marketing team had no BI/reporting prior to me, so I got us into the Tableau ecosystem
   * I currently create and manage multiple dashboards for 15-20 people, am the administrator for our Tableau portal, handle all Tableau Prep Flows, and the SQL pipeline used to power the viz layer
   * All imposter syndrome aside, I would say I'm pretty good with Tableau. Various people on my team have said they've never seen somebody do in Tableau what I can do as quickly as I can do them, including using Tableau for live table QA
* SQL and Data
   * I am responsible for a large chunk of financial and virtually all surfaced marketing data at the company. My job is heavily SQL based and touches on many different types of data, from performance marketing to user retention to in-product analytics using log-level Mixpanel data
   * I work with various teams to spec out important metrics, where to source from platform portals, working with devops to develop connections to our database, mapping out how this should look at the table and reporting level, and all QA– from the digital channel to the database to the dashboards
   * Various team members have told me that my SQL readability is ""unparalleled"" and window functions and stuff are pretty easy
* QA
   * Our data is very dirty, so a huge chunk of my time is spent doing QA – unit tests on data, finding discrepancies and putting in tickets with our data team to rectify
* Reporting
   * I’m responsible for putting together all marketing reporting for c-suite and the board of directors
   * Due to a large chunk of our data not existing in the database, and because marketers will never leave gSheets, I also maintain every gSheet report
* Things I wish I had more opportunities to explore:
   * I help surface insights when I can, however given my bandwidth it’s not feasible to do in-depth exploratory analyses for and/or alongside subject matter experts on each team
   * I feel like I’ve been funneled into a role building and maintaining reports, and while I can and do offer insights into how campaigns perform, I don’t have a hands-on look into how our performance marketing is being tweaked by our agencies
   * While I know how to use Python, I haven’t done even a fizzbuzz in years– I mainly use it for very basic API pulls in Databricks but haven’t had much experience with using python for stats
   * My math isn’t the strongest. I have a degree in Econ and haven’t done a linear regression in over a decade at this point, hah

For full transparency, doing the above, I an full time and currently make $133k a year and have had a consistent 20% bonus every year. (Bay Area) 

Thanks in advance for any insights :)",53,24,1734048952.0,analytics
1hdhlyj,Should I Accept McGill University’s MS in Management Analytics Offer or Wait for Other Options?,"I recently received an offer from McGill University for their MS in Management Analytics program, along with a CAD 10,000 scholarship. The tuition fees are CAD 70,000, and with living expenses, the total cost will be around CAD 80,000 (\~₹50–55 lakhs). I have savings of around ₹20 lakhs and will need to take a loan for the remaining amount.

For context, I have five years of experience working in an analytics role in a government organization in India. My long-term goal is to transition to an international organization. I initially applied to McGill because of its reputation and the program's alignment with my career aspirations, but now I’m reconsidering due to the financial burden.

Additionally, I’ve applied to the Rotman School of Management (University of Toronto), and their decision will be released on December 23. McGill requires my decision by December 20.

Here are my questions:

1. **Is McGill’s MS in Management Analytics worth the investment of CAD 80,000?** What is the ROI, especially for international students with prior work experience?
2. **How does McGill compare to Rotman for analytics-focused programs and job prospects in Canada?**
3. **Does McGill allow extensions for decision deadlines?** Has anyone successfully requested one?
4. Alternatively, would it make sense to continue upskilling in India (through certifications, online courses, etc.) and apply abroad later when I have more savings or a clearer plan?

I’d appreciate any insights from current students, alumni, or anyone familiar with these programs or the Canadian job market. Thanks!

  
Edit: I also got into Rotman. But as mentioned in the comments I think it is better to consider doing it in India.  As there are pleothra of courses on data science these days , will be happy if someone suggest a reputed course from IIT or IIMs for working professionals. 

",1,15,1734111487.0,analytics
1hdk3n1,Choosing between graduate programs,"Hi y’all,

I’m looking for some advice on grad school decisions and career planning. I graduated in Spring 2024 with my BcS in statistics. After dealing with some life stuff, I’m starting a job as a data analyst in January 2025. My goal is to eventually pivot into a data science or statistical career, which i know typically requires a master’s degree.

I’ve applied to several programs and currently have offers from two for Fall 2025:

1: UChicago - MS in Applied Data Science
* Cost: $60K ($70K base - $10K scholarship)
* Format: Part-time, can work as a data analyst while studying.
* Timeline: 2 full years to complete.
* Considerations: Flexible, but would want to switch jobs after graduating to move into data science.

2: Brown - MS in Biostatistics
* Cost: $40K ($85K base - 55% scholarship).
* Format: Full-time, on-campus at my Alma mater.
* Logistics: Would need to quit my job after 7 months, move to Providence, and cover living expenses. My partner is moving with me and can help with costs.
* Considerations: In-person program, more structured, summer internship opportunities, and I have strong connections at Brown.

My Situation
* I have decent savings, parental support for tuition, and a supportive partner.
* I want to maximize my earning potential and pivot into data science/statistics.
* I’m also considering applying to affordable online programs like UT Austin’s Data Science Master’s.

Questions
1. Which program seems like the better choice for my career goals?
2. Are there other factors I should think about when deciding?
3. Any advice from people who’ve done graduate school or hired those fresh out of a masters program?

Tthanks in advance!",0,2,1734117985.0,analytics
1hcv0b1,Analytics Director Salary?,"I'm actively applying to director level roles at larger, non-tech organizations and the salary range on the job description is usually pretty wide (e.g., 140K - 300K base salary at one place and 180K - 320K at another).

I'm currently a Senior Analytics Manager making approximately 175K base + 25K bonus with 3 years of experience in this role but more than a decade of experience in analytics.  Looking for the next step in my career and a pay bump.  

Three years ago I was in the running for a director role and was told they were looking to hire at 190K, so I imagine it would be more now.  What's a reasonable base salary for a director level role in the analytics field?

Would 210K - 220K be pushing it given the wide range (140 - 300K)?  220K is the mid-point of the aforementioned range.",33,24,1734036650.0,analytics
1hd3pp4,Will a masters in Data Analytics help me get into becoming a ML/AI engineer? ,"Am I delusional for thinking that a data analytics masters sounds like it can help me pivot into a Machine Learning role? 

Are these types of degrees cash grabs that have no real substance to them? I am looking specifically at the degree from University of Maryland Global Campus


Background: 

I am currently a IT system administrator working for an employer where they will cover the cost of any graduate degree completely, so I would like to take advantage of this opportunity.

I would like to eventually take my career to possibly becoming a Machine Learning engineer away from general IT. I have a bachelor in IT and one in cybersecurity (dual major). I know python pretty well as I use it a bit at my job for automation. Have used pandas and numpy for projects in school but that’s about it. ",3,15,1734061958.0,analytics
1hd3osz,"From your experience, what are the best practices to really learning your org's data?","I will speak objectively to focus on the question. The org has some DEs who have deep knowledge in our product data, sources of truth, linking data together, etc. Assuming you want to get to the same level for the purposes of analytics and be self-reliant, what would your approach be? Of course, DEs are busy with their day job and you can only meet with them so much - how would you explore your data tables and understand your environment? There are a ton of tables and variables within - what’s a best practice approach to exploring these elements and starting to document and track variables in unfamiliar territory?

The data are in Google Cloud Platform and can be accessed  with SQL BigQuery. Thanks!",2,5,1734061877.0,analytics
1hbxgm9,Director of Data Science & Analytics - AMA,"I have worked at companies like LinkedIn, Pinterest, and Meta. Over the course of my career (15+ years) I've hired many dozens of candidates and reviewed or interviewed thousands more. I recently started a podcast with couple industry veterans to help people break in and thrive in the data profession. I'm happy to answer any questions you may have about the field or the industry.

PS: Since many people are interested, the name of the podcast is Data Neighbor Podcast on YouTube",577,251,1733934727.0,analytics
1hcaiyr,Job Search Vent,"I know I’m not alone in this, but I am so frustrated and beat down right now. After over 200 applications, over half of which resulted in absolutely no response whatsoever, I landed an interview. And advanced round after round. All in all over the course of 2.5 months (yes, months) I completed 7 interviews. Yesterday I found out I didn’t get the job and received no feedback as to why. 

Seriously- anyone who has landed an entry/lower level remote analytics job recently, how? What did you do to stand out? ",32,22,1733969196.0,analytics
1hckimc,Will this lead the career to  data analysis?,"Will this role lead to data career?

Will this lead the career to  data analysis?

Started with the product support where I was more kind of helping with conversion tracking for users online stores and help issues related to it and some errors by google.

After 1.6yr I got offered a role in other company as Product support/Data analyst whose JD is this:

“Provide support to the e-commerce and logistics web projects by examining and optimizing integrations and configurations. Resolving incoming client issues through the ticketing system to maintain a seamless operational process.

Analyze processes to identify areas suitable for automation and working with the team to implement automated solutions. Contribute to data enrichment initiatives and develop skills to optimize business logic and improve overall efficiency.”

Hr confirmed me that this role will lead the way and according to interview a team member also asked me interview question on etl, advance excel and whether I know tableau. 

After landing this job mostly I am doing product support, however, they say find the area suitable for automation. Further, there is as such no tool involved as of now don’t know if I get to use data analysis tool but wanted to know is this career path will lead to data? Or will stuck to product support any advice would be appreciated!! :)",4,7,1734008247.0,analytics
1hcir6c,Looking for product&marketing&revenue analytics tool,I have been wondering whether there are analytics tools which have all of these analytics at the same place. Do you know anything?,2,5,1734001561.0,analytics
1hc8j3e,Data Help,"Hello! I am currently in need of help with some HR analytics excel regression models. I was wondering if this subreddit would be right one to ask for help, or if anyone from this subreddit would be willing to help? Thanks!",4,4,1733963210.0,analytics
1hbhr1o,Data Analytics worth pursuing in 2024 with no experience?,"Hi everyone,

I’m in a bit of a crossroads in life and looking for some honest advice. I’m a 30-year-old with a degree in Statistics from a university in the US, but due to personal struggles (including some mental health challenges I’ve worked hard to address), I’ve been out of the workforce since graduating 8 years ago. Unfortunately, I have no professional experience in my field—or any field, really.

That said, I’ve recently gotten serious about starting a career, and data analytics has always intrigued me. I enjoy problem-solving, working with numbers, and tools like Python make me feel confident and capable. However, I’ve been struggling with doubts because the job market is obviously brutal right now. Every time I hop on Reddit or LinkedIn, I see posts about people with years of experience and polished portfolios struggling to get interviews. It’s discouraging, to say the least.

I’ve been researching and planning to relearn SQL, Python, Tableau, and Excel over the next few weeks. My goal is to build a strong portfolio, apply to small-to-mid-sized companies, and avoid big tech giants where competition seems overwhelming. But I can’t shake the fear that my lack of experience and the 8-year gap on my resume will make this a lost cause.

So here’s my question:

* Is it realistic for someone in my situation to break into data analytics in the next year, especially in the current job market?
* Would upskilling and building a portfolio be worthwhile, or should I pivot to another career path altogether?

I’m open to honest feedback, even if it’s not what I want to hear. I don’t want to waste any more time if this path is a dead end. Thanks so much for taking the time to read this—I’d really appreciate your advice or any words of encouragement (or a reality check).

**TL;DR:** 30, degree in Stats but no work experience, 8-year gap. Want to pursue data analytics but worried the market is too saturated. Is it worth it?",47,18,1733879627.0,analytics
1hbj0ns,"NYC Area Analysts, please reach out, I may be hiring for full time at a we’ll known company","We haven't opened the headcount yet. But I expect within 3 months I will be hiring for a business analyst role or 2. We have not yet decided on seniority, there's some flexibility at this point. Would be hybrid in nyc. Bonus points for experience in Media and / or Data Visualization. ",9,25,1733883483.0,analytics
1hbehl3,Entry job,"I'm searching for a data analyst  job, I've completed my master degree in applied maths, did internships, even small freelance jobs, but now I'm really struggling getting an entry job, my moral is getting lower by the day, I'm not the type of guy that barely code in a jupyter notebook, I can actually code with a more than intermediate python and SQL, I have some github projects, know a bit of aws, databricks, yet it seems lost, how can I pivot, is all hope really lost, entry jobs in data seems to just not exist anymore.",17,21,1733870474.0,analytics
1hb8zk2,Math degree but 15 years in unrelated field - transitioning to data analytics?,"Hi everyone, I’ve been browsing this subreddit for a while, and have found it helpful. Would appreciate some insight on my situation. Some background: I have a quantitative (Math) undergrad degree and did well. However, this was 15+ years ago. I’m in my late 30s and, long story, ended up doing a few things that are completely not relevant (think legal/compliance) after graduating. I hate what I’m doing now and want to pivot to data analytics. But it’s been a long time since undergrad and I have no even remotely relevant work experience, nor am I in a field of work where I even have datasets to play with.

Realistically, how difficult would it be to make this transition, especially relative to someone fresh out of school? Has anyone here made a similar change? I’m willing to start from scratch as I haven’t progressed nearly as much as I’d like in my current field anyway, but would rather not go back to school for too long although I will if I have to. Thanks so much for any insight / advice.",5,8,1733856438.0,analytics
1hb1ukh,HR Data Analyst: Where should I go next with my career?,"Hi everyone, I am currently an HR Data Analyst in the UK. I have been in HR for 5 years now and the last 2 years in HR analytics. I am in a situation where I identified 3 possible routes to further develop my career, but none feels right at the moment, so if any senior data analyst or hr data analyst can give me some advice, it would be very much appreciated.

1. **SQL & Python**: This is the path I find most appealing technically, but because the last time I used SQL and Python (Pandas) intensively at work was already in 2022 and at the moment I mostly build things in Workday, I don't have practice space and things get rusty. I can still self-learn using Leetcode, and building my projects, but I am not sure if it would be enough to get my expected salary if I don't use it at work every day. Due to personal responsibilities, I am not open to a pay cut for learning at the moment.
2. **Advanced Statistics & Regression**: The good thing about HR data is that even though the size is smaller than product data or sales/marketing data, I have room to play with most of them. If I can do some data science work I think I can unlock a lot of insights for the organisation. The problem is this is way harder to self-learn compared to SQL & Python. I tried Google Advanced Certificate for a bit but after finishing the courses, I still feel I am not ready to initiate an A/B Testing or Multiple Linear Regression for a Turnover project in HR at all without guidance. I have an assumption that only a Data Scientist would be the proper person to run this kind of project.
3. **Workday Data & Reporting**: This is the path that technically and financially makes the most sense to me, as I am quite strong in building things in Workday already and can look for Senior/Lead Workday Reporting & data roles in the future. However, whoever knows Workday will understand that its syntax is quite rubbish, with a lot of repeated 'code' and unnecessary manual work; and I feel once I go deep into Workday it is hard to get out and learn any industry-wide skills like SQL and Python anymore.

I do want to get better in my career, but at the moment all 3 options are not so clear to me. Very much hope for some advises and thank you so much in advance!",11,5,1733837660.0,analytics
1hbeyjg,Level 4 data analysis qualification without apprenticeship (UK). Is this possible?,"Hi I am interested in learning more about data analysis but I don't want to an apprenticeship scheme. I'd rather just study for it independently. Is there a way to do this?

Sorry people outside of UK :)

Thank :)",1,4,1733871733.0,analytics
1hb6f2q,Business Analytics major vs MINS,"Hi all, I will be attending Chico State Univeristy next sem to major is Finance but have been really interested in either **Business Analytics** or **Management Information Systems** as a double major (or double concentration). Im a little confused on what the main different is between BA and MINS. According to my program descriptions it seems like BA is more managing data for business decisions with integration of machine learning, AI, and programming languages for processing data. MINS is more designing and maintaining the data bases. Both concentrations have a lot of overlap in the description, and I'm honestly surprised that the Business Analytics option would have techinical skills such as machine learning. However, it seems that MINS would cover the same topics.

In the description of the MINS program it states ""The MINS option is the bridge that connects business with technology by focusing on innovative uses of technology to address business needs or to harness business opportunities"". However, isn't Business Analytics kind of the same thing? I have looked at the courses for both, and they seem similar maybe just more data analytics for BA. I have also done plenty of research online and it seems that the difference varies between universities.

**Here are some of the classes for each option (for reference):**

**Business Analytics:** Data Analytics, Database Application Dev., Business Intelligence & Data Warehousing, Cloud Computing in Business

**MINS:** Business Programming, Data Communications & Networking, Enterprise Resource Planning- Systems Configuration and Use, Business Systems Analysis

 I just wanted to get some opinions from folks who have majored in either of these?



(Yes, I understand that Finance is a very different field, but thats why I'm interested in BA and MINS. While these concentrations may not aide in finding a finance related job/career, it would make me feel confident knowing I have the skills if I ever wanted to change paths... and they just seem fun!! I also posted this on my colleges' reddit forum but have not got any responses yet so I figured this would be the next best place. Thanks!!!)",2,6,1733850015.0,analytics
1haj086,What are some data-adjacent jobs I could do with my skills?  ,"Hey everyone,  

I’m 24 and graduated in May 2022 with a bachelor’s degree in math (stats emphasis). Right now, I’m working as a substitute teacher, but I’ve been trying to break into a data analyst role for the past couple of years. 

I’ve had a few interviews, but most of my applications end in rejections. So now, I’m wondering—what are some data-adjacent jobs I could look into with my background and skills?  

Here’s what I’m working with: Python, SQL, R, Tableau, Excel, Git  

Any suggestions for jobs or areas to explore that could help me pivot into a data-related career would be super helpful. Thanks!  ",26,33,1733774887.0,analytics
1hakmp7,Looking for a data mentor,"I am a data professional with over 5 years experience and a strong background in statistics and policy work. I’ve spent the last few years in UK government roles, leading fraud policy projects,creating automated data pipelines and publishing statistical commentary on behalf of government. While I have experience in tools like R, SAS, and SQL, I’ve been in more policy-focused roles recently and want to dive back into hands-on technical work.

I’m looking for a mentor in data analysis from the private sector who can help me sharpen my skills, stay up-to-date with industry tools, and guide me in aligning my public-sector experience with private-sector opportunities. If you’re passionate about data and willing to share your insights, I’d love to connect!",10,8,1733778940.0,analytics
1h9uhug,I do analytics for a college basketball team - AMA,I love sports and I love analytics. Ask me anything - and I’m also trying to learn more about non-sport analytics ,97,62,1733696468.0,analytics
1hafm2r,Analytics a Skill vs Career,"I saw this post and thought I would see what others think about the idea that analytics is more of a skill than career. I believe it’s both. It is a skill but to understand how to interpret and use the data is what leads to it being a career. 

https://www.reddit.com/r/dataanalysiscareers/s/pzgaodIVvr

",2,14,1733766578.0,analytics
1hah6yp,Do you need to know Javascript?,"Hi everyone. New member here but I have been working in Digital Analytics for over 15 years. I used to be a web developer before and I have been coding in Javascript since 1998. I am also the dude who's been to 58 MeasureCamps and I always present.

If Javascript drives you nuts, I can help. I have a monthly Javascript course specialised for Digital Analytics practitioners. I have just released lesson #6 and I have the code for another two dozen lessons already! Drop me a PM if you want to know more!",0,3,1733770420.0,analytics
1h9kodp,How good do you need to be at SQL query optimization?,"Incoming DA here, from a CS background. Currently relearning SQL.

How good do I need to be at query optimization? Do I need to know that databases are implemented with a B-tree, and reason from the data structures? Or is it more like: apply indexing effectively and you're good to go?

If I need to be good at it: any courses you'd recommend or books? I'm fine with going deep.",12,17,1733670195.0,analytics
1h9umlj,Anyone switched from Analyst > SWE?,"I'm a Senior Data Analyst and have been accepted onto a Conversion MSC in CS at Uni of Liverpool (part-time study whilst I work my analytics job).

Is it worthwhile spending 2.5yrs doing this? It would be 20hrs pw on top of my full-time role.

I earn £45k now at 28 years old, and would be 30 when I finish this masters. l've done python courses etc in the past and enjoyed it at the time.",2,4,1733696818.0,analytics
1h9bqjp,What can I an Analytics Engineer (Laid off) do to get hired,"As noted I'm an Analytics Engineer laid off but there is more story to my career:

Been in the Healthcare industry since 2014 in various 'Data Analyst' positions using SQL mainly.  
First Job 2 years: SQL + BizTalk rules composer to automate client revenue cycle systems

Second Job 3 Years: SQL + SSIS + Various Internal tools to do audits, create reports, and work with State Government on Medicaid.

Third Recent Job - 5 Years:  
\- Did 3 Years without any SQL, mostly using the system to create reports, work with our clients to set up the product,and create automation using the system's internal tools.

\- About 1.5 years ago was promoted to our Data Team, and became ""Product Analytics"" but in reality did mostly Analytics Engineering stuff, b/c of internal politics/BS. Here I used dbt, snowflake, CRMA (salesforce visualization), and Metabase to create reports, automate audits for internal teams, and a few KPI dashboards for our products sold to clients.

Got laid off 2 weeks ago along with half the data team, the company just wasn't mature and ready for it, especially leadership. Since then I have been learning Python hard to up my skills. Did some courses on Looker as it seems that's the other big thing right now.

Analytics engineering is definitely the career path I want to be on, I don't want to go back to 'Data Analyst'. I could really use some experienced advice on what can I do stay on this path? I feel like I was kind of shafted, with less than 2 years of ""Analytics Eng"" exp and online all the jobs postings are asking 3-5 years.

Been getting rejected within 1-2 days for any job I apply for. Its rough out there :/",27,35,1733635333.0,analytics
1h9bbiz,CTV measurement ,"Outside of platform pixels/identity graphs within programmatic, how are people measuring CTV (non clickable) media. Any proven 3P parties that have a strong methodology? In particular looking for solutions that work for sensitive categories.",7,2,1733633846.0,analytics
1h9uu1n,PROJECT SOS,I have a group project due in 9 hours and I need help on a question. can anyone offer their assistance? it’s really urgent. ,0,5,1733697389.0,analytics
1h8wzlf,Marketing to Analytics ,"Who has made the jump from pure marketing to Analytics? What was your path, why did you switch, and are you still doing marketing or solely analytics?",28,13,1733591289.0,analytics
1h95t22,Do I have Data Analyst experience?,"Hey all,

My first job out of college was an Analyst at a PR/Communications agency, was there for nearly 5 years. 

It became more quantitative as time went on, but all the data/calculations were done/delivered in Excel, so I didn't get any experience in SQL/Python. I also did some web analytics and social media reporting (trends off of social reporting dashboard services).

I was then on a web/software development team, but now wanting to get back into data related jobs.

I have a bachelors, but it is not STEM (BA in New Media Communications).

I definitely see that many jobs ask for a stem-related degree, but I know experience is valued- do you think my experience is close enough to ""data analyst experience""?

I have been thinking of getting an MS in Data Analytics--and since the market is competitive, do you think the experience I have negates needing an MS?

Thanks!",6,10,1733615818.0,analytics
1h8w0g4,Looking for SQL Project Ideas and Resources,"Hey everyone!

I’m trying to improve my SQL skills and thought it would be a good idea to work on a project that I can also add to my GitHub portfolio. The goal is to learn while building something useful and showcasing what I’ve worked on.

Does anyone have ideas for interesting projects I could try? Or maybe recommendations for websites or platforms where I can find databases to practice with? I’m open to anything that challenges me and helps me get better.

Thanks in advance for your suggestions! I really appreciate any tips or resources you can share.",7,8,1733588626.0,analytics
1h8lm2n,What Project helped you land your first Analysis Job?,"Hello all,

I want to transition careers into Data Analytics. My background is in teaching Mathematics. I have a degree in math and have programming knowledge from college however i am rusty as i graduate 8 years ago. 

I’ve lurked this sub for a while now and understand that having a portfolio with projects is the best way to develop skills and showcase them. 

I currently am almost complete with google data analytics on coursera and i am starting to think about how to develop a well rounded project that is interesting but i feel like I don’t have enough business sense to make a complete project.

I feel like my a lack of business acumen is making it difficult for me to plan out a project. I know the fundamentals of sql and excel and I can play around with the data but is there a process to follow that will guide me towards proper analysis. I am not interested in following YouTube tutorials because i learned better from diving straight into data with tasks and questions to guide me. What are common analytics task you all do at work? 

With that being said, how did you all decide your beginner projects? What type of analysis did you do? How do you come up with questions that are important and interesting? How can i showcase data cleaning? 

It would be cool if y’all can share a link to your projects that helped you land an entry level job. I am curious to see what original projects look like.

Thanks for the help!",48,23,1733549469.0,analytics
1h8wvki,Data Science Learning and Career,"Hi Everyone, I'm a b2b market research professional looking to learn data science from scratch. I've completed a course in data science from Great Learning couple of years back and haven't been able to use the skills. I have beginner level knowledge but now want to brush up on my data science skills to move up to the next level. What is the best way to do this in quick time, say couple of months time? Where can I get access to projects to learn from so I can move to a level where i can do lot of freelancing projects? I'm doing this to build a freelancing career and not be dependent on a salaried position.",4,6,1733590992.0,analytics
1h86m0h,Finance in Manufacturing? What concepts are critical to know?,"Hey Guys,  
I really need help in figuring out how to prepare for this interview, it will be a case study focused on the finance side of things in improving Manufacturing processes. I have to present my findings based on a dataset they will provide.  
  
The industry is food mfg, the job desc mentions financial insights and operational knowledge to create reports, develop business models, and provide recommendations. The goal is to drive continuous improvement.  
Report KPI improvement and the financial outcome. 

I am well versed in analyzing mfg data for improvements, but never did financial analysis based on them. So any help would be highly appreciated!

If you could also give me an idea of what features could be in the dataset, (Planned Downtime, defect quantity??)",3,30,1733505908.0,analytics
1h8iwbv,Veteran - thinking about transitioning to data analytics but hesitant if oversaturated,"Ok my fellow redditians. I is a veteran. I is a smarty (not really lol). I gots me 2 bachelors degrees, one in Pyschology (useless), and one in Information Technology. I never got really far in IT, because basically my school sucked, I had the Comptia A+ cert and let it expire because with all the studying I did I could never get a job that wasn't a call center/help desk. I can't do those jobs well bc I is also a deafy boi from big boom boom in sand land. I have some somewhat relevant SQL and Excel experience, and have reviewed a few Tubers talking about blah blah, you need excel, sql and tableau and you can get a job if you do my course and network blah blah. I am trying to see if I actually put the time into this, make my resume look shiny, don't list my crappy employment hx bc of my disabilabuddies from the military if I stand a chance after 6 months of study and maybe that google cert. I think I can be a shiny turd on paper, but looking for opinions from those that have tried, those that have failed and those that are lucky enough to have succeeded plz. Thankee. ",0,34,1733539987.0,analytics
1h7qgv6,A government job in NYC.,"do you think this job offer is a good deal?

job title: IT Software Developer.

employer: Government of NYC.

location: lower Manhattan, around the financial district.

work schedule: 2 days onsite, 3 days remote.

work hours: around 35 hours a week at the most, most weeks are around 30 hours of work.

Salary: $110,000. city government pension after 22.5 years of employment.

benefits: 12 days of paid vacation a year, health insurance, and 13 days of federal holidays.

culture: very relaxed as there are no hard due dates and work is fairly easy.

Job security: It is fairly secure, insulated from layoffs, and hard to let go of as it is unionized. 

for context, I am 35, 5 years or so of experience in IT, if I take this job then I am settling down for at least 10 years because after 10 years you get what is called a healthcare pension, healthcare for life basically.",33,28,1733450882.0,analytics
1h7vvf5,How should I prepare for my internship?,"I’m going to be a summer analyst at GS next year on a data analytics team (it’s really a data science team though). I’m a computer science major, but have very limited knowledge of SQL, Python, and Tableau which is what GS mainly uses along with their own proprietary language. How can I prepare myself best to be successful? I’m especially looking for good resources for learning SQL or projects that cover a lot of key concepts.",11,16,1733469395.0,analytics
1h7jw1i,Google Advanced Data Analytics Certificate?,"Hey everyone I have just recently completed the advanced Google data analytics certificate via Coursera and am in the process of building a portfolio on Kaggle and Github of my various projects in Python, Tableau, SQL, R and Excel/Google Sheets. I'm still in the process of fine tuning it and hope to be applying for jobs fairly soon here. I'm just a little worried as I don't have much experience in this field yet since I'm only 24 years old. I do have a college diploma in chemistry which utilized lots of Microsoft Excel. I do not have a degree in this field and my only certifications are the Google data analytics certificate and the advanced one. Is there any other skills I should try to add to my tool belt? What are my chances of actually landing a job in the future with my current skulls and experience? I'm open for any advice as I really enjoy this field of work. ",12,24,1733432896.0,analytics
1h7hflg,What’s a Normal Application-to-Interview Ratio for Analytics Roles with 2-4 Years of Experience in current market?,"Hi everyone, I’ve been applying to analytics roles, and I’m curious about what others have experienced when it comes to the application-to-interview ratio.

For context, I have 2-4 years of experience in eCommerce analytics and a Stats/math degree. Out of 100 applications, I’ve received about 5 interview calls. I’m wondering if this is normal for someone with my background or if there are ways I could improve my approach. FYI, I am tailoring resume for each job application, and have projects/github etc on resume. Also, applying outside of ecommerce domain. Applying to new job postings only so within 3-5 days max. 

If you’re in analytics (data analyst, business analyst, BI, etc.), how many applications did you have to submit to land an interview? 

Also, any advice for improving response rates would be greatly appreciated.",7,14,1733426749.0,analytics
1h74to7,"First year done, What to do now? ","Hello,

I ve spent the last year working as a health data analyst for a health insurance company. We mostly use excel, SSMS,T-SQL, Python and we are getting into using snowflake and salesforce. We are a small team that supports the account management team. Recently our manager took a new role and our most senior analyst has also left, Making myself with one years of experience in the field the most senior on our now small two man team. I find myself now answering for our department on my own making manager level decisions, and have very little direct over sight.  I also now  only report to the director who reports to the CMO. I m probably due to get a 2-3 percent Cost of living raise this year that will put me right over 60k per year. I live in a Medium to high Cost living, for example a average 2-3 bedroom apartment here for rent is around 1500-2k a month. A house 2k plus. Our small team is putting out around 50-70 ad hocs a month and another 130 or so reoccuring and we are responsible for side projects that take serious development time. I m also asked to provide custom reporting solutions, write  complex sql with builds in the thousands of lines, and edit and make changes to our different python applications that automates hundreds of reports.  I regular manage stakeholders, mostly clients and account management with the occasional work done for another department. We are expected to complete all this work between 40 and 44 hours per week, I m getting burnt out, Is this a fair wage in this economy for this type of work? I know the market is bad, How do I make the most of this? In this economy I dont really feel like I can jump ship with only one year worth experience in. On the other hand Given the work I feel like I should be making close to double what I m paid, But with my limited time in, nobody is going to offer that. ",10,20,1733388395.0,analytics
1h7a5bn,Pressure on work,"Im currently working 3 months as data analist consultant. My background is that i came from a fabric-operator. So im new in the field data analytics. 

My experience is good so far but i feel some some pressure from my senior colleague to finish some tasks. Also i get monitored. I have done 2 months training and get rolled into the real world usecase. 

Over time its getting frustrated that they expect to much of me. They know my situation and hired me because they want to train people from the scratch. I got a bachelor degree en the guys whom i work got master degree. 

Sometimes i think : is this the right choice to choose this field. ",1,19,1733408208.0,analytics
1h7gpj4,"Is to become data analyst, strongly required any certifications","Learning Data Analytics from Alex... The Data Analyst. If anybody could tell me, whether any certifications required to land a job. ",0,19,1733424946.0,analytics
1h6up56,Manager questions the value of historical data,"What is everyone else's opinions on historical data reporting and analysis?

Note; my team is a dev/tpm team with myself as the sole analyst.

So I work in SaaS looking at enterprise customer tickets and analysing the relevant data etc. We've had a past migration of systems in 2022 and a lot of my initial work (under a different manager) as an intern was stitching together the data to keep a continuity so we have several years of historical data for analysis and forecasting. Now, obviously internal processes change over time so. So in my current manager's eyes, this historical data is not important for new data visualisations and projects as it doesn't represent the current situation. While this is true I don't believe we should be casting aside data just because it doesn't fit a certain narrative.
Thoughts?

",3,13,1733355169.0,analytics
1h72mz7,Tableau with LLM,Has anyone integrated LLM/gen AI with their tableau dashboards? Is it helpful and what are the steps to it? ,1,10,1733379120.0,analytics
1h6rwdl,"Presentation to execs, what to expect","Hi, I just started my first full time job 6 months ago and haven’t had much experience presenting to leadership. For some context, I developed and deployed an operations tool that my boss wants me to present and demo to execs including my SVP.

I’m super nervous for it and presenting isn’t my strong suit :/ 

Could anyone give me some tips on how to present and what I should focus on, as well as what kinds of questions I can expect to be asked. 
I have a script drafted and a couple of use cases on the business impact, but is there anything else I should focus on. I’m also scared that I’m going to freeze on the spot so don’t know if the script will even help lol 

",3,17,1733348118.0,analytics
1h6wpfj,Should I ask for a title change?,"About a month ago, I started a job at my college’s Career Design Center as a Career Peer. I’m a sophomore studying Data Analytics with a minor in Computer Science, and I was told my role would involve a little bit of data work (relevant to my major) but would mostly consist of administrative tasks like answering phone calls, reviewing resumes, and setting up appointments.

However, after my training period my responsibilities turned out to be mostly data-focused. Here’s what I’ve been doing:

	•	Collecting and cleaning data to optimize our social media content types and post timing.
	•	Analyzing data on previous career fairs, including trends in attendance by major, which companies are hiring specific majors (both for jobs and internships), and associated salary data.
	•	Using this analysis to suggest which companies should return, which new companies we should contact, and how to address underrepresented majors at our fairs.

Basically I’ve been using Meta Business Suite to create excel files or cleaning existing excel files from career fairs to then import them into an SQL server, analyze the data and create Tableau reports to share insights with my supervisors. I definitely am not an expert at any of this and am only using what I’ve learned the past couple years in school. I could definitely be doing things more efficiently but I’m just working with the knowledge I have for the time being. If I would’ve known I had these responsibilities when applying for the job I don’t think I would’ve qualified haha. 

Either way, given these responsibilities, I think that my current title, “Career Peer,” doesn’t reflect what I’m actually doing. I want to ensure this experience stands out on my resume, especially as I apply for summer internships in data-related fields.

Do you have suggestions for more accurate job titles I could propose or use on my resume? I’m not entirely sure what the best and most accurate data related title I could use. Should I even change my job title? I’m not doing it for a pay bump (even though I’m paid minimum wage), I just want to make sure my work is reflected properly. Thoughts?",2,3,1733360636.0,analytics
1h6d3ep,How Much Math and Programming Do You Actually Need for Data Analysis?,"I’m curious how much you actually need to love math and programming to work in data analysis or ICT.

For data analysis, is it all about Python and SQL, or do you really need to dive deep into stats and math?

For ICT, how much programming (like Python) do you really do day-to-day?

What kind of tasks should you enjoy to thrive in these fields?

Would love to hear from anyone working in these areas!",25,36,1733308523.0,analytics
1h6k97z,Survival analysis with a twist,"Hello. 
I have a problem that is close to survival analysis, but not quite.

The analog to my actual problem is this - 
let’s say I have an ad campaign through FB, IG and Tk - specifically targeting 1000 known user in my members list. If one buys the product the campaign in the other platforms automatically closes for that user.

I can use survival analysis to see which campaign is faster and more effective by testing time to buy.
But it’s not classic. The three campaign “eat” the same population. Lets say instegram get 10 buys on the first day. These users can only be counted as censored for the other platforms.
But it feels too simplistic and makes it seem like the losing campaign “succeeded”.
I read about “competing events”, but it doesn’t sound right eather, as one event doesn’t logically block the others (like death event and intubation event), they can randomly “block” each other.


So to summarize - 
Is there a version of survival analysis where multiple risk factor with different half-lives and different plateaus “pray” on the same population? And can the parameters be estimated in the live observation with all risks running, without conducting a different test for each?",5,5,1733329802.0,analytics
1h6qe4l,Cheap Embeded Dashboard?,Any affordable products to get an embeded dashboard connected to my database in the Backoffice website of my business?,0,4,1733344427.0,analytics
1h6ng89,Need suggestion for web scraping ,Can I scrape.content from a web page which has restricted access? It's showing 403 forbidden when I try to user bs4. And I am.thinking of using selenium but it's not working out. I need to access a client portal and scrape data from the website / link. Any suggestions 🤔 ,1,13,1733337338.0,analytics
1h6lqno,Excel Copy/Pasting Issues,"Scenario:

 \- I have a table, where I create a bullet point list (using \[SPACE\] ""-"" \[CONTENT HERE\] \[ALT + ENTER\]

Example cell in a table:

 \- Item A

 \- Item B

  
When I copy this cell in one spreadsheet, and paste it into another cell in another table, it breaks it into 2 separate cells, and breaks everything. Also, i'm collaborating with others, and if I know how to prevent this, I can't prevent others from accidentally doing this.

What is going on?

How can I format my spreadsheet (multiple lines in a cell) without this happening?",1,7,1733333328.0,analytics
1h6ja1u,AB testing - observed difference higher than MDE without collecting minimum sample size,"In the AB-test summary dashboard results are shown as follows:
- If the minimum sample size has not yet been collected, it shows how many more days are needed to collect it (to avoid stopping the test too soon).

- If the minimum sample size has already been collected, it shows whether the result is statistically significant.

This approach can sometimes be problematic, let's say my data is:

baseline conversion -1.05%

assumed MDE - 5% relative

minimum sample size on this basis: 596 k sessions per variant

So after 2 weeks of the test, I still get information in the dashboard that I need data for several hundred more days. Now 2 examples of the results on the dashboard:

a) ver A: 1.05% ver B: 1.24% (18% diff) - difference not statistically significant

b) ver A: 1.05% ver B: 1.41% (34% diff) - difference statistically significant

So I'm aware that I haven't collected enough traffic based on my assumptions, but I see differences much higher than the assumed MDE, even significant for (b). My questions are:

-How should i approach this? Should i adjust my initial assumptions?

- Can i trust the result b) if it shows significance without collecting enough traffic? What if these results are observed after 2 days, should i still trust them or can assume it's due to random noise? Where is the line?

I have read the What if the Observed Effect is Smaller Than the MDE? | Analytics-Toolkit.com article. I remember coclusions that MDE and observed effect shouldn't be compared, but with such big differences it doesn't seem to be intuitive.
I would be very grateful for any help",1,2,1733327465.0,analytics
1h5qewt,Is analytics a young person's game?,"Have you seen fewer older ICs in analytics than in other technology fields? I work for a non-FAANG tech company, and I realized that there are essentially no older analytics ICs in the entire org. I'm in my late-thirties and recently realized that I'm the pretty much the oldest person in my entire analytics department. Is this an industry-wide thing or a company thing?

Part of that is definitely due to tech generally skewing younger, but analytics seems to skew even younger when I compare it to SWE, DE, and DS. Those departments seem to have more older folks with families while DA is pretty exclusively younger people. 

What do you think?  None of what I said applies to management paths - I'm talking about specifically IC tracks.",29,37,1733240772.0,analytics
1h5z9ix,Data Analytics Project Ideas,"Hey everyone! I’m getting started in data analytics and looking to build some projects to sharpen my skills and grow my portfolio. I’m aiming for roles like Data Scientist or Business Analyst. I work with SQL, R, Excel, and Tableau. what are some project ideas that make good use of these tools? Also, what kinds of projects do you think stand out most to employers? Appreciate any advice!",2,7,1733262674.0,analytics
1h4zxe6,Math & Statistics in Data Analytics,"I've been doing a bit of researching when it comes to moving into a data analytics The usual 3 things you are told to learn is: Excel, SQL and a data visualization tool (which I'm going to work on). But one thing I've been seeing mixed responses is needing to know math and/or statistics.

So I'm here to ask how much math/statistics should someone dive into if you are looking to aim for a entry level to mid analytics role? I've seen others say it varies from job to job. But I'm thinking it might not hurt to learn some of it. I was looking at taking an intro to statistics course (took a stats course back in grad school but that was many years and never used it) and maybe a basics/fundamentals algebra course.  I'm not looking to get into data science or engineering right now.

Would love to know others thoughts/ideas. Also if you have suggestions on courses/books? Something relatable as I'm not good at math at all and it can take me awhile (along with repetition) to understand things. ",65,22,1733159357.0,analytics
1h5p7rb,How Can Flask Help in Data-Related Roles?,"Hi everyone,

I'm starting an internship in about three months as an Analytics Engineer. My mentor mentioned I'll be using Flask during the internship. I want to train and be fully prepared before I begin.

I have a few questions:

1. How does Flask help in data-related roles like mine?
2. What kind of resources should I explore to get better at Flask?
3. What types of projects should I try to build with Flask to improve my skills?
4. Do you have any ideas on where I can find project datasets or examples? I’ve checked Kaggle, but it doesn’t seem to have anything Flask-specific.

Thanks in advance for your suggestions and advice!",1,5,1733237720.0,analytics
1h5m5du,GA4 Question: How Do See Previous Page Path/Referrer (Which Pages Lead to Conversion Pages),"Hello from the Azores! I have a question about GA4. I used universal analytics somewhat and could find my way around somewhat. But GA4 Is a black box to me.

The answer to my question should be intuitive and clear but I can't figure out how to determine the previous page path for our conversion pages.

For example, I want to know which blog articles drive the most traffic to the contact us page (a conversion goal).

Can someone clearly and concisely explain how I would see that?

I'm mostly interested in organic traffic user pathways here. So I can optimize the top of funnel/ entry points and optimize them as needed.

Obrigado!",1,2,1733228428.0,analytics
1h57yxz,Offered a promotion - how much should I ask for?,"There’s a lot of detail here but I’ll try to keep it as short as possible.

I’m on contract right now as a manager-level IC at a growing company that does not have a formal data analytics function yet.

Rather than just converting my contract, I’ve been offered the opportunity to take a Director level position and head up the new DA team.

I’m interested in and qualified for the position, my only hesitation here is wanting to land the right compensation.

A few pertinent points:
- total analytics experience is 7 years
- industry experience is 5 years
- I have been in my contract role for 4 months but have previous experience at another subsidiary of the parent company
- I work in a field that does not attract strong data talent, and hiring an outsider would likely cost the company industry knowledge
-company is around 250 employees but experiencing rapid growth 

My current contract rate is $60/hr ($125k annually, no benefits while on contract).

The new role’s incremental duties include P&L responsibility, managing 3 direct reports, scaling out a non-existent centralized data function.  I’ll be reporting directly to the president of the company.

What are everyone’s thoughts on what I should come in asking for?",7,3,1733178986.0,analytics
1h58o8t,What Experience is Worth Getting?,"I’m currently finishing my Master in HI. I’m actively a Physical Therapy Assistant. I just applied and have an interview for a Medicaid Analyst position for the state of Louisiana. I’ve had interviews for jobs involving HI that have interested me. One was for a children’s hospital that was collecting data on the effects of consistent wear of braces for kids that had varying spine diagnoses. I want to work in the healthcare and contribute to projects like this. The hesitation is the Medicaid Analyst job is a significant pay cut than other jobs I could be taking in the PT world. I know in order to make the transition I will need to take entry level jobs, but will this experience translate towards my career goals if I do accept the Medicaid job?",2,5,1733180803.0,analytics
1h5da10,Minor in Communication as a DS major,"Hi everyone, I'm a freshman majoring in DS at a top 50 university. I have decided to minor in Statistics and now I'm considering about Economics. I am also an ISTP and not so good at talking 🥲 

I think I'll work in FinTech, but I still do not really know what I should do. How is FinTech for a DS major?",1,6,1733193863.0,analytics
1h5b0gy,Cross Domain Tracking,"Hi, I'm working on cross-domain tracking between a hotel website and a Mews booking engine. The setup in GTM is done but:

* GTM preview mode often asks me ""Unknown domain  The domain mews.com is not enabled for debugging. Would you like to enable it?"" when I do I can see the tags firing with the page location and page referrer parameters but it keeps showing that warning.
* Debug View works on the hotel website but breaks on app.mews.com transition.

Both domains have GTM implemented. The main goal is tracking the user journey properly between these domains. What might be missing?  


Current setup:  


* GA4 Cross-domain Linking listed as contains mews.com
* Conversion Linker tag configured: enable linking on all page URLs, enable linking across domains and auto link domains mews.com

Thanks in advance.",1,1,1733187194.0,analytics
1h4zrs4,Got fed up with traditional analytics tools… Any suggestion for warehouse-native?,It just so much time to import-export the data. We tried reverse ETL tools as well but its still so overwhelming. does anybody have experience with warehouse native tools? We woild like to test one,3,7,1733158974.0,analytics
1h524nx,non inferiority testing in A/B testing,"Heya,

I work as a product analyst and one of my task is doing A/B testing.

However, sometimes the goal of the A/B test is not so much is A better than B (or vice versa) but is B not worse than A. In normal terms; they have put out a change, and mainly want to know if it isn't performing worse than the first change.

In my general statistics courses I've only learned the many techniques for rejecting null hypothesis rather than proving them...

Any of you got experience with this?

Currently this is mainly for binary variables",2,8,1733164644.0,analytics
1h4ry83,Guys i dont know if i made a mistake on a call from a recruitment team,So here i am on a good morning and a recruitment team called me askasking about my current job i was speaking and he asked what does my company do usually i said they take in project and sometines outsource staff members they asked where was i outsourced to ? I said i have signed an nda can't say that eventhough i could say i dont know what came in to me then later they said they would call me after 2 weeks did i just get myself fuckedwith that answer ?,8,9,1733135015.0,analytics
1h55y1b,Looking for Projects to Build Experience as an Aspiring Analyst,"I have around 7 months of experience in hotel operations and a few months in business management. I also have an undergrad in Finance and HR and am pursuing a Master’s in Applied Business Analytics at Boston University. Since most employers require direct experience, I’m looking for ways to gain hands-on practice outside of traditional jobs. What projects, platforms, or volunteer opportunities would you recommend to build relevant experience and strengthen my resume for analyst positions? Any advice or suggestions would be greatly appreciated!",0,15,1733173954.0,analytics
1h55afx,How can I apply statistics to my job?,I work in corporate finance. I want to start creating tools for my team to help give us better insight into the reports we process each month. What are some useful statistical techniques I can apply that can be applied universally?,1,5,1733172370.0,analytics
1h4rz44,I need ur opinon please help 🥲,So here i am on a good morning and a recruitment team called me askasking about my current job i was speaking and he asked what does my company do usually i said they take in project and sometines outsource staff members they asked where was i outsourced to ? I said i have signed an nda can't say that eventhough i could say i dont know what came in to me then later they said they would call me after 2 weeks did i just get myself fuckedwith that answer ?,2,6,1733135128.0,analytics
1h4pjs2,Sales data analyst case study ,"I have been asked to work on a case study for sales data analyst position for a firm which deals with staffing for nursing homes. They have asked me to give five recommendations to sales teams, providing the graphs and tables from the query I would write along with how I achieved it. I have never work with sales data, any recommendations on what I can focus on? 
",4,3,1733124160.0,analytics
1h4cmfb,Help! Cognos Report list view,"I have a current report in Cognos that calculates sales made by contractors. Report gives the contractor names in rows while the columns have the sales they made that day (sales count, sales volume, net). This report is executed daily and exported as html. I am adding a YoY view also to this. Initially I had created a second table below the main one to show the YoY view but I want to have them in one table. When I try to combine them all into one list table it changes the data from previous year to current e.g moving sales count may 20 2023 right next to sales count may 20 2024 in the Cognos design view changes the count of the 2023 data to 2024. How can I avoid this?",2,2,1733084819.0,analytics
1h3m6hc,Data analysts! What was your college major?,What did you study in college? And did it prepare you well for your current role as a DA?,144,391,1733000221.0,analytics
1h468dh,Working at Amazon as an Applied Scientist,What’s it like? Pros and cons? I have an interview there soon ,4,2,1733068388.0,analytics
1h3yqp1,Unemployed for a year with 1 YOE. How hard will it be for me to get a new job?,"I was laid off last year and spent about three months actively applying for data analytics roles. During that time, I noticed a lot of news about layoffs and saw some of my friends go through similar challenges. Around January 2024, I decided to take a 9-month sabbatical to travel. I had always wanted to experience extended travel, and I knew a few people who did something similar after college. Considering the tough job market at the time, it felt like the right decision.

Now that the job market seems to be improving, I’m looking to transition back into analytics. Has anyone here navigated re-entering the field after a year-long break? How challenging was it? And is it advisable to be transparent about taking time off to travel when discussing the gap with potential employers?
",13,11,1733041177.0,analytics
1h3gyiu,How did you get your remote job oppurtunity?,Hi dear data analysts how did you get your remote job oppurtunity?,13,86,1732986034.0,analytics
1h3tpvu,Manufacturing Data Reality. What do these datasets typically look like?,"Hey Guys,  
So I have an interview coming up for a food manufacturing company and they are going to give me a case study on Excel to work on. The job desc is focused on continuous improvement on sugar lvl.  


Does anyone here work in manufacturing and help give me an idea of what a typical dataset could look like?

I would love to start practising on some fake datasets, I asked ChatGPT but it isn't giving the most realistic datasets.

Any help us much appreciated!!",1,7,1733022518.0,analytics
1h3xgmb,What are the skills I should focus on to move on to the next level?,"I am working as a DA for 4+ yrs but I am not getting any good projects and even when they come it's very rare I get to work end to end. I want to switch into a senior role based on my experience.  Since mainly I am getting involve in non-da work and whenever I get da it's not end to end which kin of is effecting my overall experience. 

What skills would you suggest me to learn?
Also as for my DSA it's like 5/10 and it's been long since I practiced.

These are my current skill sets:
Tableau (not prep)
SQL (not advanced like procedures or windows)
Python (pandas mostly)

What should I be focusing on? Top 4 skills.
",0,4,1733035799.0,analytics
1h31azc,How much is worth to learn R outside of academics & research?,"Hey all. In a professional environment working in Data Analytics, or as Business Analyst or in a related field how much is worth to learn R? Outside of Research & Academics there are advantage of learning and then using R over say Phyton.
Phyton seems much more versatile and valuable and a requirement for many companies along with Excel, SQL + dashboarding.",13,23,1732930127.0,analytics
1h35qkq,How Do I Find An Analytics Internship As A College Student?,I'm currently a college student that is studying marketing analytics and I'm wondering where I can find internships for the summer. I'm honestly worried about finding one. I got invited to an interview where I record my answers but I completely bombed it and now I'm worried I won't be able to find anything. I seriously need help.,2,12,1732945217.0,analytics
1h30a1t,Querying multiple large dataset ,"We're on a project requiring to query multiple large dataset & multiple table using GPT to analyze the data (postgresql). Some of the tables have like 2,000 words text or more.

Any recommendations to tackle this issue?",3,18,1732926978.0,analytics
1h2rcfn,Freelancing as a Marketing Analyst,,4,2,1732902422.0,analytics
1h2ak46,Which age group or generation is more prevalent in Reddit more than any other social media platform ,It could apply to today or all time,9,6,1732843482.0,analytics
1h2ctbb,Any Posthog users here?,"I just watched Posthog's demo video and was mind blown. If you have used it in production, please share insights.

* It offers 8 different products, does it become confusing at some point? Is there a real advantage to having all those tools in one place?
* It seems to be feature rich, but do you see any major gaps compared with mixpanel/amplitude for analytics?
* How is the pricing compared to what you were using before?
* Any reasons why you wouldn't prefer such a tool over individual tools for analytics, experimentation, session replays etc?",2,2,1732851461.0,analytics
1h1vqx9,Microsoft Clarity: Heatmap Download doesn't work correctly ,"Hello everybody,

I'm using Clarity for a few weeks now and wanted to download some heatmaps as PNG. Unfortunately everytime I download it it doesn't load the CSS and/or JavaScript. But in the application itself everything is fine.

Do I have to configure something that I miss?",1,2,1732799837.0,analytics
1h1fslx,If you could automate one thing when analyzing data what would it be?,"If you could automate one thing when working with your data, what would it be? Cleaning up messy data? Creating dashboards? Finding insights faster?",13,39,1732743802.0,analytics
1h1bmp6,What data do you consider most essential for judging the success of social media content?,Title says it all. What do you consider the most accurate measure for success when it comes to running a company's social media account? Would you prioritize engagement or sales?,3,10,1732733182.0,analytics
1h15h17,Sessions attributed in Direct channel when consent is denied?,"Following a spike in Direct traffic in GA4, I’ve been told by our cookie banner support that when a user refuses cookies or makes no choice in Consent Mode, this traffic falls into the Direct channel.

Has anyone ever truly experienced this? 

I’m somewhat doubtful about that as I always thought sessions were not processed when a user refuses cookies (except through modeling, which is not active in the property anyway).

Please note that navigation is blocked prior to consent and the page is not reloading on consent.",4,4,1732717461.0,analytics
1h17sul,How to Build an In-House Tool for Tracking EMV and VIT?,,2,1,1732723643.0,analytics
1h0ty6j,"New grad, entry level job responsibilities ","Hi,
Can anyone let me know if this is normal for an entry level new grad data analyst role?

For context, I joined a new grad rotational program and I feel like I’m being over worked and also doing a lot more product/management work than data work.

I’m on a team of all product/non technical people and I’m the only data analyst/technical person on my team. Because of that I’m in charge of developing dashboards for my team, as well as requirements from cross functional teams. Getting data from different teams takes weeks of communication and negotiation, often escalating to their supervisor and it puts me in a really awkward position since, as a new grad, I feel uncomfortable demanding/requesting said confidential data from different teams . 
Since I’ve started I’ve designed and am now maintaining 4 different dashboards. My boss wants me to start writing requirement docs and also leading product meetings with different teams. 

I guess I thought that being in a new grad entry role position, I would have a mentor to gain data analyst/bi skills but instead I feel like I’m overwhelmed with responsibilities and since it’s my first time doing bi and there isn’t a mentor/another data analyst on my team to bounce off of ideas and get help etc . there’s also an added learning curve with bi tools and cloud data management, which is hard to learn while also having to keep up with deadlines.

I don’t mean for this to come off as complaining, I’m super grateful to have a job and have people that trust me to develop these reporting tools, but I’m wondering if this is normal ",21,11,1732674953.0,analytics
1h0cb7x,"I hate python, should I give up the pursuit of this career?","Can't stand the language, tried it many times over the course of my life, with many different platforms/websites. Absolutely hate it, makes no sense to me whatsoever.

I like SQL though, but I see that python is a reocurring skill being mentioned for DA's so I'm just asking, how important is it? and should I honestly just give up the pursuit to transitioning to this space if I can't learn it?

Thanks",57,106,1732629007.0,analytics
1h0z4ft,Is Tableau Desktop Specialist Certification worth it?,"Hi everyone,

I’m graduating next May and looking to start a career in **Business Analytics**. Would you recommend the **Tableau Desktop Certification**? Has it helped you professionally?

I appreciate your time and insights!",1,3,1732693458.0,analytics
1h0s8w6,Trying to get back into Analytics for a DA job,"Since graduating college, I've been searching for a DA job for a year and half. I've learned R through college. Learned SQL, Tableau, and a little bit of Python and Power BI. Even got a certification in Google Data Analytics. I settled for a accounting job and eventually went up to Pricebook Analyst (current job). Since my current employer has changed a lot, I've been seriously been getting back into learning programming again and seeking a Analytics role. Problem is: It's way harder now than it was a couple years ago.",3,2,1732669814.0,analytics
1h0lhf8,Data Quality vs Data Architect,"Hey everyone!

Being direct:  
Data Engineer with 10y experience, trying to do a move to Data Governance side. I want to be part of the DG Team, somehow in the future. Ideally I'll be a CDO.

I've got three proposal in hands:  
(1)  
1x for a Data Quality Analyst role, working very close to the Data Governance Team.  
Business: bank industry  
Tasks:  
Maintain DQ  
possibilities to evolve the DG framework, but it's not the focus. but I'll work close to the team.  
Work with MS Purview  
Work as a team in a ""agile"" methodology  
Concerns: business limited scope, since I'll be QAing data for reports.  
Pros: I'll work exactly on the field I want

  
(2)  
2x for Data Architecture  
Both companies (Industry Innovation and Telecom)  
Both are trying to follow a Data Mesh Architecture.  
Both don't have a strict DG framework implemented ; I'll have contributions to it, but will not implement it directly.  
Don't know exactly which tools I'll work with.  
Both don't work in agile methodology, since I'll work ""alone"", defining something and not necessarily build projects.  
Concerns: tasks are not directly related with DG tasks.   
Pros: Strategic position. Possibility to work with DG. Working on a Data Mesh centric idea, possibly designing data products (oh sh\*, developing data products without a DG framework already implemented)

  
What brings me here:  
How the f\* do why choose? I really need to decide. All the three came to me on linkedin, are good companies with good salaries. Bank industry is in general more ""close"", no big progress, I think. But it's DG. However working as a Data Architect I'll be more strategically positioned to help on DG tasks, I believe.  
Bank pays a little less.  
All have good facilities and perks.  
The thing here is essentially the tasks.

  
Any Data Architects/Data Quality/Governance Analysts here?

Thanks for your inputs!  
",4,3,1732651867.0,analytics
1h0e8lv,Industrial Engineer Looking to Enter the Analytics Field,"I’m a final-year industrial engineering student, and I’ve recently started learning Power BI, Tableau, and Excel. I’ve been enjoying the field of analytics so far, but here’s the thing—I absolutely dislike coding. Even for VBA in Excel, I rely on ChatGPT to write the code for me. Luckily, my professor thinks it’s okay to use tools like that, so it hasn’t been an issue.

I’m curious to know more about the nature of work in analytics roles, such as Business Intelligence (BI) or Business Analysis. What does a typical day look like? Are these roles heavily reliant on coding, or can they be more focused on tools and insights?",8,12,1732634282.0,analytics
1h0cxo0,How to get into analytics?,"I have a BA in Psychology and Gender Studies and for the past 8 years I've been working as a welfare caseworker for the state, before that I worked for 2 years at a crisis center as a service coordinator for clients. 

I'm interested in finding a job as a data analyst but I'm 37 and I don't have relevant experience or education for it as far as I understand. I did take some statistics classes in college and was an RA for 4 years in college but that was a really long time ago.


Would I benefit from a bootcamp program? There is one offered at a local college but I think it's like 10k. Or do I need to go to grad school in data analytics? Or is this something that can be self-taught? Any help/advice would be appreciated!",3,8,1732630756.0,analytics
1h04rih,Is it possible to become a data analyst without a degree or relevant work experience?,"I know this topic has been answered many times before, but I wasn't finding enough relevant answers to my situation. For context, I'm 19 years old and I live in the United States. As the title states, I don't have a degree as I dropped out after a year because I felt the vast majority of my classes were useless and not applicable. I've been running a 3D printing business for the past 1.5 years and it has been pretty successful, but also inconsistent. Before the business, my only work experience was a couple of busser roles at restaurants.   
  
Anyway, I wanted a good job to fill in for slow months in the business. I decided on data analytics because it was interesting to me, and it seemed like there was a lot of room for growth/learning. So I did the data analyst career track on Datacamp and really enjoyed it and want to continue my learning.

I'm just trying to figure out if this is possible, and if so, what my next step should be. I would prefer not to go back to a university, but I do like structured learning. So I have been considering potentially doing a BootCamp. I know that it will be harder for me since I'll probably get filtered out for not having a degree a lot of the time. I was thinking it might be better to try to get an internship before a job. Thanks for reading all of this, and hopefully some of you can give me some guidance on this.",9,35,1732599815.0,analytics
1gzt3ln,Getting masters in Data Analyics while having a BS in Computer Science?,"Firstly, I apologize since I know this is a FAQ, I just wanted to ask it given the context around my circumstances. For context, I'm 22 and graduate with a BS of Computer Science & Engineer from OSU back in May. I've been applying to a lot of jobs(mostly software dev), since my senior year had started but am still unemployed and living with parents. I've had this thought in the back of my head the whole time though about how I didn't really know if I wanted to do general software development. It just always continuously bored me, but I guess I was sticking with it out of ""expectation"" and not wanting to deal with feeling like my life plan was falling apart. I ended up finally thinking it over lately and realized I mainly enjoyed the handful of classes I had focused on databases, cleaning up data, analyzing data, etc. 

So, I've been starting to go through some decent tutorials I've found online to refamiliarize myself with the processes I learned in class.(mainly Python and Jupyter Notebooks since that's what school used). I plan to learn about using other tools as well, such as Tableau, and I have a personal project I'm planning as well. Tbh, I've had way more fun doing this compared to even just thinking of writing code for things like apps and websites. 

That's gotten me thinking about doing a Master's via something like WGU or Georgia Tech's online program, since I can afford them(I have money saved up). But I also don't want to make a decision to drop that much cash lightly, so I wanted to ask here: How worth it would it be to get a Masters, and should I just focus on finding free/cheap courses online to take and doing personal projects instead?

Edit: alright yeah yall have definitely convinced me to not go for it rn, i’ll keep working on projects/my self-learning and finding a part time job. Thanks for the responses! :)",24,31,1732566876.0,analytics
1h05yi8,Reason for rejecting an offer,"I interviewed in a company in 2021 and rejected the offer since i was not super happy with it and was getting promoted in my current company. The offer basically helped me negotiate my ctc in the existing one.

Now i am reapplying to this company and i really want to get in! I am 100% sure i will be asked why i declined the offer earlier. 
What can i say to let my previous decision sound reasonable?",3,6,1732604483.0,analytics
1gzozxk,Advice on Preparing for Healthcare Data Analyst Interviews,"Hi,

I’m currently applying for Healthcare Data Analyst positions and want to prepare for potential interviews. While I don’t have an interview scheduled yet, I’d love to get advice on resources and tips to prepare for it. I am looking for an entry-level position. I am based in the USA.

Some details about my background:

* I have an MD degree and a Master’s in Health Informatics.
* I have intermediate SQL skills and Excel proficiency.
* I’m familiar with healthcare ontologies like ICD-10, RXNorm, and SNOMED.
* I’ve worked with data cleaning, analysis, and visualization (using tools like R).

Interested to know the following:

1. Key skills or concepts I should focus on 
2. Common interview questions for Healthcare Data Analyst roles.
3. Recommended resources to practice technical and analytical skills and behavioral interview questions

If you're a healthcare data analyst or have recently interviewed for a similar position, please share your insights. Any advice is greatly appreciated!",13,13,1732557084.0,analytics
1gzfybj,Starting on data analytics journey,"What do you guys recommend the best way for  a beginner to proceed, I wish to learn python, sql, power bi and tableau. I have very basic exposure to sql since I worked on zoho analytics.
1. Should I undertake projects to make it stronger and then move to python?
2. Should I enroll in bootcamps that span over 6-7 months and do a proper course with certification? (I have done a professional course in accounts and finance so basically I have no prior knowledge in using these tools, nor do the typical job profiles in my line require the same, however I wish to switch streams, get into more technical roles as this looks more fun and intriguing)
3. I know there isn't an end to the knowledge you acquire but still What should be the level of various tools (even if other than above) I should acquire to be able to freelance maybe and unskilled myself.

If possible please guide me with the best source of acquiring such knowledge as well.",11,26,1732530893.0,analytics
1gz5k1c,"I work in analytics, but need recommendations on a new job title based on my new role","My company is looking to change/update my job title (including a raise) because I started as a data analyst but I’ve taken a larger role over the past year. Problem is, leadership has left it up to me to decide first on what I think the job title should be and then they would discuss it with me. 

Here are some of the things that I do:

- SQL Reporting
- Create Power Bi and Tableau Dashboards
- Create ETL processes using SSIS and C#
- Manage database and data governance
- Part of the team designing internal applications

Would appreciate any recommendations! 



",45,22,1732493139.0,analytics
1gz6wrn,"In your opinion, has the optimization pendulum swung too far?","What I mean by this is have we gotten to a point where companies are investing way too much for way too little gain?

For example, demand forecasts can be useful. And they might even be pretty damn accurate with 5-10 variables. Is searching for and applying those next 5-10 variables really helping that much. Is the team dedicated to optimizing inventory and merchandise layout in stores really worth the ROI? 

I am not at all saying no analytics is useful. I think data is useful in some industries and extremely useful in others. But have some companies gotten to fixated on data/optimization/forecasting to the point there’s an excess of analysts who are not providing any additional value?",19,10,1732497013.0,analytics
1gzehik,Is there any Analytics for BlueSky?,"Is there any out-of-the-box solution similar to what Twitter has but for BlueSky? I see more and more hype around BlueSky, but I need help finding guides or descriptions on ready-made analytics for this social network.",3,2,1732524324.0,analytics
1gyus70,Need some help with HR analytics,"Hi, all,

I'm working on a project with some HR data. I'm trying to create some useful dashboards (using Tableau) from this information, but could use some guidance on what to look at.

I have three excel spreadsheets.

The first is employee\_data. The fields are as\_of\_date,	employee\_id,	employmentStatus,	location,	division,	department,	tenure (months),	tenure\_in\_position (months).

The second is hires. The fields are as\_of\_date,	employee\_id,	employmentStatus,	location,	division,	department

The third is terminations. The fields are as\_of\_date, employee\_id, employmentStatus, location, division, department

So far, I've thought of the following metrics: Total headcount per division/department/location, average tenure per division/department/location, and average tenure of terminated employees (again per the usual dimensions).",5,11,1732465443.0,analytics
1gyvqu3,How to train a multiple regression on SPSS with different data?,"Hey! Currently I'm developing a regression model with two independent variables in SPSS using the Stepwise method with an n = 503. 

I have another data set (n = 95) in order to improve the R squared adj of my current model which is currently around 0.75. 

However I would like to know how I could train my model in SPSS in order to improve my R squared. 
Can anyone help me, please?",3,2,1732467834.0,analytics
1gykfd8,How should I go about  making things more efficient?,"Some context

I interned at this organisation for a year and now that I am expected to graduate, I have been contracted by the organisation to help out with making the data analysis and validation process efficient and as automated as it can be. The oganisation uses Microsoft 365 license and hence has all the access to the Power apps. Unfortunately, the team is too busy with their portfolio (it is not a data team) to really find the time for improving efficiency.

The expectation is that I will be able to help them out, make things more automated and use my data analytics skills to provide them with monthly insights. It is a government organisation, and hence, will not allow the usage of Python/R as they deem it to be unnecessary and potentially dangerous. I do not mind that as I have had good experience with working on PowerBI and Excel. The issue however is that the data is not on a SQL server or a datalake which will allow PowerBI to perform the required ETL and allow me to analyse. 

What is the problem statement ?

The data comes in as a standalone Excel submission from the service providers. These files are then individually validated, analysed, and insights gathered. This can get really inefficient and overwhelming very quick as the project keeps moving forward. The Excel file has data that would literally be nightmare for PowerBI to work on (It is not in a Tidy format). 

What I intend to do ?

I have this idea in my head where I could potentially automate the data cleaning process using Power Query. Our service providers submit the data in the exact same format each month. So, using Power Query would allow me to convert the data into a tidy format, allowing me to feed it into PowerBI and analyse/create dashboards. As the Excel template does not change, a simple refresh should update the data each month. The Microsoft 365 license means that we also have Sharepoint access. I also intend to make good use of this, so that everything is now centralized, easily accessible, and updated on the go. I also hear a lot about the automation benefits of Power Automate but have never used/experienced it. I am more than willing to learn this and implement

Your inputs in this which would help a lot:

Firstly, how I could use Power Automate as a resource to help streamline the process?  Do you think I have the right approach to the problem? What are some of the pitfalls I could fall into, considering I will be the only ""data"" person in the team and will need to contact the business intelligence team of the organisation for any specific help (my manager is willing to support me and get help from other departments but this is usually easier said than done)?



",6,3,1732428892.0,analytics
1gy464m,Ask me anything: 3+ YoE and Just Accepted a New Offer,"I'm still fairly new in my career as a DA but I recently went on the job hunt for a new role and want to share some stats real quick!

**Total Duration:** 1.5 months  
**Applied:** 137 companies  
**Interviewed:** 12 companies  
**Interviews Held:** 27 interviews  
**Final Stage:** 4 companies  
**Offers:** 2 companies  
**Accepted:** 1 company

It seems like we have a lot of people in this channel asking for career advice and while I'm not an expert, feel free to ask anything! Happy to share what I can.

EDIT: This is US based and in the SaaS space.",60,61,1732381067.0,analytics
1gyh8iv,Preparation tools and resources,"Hey Community , 

  
I am actively looking to change and would appreciate if you could send me the links , references to the training , preparation material  that you might have.

  
Thank you very much in advance. 

  
",0,5,1732417417.0,analytics
1gy9x19,Learning Excel as a CS student ,"I’m currently back in university majoring in computer science (post bacc program) with a concentration in data science/big data analytics. But I already have a BA/MA. My MA was paid by my previous employer and I pursued a certificate in data analytics which was how I was able to convince them to offer tuition reimbursement for the program (I worked in data entry). I graduated but I was never able to find a data analyst. So I started looking elsewhere and I was able to land a remote role in digitization. 

However, I’m still interested in finding a data analyst role as I believe that work experience as a data analyst and a CS degree can help me stand out for a data engineer role once I graduate. Ideally I would like to work as a data analyst while I’m still in school for CS. I’ve been focusing on improving my skills in SQL and Python, but I wonder if I should focus on improving my skills in Excel? I was thinking about taking a few of Maven Analytics Excel courses and create some Excel projects to add to my resume. Are there any other resources that help with learning Excel? Thanks in advance! ",3,10,1732396124.0,analytics
1gx83uu,Rant: Companies don’t understand data,"I was hired by a government contractor to do analytics. In the interview, I mentioned I enjoyed coding in Python and was looking to push myself in data science using predictive analytics and machine learning. 
They said that they use R (which I’m fine with R also) and are looking to get into predictive analytics. They sold themselves as we have a data department that is expanding. 
I was made an offer and I accepted the offer thinking it’d be a good fit. 
I joined and the company and there were not best practices with data that were in place. 
Data was saved across multiple folders in a shared network drive. They don’t have all of the data going back to the beginning of their projects, manually updating totals as time goes on. No documentation of anything.
All of this is not the end of the world, but I’ve ran into an issue where someone said “You’re the data analyst that’s your job” because I’m trying to build something off of a foundation that does not exist.
This comment came just after we lost the ability to use Python/R because it is considered restricted software. I am allowed to use Power BI for all of my needs and rely on DAX for ELT, data cleaning, everything. 

I’m pretty frustrated and don’t look forward to coming into work. 
I left my last job because they lived and died by excel. I feel my current job is a step up from my last but still living in the past with the tools they give me to work with. 

Anyone else in data run into this stuff?
How common are these situations where management who don’t understand data are claiming things are better than they really are?",238,127,1732282899.0,analytics
1gy4yhx,Blindspots as a CS student who pivoted to DA?,"US Citizen, graduated January 2023 in CS and wasn't able to find a job for a year mostly due to poorly managed ADD getting in the way of me actually applying, so I did a basic DS bootcamp end of that year  through simplilearn (I know) to get certified and try and pivot out or just show that the time wasn't entirely wasted. Even though the bootcamp was pretty crap, I learned from it and was able to start using some of that knowledge this year in some of my jobs.

This year I've been doing very simple webpage design, some digital marketing/SEO stuff, had an IT job doing hardware repair and data analysis on server failures + technical writing for that until the company fell apart a few months ago, and some claims and referral data analysis for a bunch of small health practices in my area. I'm coming up on a year of experience overall, specifically in those web design and data analysis areas since I've been freelancing the whole year on the side, and I'm hoping to use that to get a role that pays more than $18 an hour once hiring season hits, but I know for a fact there are areas I need to improve in because I've never been hired as a data analyst with a team to work with and learn from. The only thing I've really been asked to do is make reports for different clients in excel, and I ended up using the opportunity to gain experience with both Power BI, more excel knowledge like pivot tables, and Tableau. The problem is outside of specific instances, I really don't know what it is that I don't know.

Here's what I'm current doing to get ready for interviews: I'm working on getting my SQL back up to par by going thru sqlzoo as well as a GitHub SQL course that someone posted here a month ago, since my previous data engineering mentioned my SQL skills specifically needing work. I'm also trying to learn react & typescript for web development instead of just using basic CSS with one of those pagebuilder services like Wix. Planning to build a portfolio site with those skills to showcase my freelance work as well as a site for my own music/visual art career and use that specific project to strengthen my knowledge on database design and front-end/back-end development. At some point I also want to try and train some image generation model on my own visual art and make it generate new art just to see what it spits out, but that's something I would do after my own portfolio site is ready.

I've done a bit of research on how to prepare for these interviews like watching one or two mock interviews, but I'm still a little lost on what my next steps should be. What other things should I try and do for getting ready for interviews or just strengthening my knowledge base? Are there any specific resources similar to leetcode that people in this industry use?",1,5,1732383099.0,analytics
1gxvent,Looking for advice,"Not sure if this is the right subreddit but hi, I'll make this quick.

I have a degree in mathematics - applied statistics and I want to get my foot in the door of the Data Analytics industry. I'm a very quick learner and self-teacher but I have no experience and about to turn 30. What would help my resume the most? Certificates? Portfolio of projects? ",1,4,1732352114.0,analytics
1gxhkvb,How Can I Push Through and Advance My Career?,"Hello everybody, I really need your help because I'm struggling so much mentally on how to continue my data analytics path. First of all I have a bachelors in Economics and a masters in Data Science, I graduated this March from my MSc, so as you you can understand I have a very big problem on finding a job, it's taking a huge toll on my mental health and I've forfeited for the last 3-4 months. 

  
Main thing I struggle with is that I don't really know where to focus, what tech, what programming language, etc. I know Python (not so well but with the help of ChatGPT I can do everything, coming from economics I don't really know how to code good, but throughout my studies I know how to spot mistakes and adjust code to where it needs to be or tune models or anything, so the knowledge is there but not the coding, ikr its bad I will try to work on that). I also know SQL and have done dashboards in PowerBI and Tableau, I could really easily learn Excel too if needed. What should I work on? should I for example try to master Python and SQL? Then choose either PowerBI or Tableau and work on them too?

  
Also second thing the industry is using too much different tech, for example I search for the limited jobs that currently exists and everyone needs something different. For example I saw ads asking for AWS, Azure, MLOps, from the data engineering side that I'm also looking for to break in data analytics, snowflake, mongodb, Apache Airflow, databricks, SSIS, and all that chaos. 

  
Like literally what could you suggest me? I don't know how to continue with meaningless projects with toy data or what tech to focus to guarantee me a job as a starting point. I'm feeling so lost and devasted that I studied for 6 years to deal with this.",8,9,1732307484.0,analytics
1gxpigx,Is analytics for me?,"I’m a freshman and wondering what to major in college.  I’ve always had an interest in numbers and math looking at charts etc, but no so much theoretical math.  Physics is cool but it’s not really my thing so probably not gonna in any type of engineering.  With CS the classes doesn’t seem to interesting to me and I heard it’s pretty theoretical.  After doing some research I heard analytics might be good for me I’m good at math, it’s practical, and it’s businessy which I’m also interested in. 
1. Hows the pay?  And maybe in comparison to other tech roles like software engineering
2. Work life balance? 40 hrs? WFH? Stressful? Etc.
Saturated? (I plan on doing internships and a lot of outside stuff other than grades)
3. Career progression/exit opportunities?
4. Anything else many people overlook
",0,12,1732329754.0,analytics
1gx8b8t,Final class for Master's - Enterprise Data Management or Machine Learning / AI?,"I have one semester left in my master's program, and I'm only able to take one more class. I can't decide between them! Can any experienced analysts help me out? I know analytics jobs are diverse, so your experience will likely be different from mine, but it would be helpful to hear from you anyway.

The two classes are Enterprise Data Management and Machine Learning / AI Applications with Python. My current job entails wrangling data from SQL databases and creating dashboards in Tableau, so the Enterprise Data Management class sounds more relevant to my current role. However, I personally love working with Python, and while I think an AI class may not be as relevant, it sounds very fun and might be more future-proof.

What do you think? Leaving the descriptions of the classes below if that helps!

""Enterprise Data Management: Explores how the data warehouse provides the foundation for analytics within the enterprise. Topics include: dimensional models, design and creation of data warehouses and data marts, ETL process and the extension of the data warehouse concept to big data.""

""Machine Learning / AI Applications with Python: Machine learning is pervasive, with high-stakes applications spanning all business sectors, including fraud detection, high-frequency trading, and highly personalized and relevant marketing campaigns. Machine learning requires interdisciplinary techniques to create algorithms that sift through large volumes of data to support business decision-making. This class will equip students with the analytical techniques and skills to build and evaluate machine learning models using Python. In addition, students will use Python for a hands-on exploration of a broad crosssection of algorithms for machine learning, including linear models and dimensionality reduction. Students will gain additional familiarity with deep learning models such as artificial, recurrent and long shortterm memory neural networks. Cloud-based resources and the open-source frameworks TensorFlow and Keras will be leveraged. At the end of the course, students will be prepared for accurate, effective and ethical research or industry application of machine learning techniques.""

Thanks in advance!",2,3,1732283504.0,analytics
1gwl77t,Anyone notice lower salaries for analytics roles?,"I'm currently interviewing with 3 companies for roles that require 3-5 yoe in a HCoL area in the US and their salary range are around 70-85k. Some even have an analytics manager title but the pay is 70-80k. Anyone else notice salaries being lower while also requiring more experience?

PS: they're more focused on marketing analytics but require (again ,3-5 yoe) in analytical and BI tools",62,45,1732210871.0,analytics
1gwae96,"Analytics market is rough. Officially checking out, changing careers","As above. Every job had 100 plus applicants, tech is evolving fast. Employers have an ever increasing amount of skills they want for less pay.",111,111,1732172429.0,analytics
1gwmw0a,Tracking KPIs for an AI Video Maker Launch,"Hi analytics enthusiasts,

I’m working on the launch of Calvin AI, a video maker app for creators. It automates video creation and uploads to platforms like YouTube and TikTok.

Which metrics or KPIs would you prioritize for evaluating the app's performance in the market? Are there any specific analytics tools you'd recommend to track user behavior and engagement with the app?

Looking forward to your expert opinions!",1,3,1732214805.0,analytics
1gwcm64,Trying to get a Pulse,"I’m thinking about switching careers and was thinking about going into data analytics. I’m trying to get a pulse on the market. From what it seems the sentiment is looking a little grim from some of these posts. Examples being tech layoffs, over crowded market, bloggers saying this field is the best thing since sliced bread etc. 

Do you think sentiment is a byproduct of a difficult job market overall or is this field really having challenges for the foreseeable future due to companies not valuing the position or thinking AI can solve their issues? I was going to enroll for a class to learn SQL as a starter. Any advice and opinions welcomed. 

Thanks!",5,14,1732182336.0,analytics
1gwkq6j,Advice and opinions on getting Magister's degree for Data Analyst role,"Hi everyone, I am in my 3rd year of BCs in Marketing and communications. Looking for a change to Data Analytics. Throughout my time at uni I have realized that I want to work in a field related to analytics. I am wondering if it is worth to get Magister's degree. I am also currently learning SQL with Data camp and planning to learn Power BI to get PL-300 cert. After reading some threads I guess it is better to get a degree in Statistics?
Maybe you could give me a relity check.
Any advice or guidance are appreciated. 
I am currently located in Prague, Czech Republic.",1,1,1732209796.0,analytics
1gwd8e1,Help- best practices for Analytics and tracking strategy for a website in multiple regions ,"Hi everyone,

Hope I can grab some help here. 
I’m currently working on a tracking strategy for a website that operates in multiple regions (e.g., France, Japan, Singapore) with plans to expand further. 

The main goals are to:
1. Maintain accurate, region-specific insights for stakeholders while also having a centralized view of global data.
2. Comply with local privacy laws (e.g., GDPR in Europe).
3. Optimize costs and resources while ensuring the system is scalable for future expansion.

Some initial thoughts and questions I’m
- I’m thinking of creating separate properties or data streams for each market?
- with this in mind, how do I ensure event consistency across regions while allowing for region-specific tracking?
- Has anyone implemented server-side tracking for such use cases? How did it impact costs, implementation complexity, and reporting?

Any insights, examples, or resources would be greatly helpful. 

Many thanks ",3,5,1732184934.0,analytics
1gw33di,is it normal for me to have no cloud knowledge ,Hello  im a data analyst intern here i have been in an internship for about 8 months (still ongoing about to end in 4 months ) and now im looking for a job as an analyst for some other company but they are asking for cloud knowledge is it normal for me to have no prior knowledge on cloud eventhough i have expereince for 8 months ? ,11,8,1732149120.0,analytics
1gw6r3r,Career Advice,"Hi all. I’m looking for career advice as I’m entry level into data analytics/science and I’m unsure if I want to continue to be in this field or not. Although I love data and performing analyses and building machine learning models, this unforgiving market has me rethinking things.

Some context, I have a BS in Statistics (theoretical maths minor), a couple internships, and am currently doing a MSCS with a concentration in ML. My BS and MS are from fairly prestigious universities, and hold a decent amount of influence. I like to believe they’ve helped me get some interviews, although I’m not certain.

As for the job search, I’ve been trying to get entry level data (fill in the blank). From healthcare, tech, finance, etc. I’ve probably applied. I’ve gotten a couple interviews, and earlier on (about June) I know I fumbled due to self inflicted poor interviewing skills. However, I spent the time to learn how to interview and have stepped it up quite a bit. However, recently I’ve either been rejected due to: HM doesn’t like me, budget cuts (final candidate after numerous rounds), or ghosting (after numerous rounds and/or projects). The latter seriously make me reconsider everything I’ve done up until now. 

Tl;dr, I’m lost and looking for something. Any and all advice appreciated.

Edit: more context. Originally, I had planned on pursuing a PhD in Stats (hence minor in theory maths) but my GPA wasn’t competitive enough. It’s above a 3.3 but no Latin honors.",3,4,1732159606.0,analytics
1gwbbhz,somebody please help ,"
is bachelor of science in business data analytics a good degree especially in pakistan and which universities should one go for? what’s the scope and is maths and cs compulsory for it? can’t one take foundation or extra classes for these subjects if one hasn’t took these subjects ever before?",0,1,1732176426.0,analytics
1gw9hp3,random tool stack question,"I have a new project coming up and need to build a simple back end and front end to enter and update some data. My company pays for every tool under the sun. for those that have used them, would you rather use snowflake + streamlit or mssql server + power apps? ",1,1,1732168883.0,analytics
1gvwseu,Looking for Recommendations: Best Sources for Analytics News and Updates?,"Hey everyone! I’m on the lookout for great accounts, websites, or blogs that regularly post about analytics. Ideally, I’m interested in sources that cover a range of topics, like:
• New product launches in the analytics space
• Emerging startups worth keeping an eye on
• Cool tools or fresh features from existing products
• Unique implementation ideas or case studies with existing analytics tools",4,4,1732128235.0,analytics
1gw0feu,Small DA Gigs,"What are some DA projects that can readily be offered to local small businesses or individuals, where the data is already there?

For example, helping a small business optimize social media postings, or a restaurant's staff numbers and sales to better overlap.

Any ideas or proven experience selling small gigs to locals?",1,0,1732139492.0,analytics
1gw0eve,Small DA gigs,"What are some DA projects that can readily be offered to local small businesses or individuals, where the data is already there?

For example, helping a small business optimize social media postings, or a restaurant's staff numbers and sales to better overlap.

Any ideas or proven experience selling small gigs to locals?",1,3,1732139450.0,analytics
1gvcp6w,Sports Analytics Career,"Hey everyone!

I am very interested in getting into the sports analytics industry. I have a bachelors in psychology, but sports is my passion, and I also love math. Do you have any recommendations on what route I should take?",12,7,1732062777.0,analytics
1gvg0r5,How important are degree titles?,"I’m currently in my undergrad studying Data Analytics with a minor in Computer Science. I’ll be applying for masters programs soon and my school offers a Data Science masters and Data Analytics: Statistics masters. These programs are nearly identical depending on what electives you pick, but the Data Science program has a required thesis whereas the Data Analytics: Statistics program has a required capstone project. I’m wanting jobs as a data scientist, machine learning engineer and I know my coursework in my undergrad and either graduate program will prepare me well, but will employers really look at the degrees differently? I’d rather not do a thesis, but would do it if Data Science looks better than Data Analytics: Statistics on my resume. Thoughts?",7,5,1732072588.0,analytics
1gvc68f,Does anyone else find being able to apply the 'basic' stuff they learned in education to a job fun?,"TLDR ; enjoyed using Monte Carlo esque methods for an experiment at work, despite the seemingly miniscule business value.

Context - I work in SaaS as a data analyst looking at customer tickets, things like time to resolution (TTR) etc. 

I thoroughly enjoy the freedom I have in my job to explore data and not just deploy XGBoost models and create dashboards, and one question my manager (a very open minded person) had was ""given our distribution of TTR samples, if a particular customer was to have 20 or 70 tickets, how does our expectations for their average TTR change?"" 

Feel free to critique my methods (recent grad), but what I did was take this distribution (heavily right skewed) find the mean and SD, and use those values to solve for the rate and shape parameters of a Gamma distribution (I felt it was most appropriate). Once I had this approximate distribution I did a loop of taking random samples for n=1 to 300, many times to get a distribution of sample means.

Now that I have my different distributions of means, when you plot them you see most are approximately normal (CLT) but what I was interested in was how the tails would become shorter as n gets larger (a customer having more tickets), so now what we did was compare that to our observations and see which customers have a TTR that fall outside of our 95% range of means for the given n, hence better highlighting the customers that received an especially bad service.

While I believe the applicable business value of this is quite minimal (a customer doesn't care about probability distributions, just their own individual service, and just looking at the data before this experiment would tell you who has been receiving a poor quality service) I did find this to be quite fun, especially for a work environment. So maybe this could serve as a message to those that don't enjoy their job enough that maybe we can create our own opportunities to do fun experiments.

Thoughts?",10,4,1732061305.0,analytics
1gv4ift,Deploying models,"Guys i have a couple of questions about deploying models:

1. is it difficult for someone with a ds background to learn how to deploy a model? i mean can one or two courses/certificates teach that or a strong swe background is needed?

2. do data analytics/data science master degrees (for example MIT MBAn) teach how to deploy models and other MLE stuff or they generally only teach how to analyze data and build models?",10,3,1732041723.0,analytics
1gvcqax,Looker Studio - Event Count w/filters different from calculated field,"Made an event in looker studio called ""error"" and I wanted to create an error/user metric using calculated fields, but my issue is that I have error defined in two different ways. One is using ""Event Count"" (I'm using GA4) and filtering the event name to ""error"" and get a count of 5. I then tried to make a calculated field like this 

  
COUNT(IF(event\_name = 'error' then 1,0 END)) or something to that degree and the field itself works! But I get a count of 1 instead of 5 like my event count field. The calculated field needs to work because I am trying to make that error/user metric. Any reason why I'm running into this? Thanks!",3,2,1732062862.0,analytics
1gv0ice,Monthly Career Advice and Job Openings,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link",10,12,1732032016.0,analytics
1gvfjdn,Bachelor's in Education to Masters in Business Analytics or Business Administration?,"Hello,

I'm currently working on my Bachelor's degree in Early Childhood Education, but I know I want to pursue a Master's degree right after I'm done. I don't know if I want to pursue business administration or business analytics or if there is a better route to take. I don't even know if business analytics is a good route to take but I am willing to do the study time and work for it. I wanted to stay in education but the way the fields going and the fact I want to provide for a family weighs heavy on my shoulders and I feel like I have to do what's best for my future. I have my next job locked in with career counseling when I'm finished with the bachelor's if that helps with experience in analytical work. I'm m/23 with five years of educational field experience under my belt. Any advice? ",0,8,1732071109.0,analytics
1gv4d8g,Google Analytics 4 finding referrals from job boards we don't post to,"I've recently been digging into the last year of GA4 data to see where our candidate traffic is coming from. There are several sites and job boards we don't post our opportunities to that are showing up as referral sources. This also includes some ""AI-powered"" job boards. On some of those sites, the information isn't even correct.

Are there companies scraping popular job boards for openings and posting them on their own sites?",1,3,1732041377.0,analytics
1gvbubj,HS senior son considering analytics - recommendations for or against?,"My son is a HS senior, and, based on his strong background in math - finished AP Calculus AB & BC with A's as a junior - he is considering analytics and/or applied math as his concentration in college. He will either be attending Johns Hopkins University or Harvard University - has an offer to play football at JHU in-hand and is currently applying restricted early action to Harvard, where he would be a walk-on football player, so needless to say, he's going to have an opportunity to get a great college education.  

All that being said, we don't really know a lot about the actual analytics profession, so would any of you analytics professionals please be willing to share some advice for him at this point in his path?  Is there a specific major that would best prepare him for the field?  Is there anything that you would have done differently on your path to your current job?  How will AI impact the profession in the near future?

I appreciate your help!",0,29,1732060403.0,analytics
1gupieb,How long do you keep your web analytics?,"I'm running a WooCommerce store with Matomo for web analytics. I'm trying to decide how long should one keep web analytics for?

I was thinking at least 18 months to see trends from prior year, but beyond that, are web analytics needed?",6,10,1731992327.0,analytics
1gubrrn,"How Important is Linear Alegebra, etc. Truly in Data Analytics?","Pretty much the title. I'm someone who came from a business background (finance/accounting) and have a good amount of experience transforming/analyzing data from large/disparate sources and presenting key findings to executives across a range of business problems. While I'm certainly not THE most technical or quantitative person on an analytics team, I do have a relatively strong, albeit limited, background in certain data skills, such as Python/statistics, such that I was able to solve problems or do some of the work myself when more technical folks were busy or otherwise unable to help.

I want to keep building on my data skills because I frankly enjoy analyzing and explaining data/generating insights moreso than I do the regular cadence of reporting that I am forced to do in finance/accounting roles. I also want to analyze and solve problems beyond just profit/loss metrics. 

When I look online, I keep seeing that fairly advanced math (i.e. Linear Algebra+) is often seen as foundational knowledge for data science/analytics. My question is how correct is this outside of the highest levels of data science (i.e. FAANG or other very data-centric organizations)? To be blunt, I've found the following to be most useful in my career so far:

1. Being able to transform or build data models that aggregate/generate reports that a business partner/stakeholder can understand quickly and without error. To me, SQL/Python are generally good enough to solve this as you can use these tools to ETL the data and then Excel to put it into a spreadsheet for folks to see trends or create their own ad-hoc analyses

2. Once step 1 is done, simple definition of KPIs that are meaningful, being able to track them, as well as some visuals, dashboards, etc. to slice and dice data. To be honest, I can solve for this via PowerBI, maybe even Excel using pivot tables. The first part of defining business requirements, etc. mostly comes from having good business sense or domain knowledge. Don't really see a use case for linear algebra, etc. type of math here either

3. Strong communication skills and being able to present the ""so-what"" in plain english. Again, I'd almost argue that using really complex algorithms or advanced math will confuse the average business user. Candidly, I've never found much use for executives to present anything beyond some regressions, which I don't believe requires a ton of advanced math (correct me if I'm wrong here). 

So can someone help me understand where the major use cases for really advanced algos/math come up within the data world? I feel like there's something I'm missing, so would really appreciate some insight. Further, if anyone has good resources that explain practical use cases of linear algebra, etc. when coding, that'd be great. I find trying to pick up linear algebra by studying the theory hasn't been helpful, and I'd love to understand more practical examples of how I can apply it while furthering my education.

Thanks for the help!",36,33,1731954685.0,analytics
1gueinu,"Currently in cloud administration, debating switching to data analytics or marketing?","I'm a cloud admin thinking of switching careers to data analytics or marketing. The interviews in tech seems really intense even after working in tech for a few years as a system or cloud admin. The interviews feel like tests where they want you to memorize multiple applications, processes, and steps. The hiring for the last year has been ruthless too, and I've had less responses from jobs even though I have more experience. 

I thought of data analytics first because it relies less on programming like powershell, javascript, or cisco commands. It also is more interesting analyzing charts. I'm interested in investing so observing patterns and seeing how changes can improve company earnings interests me because you actually see a result from your work. I feel the charts are less abstract than random powershell scripts that you would use as a cloud admin. 

Idk if it'd be possible for me to switch to data analytics? I don't have a tech degree. I do have 4 cloud certs and CompTIA. I've been in a few tech jobs over the last 4 years. Would I need an MBA or to go back for another bachelors?

My last option is marketing. Because I like the analytical nature similar to data analytics. The different advertising creative ideas interest me as well. I also like that it's not as technical. However, I'm an introvert, so idk if it would require a lot of direct facing customer work. I've heard some say the pay isn't great and it's like a sales job, is this true?

From my experience, interests, and qualifications. Should I stay in tech as a cloud or system admin or switch to marketing or data/business analytics?",6,34,1731961395.0,analytics
1gu7zjt,Whats the perfect career path for being a Business Analyst,"Hi everyone. In your experience, whats the best course of education one should take to land a job as a business analyst(BA) in the Tech sector? Some say do your undergraduate in BA and then an MBA, or the other way round?

CONTEXT: Im a fresh high school graduate and I have a huge creative side. I like to sing, film edit funny videos and extroverted too. I talk alot too💀. I took Business Studies in High-school and loved it so much I got the highest score.  I like the idea of applying business knowledge in an environment such as Spotify, Netflix, or any other major tech corp to make it more innovative and accessible.",8,6,1731945507.0,analytics
1guftnv,Guidance neeeded: leading bank Data scientist interview in 6 days,"Hello Everyone! 

I have an interview schedule for Data scientist at a leading US bank

Job role requirements: SQL, PYTHON , TABLEAU

my skillset: SQL problem solving( writing SQL queries)

To what I have understand and got the information is that 
The first round will be technical 
Consisting of python ,sql and case studies 

Please guide me on 
1.theoretical and hands on problems for python how I can watch yt videos if any and solve python problems 
2. Theoretical question on DBMS 

Any and all suggestions are welcome",2,3,1731964651.0,analytics
1gu5wda,Question on presenting multivariate categorical data,"Hello! I have a dataset with people who answered multiple (five to be exact) questions on disabilities in their families, and turns out that many of the types of disabilities co-occur. I wanted to show this in a report somehow, but I really struggle to find an appropriate way of presentation. I would like to show how many people have co-occurring disabilities, and which disabilities co-occur. I do not want to use an alluvial graph or parallels sets, I would rather have something like a Venn diagram, but I don't think anything like this is used for presenting data.

Could you please help me?",2,3,1731939966.0,analytics
1gtg9pt,WHAT DO I DO. If I can't land a job NOW and the market is only going to get worse THEN WHAT DO I DO,"I cannot continue to be poor. I cannot enter my 30s with no career making shit money living paycheck to fucking paycheck. Not after all the hard fucking work I've put in and all the suffering I've had to experience just to get my fucking education. 

  
**MA Mathematics, Certificate Computational Linguistics - A** university

**AS Data Science and Computer Science - A** community college

**Certificates in Java and SQL/Database Development - A** community college

**Data Analysis:** Python, SQL, Excel, Snowflake, PowerBI, Tableau, Data Visualization, Natural Language Processing, Large Language Models

Why isn't this enough to get an entry level job? Even with relevant work experience? I get interviews, sometimes I get deep into the process. One job interviewed me SIX TIMES. NO OFFER. WHAT DO I DO. I cannot continue like this with no future and no job prospects.  
",62,245,1731858157.0,analytics
1gtjumd,Is it worth it to get a MS in Data Analytics?,"Hi everyone! I (23F) have wanted to pivot into data analytics for a while now... Is it worth it to get a MS in data analytics with my current credentials or will my path be enough?

As background, I graduated last May of 2023 with a BFA in Industrial/Product Design (STEM-certified major) and a minor in Business from UIUC. I have 2 internships under my belt, one being with a non-profit where I did social media marketing and 2. at a audio electronic company as their HR/Marketing intern. After graduation, I took up a HR sales consulting role for a year where I was super client-facing and managed my own book of business. I did NOT like this role, as I had to serve as an admin for the team, an EA for our CEO, all while handling all of the incoming website leads.

Since leaving that role, I started to self-study with Alex the Analyst beginner SQL tutorials on Youtube as well as making my way through the Data Analyst in Power BI track via DataCamp. After I finish this course, I was planing on taking the PL-300 MS test to gain a certification. I've built one project so far and have posted it live on my GitHub portfolio, and this went through my process of merging in SQL, data cleaning in Power Query, and visualizing in Power BI. I found that I really like my creative side when visualizing and am interested in a Power BI analyst role. 

We all know how saturated the job market is and transparently, I haven't had much luck cold applying to entry-level roles. Even internships, they require you to be enrolled in a master's program for institutionalized benefits (ugh). Considering that I don't have experience with hands-on data, I am in the position where I have an unrelated bachelor's + no experience.

Wondering if I enroll in a master's program to gain education + ability to apply to internships? Is this my best bet?",30,27,1731867493.0,analytics
1gtrs03,Tips or help for Business Analytics Internship Interview,"Hi everyone,

I’m currently interviewing for the **Business Analytics Internship** at Comcast, it’s an 11-week paid internship that offers exposure to work on thought-provoking projects involving data analysis and reporting.

Here’s a summary of the role:

* Work with large datasets to conduct analysis and derive insights that drive change in KPIs.
* Utilize tools like SQL, Tableau, Power BI, and Excel to create visuals and reports that align with business strategies.
* Collaborate with teams to improve processes and adopt best practices.
* Present findings and recommendations through storytelling and visuals that resonate with business goals.

**Preferred skills include:**

* Basic SQL/relational database querying
* Experience with Tableau or Power BI
* Understanding of data models
* Proficiency in Excel and PowerPoint

I’m looking for advice from anyone who has experience with **business analytics, internships at Comcast (or similar companies), or interviews for data-related roles.**

# Specific Questions:

1. **Interview preparation:** What are the key technical and behavioral questions I should prepare for?
2. **SQL/Tableau:** Any recommendations for resources or practice projects to refresh these skills?
3. **Storytelling with data:** How can I effectively present data insights during the interview if asked?
4. **General tips:** What’s the best way to stand out in a competitive internship like this?

I have a background in **MIS**, experience with SQL, Tableau, and dashboard design, and a genuine interest in leveraging data to drive decision-making.

Any tips, advice, or personal experiences would mean the world to me! Thanks in advance for your help 😊",9,8,1731888827.0,analytics
1gtp17o,FP&A to Analytics?,"I’m hoping to get a bit of guidance on what my potential next steps would be career-wise. I took an FP&A Manager position about 2 years ago. This position is within our Analytics team. I’m able to get involved in projects that are both financial and analytics in nature. I’ve gotten experience with SQL, PowerBI, and soon to be Python. I’ve had quite a few projects that have been focused around process improvement and restricting teams. The analytics portion and process improvement/restructuring have been very enjoyable for me, and I seem to be fairly good at it.

I find myself wanting to do more analytics focused work, opposed to finance. I’ve got a natural talent for finance, but I’m much less interested in it. I’m self-evaluating on where exactly I should go from here. For those that have made a jump from finance to analytics, how did you find it? I feel like the skills are very closely aligned. I’m likely behind the curve for others in my peer group that have been purely in analytics, but my finance skills may fill a bit of the gap. The general consensus pay-wise, seems to be that analytics has a higher floor, but finance has a higher ceiling, due to executive opportunities.

Any feedback would be appreciated!",8,6,1731881157.0,analytics
1gtb4xl,Christmas gift for a budding data analyst,"Hi all,

A friend of mine wants to switch careers and become a data analyst. I'd like to support them by giving a Christmas gift that will help them along the way.

Are there any books, productivity accessories, or paid resources you recommend that will help with their learning journey?

Thanks in advance.",2,9,1731840924.0,analytics
1gtgrjc,"I have completed my bachelor's in Statistics, Is it mandatory to complete masters if I am looking for entry level job in data analysis? ","I live in India, In 2023  I completed my bachelor's in statistics with CGPA of 9.72  ,For period of one year I worked in a BPO just to support my family and some personal expenses, Now I am persuing Full Stack Data Science Course From Ed Tech institution which is providing this course in association with IIT Guwahati ( A prestigious institute in India ).
I know python and Sql to the level that I am comfortable in solving leetcode Database problems, I invested 2 months in prepration for Power BI Data Analyst Associate Exam of Microsoft and Successfully Cleared it ,Now I am preparing for AWS machine learning Speciality exam, My question is , considering my educational background, Courses and certificate what are my chances of securing entry level data analyst job as a fresher , I just have 3 years degree in Statistics without any master ,So guys  just want to know your  view on Persuing Master's ",0,7,1731859503.0,analytics
1gt4jhp,Sports betting analytics/modeling,"I recently made a post in 
r/algobetting showing the results of some deep analysis I've been doing on a sports betting moneyline model. If anyone wants to comment or bud in on the conversations about expected value I would appreciate some other opinions from experts in this sub since that sub can be full of pessimist sometimes. 
Here's the link to the post: https://www.reddit.com/r/algobetting/s/RrZ23ovl3o",1,1,1731813873.0,analytics
1gsvg8i,bootcamps for a beginner?,"any advice or reviews for bootcamps for someone looking to get into data analytics? i haven’t been in school since i was i believe 19 or 20, i’m now 25 and living in the US, recently received a SSN so looking to get a job (in anything) or start some form of education again. data related things interest me, although my last education experience was with computer science (i left before taking my college exam.. fuck up on my part, so my highest level of education is high-school).

i’ve been reading negative things about generalassemb.ly or bootcamps in general, but i’m struggling to see how to get my foot in the door with something that isn’t being a waitress or cashier 

any advice is welcome, thanks for reading",3,14,1731786786.0,analytics
1gsonp6,Looking for feedback!,"I am an analytics professional with over 10 years of experience in Marketing and Product Analytics. One of my biggest pain points has been spending hours every week checking multiple dashboards to monitor trends.

To solve this, I built a Chrome extension that automates insights from Google Sheets and delivers them straight to your inbox on a weekly basis. All you need to do is pipe your data into Google Sheets, and the extension takes care of the rest.

I’m genuinely looking for feedback and would greatly appreciate your time and thoughts. Thank you

Here is the [link](https://chromewebstore.google.com/detail/octo/obloifdoiohdfcnimbahlphceecmjoci).",7,6,1731768085.0,analytics
1gstq1f,Time Series for Business Forecasting or Multivariate Analysis,"I'm a student in college studying mis/ business analytics. I have to pick one elective between these two then I graduate. I'm looking into careers in data analytics. Which one would be more beneficial? 

Time series is more interesting to me and multivariate analysis seems very useful. 

  
",2,2,1731782096.0,analytics
1gsg5s0,Has anyone had experience with data engineering academy? ,I keep getting an insane amount of ads the past few months from them ,0,1,1731734113.0,analytics
1grplu5,Proficient in SQL,"For a data analyst how proficient in SQL should we be ? 

When applying to job they usually say knowledge or proficient in SQL. I get nervous applying to them because I don’t know if they expecting 100+ lines of code or just being familiar with the six SQL clauses. 

This is my second data analyst job. And I still have a beginner-ish knowledge of SQL. I want to get an analyst job that uses SQL frequently however I am nervous applying to them.  ",59,37,1731649712.0,analytics
1gs737r,Help Tracking Leads to Phone Calls?,"What's the best way to track leads from a website to a phone call? This client I just started working with doesn't have any kind of shopping cart system- merely a call to action to call their office. I know that's not great, but- that's what we have to work with. I'm guessing I need to just setup a special tag in GTM and apply it to the call to action buttons so I can at least track conversion rates. Any other suggestions on how I can track/manage this data and get some good insights on the marketing we're starting to do? Any way to track actual phone calls, and gain some metrics to tie that to the sales software (Housecall Pro, Quickbooks, etc)?  
Thanks!",1,7,1731705769.0,analytics
1gs4ige,Connecting Tableau To US Census Data / API,Has anybody on here had success connecting Tableau directly to the US Census data / API? Looking to get as close to real-time as possible without having to export .csv files.,1,3,1731698910.0,analytics
1gs0kmn,Podcast Database Auto Update in SQL,"I created a database for a podcast I host to keep track of shows and movies we have discussed, the topics within that episode (or more so main topic), guest we have had on, (if so, their name and profession) whether the episode was a re-releaes or not (have only  had that happen a couple of times).  This is to keep track of our content and so that we are not covering the same thing twice or multiple times. We use Buzzsprout as our hosting service.  What I would like to do is somehow have that table in SQL updated each time we upload an episode to Buzzsprout and it is published, to update the table in SQL. with the fields needed (I haven't worked with Buzzsprouts API and not too sure how to ago about doing this or if it is possible).",1,1,1731688943.0,analytics
1grzzei,How to track conversion in mobile app for campaigns that forwarded to landing page?,,1,1,1731687414.0,analytics
1gridcq,How do you quantify the value of Analytics?,"Do you come up with a value add metric for analytics at your organization?  Is so, how? 

The clearest way would be the incremental lift from predictive models etc.  Which is great. 

I'm asking from a different angle... The overall value of being ""data driven"" as an organization.  So for example, enabling people to use self service analytics tools vs submitting a request and waiting.  Or ensuring that analytics are built into workflows so that people don't need to ""do the math"" in addition to their day jobs.

I hope that makes sense.... And clearly not an easy answer, but would love to hear different ideas or approaches.",12,19,1731626648.0,analytics
1grcvec,How much easier is it to get the next job after your first analytics job?,Just wondering if anyone had personal experiences or thoughts on this. ,24,35,1731612184.0,analytics
1grmppy,how to transition to different industry; From e-commerce/marketing data analyst to health or finance or supply chain/operations industry ?,I have been looking for a job within data analytics or business analytics field and I see where I live currently has lots of jobs within healthcare or finance sector although all these jobs somehow ask for industry experience. Is there a way to get my resume selected in different industry by putting 2-3 portfolio projects in that industry even though my only experience(4+ years) of data analyst is in e-commerce? Any suggestions will be helpful. I was also thinking that job market is tough so therefore I am not getting response from those jobs but also in the requirements/ preferences they always mention experience in healthcare etc. Those people who transition successfully or in hiring positions can give any advice will be helpful.,3,1,1731639681.0,analytics
1grlr5b,Predict Red Flags in Healthcare Insurance Claims (Fraud),I was asked to do a dashboard showing possible fraud in healthcare insurance claims. Can you give me any advice how to start on this. ,3,4,1731636666.0,analytics
1grjfim,Entry Level Job with no College Degree,So I am pretty(intermediate level) well versed with Python's data science/analysis libraries and have done a lot of smaller projects. I also know a little bit of SQL. Are there any entry-level jobs I can get without any college degree? Any feedback would be great. Thank you.,2,40,1731629648.0,analytics
1gr3wu8,Is It Worth Considering an In-House Content Creation Team for Your Store in 2025? Or Just a Waste of Time?,"**Looking at the trends in content creation, we see a major shift starting from 2020.** Yes, creators existed before then, but in much smaller numbers and without the level of influence they have today.

In 2020, with global lockdowns and the explosive growth of TikTok, **the number and significance of content creators skyrocketed.** TikTok in particular was initially dismissed by many brands as a passing trend for teenagers, yet it’s now a *dominant platform in online engagement.*  
*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# TikTok Demographics (April 2024)

* **Women 18-24:** 17.3%
* **Men 18-24:** 18.9%
* **Women 25-34:** 14.6%
* **Men 25-34:** 19.3%
* **Women 35-44:** 6.9%
* **Men 35-44:** 8.9%
* **Women 45-54:** 3.7%
* **Men 45-54:** 4.2%
* **Women 55+:** 3.1%
* **Men 55+:** 3.1%

**63.8% of TikTok users** fall into the over-25 age group, with **49.7% in the 25-44 range**. These users are not only *digitally savvy* but also *trust online transactions* and actively engage with social media, seeking information from influencers who play a significant role in shaping opinions and driving purchasing decisions.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# A Four-Hour Audit with a Jewelry Store

Yesterday, we spent four hours auditing a jewelry store that launched in 2018. This store had effectively used influencers and content creators to drive sales until mid-2023.

Our audit focused on data from 2022 onwards. Together with the owner, we analyzed **current sales numbers and past marketing activities** to understand the impact of various strategies.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Snapshot of Online Performance and Advertising Costs

* **Until Summer 2023**: The store maintained a network of relevant influencers and content creators who consistently converted traffic into sales.
* **Since Mid-2023**: The store reduced its partnerships, retaining only a few key creators from earlier years. Since then, **performance has gradually declined**.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Performance Highlights from the Audit

**2023 Summary:**

* **Sessions (GA4 data):** 541,203
* **Contacts in Database:** 13,935
* **Orders:** 6,538
* **Average Order Value:** 248.58 RON
* **Revenue:** 1,625,216 RON
* **Unconverted Contacts:** 7,397
* **Potential Losses:** 1,838,746.26 RON due to weak communication with existing customers

**2024 Year-to-Date (through Nov 13):**

* **Sessions (GA4 data):** 323,374
* **Contacts in Database:** 20,023
* **Orders:** 4,491
* **Average Order Value:** 258.79 RON
* **Revenue:** 1,162,225.89 RON
* **Unconverted Contacts:** 15,532
* **Potential Losses:** 4,019,526.28 RON from poor engagement with the existing audience

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Key Insights and Takeaways

1. **Organic Reach Builds Long-Term Value:** Organic engagement is crucial for *building a durable, profitable brand.* Brands that establish a strong online presence through organic channels stand the test of time.
2. **Content Creators Are Essential:** Content creators don’t just put your products in front of an audience; they *lend credibility*. Their influence builds trust and can make a significant difference in purchase decisions.
3. **A Well-Defined Strategy Drives Success:** Brand strength isn’t just about ad spend but also about *market relevance.* Consistency, visibility, and perception are powerful elements in brand growth.
4. **Customer Neglect Can Be Costly:** Neglecting your audience, especially those who’ve already shown interest by engaging with your promotions, can cost you. *Failure to nurture these relationships can lead to substantial revenue losses.*
5. **Understanding the Digital Landscape is Key:** The internet works through specific mechanisms, particularly those focused on capturing attention. *Understanding these can unlock new opportunities, increase sales, and open doors for growth.*

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Platform Performance Analysis (2022)

From the analysis conducted on **Meta, Google, TikTok, and Email Marketing**—platforms the store has utilized since 2022:

* Out of **6,872 total orders on the site in 2022**:
   * **Meta** generated: 3,466 orders
   * **Google, TikTok, and Email Marketing** combined: 20 orders
   * **Organic (Influencer-Driven)**: 3,406 orders

*Key Insight*: Nearly **50% of total sales came organically**, largely driven by influencer collaborations established years prior to 2022. When the store reduced these partnerships, **performance began to gradually decline**.

*\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_*

# Conclusion and Next Steps for 2025

This audit revealed core issues facing the store. During the four hours of analysis, we focused on **hard data and critical metrics** that used to be strengths but are now missing. We didn’t even look at creative elements like ad copy or target audiences.

*Thanks to the store owner for dedicating time to this audit.* We’re excited to start working together and unlock the store’s full potential in the upcoming year!

Picture of Store Performances from 2022 to 2024, in Comments.",10,7,1731588196.0,analytics
1grc9ad,Interview: take home assignment,I have an case study coming up next week where I’ll be given several CSVs to analyze and come up with recommendations using any tool i choose. Does anyone have recommendations on what to use besides excel?,0,20,1731610647.0,analytics
1gqmtiz,Some days I want to scream into the abyss,"The boss wanted to create his own metric to calculate user engagement across several platforms. He came up with his own methodology and everything.  

He put the results in a PowerPoint.  

A few days go by and I just spent the last half hour explaining his OWN metric and methodology to him because he didn't know what it was or how we got that number.  

Why",44,14,1731530087.0,analytics
1gqw0rt,Interviewing into new job,"Background: I was laid off in June but had a solid severance package, which gave me some time to find the right next step. After a lot of rejections and auto-responses, I finally accepted an offer for a director-level role in my industry with a small company. I’ve been in the role for a couple of weeks now. The pay is decent (a bit below my previous package), but compensation wasn’t my main concern. The smaller company size means less corporate red tape, and the people seem great—but I do have some concerns about long-term stability.

Now, just as I’m settling in, a wave of interview requests is coming in for roles I applied to earlier. Most are with larger companies offering strong compensation potential, LTIs, and career growth, though they’re hybrid, while my current role is fully remote.

Question: Would it be unprofessional to keep interviewing for these other roles? I’m not applying to new positions, but I want to make sure I’m making the right call before fully committing. For context, I was with my previous company for 10 years (so, not a job-hopper) and I’m in my mid-40s.",9,10,1731555942.0,analytics
1gqw864,Analytics student project,"Hi! 
I hope everyone is well! I’m a student close to finishing my degree and trying to build a portfolio of projects to show off my analytical skills since my experience so far is mostly in retail. I’ve included a uni project on my resume (even though there were some errors in it when graded), and I’m hoping to work on more projects during my break.  I found a Kaggle dataset I’d like to use for market analysis via sentiment analysis, but my questions and approach are pretty similar to what’s already there. Is it okay to use these datasets even if my analysis overlaps a bit? I really want the projects to be my own—not just for recruiters, but for my growth too. To summarise, is it okay to use kaggle datasets and just analyse from there even if the questions are quite similar? 

For background, I’ve mainly been using R during my degree as that’s what has been taught. I know SQL and PowerBI is something that’s highly regarded or required and it is something I am working on learning through out my break and last year of university. Any advice on standing out with these projects or anything else is super appreciated!

Thanks!",3,6,1731556618.0,analytics
1gqf6us,What are fields that analytics is not very prevalent in ,"I was reading about sports analytics and someone mentioned that certain sports are less explored with analytics.

This made me think. Outside of sports what are some fields that have a lot of untapped potential in terms of analyzing and using data?",23,31,1731511061.0,analytics
1gr2zim,Most common ad hoc (on demand) analytics tasks for e-commerce?,"Hey guys,

I built an AI agent for ad hoc data analysis. The process is simple: the analyst uploads data, types in a question, and the AI writes custom code and iteratively extracts insights. Analysts can then validate the code & results and ask follow-up questions, creating a human-in-the-loop workflow for deeper analysis.

Now working on cold outreach and aiming to personalize use cases for potential clients.

**Question for data analysts in e-commerce:** what are the most common inquiries you get? Some of the most common (weekly/daily) custom analytics tasks, the most time-consuming, if applicable.

Thanks!",0,14,1731584806.0,analytics
1gqglzc,Can I realistically get hired with a degree and internship in Accounting?,I graduated with an Accounting degree this year and finished a 3 month accounting internship. Can someone with my resume realistically get hired as an entry level data analyst? ,2,15,1731514686.0,analytics
1gqcyid,Missing data in Google Analytics,"Hi

I have a WordPress site which is notoriously missing data in Google analytics.  
Generally the setup seems to be correct, we're using gtm4wp plugin to connect everything.

However, it seems that sometimes even 30-40% of data is missing. There is evidence pointing to google consent mode v2 blocking some data.

How would you go about it finding where the problem is? What would be the order of steps to take?",2,2,1731504762.0,analytics
1gqg1o1,"JuicyAds data is showing up as ""not set"" in GA4","We’re using JuicyAds for PPC, but GA4 is categorizing traffic coming through UTM-tagged links as ""not set,"" so I’m unable to see this traffic in the analytics.

Additionally, JuicyAds offers various tracking options, such as for registrations and payments. Have you had experience setting up these kinds of measurements?

Please let me know if you can help with the above.",1,1,1731513279.0,analytics
1gq7tm5,I get data out of customer systems and into monitoring systems. Where do analysts come in?,"Splunk etc, we create visibility of complex systems and especially issues. 

At what point can I call an analyst /data engineer/ data scientist in and they'll be able to be helpful?

If I were to come into a computer that has analysts, what would I need to give them to be relevant?

",1,1,1731484080.0,analytics
1gq6but,How And Should I Start To Learn,"Hello I’m 16 and i want to get into this field whenver i’m older. My first question, Is it even worth getting into becoming a Data Or Financial Analyst at this time in the job market. Secondly where should I start learning and what should i learn to be able to get entry level jobs right out of college with little competition against experienced people. Thank you for your time.",0,9,1731477744.0,analytics
1gp7a68,Boss Gifted me a SQL Course,"So I checked my email today and my boss gifted me a mastering SQL for data science course via Udemy. However, I have a certificate in data analytics from a bootcamp I attended for a few months and took somewhat of a similar course in Datacamp during that time. I use SQL almost daily and I am not an expert by an means but feel I am atleast decent.  My most recent performance review was meeting expectations. However, this has me a bit worried, that I am going to get let go and this is there way of telling me I am not good enough. Though there have been no other warning signaling this. Am I just being paranoid?

  
I have been in my role for about a year and this is my first data analytics job.",71,24,1731369615.0,analytics
1gpn664,Working in Logistics with an MSBA,"When I graduated college with a Bachelor's in Business Administration I got a job at a consulting firm doing what they called ""project steering"" (basically junior PM). While I was in this role my company paid for half of my MSBA and I planned to make use of it, but ended up being comfortable in my job even though I wasn't making very much money. I switched projects and roles several times and got a promotion during that time but still hung on because of comfort and the hope that I could make the leap into project management at some point. 

During the summer of 2023 after 5 years with the company I got laid off and had to quickly find another job. The firm I was with did packaging planning for a large automotive manufacturer and so I was able to land a packaging role with one of this manufacturers biggest suppliers. Huge pay increase from my previous role but now 1 year in and I'm unsure of what I want to do with my career. I recently got a promotion to Logistics Planner but haven't started yet. 

For some background, we worked in Tableau and briefly in R during my MSBA but other than that learning hard skills was on our own time. I know that I would obviously need to dive into SQL and Python, but my question is if I wanted to explore business analytics again what would that look like? I understand it's different for everyone but I cant help but think I would basically need to start over given that I don't have any work experience in analytics. I fear that I would take a sizeable pay cut (in my new role that I haven't started yet I will be making $82k). In your opinion would it be worth it for me to start looking at making the switch at this point in my career? I understand the job market for these jobs can be brutal.

",2,8,1731425020.0,analytics
1gpm67o,"Zip codes labeled as Urban, Suburban, and Rural","Does anyone know of a good datasource for labeling zip codes by the categories (Urban, Suburban, and Rural)? 

Thanks ",3,4,1731422340.0,analytics
1gpum4c,Sample size for weighted sample?,"I'm working on a project where we want to do a weighted sample of contracts. Each contract will be weighted according to its dollar-value.

Our goal is to collect some data from each contract and estimate a total number for the entire population. Its a dollar value, not a proportion.

I'm struggling to find any resources regarding sample size or confidence intervals for weighted samples. Does anyone know how to handle this, or could recommend a resource? ",1,5,1731443412.0,analytics
1gpg37g,How useful is inferential stats for a data analyst?,"So I've just finished a university course in descriptive stats, and now I can do the more advanced module. Descriptive stats has been very helpful for my day to day job as a data analyst (digital analyst). It took me about 9 months to finish, but was quite useful.

For the new course I'll again have about a year. I'm curious how useful the advanced module will be and which specific techniques I'll most likely use, and should pay the most attention to:

  
• Study Task 1: t-test for independent observations / Mann-Whitney U test

• Study Task 2: one-way ANOVA / Kruskal-Wallis test

• Study Task 3: Pearson correlation / Spearman correlation

• Study Task 4: Regression analysis

• Study Task 5: t-test for paired observations / Wilcoxon signed-rank test

• Study Task 6: Repeated measures and GLM (General Linear Model)

• Study Task 7: Chi-square test",5,10,1731399677.0,analytics
1goy7ph,28M Regretting My Move to Tech Sales—How Can I Rebuild My Career in Data Analytics?,"Back in 2021, I landed a data analytics role through a grad scheme at a Big 4 firm. It was a great start, learning SQL, Power BI, Python, and gaining consulting skills. But over time, the repetitive tasks and limited pay progression made me consider other options, so I switched to tech sales, hoping for better earnings.

Unfortunately, sales wasn’t the right fit. My first company lacked proper training and direction, leading to layoffs. My second role also struggled with product-market fit and management issues, and I eventually decided to leave.

Now, I’m looking to rebuild my career in analytics. Has anyone here navigated a similar career switch or returned to analytics? Any tips on re-entering the field or insights on interviews would be amazing.

**Additionally....**

A friend of mine, who’s a founder, suggested that I consider *“enhancing”* my CV by adding experience I don’t have in this field, to improve my chances of landing an analytics role. The only challenge would be preparing well enough to handle any specific questions during the interview.

Has anyone else faced similar advice or have thoughts on the risks and benefits of this approach?",15,25,1731347226.0,analytics
1gpcehg,Medical Science graduate looking for a career in data analytics,"Hey guys, I'm about to graduate from a Medical Science from the University of Sydney with a major in pharmacology.  I understand its hard to find a job with solely a science degree however I am still hoping to get one as I do not want to pursue postgraduate study as of yet due to financial and time constraints.  My plan during the summer break is to complete the Data Analysis with Python Certification on freecodecamp and complete some projects.  I am hoping to find an entry level job in data analytics in the health or pharma industry.  Any recommendations or suggestions? Thanks. ",1,6,1731385029.0,analytics
1gp4nwe,Topic Modeling - Support/CSAT/NPS ,"So I've done some topic modeling in college but I'm not an expert at it. We use intercom as our supporting system and I've been peaking at there topic feature set. The cool part about it is that they do all the heavy lifting where you can create your own custom topics, and it also identifies emerging topics. 

The challenge then is when you have support conversations with multiple topics. How do you identify the most pressing? If you have someone mention billing once but then mention videos ten times, intercom doesn't weigh the topics per conversation. 

I've thought of dusting off some ole python scripts I've written in college, but now with AI I wonder how beneficial it would be to just pipe these all into GPT and have it create topics and weigh them for me. 

So question: How have you successfully implemented topic modeling in your orgs for either Support, NPS, etc?",0,1,1731362811.0,analytics
1gp4e42,Are you changing Career to Analytics?,"This post is for you!

I have seen a lot of post lately for people who are planning or already switched to analytics.

I would like to know your progress and see if I collaborate with you.

How much time it took you to fully change your career path and secure your first job? 

During the switch process did you worked with any mentor or trainer to help you? 

Why do you find analytics interesting?

I run a growth consulting and have 14 years of experience out of which 9 years is digital analytics. 

I am happy to work with people who are genuinely passionate about data, digital marketing, analytics or AI to assist and share my insights.

Please comment below with the answers to the questions above. 

EDIT: I am not charging any fees!",0,8,1731362128.0,analytics
1gotyxy,"assigned to a pricing project, no prior experience help !","Hello everyone,

I was recently hired as a corporate financial analyst in an energy sector company. For context, I previously worked in banking as an analyst securities portfolio valuation. I wanted to move into the corporate world to pursue a career in strategic finance, which greatly inspires me. I have a decent academic background in finance that helped me get through the technical questions in the interview (DCF valuation, investment decision-making, Excel, stress testing, etc.).

Currently, the company offers a natural gas appliance protection plan with a monthly fee, and maintenance services are provided by a third-party contractor. Recently, this contractor raised their prices, which has reduced the current margin to nearly zero. I've been assigned the task of reviewing the pricing, but I have no prior experience in service pricing and don't know where to start. also, this position was created and there is no significant work done before.

Logically, I believe I should list all the costs related to the service, calculate a cost price, set a margin considering the competition, and then decide on the new price. However, how is this practically done? Do we project costs and revenues over a certain horizon? If so, how do we determine that horizon? How do we calculate the overhead allocation ratio for this service? How do we assess profitability? Where exactly do we apply the desired margin? Should we stop at EBITDA or continue to net profit?

Can anyone provide an overview of the method and model for a beginner in corporate pricing? is there any resources or website where I can go to learn ?

Thanks in advance for any insights!",3,1,1731336772.0,analytics
1gotcrq,Intelligently Calculating Return on Ad Spend,"Am looking for a way to project return on advertising spend (ROAS). Does not need to be a complex model (in fact would much prefer it if it isn't!).

We have all the data coming in through our SQL pipeline (revenue / subs by campaign, source, adset etc, and based on the attribution date have ROAS after 3 days 7 days 21 / 30 days etc).

What I want to be able to surface in our dashboard is some kind of prediction which is smart enough to update the predicted roas for (e.g.) 30 days, but as time goes on this prediction converges on the actual amount. I.e.. your prediction is calculated from the present performance, so the 30 day roas prediction made for customers converting on (e.g.) 1st October would have some value x\_1 predicted on the 8th October, but you would expect x\_2 from (again, e.g.) 28th October to be much closer to the final amount.

Anything that can just be calculated in a Python script or something on a daily run would be absolutely perfect. Any help v much appreciated!",2,2,1731335108.0,analytics
1gotdhb,Qlik data ingestion suite ,"I worked on a data migration project for a major financial institution and would like to discuss one of the powerful tools they used for data movement

It’s called Qlik Enterprise Manager and Qlik Replicate. I wrote an article on how it works.

I worked extensively on the infrastructure setup for Qlik Enterprise Manager and Qlik Replicate in a public cloud environment. Let me know what you’d like to learn about it, and I can cover it in my next article

you can find me on medium (@rasvihostings)",0,1,1731335166.0,analytics
1goizfd,Harvard For Data Science,"hello gang, after much thinking, i decided I want to join harvard extension school master of data science program, whole thing with the MIT Micromaster will cost me around $35,000 or so. what do you all think? any one has experience with this program before? any one can share experience on how good the program is? thanks.",4,21,1731296136.0,analytics
1gohe2s,Where to progress from a Business Systems Analyst job title specifically ,"For those of you who were BSA’s in the past,   what roles do you hold now? 

- I’ve done some research and my target roles seems to be Analytics Engineer, Data Scientist, Data Engineer. I have 11 years overall working experience but made the pivot to tech in 2022. I graduated last year with an MBA in MIS & currently doing the OMSA at Georgia Tech. (Pls don’t ask me why I’m doing a 2nd masters & why can’t I just get a job first. Short answer is that I come from a non traditional background in healthcare & didn’t have proper guidance. That’s all you need to know thank you!)

- Although I prefer having to do less presentations or managing people, I still welcome your responses and advice on how to advance.

- For this job title,  I want my tasks to be confined to 9-5 I don’t want to do anything related to work outside of that. 

- I’m talking BSA not BA or SA. Yes I know they’re interchangeable but I want to know how you fared after you moved on from the BSA title specifically.

Thanks in advance!",5,3,1731291014.0,analytics
1govbid,"l’m Looking for a Remote Internship in Data Analytics, but My Applications Haven't Been Successful. Any Advice?""","I graduated from physics, then I worked in the Front-End field. This year, I decided to focus on developing my skills in data analytics, a field where I feel more enthusiastic and motivated. I've learned Python, Numpy, Pandas, and Tableau, and I’m continuing to learn and work on projects. I am currently looking for a remote internship. My English level is between B1 and B2, which sometimes makes me feel less confident. Although I have applied to many positions, I haven’t yet found a remote internship. What steps should I take?

",0,6,1731340216.0,analytics
1goaj11,Business Analytics Engineer,"Anyone ever heard of this job title or know someone who does? If so, may I ask what your day to day looks like and your salary especially in the USA? 

I think this may just be another buzzword for the specific analytics engineer they want but I want to know if $90-$110k is a good range for it. There’s no indication that it’s a mid level or senior role but the job description includes  expectations to lead etc",9,19,1731271771.0,analytics
1gojgd3,What does a good data analytics project look like?,I'm having a hard time finding quality data analytic projects out there. Most of the projects on YouTube use simulated data with no real world impact of their findings and its super basic like cleaning in Python and then visualizing in a BI software.,2,3,1731297719.0,analytics
1goa9ml,Tools for A/B Trsting?,"Been out of the dev game for a few years. What’s the best way to split test landing pages, overall site design and individual elements? Mainly looking at conversion rates. 

Free is good but not required. I read that Google’s optimize is going away- is that still a good option for now? Optimizely? Anything else?

Thank you!",2,10,1731271100.0,analytics
1gnqsh5,How does your work impact business you are working for ?,"I aspire to be a data and BI analyst, I wanted to gain vision of how analytics work that data analyst like you do impacts the business you are working for.

If you were to crisply describe what your work achieves and why exactly it is critical to your particular firm and its operation, it will give valable insight in developing my thinking about analytics as well as better me skillset.





",15,18,1731206781.0,analytics
1gneqkt,Do you feel you are responsible for EVERYTHING?,"I am business side Power BI developer for last 5 years, but I found myself not only doing the typical front-end stuff, but also
- stakeholder management,
- creating adoption frameworks,
- being product owner,
- running team of data engineers, BI developers and business analyst 
- responsible for WHOLE data quality in the domain
- doing simple data engineering stuff
- conducting business analysis
- creating roadmaps for future analytics development

The scope creep is real and I kinda envy external consultants „do my stuff only” and getting even better rate and overtime, whereas being employee while having more security it means I do unsaid Data and Analytics Manager work. Do you have similar experience?

I seriously thinking about going consultant route, moving to IT department with goal of having less scope and more focus. I am not sure that being covert manager is way to go.",39,28,1731172166.0,analytics
1gnlgah,How do you like to do your process of ETL?,"I’m currently learning about data analytics and curious to hear how people like to Extract, transform, load the data. 

What’s your process for it? What advice do you have? 

",8,12,1731190494.0,analytics
1gn7tnt,Business Project - Weather dataset,"I have a group project in my business analytics class that requires using data to get meaningful information from the set. 20 hours of work must be put in to receive full credit. 

I decided to get 5 years worth of datasets of the weather in florida. What are some good data visualizations to choose from? would excel or power BI be the best option to work with? honestly any suggestions would be awesome, as my 2 other group members are not helpful ",12,11,1731150528.0,analytics
1gn2emv,A Fresh Data Analyst Aspiring To Meet Others & For More Opportunities,"Hey, everybody! Hope you all are well and safe.

I am a new Data Analyst - I transitioned into this field last year, earned some certifications and did some full-scale projects, and thankfully landed a full-time job as a Data Analyst. Now, I am working in a Data Analytics role full-time and also continuing my academic education.

I just wanted to post here and welcome anyone who is willing to connect and exchange ideas and grow in the professional world together! If you are interested in discussing and helping each other and growing in the field and networking together, please do DM me. I am also on LinkedIn.

Also, it would be great if I can get some consulting work - entry level consulting projects would be great and I am now trying on Upwork to earn some. Please do connect me to anyone who can help my chances of getting beginner-level consulting projects so I can build my experience and forte there as well.

Thank you! Welcoming responses.",6,5,1731127707.0,analytics
1gn1ijl,Does Google penalize sites that don't use Google Analytics?,"I used to get signficant search traffic from Google. I removed Google Analytics. Now very little. It's wierd, all my sites are open source, super fast, no ads, no trackers, have sitemaps/accurate titles and descriptions, useful original content, inbound links. I can't figure out what's different, other than I no longer have GA on the site.",4,1,1731124612.0,analytics
1gn6i6n,I am presently doing an MBA in India and want some advise on career options : I want to hear my career towards analytics field,I am presently doing an MBA in India and want to hear my career towards the analytics field. Our internships are going on. Is it prudent to take an internship in sale/marketing or some other domain( based on brand name or salary). Would it affect my original goal ?,0,2,1731144663.0,analytics
1gn4fzu,I made a Customer Journey Tracker & Analytics for WordPress. (Plus chat with data) AMA,Designed for Martech.,0,1,1731135625.0,analytics
1gn2fle,American Data Scientists With International Degrees?,"hello gang, I am an American, I work in IT and I see people all the time with international degrees, from India, from China, other times they have degrees that have nothing to do with IT at all, like my team lead at my company has a master's in counseling. so here is my question, theoretically one with an Indian bachelor's degree in computer applications, and a master's in data science, with us citizenship or a green card, can be someone perfectly qualified to work as an American data scientist right? Indian data science can't be much different than American data science am I right?",2,5,1731127804.0,analytics
1gmjnzn,How would you go about this?,"Hi all,

So I recently started a job this week working as a game master for a business that just opened up this past month. 

Looking into the business, I can see that the way they collect and store customer data is very basic, and leaves me the opportunity to create a DB for them. It also gives me the opportunity to do DA for them as since it's a new business, DA can exponentially make a difference in business decisions.

However, I don't want to just create all these and still get the title Game Master. I'd like to negotiate with a title change as Data Analyst and if possible an increase in wage, as it is a field I want to get into, but with the market right now, can't.

How would you go about this?",3,13,1731075795.0,analytics
1gluqt0,I want to start my first portfolio project as a beginner data analyst,"I wanted to create my first portfolio project as a beginner data analyst. I am a person who learns by doing and not studying for 5 hrs straight. I am having difficulty in choosing what is the best data set i should focus for my first project that can be showcase in my portfolio website for applying a job.

Any insight from you guys who started also from the bottom will be very helpful and i know its too much to ask but i hope someone who has the time can comment here an exact roadmap what to do. There are guided projects on youtube but i think following along will not make me learn what i needed to do. Its like i’m just copying somebody’s work. 

I hope someone can help me 🙏",38,34,1730997575.0,analytics
1gm53au,Looker Studio - Report level filter for session duration?,"
I am looking for a way to filter traffic at the report level by session duration. I want to remove any traffic with a duration on the site of 0s. So far, the only way I have found to do this is to view a table of cities by average session duration, look for cities with an average session duration of 0 seconds, then create a report level filter removing those cities. 

I feel like there has to be a better way to do this. Any ideas?",5,2,1731024004.0,analytics
1glv0dq,Should I attend the Gartner Data & Analytics summit?,"4 YoE in analytics, manage a small team of analysts. I am interested in attending but know the cost is not insignificant when factoring in travel and accommodations and don’t want to blow a massive hole in our professional development budget if it’s just meh. Has anyone here attended previously? Are there other conferences that you’d recommend as an alternative? Interested in any feedback - positive or negative. ",7,3,1730998241.0,analytics
1gm00w2,Coursera/ google data language courses ,"I’m a undergrad currently pursuing a degree in sciences but want to move into data analytics after I graduate. I’ve been using coursera courses, including the Google ones that give certificates, to learn the basics, but each course is around £38. Are these courses/certificates worth it? Or is it better to do some bootcamp instead? ",1,3,1731010750.0,analytics
1glxwyc,looking for a partner to make a data bank,"I'm working on a personal data bank as a hobby project. My goal is to gather and analyze interesting data, with a focus on psychological and social insights. At first, I'll be capturing people's opinions on social interactions, their reasoning, and perceptions of others. While this is currently a small project for personal or small-group use, I'm open to sharing parts of it publicly or even selling it if it attracts interest from companies.



I'm looking for someone (or a few people) to collaborate with on building this data bank. 



Here’s the plan and structure I've developed so far:

# Data Collection

* **Methods:** We’ll gather data using surveys, forms, and other efficient tools, minimizing the need for manual input.
* **Tagging System:** Each entry will have tags for easy labeling and filtering. This will help us identify and handle incomplete or unverified data more effectively.

# Database Layout

* **Separate Tables:** Different types of data will be organized in separate tables, such as **Basic Info**, **Psychological Data**, and **Survey Responses**.
* **Linking Data:** Unique IDs (e.g., `user_id`) will link data across tables, allowing smooth and effective cross-category analysis.
* **Version Tracking:** A “version” field will store previous data versions, helping us track changes over time.

# Data Analysis

* **Manual Analysis:** Initially, we’ll analyze data manually but set up pre-built queries to simplify pattern identification and insight discovery.
* **Pre-Built Queries:** Custom views will display demographic averages, opinion trends, and behavioral patterns, offering us quick insights.

# Permissions and User Tracking

* **Roles:** We’ll establish three roles:
   * **Admins** \- full access
   * **Semi-Admins** \- require Admin approval for changes
   * **Viewers** \- view-only access
* **Audit Log:** An audit log will track actions in the database, helping us monitor who made each change and when.

# Backups, Security, and Exporting

* **Backups:** Regular backups will be scheduled to prevent data loss.
* **Security:** Security will be minimal for now, as we don’t expect to handle highly sensitive data.
* **Exporting and Flexibility:** We’ll make data exportable in **CSV** and **JSON** formats and add a tagging system to keep the setup flexible for future expansion.",1,1,1731005470.0,analytics
1glwo9w,How to pull all ads data for Meta?,I am working to build a data pipeline to get metrics for all Meta ads. The problem is with Insights API you need to pass ad id to get the data and doesn’t allow more than one ad id. I want to system to be dynamic meaning it pull all the ads data for provided campaign ids. I am using Make to build the pipeline. ,0,8,1731002385.0,analytics
1gl3vea,Data Camp Premium is free this week!,"Good for learning & showing certifications, if anyone's interested in transitioning to data/product analyst. 

It lasts for 4 more days i think. ",20,15,1730913999.0,analytics
1gl4hu2,Struggling to break into data analytics—any tips ,"I'll cut to the chase: I’m looking for guidance on securing a job or internship as a Data/Business Analyst. I’m in my final year of undergrad, with skills in Python, SQL, Power BI, and Tableau. I’ve also explored basics in deep learning and AI, and I have experience with Unreal Engine, Android Studio (Java), and am currently studying data mining and image processing. For my final year project, I’m working on a hand gesture-to-speech app. I've completed at least one project in each of the areas mentioned.

Despite these skills, I’m struggling to get responses to my applications. I’ve created an ATS-friendly resume with a strong score, and I’m actively networking on LinkedIn. However, I’m still not seeing progress, even for internships.

I’d really appreciate any advice on what I might be missing or areas I could improve to stand out in my job search. Any feedback on skills or experience I should build on would be a huge help. ",6,19,1730915581.0,analytics
1gle2z4,Best election analytics resource for interesting visualizations? ,Does anyone have any recommendations on websites that do in depth election analysis with visualizations? Preferably interactive. ,0,1,1730940357.0,analytics
1glamma,Job Hunt Guidance ,"Hello!!

I am a recent Masters graduate from a Canadian university with 1.5 years internship experience in Power BI and Power Platform. (2 years old experience in SAP ABAP). I cleared my PL-300 certification and have some projects.

I am searching for data analyst jobs for the last 2+ months but no luck for interviews. My resume should be fine as I am using the FAANG path LaTex format and GPT 4 for increasing ATS chances.

I am a bit down on the networking game and I would appreciate any feedback you guys might have if I am doing anything wrong.

Thanks <3",0,3,1730930978.0,analytics
1glgtvg,"Given a take home assignment, I'm clueless as to where to start. Can i get a little guidance?",I was given 5 different data sets and i was to choose one dataset to analyze. The goal is to create a question and find an answer to the question using analytics. I was given 4 hours and it's due in 2 days... Can someone give me some guidance as to where to start?,0,11,1730948704.0,analytics
1glbm7j,Crazy Question!,I'm a Senior Claims Adjuster wanting to go into Data Analytics (working on a grad cert in Business Analytics) ... Possible?,0,7,1730933579.0,analytics
1gkplvb,"I am trying to decide between a business analytics masters or information systems and analytics masters. BA is in person while IS is online. Which degree will allow me more job opportunities and growth, or will the job pool be about the same? ",Which degree? ,0,12,1730865160.0,analytics
1gk4e5a,"CS to DA career path switch, where to begin?","Hi! I'm a CS graduate, more specifically cyber security, but i've had little to no luck in landing any jobs in my field, I've been applying relentlessly for exactly a year now, not even internships where I live!:(

Market is super saturated for anything remotely related to tech, even for DA, however, I find it still has good opportunities where I can land at least an entry level position.

I do have some background on analytics, I've done some research on SAS, completed the Microsoft Power BI course for data analysis, and I also have basic knowledge of SQL, but I still don't feel as though I'm ""ready""- I don't have any work experience in DA, I see many posts about how courses are not enough until you apply DA on real work, and how recruiters won't take your certifications/courses into consideration and really, will only look at past experience in Data Analysis, so how am I supposed to start?

I currently have a job in sales, it's all over the place as I do supply chain and some sales reporting here and there and I've tried to incorporate my own version of ""analytics"" into my reporting, but I do not see excel reporting as enough, and for the life of me I wouldn't have a clue on how more to incorporate DA into my work/life.

So, dumb it down for me, tell me where to start, what to focus on, what certifications I should do, what has helped you, what tools you find best personally, what suggestions and example sets should I try to work on at my own pace, all of it.

My issue is I need to follow a set of rules, but I can't even begin to make myself a schedule to follow with the end goal being a DA job, because I truly have no clue where to begin.

Sorry to sound like a child, but it's really one of those moments where my brain is insisting I get into it, but is not leading me towards any clues or starting points.

EDIT: I forgot to mention, I'm open to learning more about any other analytics-related field, as long as I feel like I am a good fit to understand it and I can follow through, but I don't think I'm knowledgeable enough on other analytics fields to make that decision, but I would like to give myself enough credit to note that if I really put my mind to something I'm more than likely to excel by all means, so feel free to suggest anything for me! Thanks a lot",10,12,1730805539.0,analytics
