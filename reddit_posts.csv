subreddit,title,score,id,url,created_utc,body
datascience,"Weekly Entering & Transitioning - Thread 13 Jan, 2025 - 20 Jan, 2025",3,1i06k3y,https://www.reddit.com/r/datascience/comments/1i06k3y/weekly_entering_transitioning_thread_13_jan_2025/,1736744505.0," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new)."
datascience,E-values: A modern alternative to p-values,34,1i1bjhi,https://www.reddit.com/r/datascience/comments/1i1bjhi/evalues_a_modern_alternative_to_pvalues/,1736876152.0,"In many modern applications - A/B testing, clinical trials, quality monitoring - we need to analyze data as it arrives. Traditional statistical tools weren't designed with this sequential analysis in mind, which has led to the development of new approaches.

E-values are one such tool, specifically designed for sequential testing. They provide a natural way to measure evidence that accumulates over time. An e-value of 20 represents 20-to-1 evidence against your null hypothesis - a direct and intuitive interpretation. They're particularly useful when you need to:

- Monitor results in real-time
- Add more samples to ongoing experiments
- Combine evidence from multiple analyses
- Make decisions based on continuous data streams

While p-values remain valuable for fixed-sample scenarios, e-values offer complementary strengths for sequential analysis. They're increasingly used in tech companies for A/B testing and in clinical trials for interim analyses.

If you work with sequential data or continuous monitoring, e-values might be a useful addition to your statistical toolkit. Happy to discuss specific applications or mathematical details in the comments.​​​​​​​​​​​​​​​​

P.S: Above was summarized by an LLM.

Paper: Hypothesis testing with e-values - https://arxiv.org/pdf/2410.23614

Current code libraries:

Python:

- expectation: New library implementing e-values, sequential testing and confidence sequences (https://github.com/jakorostami/expectation)

- confseq: Core library by Howard et al for confidence sequences and uniform bounds (https://github.com/gostevehoward/confseq)


R: 

- confseq: The original R implementation, same authors as above

- safestats: Core library by one of the researchers in this field of Statistics, Alexander Ly. (https://cran.r-project.org/web/packages/safestats/readme/README.html)

"
datascience,Fuck pandas!!! [Rant],334,1i0x2pm,https://www.kaggle.com/code/sudalairajkumar/getting-started-with-python-datatable,1736825814.0,"I have been a heavy R user for 9 years and absolutely love R. I can write love letters about the R data.table package. It is fast. It is efficient. it is beautiful. A coder’s dream.
 
But of course all good things must come to an end and given the steady decline of R users decided to switch to python to keep myself relevant.

And let me tell you I have never seen a stinking hot pile of mess than pandas. Everything is 10 layers of stupid? The syntax makes me scream!!!!!! There is no coherence or pattern ? Oh use [] here but no use ({}) here.
Want to do a if else ooops better download numpy. 
Want to filter ooops use loc and then iloc and write 10 lines of code.

It is unfortunate there is no getting rid of this unintuitive maddening, mess of a library, given that every interviewer out there expects it!!! There are much better libraries and it is time the pandas reign ends!!!!! (Python data table even creates pandas data frame faster than pandas!)

Thank you for coming to my Ted talk
I leave you with this datatable comparison article while I sob about learning pandas 

"
datascience,Seeking Advice on Amazon Bedrock and Azure,8,1i13e03,https://www.reddit.com/r/datascience/comments/1i13e03/seeking_advice_on_amazon_bedrock_and_azure/,1736851459.0,"Hello everyone. I’m currently exploring AI infrastructure and platform for a new project and I’m trying to decide between Amazon Bedrock and Azure (AI Infrastructure & AI Studio). I’ve been considering both but would love to hear about your real-world experiences with them.

Has anyone used Amazon Bedrock or Azure AI Infrastructure and Azure AI Studio? How would you compare the two in terms of ease of use, performance, and overall flexibility? Are there specific features from either platform that stood out to you, or particular use cases where one was clearly better than the other?

Any advice or insights would be greatly appreciated. Thanks in advance! "
datascience,Dash Python Incosistence Performance,2,1i18xcv,https://www.reddit.com/r/datascience/comments/1i18xcv/dash_python_incosistence_performance/,1736869506.0,"I'm currently working on a project using Dash Python. It was light and breezy in the beginning. I changed a few codes while maintaining the error at 0, test-running it once in a while just to check if the code change affected the website, and nothing bad happened. But after I left it for a few hours without changing anything, the website wouldn't run anymore and showed me an ""Internal Server Error"". This happened way too many times, and it stresses me out, as I have to update most of the backend ASAP. Does anyone has any similar experience and manage to solve it? I'd like to know how."
datascience,Mastering The Poisson Distribution: Intuition and Foundations,122,1i0dbaj,https://medium.com/@alejandroalvarezprez/mastering-the-poisson-distribution-intuition-and-foundations-d96bae3de61d,1736772966.0,
datascience,Where do you go to stay up to date on data analytics/science?,285,1i03pk7,https://www.reddit.com/r/datascience/comments/1i03pk7/where_do_you_go_to_stay_up_to_date_on_data/,1736735034.0,"Are there any people or organizations you follow on Youtube, Twitter, Medium, LinkedIn, or some other website/blog/podcast that you always tend to keep going back to? 

My previous career absolutely lacked all the professional ""content creators"" that data analytics have, so I was wondering what content you guys tend to consume, if any. Previously I'd go to two sources: one to stay up to date on semi-relevant news, and the other was a source that'd do high level summaries of interesting research papers. 

Really, the kind of stuff would be talking about new tools/products that might be of use, tips and tricks, some re-learning of knowledge you might have learned 10+ years ago, deep dives of random but pertinent topics, or someone that consistently puts out unique visualizations and how to recreate them. You can probably see what I'm getting at: sources for stellar information."
datascience,exit cmd.exe from R (or python) without admin privilege,0,1i1951j,https://www.reddit.com/r/datascience/comments/1i1951j/exit_cmdexe_from_r_or_python_without_admin/,1736870071.0,"I run:

system(""TASKKILL /F /IM cmd.exe"")

I get

Erreur�: le processus ""cmd.exe"" de PID 10333 n'a pas pu être arrêté.

Raison�: Accès denied.

Erreur�: le processus ""cmd.exe"" de PID 11444 n'a pas pu être arrêté.

Raison�: Accès denied.


I execute a batch file> a cmd open>a shiny open (I do my calculations)> a button on shiny should allow the cmd closing (and the shiny of course)

I can close the cmd from command line but I get access denied when I try to execute it from R. Is there hope? I am on the pc company so I don't have admin privilege"
datascience,Humana Senior DS Position merry-go-round,21,1i0c3x8,https://www.reddit.com/r/datascience/comments/1i0c3x8/humana_senior_ds_position_merrygoround/,1736768520.0,Anyone in the US apply to the Humana revolving Senior DS position over the last 5 months? They continuously post this position and never seem to fill it. Wondering if anyone has gotten an actual interview. I make it to the prescreen rounds  every single time I apply and then it just gets reposted.  
datascience,Advice on stabilizing an autoencoder's representation?,3,1i0m1ts,/r/learnmachinelearning/comments/1haqmu6/advice_on_stabilizing_an_autoencoders/,1736795972.0,
datascience,Mistral released Codestral 25.01 : Free to use with VS Code and Jet brains,0,1i0wxxt,/r/OpenAI/comments/1i0wwxm/mistral_released_codestral_2501_ranks_1_on_lmsys/,1736825403.0,
datascience,"How we matured Fisher, our A/B testing library",63,1hzpcuv,https://medium.com/@alejandroalvarezprez/how-we-matured-fisher-our-a-b-testing-package-6f2294746a56,1736696534.0,
datascience,Sky-T1-32B: Open-sourced reasoning model outperforms OpenAI-o1 on coding and maths benchmarks ,2,1i0czn6,/r/ArtificialInteligence/comments/1i0cyyw/skyt132b_opensourced_reasoning_model_outperforms/,1736771828.0,
datascience,Seeking Advice on GPU Comparison: GreenNode vs FPT,0,1i0bhi3,https://www.reddit.com/r/datascience/comments/1i0bhi3/seeking_advice_on_gpu_comparison_greennode_vs_fpt/,1736765919.0,"I’m currently exploring GPU options for my projects and I’m curious if anyone here has experience using GPUs from GreenNode or FPT. I’m looking for real feedback on how they compare in terms of performance, pricing, and overall experience.

Has anyone used GPUs from either of these providers? How do they stack up against each other in terms of power efficiency, speed, and reliability? Are there any specific use cases where one outperforms the other?

I’d love to hear your thoughts, personal experiences, or any suggestions you might have on which GPU might be better for intensive workloads. Thanks in advance!"
datascience,"200 applications - no response, please help. I have applied for data science (associate or mid-level) positions. Thank you ",403,1hyploh,https://www.reddit.com/gallery/1hyploh,1736575525.0,
datascience,Feeling stuck in my career. Please help,56,1hyte5x,https://www.reddit.com/r/datascience/comments/1hyte5x/feeling_stuck_in_my_career_please_help/,1736592284.0,"I'm in a weird position, where I feel like I'm stuck in my career. I really enjoy mathematics, ML/AI, implemented a lot of algorithms from scratch in C, developed new models for business purposes, presented at some internal/small conferences, and developed entire ML infrastructures for startups, but having no real opportunities to grow more.

At the moment I'm making over 100k$ working remotely from eastern Europe for a FAANG in the US (they have an office here, but my entire data science team is based in the US and I'm working on the same things as them).

When applying to companies in the US/UK I'm receiving zero callbacks (willing to relocate), although companies from the same areas are reaching out with remote offers of \~100k$/year. Those don't have the benefits of my current company, and are not attractive opportunities. I'm looking to relocate and get 200k$+. Current internal transfers to the US are closed, as they are looking to expand in east Europe. I've also asked for more difficult projects, but those are only available for US, not for my region.

The projects that are open to me at the moment offer zero satisfaction and I want to solve more complex problems and continue to expand my skills, but I'm stuck for the only thing that my studies are in eastern Europe and that I don't hold a PhD, even though I've already worked on novel models in industry, and speaking with friends and colleagues that hold a PhD, my skills are on par.

I'm at a point where I feel like skills and projects don't mean absolutely anything, and the only thing that has any weight for getting a job are diplomas and people you know... Maybe I'm exaggerating, but from all of my experiences I'm starting to feel like people from my region without studies abroad are seen only as cheap labor that should never be given the chance to work on real problems and be paid accordingly (a shitty company directly told me that, while another told me explicitly that my skills don't matter and they're only offering bad projects with bad pay in my region). It's like, there's a limit to the level of difficulty I can work on and the pay I can receive, regardless of how much I outcompete others...

At the moment, I'm working on a side research project that I'll be sending to some top tier conferences, and then try getting a PhD in the west... but that will take years, and if I already have the skills it's so frustrating to be stuck for so long just for a diploma and a title...

Or maybe my skills are really not on par, and I'm only good compared to the people in my region? Here's my resume if anyone would be willing to offer me some feedback."
datascience,SQL Squid Game: Imagine you were a Data Scientist for Squid Games (9 Levels),523,1hy7g0m,https://datalemur.com/sql-game,1736524298.0,
datascience,How to communicate with investors?,13,1hyaw2t,https://www.reddit.com/r/datascience/comments/1hyaw2t/how_to_communicate_with_investors/,1736532961.0,"I'm working at a small scale startup and my CEO is always in talks with investors apparently. I'm currently working in different architectures for video classification as well as using large multimodal models to classify video. They want to show how no other model works on our own data (obviously) and how recent architectures are not as good as our own super secret model (videoMAE finetunned on our data...). I'm okay with faking results/showing results that cannot be compared fairly. I mean I'm not but if that's what they want to do then fine, doesn't really involve more work for me.

Now what pisses me off is that now I need to come up with a way to get an accuracy per class in a multilabel classification setting based solely on precision and recall per class because different models were evaluated by different people at different times and I really only have those 2 metrics per class - precision and recall. I don't even know if this is possible, it feels like it isn't, and is an overall dumb metric for our use case. All because investors only know the word ""accuracy""....

Would it not be enough to say: ""This is the F1 score for our most important classes, and as you can see, none of the other models or architectures we've tried are as good as our best model... By the way, if you don't know what F1 means, just know that higher scores are better. If you want, I can explain it in more detail..."" as opposed to getting metrics that do not make any sense...?

I will not present it to the investors, I only need to come up with a document, but wouldn't it be enough for the higher ups in my company to say what I said above in this scenario? "
datascience,Simple Full stack Agentic AI project to please your Business stakeholders,0,1hyxec6,https://www.reddit.com/r/datascience/comments/1hyxec6/simple_full_stack_agentic_ai_project_to_please/,1736606856.0,"Since you all refused to share how you are applying gen ai in the real world, I figured I would just share mine.

  
So here it is:  [https://adhoc-insights.takuonline.com/](https://adhoc-insights.takuonline.com/)  
There is a rate limiter, but we will see how it goes.



Tech Stack:

Frontend: Next.js, Tailwind, shadcn

Backend: Django (DRF), langgraph

LLM: Claude 3.5 Sonnet

I am still unsure if l should sell it as a tool for data analysts that makes them more productive or for quick and easy data analysis for business stakeholders to self-serve on low-impact metrics.

So what do you all think?"
datascience,Companies are finally hiring,1556,1hxalxo,https://www.reddit.com/r/datascience/comments/1hxalxo/companies_are_finally_hiring/,1736421422.0,"I applied to 80+ jobs before the new year and got rejected or didn’t hear back from most of them. A few positions were a level or two lower than my currently level. I got only 1 interview and I did accept the offer. 

In the last week, 4 companies reached out for interviews. Just want to put this out there for those who are still looking. Keep going at it. 

Edit - thank you all for the congratulations and I’m sorry I can’t respond to DMs. Here are answers to some common questions. 

1. The technical coding challenge was only SQL. Frankly in my 8 years of analytics, none of my peers use Python regularly unless their role is to automate or data engineering. You’re better off mastering SQL by using leetcode and DataLemur

2. Interviews at all the FAANGs are similar. Call with HR rep, first round is with 1 person and might be technical. Then a final round with a bunch of individual interviews on the same day. Most of the questions will be STAR format. 

3. As for my skillsets, I advertise myself as someone who can build strategy, project manage, and can do deep dive analyses. I’m never going to compete against the recent grads and experts in ML/LLM/AI on technical skills, that’s just an endless grind to stay at the top. I would strongly recommend others to sharpen their soft skills. A video I watched recently is from The Diary of a CEO with Body Language Expert with Vanessa Edwards. I legit used a few tips during my interviews and I thought that helped "
datascience,How good are your linear algebra skills?,81,1hxt0wl,https://www.reddit.com/r/datascience/comments/1hxt0wl/how_good_are_your_linear_algebra_skills/,1736472663.0,"Started my masters in computer science in August. Bachelors was in chemistry so I took up to diff eq but never a full linear algebra class. I’m still familiar with a lot of the concepts as they are used in higher level science classes, but in my machine learning class I’m kind of having to teach myself a decent bit as I go. Maybe it’s me over analyzing and wanting to know the deep concepts behind everything I learn, and I’m sure in the real world these pure mathematical ideas are rarely talked about, but I know having a strong understanding of core concepts of a field help you succeed in that field more naturally as it begins becoming second nature.

Should I lighten my course load to take a linear algebra class or do you think my basic understanding (although not knowing how basic that is) will likely be good enough?"
datascience,SAS - SQL question: inobs= vs outobs=,5,1hy8jhq,https://www.reddit.com/r/datascience/comments/1hy8jhq/sas_sql_question_inobs_vs_outobs/,1736527127.0,"Just a quick question here regarding PROC SQL in SAS.  Let's say I'm just writing some code and I want to test it.  Since the database I'm querying has over a million records, I don't want it to process my code for all the records.  

My understanding is that I would want to use the inobs= option to limit how much of the table is queried and processed on the server.  Is this correct?

The outobs= option will return however many records I set, but it process every record on the table in the server.  Is this correct?"
datascience,Microsoft's rStar-Math: 7B LLMs matches OpenAI o1's performance on maths,1,1hxxjz6,/r/OpenAI/comments/1hxxjcc/microsofts_rstarmath_7b_llms_matches_openai_o1s/,1736487671.0,
datascience,Is it necessary to understand the mathematics for data science anymore?,0,1hyhm2a,https://www.reddit.com/r/datascience/comments/1hyhm2a/is_it_necessary_to_understand_the_mathematics_for/,1736550108.0,"The general consensus has been that you need to know the maths behind the models (proofs) in data science and that it’s advantageous to do so. But in this era of LLMs making our work even easier, and all the tools we use having already baked in the math behind the models for us, I wonder if this statement remains true or if it’s outdated advice. For example, in my limited experience of doing DS work, I’m personally yet to come across a situation where I was able to debug something because I knew the deep math proofs behind it (I did stats so know a decent amount of proofs). But I’m also very new to DS work so perhaps I’m missing something. 

Obviously understanding model output and what each of them means such as AUC, residuals, checking for drift etc remains important and will always do so."
datascience,Best resources for CO2 emissions modeling forecasting,7,1hxplq8,https://www.reddit.com/r/datascience/comments/1hxplq8/best_resources_for_co2_emissions_modeling/,1736462904.0,"I'm looking for a good textbook or resource to learn about air emissions data modeling and forecasting using statistical methods and especially machine learning. Also, can you discuss your work in the field; id like tonlearn more."
datascience,I was penalized in a DS interview for answering that I would use a Generalized Linear Model for an A/B test with an outcome of time on an app... But a linear model with a binary predictor is equivalent to a t-test. Has anyone had occasions where the interviewer was wrong?,266,1hx305z,https://www.reddit.com/r/datascience/comments/1hx305z/i_was_penalized_in_a_ds_interview_for_answering/,1736391735.0,"Hi,

I underwent a technical interview for a DS role at a company. The company was nice enough to provide feedback. This reason was not only reason I was rejected, but I wanted to share because it was very surprising to me. 

They said I aced the programming. However, hey gave me feedback that my statistics performance was mixed. I was surprised. The question was what type of model would I use for an A/B test with time spent on an app as an outcome. I suspect many would use a t-test but I believe that would be inappropriate since time is a skewed outcome, with only positive values, so a t-test would not fit the data well (i.e., Gaussian outcome). I suggested a log-normal or log-gamma generalized linear model instead.

  
I later received feedback that I was penalized for suggesting a linear model for the A/B test. However, a linear model with a binary predictor *is equivalent to a t-test*. I don't want to be arrogant or presumptuous that I think the interviewer is wrong and I am right, but I am struggling to have any other interpretation than the interviewer did not realize a linear model with a binary predictor is equivalent to a t-test.

Has anyone else had occasions in DS interviewers where the interviewer may have misunderstood or been wrong in their assessment?"
datascience,Spreadsheet first cell debate ,0,1hy9am1,https://www.reddit.com/r/datascience/comments/1hy9am1/spreadsheet_first_cell_debate/,1736529004.0,"Settle this debate I'm having with a coworker. 

I say that spreadsheets should always start in row 1, column A. They say row 2, column B, [edit] so that there is an empty row and column before the table starts.

What's your take?"
datascience,Question on quasi-experimental approach for product feature change measurement,5,1hxnq3t,https://www.reddit.com/r/datascience/comments/1hxnq3t/question_on_quasiexperimental_approach_for/,1736457945.0,"I work in ecommerce analytics and my team runs dozens of traditional, ""clean"" online A/B tests each year. That said, I'm far from an expert in the domain - I'm still working through a part-time master's degree and I've only been doing experimentation (without any real training) for the last 2.5 years. 

One of my product partners wants to run a learning test to help with user flow optimization. But because of some engineering architecture limitations, we can't do a normal experiment. Here are some details:

* Desired outcome is to understand the impact of removing the (outdated) new user onboarding flow in our app. 
* Proposed approach is to release a new app version without the onboarding flow and compare certain engagement, purchase, and retention outcomes.
* ""Control"" group: users in the previous app version who did experience the new user flow
* ""Treatment"" group: users in the new app version who *would have* gotten the new user flow had it not been removed

One major thing throwing me off is how to handle the shifted time series; the 4 weeks of data I'll look at for each group will be different time periods. Another thing is the lack of randomization, but that can't be helped.

Given these parameters, curious what might be the best way to approach this type of ""test""? My initial thought was to use difference-in-difference but I don't think it applies given the specific lack of 'before' for each group. "
datascience,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,4,1hxi5em,/r/MachineLearning/comments/1hwvk9x/rn_tabpfn_v2_accurate_predictions_on_small_data/,1736443855.0,
datascience,Am I underpaid/underemployed at $65k for a Data Analyst position in a MCOL city?,66,1hx286f,https://www.reddit.com/r/datascience/comments/1hx286f/am_i_underpaidunderemployed_at_65k_for_a_data/,1736389552.0,"I'm in a mcol city. I have a master's in Data Analytics that I finished in October 2024, and I've been working as a Data Analyst for 1.5 years. Before that, I was a study lead Clinical Data Manager for over a year (and before that I was a tax researcher and worked in HR). Currently, I make $65k base salary, but $85k total compensation. 

I keep getting interviews for Data Scientist positions that are well into the $100k+ base salary range, but I haven't landed an offer yet (it's really disheartening). Am I underpaid?

P.S. I'm open to job suggestions lol"
datascience,absolute path to image in shiny ui,5,1hwmsd2,https://www.reddit.com/r/datascience/comments/1hwmsd2/absolute_path_to_image_in_shiny_ui/,1736350202.0,"Hello,
Is there a way to get an image from an absolute path in shiny ui, I have my shiny app in a .R and I havn t created any R project or formal shiny app file so I don t want to use a relative paths
for now 
ui <- fluidPage(  tags$div( tags$img(src= absolute path to image).....
doesn t work"
datascience,Change my mind: feature stores are needless complexity.,111,1hvzskd,https://www.reddit.com/r/datascience/comments/1hvzskd/change_my_mind_feature_stores_are_needless/,1736278466.0,"I started last year at my second full-time data science role. The company I am at uses DBT extensively to transform data. And I mean very extensively. 

The last company I was at the data scientist did not use DBT or any sort of feature store. We just hit the raw data and write sql for our project.

The argument for our extensive feature store seems to be that it allows for reusability of complex logic across projects. And yes, this is occasionally true. But it is just as often true that there is a Table that is used for exactly one project. 

Now that I'm starting to get comfortable with the company, I'm starting to see the crack in all of this; complex tables built on top of complex tables built in to of complex tables built on raw data. Leakage and ambiguity everywhere. Onboarding is a beast. 

I understand there are times when it might be computationally important to pre-compute some calculation when doing real-time inference. But this is, in most cases, the exception, not the rule. Most models can be run on a schedule. 

TLDR; The amount of infrastructure, abstraction, and systems in place to make it so I don't have to copy and paste a few dozen lines of SQL is n or even close to a net positive. It's a huge drag.

Change my mind. "
datascience,As of 2025 which one would you install? Miniforge or Miniconda? ,41,1hw5s76,https://www.reddit.com/r/datascience/comments/1hw5s76/as_of_2025_which_one_would_you_install_miniforge/,1736293697.0,"As the title says, which one would you install today if having a new computer for Data Science purposes. Miniforge or Miniconda and why?

For TensorFlow, PyTorch, etc.

Used to have both, but used Miniforge more since I got used to it (since 2021). But I am formatting my machine and would like to know what you guys think would be more relevant now.

I will try UV soon but want to install miniforge or miniconda at the moment."
datascience,People who do DS/Analytics as freelancing any suggestions ,76,1hvwxzv,https://www.reddit.com/r/datascience/comments/1hvwxzv/people_who_do_dsanalytics_as_freelancing_any/,1736271429.0,"Hi all

I've been in DS and aligned fields in corporate for 5+ years now. I'm thinking of trying DS freelance to earn additional income as well as learn whatever new things I can by doing more projects. I have few questions for people who have done it or tried it. 

Does it pay well? Do you do it fulltime or along with your job? Is it very difficult with a job?

What are some good platforms?

How do you get started? How much time does it take? How to get your first project? How to build your brand?

If you do it with your current job how much time does it take? Did you take permission from your manager about this?

Other than freelancing are there better options to make additional income?

Thanks!"
datascience,CAG : Improved RAG framework using cache,7,1hwcayh,/r/OpenAI/comments/1hwc8xp/cag_improved_rag_framework_using_cache/,1736313595.0,
datascience,Gradient boosting machine still running after 13 hours - should I terminate?,23,1hvy3ld,https://www.reddit.com/r/datascience/comments/1hvy3ld/gradient_boosting_machine_still_running_after_13/,1736274260.0,"I'm running a gradient boosting machine with the caret package in RStudio on a fairly large healthcare dataset, \~700k records, 600+ variables (most are sparse binary) predicting a binary outcome. It's running very slow on my work laptop, over 13 hours.

Given the dimensions of my data, was I too ambitious choosing hyperparameters of 5,000 iterations and a shrinkage parameter of .001? 

  
My code:  
\### Partition into Training and Testing data sets ###

set.seed(123)

inTrain <- createDataPartition(asd\_data2$K\_ASD\_char, p = .80, list = FALSE)

train <- asd\_data2\[ inTrain,\]

test  <- asd\_data2\[-inTrain,\]



\### Fitting Gradient Boosting Machine ###

set.seed(345)

gbmGrid <- expand.grid(interaction.depth=c(1,2,4), n.trees=5000, shrinkage=0.001, n.minobsinnode=c(5,10,15))

gbm\_fit\_brier\_2 <- train(as.factor(K\_ASD\_char) \~ .,

tuneGrid = gbmGrid,

data=train,

trControl=trainControl(method=""cv"", number=5, summaryFunction=BigSummary, classProbs=TRUE, savePredictions=TRUE),

train.fraction = 0.5,

method=""gbm"",

metric=""Brier"", maximize = FALSE,

preProcess=c(""center"",""scale""))

"
datascience,This is how l stay up to date with the latest machine learning papers and technics ,122,1huz6ax,https://www.reddit.com/r/datascience/comments/1huz6ax/this_is_how_l_stay_up_to_date_with_the_latest/,1736170682.0,"l go for the popular papers l hear about on Twitter and machine learning subreddits(Andrew Ng suggests these as great places to get the latest ml information). It won't cover everything, but it's okay and better to have some coverage than none - just because there are too many papers.

As for why l go for popular(by popular l mean a lot of technical/knowledgeable people are talking about them), well for certain things to be adopted they need some adoption, and l am sure there are great frameworks/architectures out there that just never got adopted and are not used a lot.

I will not write GPU kernels just so l can make this esoteric architecture, which l found on a paper somewhere,  work. Instead, I would use the popular transformer architecture, with lots of documentation and empirical evidence to support performance.

How about you all?"
datascience,What technology should I acquaint myself with next?,13,1hvfuwa,https://www.reddit.com/r/datascience/comments/1hvfuwa/what_technology_should_i_acquaint_myself_with_next/,1736213326.0,"Hey all. First, I'd like to thank everyone for your immense help on my last question. I'm a DS with about ten years experience and had been struggling with learning Python (I've managed to always work at R-shops, never needed it on the job and I'm profoundly lazy). With your suggestions, I've been putting in lots of time and think I'm solidly on the right path to being proficient after just a few days. Just need to keep hammering on different projects. 

At any rate, while hammering away at Python I figure it would be beneficial to try and acquaint myself with another technology so as to broaden my resume and the pool of applicable JDs. My criteria for deciding on what to go with is essentially: 

1. Has as broad of an appeal as possible, particularly for higher paying gigs
2. Isn't a total B to pick up and I can plausibly claim it as within my skillset within a month or two if I'm diligent about learning it

I was leaning towards some sort of big data technology like Spark but I'm curious what you fine folks think. Alternatively I could brush up on a visualization tool like Tableau."
datascience,data experience,471,1hurpgg,https://i.redd.it/aun922n06bbe1.jpeg,1736140819.0,
datascience,Are Medium Articles helpful?,21,1hv3gn4,https://www.reddit.com/r/datascience/comments/1hv3gn4/are_medium_articles_helpful/,1736182086.0,"I read almost every day something from Medium (I do write stuff myself too) though I kind of feel some of the articles even though highly rated are not properly written and to some extent loses its flow from the title to the content.

I want to know your thoughts and how have you found articles helpful on Medium or TDS."
datascience,What's your biggest time sink as a data scientist?,183,1huk9gq,https://www.reddit.com/r/datascience/comments/1huk9gq/whats_your_biggest_time_sink_as_a_data_scientist/,1736118652.0,"I've got a few ideas for DS tooling I was thinking of taking on as a side project, so this is a bit of a market research post. I'm curious what data-scientist specific task/problem is the biggest time suck for you at work. I feel like we're often building a new class of software in companies and systems that were designed for web 2.0 (or even 1.0). "
datascience,SWE + DS? Is learning both good,4,1hv5720,https://www.reddit.com/r/datascience/comments/1hv5720/swe_ds_is_learning_both_good/,1736186285.0,"I am doing a bachelor in DS but honestly i been doing full stack on the side (studying 4-5 hours per day and developing) and i think its way cooler.

Can i combine both? Will it give me better skills?"
datascience,"Tried Leetcode problems using DeepSeek-V3, solved 3/4 hard problems in 1st attempt",0,1hvnkbl,/r/OpenAI/comments/1hvnjf6/tried_leetcode_problems_using_deepseekv3_solved/,1736241165.0,
datascience,Do you prepare for interviews first or apply for jobs first?,187,1hudtrj,https://www.reddit.com/r/datascience/comments/1hudtrj/do_you_prepare_for_interviews_first_or_apply_for/,1736102433.0,"I’ve started looking for a new job and find myself in a bit of a dilemma that I’m hoping you might have some experience with. Every day, I come across roles that seem like a great fit, but I hesitate to apply because I feel like I’m not fully prepared for an interview. While I know there’s no guarantee I’ll even get an interview, I worry about wasting an opportunity if I’m not ready.

On the other hand, preparing for an interview when you have one lined up seems like the most effective approach, but I’m not sure how to balance it all.

How do you usually handle this?"
datascience,Meta's Large Concept Models (LCMs) : LLMs to output concepts ,5,1huz0m1,/r/OpenAI/comments/1huyy4h/metas_large_concept_models_lcms_llms_to_output/,1736170208.0,
datascience,"How are these companies building video/image generation tools? From scratch, fine-tuning Llama, or something else?
",19,1huloe0,https://www.reddit.com/r/datascience/comments/1huloe0/how_are_these_companies_building_videoimage/,1736122444.0,"There’s an enormous amount of LLM-based tools popping up lately, especially in video/image generation, each tied to a different company. Meanwhile, we only see a handful of really good open-source LLM models available.

So, my question is: How are these companies creating their video/image/avatar-generation tools? Are they building these models entirely from scratch, or are they leveraging existing LLMs like Llama, GPT, or something else?

If they are leveraging a model, are they simply using an API to interact with it, or are they actually fine-tuning those models with new data these companies collected for their specific use case?

If you’re guessing the answer, please let me know you’re guessing, as I’d like to hear from those with first-hand experience as well.

Here are some companies I’m referring to:

* **Video/image generation**:
   * [heygen.com](https://heygen.com)
   * [invideo.io](https://invideo.io)
   * [character.ai](https://character.ai)
   * [kindroid.ai](https://kindroid.ai)
   * [runwayml.com](https://runwayml.com)"
datascience,Best LLMs to use ,0,1hvk25m,https://www.reddit.com/r/datascience/comments/1hvk25m/best_llms_to_use/,1736226461.0,"So I tried to compile a list of top LLMs (according to me) in different categories like ""Best Open-sourced"", ""Best Coder"", ""Best Audio Cloning"", etc. Check out the full list and the reasons here : https://youtu.be/K_AwlH5iMa0?si=gBcy2a1E3e6CHYCS"
datascience,"Weekly Entering & Transitioning - Thread 06 Jan, 2025 - 13 Jan, 2025",7,1hurdd1,https://www.reddit.com/r/datascience/comments/1hurdd1/weekly_entering_transitioning_thread_06_jan_2025/,1736139681.0," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new)."
datascience,What schema or data model are you using for your LLM / RAG prototyping?,8,1huoyaf,https://www.reddit.com/r/datascience/comments/1huoyaf/what_schema_or_data_model_are_you_using_for_your/,1736131935.0,"How are you organizing your data for your RAG applications? I've searched all over and have found tons of tutorials about how the tech stack works, but very little about how the data is actually stored. I don't want to just create an application that can give an answer, I want something I can use to evaluate my progress as I improve my prompts and retrievals.

This is the kind of stuff that I think needs to be stored:

* Prompt templates (i.e., versioning my prompts)
* Final inputs to and outputs from the LLM provider (and associated metadata)
* Chunks of all my documents to be used in RAG
* The chunks that were retrieved for a given prompt, so that I can evaluate the performance of the retrieval step
* Conversations (or chains?) for when there might be multiple requests sent to an LLM for a given ""question""
* Experiments. This is for the purposes of evaluation. It would associate an experiment ID with a series of inputs/outputs for an evaluation set of questions.

I can't be the first person to hit this issue. I started off with a simple SQLite database with a handful of tables, and now that I'm going to be incorporating RAG into the application (and probably agentic stuff soon), I really want to leverage someone else's learning so I don't rediscover all the same mistakes."
datascience,Announcing Plotlars 0.8.0: Expanding Horizons with New Plot Types! 🦀✨📊,35,1hu5gha,https://www.reddit.com/r/datascience/comments/1hu5gha/announcing_plotlars_080_expanding_horizons_with/,1736077739.0,"Hello Data Scientists!

I’m thrilled to announce the release of **Plotlars 0.8.0** — our latest step towards making **data visualization in Rust** more powerful, accessible, and versatile.

With this release, we’ve introduced **four new plot types**, unlocking exciting ways to represent your data visually. Whether you’re working with images, geographical datasets, or matrix data, Plotlars has you covered!

🚀 **New Features in Plotlars 0.8.0**

* 🖼️ Image Plot Support: Visualize **raster data** effortlessly with our new Image plot. Perfect for embedding and displaying image-based datasets directly in your plots.
* 🥧 PieChart Support: Represent **categorical data** using elegant and customizable pie charts. Ideal for showing proportions and category breakdowns.
* 🎨 Array2DPlot for RGB Data: Introducing Array2DPlot for **2D array visualization** using **RGB color values**. Excellent for displaying pixel grids, image previews, or matrix-based visualizations.
* 🌍 ScatterMap for Geographical Data: Plot your **geographical data points** interactively on maps with ScatterMap. Perfect for visualizing cities, sensor locations, or any spatial data.

🌟 **A Big Thank You to Our Supporters!**

Plotlars is nearing an incredible **300 stars on GitHub**. Your support, feedback, and enthusiasm have been instrumental in driving this project forward. If you haven’t already, please consider **leaving a star ⭐️ on GitHub** — it’s a small gesture that means a lot and helps others discover Plotlars.

🔗 Explore More:

📚 [Documentation](https://docs.rs/plotlars/0.8.0)  
💻 [GitHub Repository](https://github.com/alceal/plotlars)

**If you love Plotlars, share it with your friends and colleagues! Let’s build a thriving ecosystem of data science tools in Rust together.**

Thank you all for your continued support, and as always — **happy plotting!** 🎉📊

https://preview.redd.it/dhd0kxqfy5be1.png?width=984&format=png&auto=webp&s=c8059ef9038fada080b033f6d89a9765409fab5b

"
machinelearning,[D] Simple Questions Thread,4,1hzprm8,https://www.reddit.com/r/MachineLearning/comments/1hzprm8/d_simple_questions_thread/,1736697638.0,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!"
machinelearning,[D] Monthly Who's Hiring and Who wants to be Hired?,40,1hq5o1z,https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/,1735615814.0,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience."
machinelearning,[R] Titans: Learning to Memorize at Test Time,21,1i1lg6o,https://www.reddit.com/r/MachineLearning/comments/1i1lg6o/r_titans_learning_to_memorize_at_test_time/,1736902312.0,"Abstract: “Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps an attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of a fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.”

Arxiv: https://arxiv.org/abs/2501.00663"
machinelearning,[R] Transformer²: Self-Adaptive LLMs,21,1i1l8d4,https://www.reddit.com/r/MachineLearning/comments/1i1l8d4/r_transformer²_selfadaptive_llms/,1736901669.0,"Paper: https://arxiv.org/abs/2501.06252

**Abstract**

Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer², a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer² employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific ""expert"" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer² demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer² represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. 

Blog Summary: https://sakana.ai/transformer-squared/

GitHub: https://github.com/SakanaAI/self-adaptive-llms"
machinelearning,[D] Machine Learning Engineer vs AI Research Scientist: Future Prospects?,75,1i18421,https://www.reddit.com/r/MachineLearning/comments/1i18421/d_machine_learning_engineer_vs_ai_research/,1736867312.0,"Some people say that AI research scientists (PhD holders) are pretty much irreplaceable because of their ability to push the boundaries of knowledge and come up with groundbreaking methods and algorithms. But let’s be real—tech companies don’t need a ton of researchers, especially if their work doesn’t directly boost profits.

On the flip side, Machine Learning Engineers are the ones putting those algorithms into action, scaling systems, and keeping production pipelines running—all things that directly bring in the $$$. That’s why some people think MLE roles will grow faster than AI research scientist roles in the future.

What do you think? Are there trends or experiences you’ve seen that suggest one of these roles will be more in demand down the line? I'm currently a PhD student by the way.

For a fair comparison, let’s assume both roles are at a FAANG company."
machinelearning,[D] How are people searching for papers in ArXiv? ,18,1i1hz8c,https://www.reddit.com/r/MachineLearning/comments/1i1hz8c/d_how_are_people_searching_for_papers_in_arxiv/,1736892699.0,"Hello,

I am wondering what is the usual way people search for or discover new papers in ArXiv? Do you just use their search engine? Any tips/hints?"
machinelearning,[R] Cosine Similarity Isn't the Silver Bullet We Thought It Was,408,1i0hfsd,https://www.reddit.com/r/MachineLearning/comments/1i0hfsd/r_cosine_similarity_isnt_the_silver_bullet_we/,1736784660.0,"Netflix and Cornell University researchers have exposed significant flaws in cosine similarity. Their study reveals that regularization in linear matrix factorization models introduces arbitrary scaling, leading to unreliable or meaningless cosine similarity results. These issues stem from the flexibility of embedding rescaling, affecting downstream tasks like recommendation systems. The research highlights the need for alternatives, such as Euclidean distance, dot products, or normalization techniques, and suggests task-specific evaluations to ensure robustness. 

Read the full paper review of 'Is Cosine-Similarity of Embeddings Really About Similarity?' here: [https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was](https://www.shaped.ai/blog/cosine-similarity-not-the-silver-bullet-we-thought-it-was)"
machinelearning,Pre-trained models for Sentiment classification and analysis [Discussion],5,1i1act4,https://www.reddit.com/r/MachineLearning/comments/1i1act4/pretrained_models_for_sentiment_classification/,1736873202.0,"Hi. I am working on a project which requires me to identify sentiments from English text and then quantify those sentiments as percentage. I need to run six models on the text and then compare the classifications.

So far, I have explored some BERT and RoBERTa based models in Huggingface, which are trained using the GoEmotion dataset provided by Google. I was curios, are there any better models that I am missing? Please leave the name of some pre-trained models which can give some good results.

TIA!"
machinelearning,[D]How do you measure improvements of your AI pipeline?,0,1i1gr9y,https://www.reddit.com/r/MachineLearning/comments/1i1gr9y/dhow_do_you_measure_improvements_of_your_ai/,1736889549.0,"I am very creative when it comes to adding improvements to my embedding or inference workflows, but  I am having problems when it comes to measuring whether those improvements really make the end result better for my use case. It always comes down to gut feeling.

How do you all measure...

..if this new embedding model if better than the previous?

..if this semantic chunker is better than a split based one?

..if shorter chunks are better than longer ones?

..if this new reranker really makes a difference?

..if this new agentic evaluator workflow creates better results?

**Is there a scientific way to measure this?**"
machinelearning,[D] Predicting the probability of default for a credit card user,2,1i1aarq,https://www.reddit.com/r/MachineLearning/comments/1i1aarq/d_predicting_the_probability_of_default_for_a/,1736873056.0,"I have an imbalanced dataset of about 100,000 rows 1500 of them are of defaultes, which has more than 1000 features and has lots of missing values. Alsothe name of the features are anonymized (like bureau\_1, bureau\_2)  so it also seems difficult and these feaures had max correlation of 0.1 with the target variable

I want to predict the probability of a customer who might default based on the data but am not able to make much progress in terms of metrics like recall (0.25), f1 and auprc.  
I have tried various tree based models like lgbm, xgboost etc with various class balance attributes but its not giving me that good of results.

If anyone of you have such prior experience of handling such datasets, can you suggest me what should i do in terms of feature engineering, modelling etc. All of your help will mean a lot to me."
machinelearning,[D]  Correlation clustering?,3,1i13aot,https://www.reddit.com/r/MachineLearning/comments/1i13aot/d_correlation_clustering/,1736851054.0,"I wanted to apply clustering algorithms on a similarity matrix. Is that possible? If yes, how?"
machinelearning,[D] Non-Person Action Recognition,0,1i194ja,https://www.reddit.com/r/MachineLearning/comments/1i194ja/d_nonperson_action_recognition/,1736870033.0,"I am working on getting object tracking working for a sports game, and would like to take the next step and be able to detect when an action has taken place (like a soccer ball has gone out of bounds, of a bowling ball has hit pins, or a ball has been thrown (as opposed to a practice throw or pump fake).  I have been doing these by hand coding heuristics for how to detect, but I would like to be more flexible.  All the libraries for action recognition seem to be about human skeleton actions.  That makes me think I am looking at the wrong problem space.  Is there existing art for taking locations of objects over time and learning when an action is taking place given training data?"
machinelearning,LLM Distributed Training [R],5,1i0vrg3,https://www.reddit.com/r/MachineLearning/comments/1i0vrg3/llm_distributed_training_r/,1736821786.0,"What are the approaches to access datasets during training? Are they downloaded to the machines/pods before starting the training process or are they network mounted?

Similarly for large models how do the models are deployed for inference? ( for auto scaling or for updating the model version)"
machinelearning,[D] Prove me wrong…,0,1i1fwsg,https://www.reddit.com/r/MachineLearning/comments/1i1fwsg/d_prove_me_wrong/,1736887221.0,"I’ve built an LGBM model to classify Parkinson’s patients using a dataset of 2,562 patients with 37 features selected through P value and correlation analysis and my own domain knowledge, questions can be binary, continuous or ordinal eg do they have Urinary Problems yes/no = 0/1, all questions are numerical answers.  The dataset was split into 70% training (1,793 samples), 15% validation (384 samples), and 15% hold-out test (385 samples). I performed 5-fold stratified cross-validation on the training set, with approximately 1,434 samples for training and 359 for validation in each fold. The dataset contains 1085 PD patients and 1477 non-PD patients. I think the performance is really good, I'm wondering if anyone has any additional tests or methods to assess whether it's a big fantasy or have I a good model on my hands?

.=== Cross-Validation Metrics ===

Mean F1 Score: 0.8860 ± 0.0210

Mean AUC: 0.9565 ± 0.0095

Mean Recall: 0.8814 ± 0.0239

Mean Precision: 0.8911 ± 0.0251 

=== Hold-Out Set Metrics ===

F1 Score: 0.8875

AUC: 0.9505

Recall: 0.8957

Precision: 0.8795"
machinelearning,NannyML chunking [D],5,1i0u5sv,https://www.reddit.com/r/MachineLearning/comments/1i0u5sv/nannyml_chunking_d/,1736817040.0,"Does anyone have experience with the NannyML library? I am having a difficult time fully grasping the reasoning behind forcing users to split data into chunks. I haven’t seen any other drift detection libraries do this. 

Let’s say I have a model on which I would like to perform drift detection. I have some reference feature data from some time ago, and some analysis feature data from today. It seems that to use this library, I am required to split these 2 datasets into arbitrary chunks (they recommend at least 6). I would actually like to perform drift detection by comparing both sets of data to each other as a whole, however. This doesn’t work - forcing the chunk size to 1 results in the upper_threshold value to be set to 0 and every feature gets alerted on.

It seems like the library is geared towards comparing some number of reference datasets  across time vs some equal number of analysis datasets across time… but doesn’t work if there is only have 1 analysis dataset (for 1 date). What am I missing here? Any help much appreciated!"
machinelearning,[Project] Hallucination Detection Benchmarks,24,1i0g71d,https://www.reddit.com/r/MachineLearning/comments/1i0g71d/project_hallucination_detection_benchmarks/,1736781378.0,"Hi Everyone, I recently noticed most LLM observability providers (Arize AI, Galileo AI, LangSmith) use a simple LLM-as-a-Judge framework to detect hallucinations for deployed RAG applications. There's a ton of hallucination detection research out there like [this](https://arxiv.org/abs/2311.05232) or [this](https://arxiv.org/abs/2403.16527) survey, so I wondered why aren't any of these providers offering more advanced research-backed methods? Given the user input query, retrieved context, and LLM output, one can pass this data to another LLM to evaluate whether the output is grounded in the context. So I benchmarked this LLM-as-a-Judge framework against a couple of research methods on the HaluBench dataset - and turns out they're probably right! A strong base model with chain-of-thought prompting seems to work better than various research methods. [Code here](https://github.com/liuzihe02/halu/tree/main). Partial results:

|Framework|Accuracy|F1 Score|Precision|Recall|
|:-|:-|:-|:-|:-|
|Base (GPT-4o)|0.754|0.760|0.742|0.778|
|Base (GPT-4o-mini)|0.717|0.734|0.692|0.781|
|Base (GPT-4o, sampling)|0.765|0.766|0.762|0.770|
|CoT (GPT-4o)|**0.833**|**0.831**|**0.840**|0.822|
|CoT (GPT-4o, sampling)|0.823|0.820|0.833|0.808|
|Fewshot (GPT-4o)|0.737|0.773|0.680|**0.896**|
|Lynx|0.766|0.780|0.728|0.840|
|RAGAS Faithfulness (GPT-4o)|0.660|0.684|0.639|0.736|
|RAGAS Faithfulness (HHEM)|0.588|0.644|0.567|0.744|
|G-Eval Hallucination (GPT-4o)|0.686|0.623|0.783|0.517|"
machinelearning,[P] Geometric Intuition for Dot Product,9,1i0ju9b,https://www.reddit.com/r/MachineLearning/comments/1i0ju9b/p_geometric_intuition_for_dot_product/,1736790606.0,"Hi Community,

First, I want to thank you for reading my earlier posts on geometric intuition and receiving with worms! I didn't expect to receive so much good feedback and also different explanations in the comment. I learned so much!

Motived by this, I wrote another post for geometric intuition and this time about ""**Dot Product**"". Here is the link [https://maitbayev.github.io/posts/dot-product/](https://maitbayev.github.io/posts/dot-product/)

Let me know what you think"
machinelearning,[P] Fast Semantic Text Deduplication,22,1i0cd4n,https://www.reddit.com/r/MachineLearning/comments/1i0cd4n/p_fast_semantic_text_deduplication/,1736769562.0,"Hi! A friend and I have been working on a project called [SemHash](https://github.com/MinishLab/semhash) which I wanted to share. We found that text deduplication is more complex than it appears, so we built this to simplify the process.

Duplicate samples can skew model training, return redundant samples in RAG workflows, reduce generalization, and cause train-test leakage—leading to unreliable results. Techniques like minhash handle exact or near-exact duplicates, but semantic deduplication also catches semantically redundant samples, which we believe is an important aspect of deduplication. Furthermore, it’s not trivial to see why something was removed with minhash, which we also believe is important. For this reason. we’ve added explainability features as well so that you can inspect why something was removed. We already found some interesting results on some well known datasets in our benchmarks which are included in the repo.

The package can be installed with `pip install semhash`, and the basic usage looks like this (this example assumes you have the `datasets` library installed):

    from datasets import load_dataset
    from semhash import SemHash
    
    # Load a dataset to deduplicate
    train = load_dataset(""ag_news"", split=""train"")[""text""]
    test = load_dataset(""ag_news"", split=""test"")[""text""]
    
    # Initialize a SemHash instance
    semhash = SemHash.from_records(records=train)
    
    # Deduplicate the train set
    deduplicated_train = semhash.self_deduplicate().deduplicated
    
    # Or deduplicate the test set against the train set
    deduplicated_test = semhash.deduplicate(records=test).deduplicated

I’m very interested in hearing your thoughts on this! Is deduplication a part of your current ML workflows, and if so, what techniques do you use?"
machinelearning,[P] What is RF and How to Implement it?,0,1i162tn,https://www.reddit.com/r/MachineLearning/comments/1i162tn/p_what_is_rf_and_how_to_implement_it/,1736861424.0,"If you're building an LLM application that handles complex or ambiguous user queries and find that response quality is inconsistent, you should try **RAG Fusion**!

The standard RAG works well for straightforward queries: retrieve ***k*** documents for each query, construct a prompt, and generate a response. But for complex or ambiguous queries, this approach often falls short:

* Documents fetched may not fully address the nuances of the query.
* The information might be scattered or insufficient to provide a good response.

This is where **RAG Fusion** could be useful! Here’s how it works:

1. **Breaks Down Complex Queries:** It generates multiple sub-queries to cover different aspects of the user's input.
2. **Retrieves Smarter:** Fetches *k*\-relevant documents for each sub-query to ensure comprehensive coverage.
3. **Ranks for Relevance:** Uses a method called **Reciprocal Rank Fusion** to score and reorder documents based on their overall relevance.
4. **Optimizes the Prompt:** Selects the top-ranked documents to construct a prompt that leads to more accurate and contextually rich responses.

We wrote a detailed blog about this and published a Colab notebook that you can use to implement RAG Fusion - **Link in comments!**"
machinelearning,[D] How to convince the stakeholders that our ML solutions is good enough? ,0,1i16ud7,https://www.reddit.com/r/MachineLearning/comments/1i16ud7/d_how_to_convince_the_stakeholders_that_our_ml/,1736863713.0,"Over the past year, we developed a solution designed to be a companion for data analysts, helping them manage and analyze their data. However, I’m struggling to demonstrate its reliability, as it occasionally fails to function properly."
machinelearning,[R] Mastering Machine Learning System Design: A Comprehensive Guide for Scalable AI Solutions,0,1i16f1c,https://www.reddit.com/r/MachineLearning/comments/1i16f1c/r_mastering_machine_learning_system_design_a/,1736862461.0,"**Key Highlights**

1. **What to Expect in ML Interviews**

• Problem-solving, system design, and hands-on ML experience.

• Real-world examples from top tech companies like Google and LinkedIn.

2. **Why ML System Design Matters**

• Addresses **scalability**, **reliability**, and **optimization** for millions of users.

• Explores scenarios like LinkedIn’s Feed Ranking and YouTube’s Recommendation System.

3. **Step-by-Step Guide to ML System Design**

• **Define the Problem Statement**: Clarify goals and assumptions.

• **Identify Metrics**: Choose relevant metrics (e.g., AUC, CTR).

• **Determine Requirements**: Training and inference needs.

• **Design High-Level Systems**: Outline components and data flow.

• **Scale the Design**: Optimize for bottlenecks and high traffic.

4. **Real-World Example**: YouTube Recommendation System

• Candidate Generation Service, Ranking Model, and Recommendation API.



**Key Takeaways**

• **Modular Design**: Ensure components can scale or be replaced independently.

• **Real-Time Inference**: Build low-latency systems (<100ms).

• **Bottleneck Identification**: Proactively address system limitations.

• **Monitoring & Maintenance**: Automate model drift detection and retraining.

[🔗 Machine Learning System Design Introduction](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)🔗 [**Machine Learning System Design Introduction**](https://medium.com/nextgenllm/machine-learning-system-design-introduction-169dc1e6cd71)

This article is a must-read for mastering ML system design and preparing for interviews at top tech firms."
machinelearning,"[D] In ""Speculations on Test-Time Scaling (o1)"", shouldn't this equation be E_(y~p(·|,z_(1:t),x))[Ver(y)]? Adding z_(1:t) into the expectation value equation's subscript. Because it depends on it.",2,1i0nmax,https://www.reddit.com/r/MachineLearning/comments/1i0nmax/d_in_speculations_on_testtime_scaling_o1_shouldnt/,1736799841.0,"In ""Speculations on Test-Time Scaling (o1)"" https://youtu.be/6PEJ96k1kiw?si=-bA2KTKbc0kPJqYX&t=1085 , in the context of https://imgur.com/2t94rWF , shouldn't the equation in https://imgur.com/6AODoeq be E\_(y~p(·|,z\_(1:t),x))[Ver(y)]? Adding z\_(1:t) into the expectation value equation's subscript. Because it depends on it."
machinelearning,[P] I made pkld – a cache for expensive/slow Python functions that persists across runs of your code,129,1hzshvp,https://i.redd.it/n5l1145dqlce1.png,1736704664.0,
machinelearning,[D] Anisotropic periodic kernel in Python with Sklearn,3,1i0et7v,https://www.reddit.com/r/MachineLearning/comments/1i0et7v/d_anisotropic_periodic_kernel_in_python_with/,1736777492.0,"Hello,

I am using sklearn in Python to perform Gaussian Process Regression (GPR) on some ocean variables through the GaussianProcessRegressor class. The domain of the parameters is a 3D spacetime domain (latitude, longitude, and time), so I am using an anisotropic kernel for the regression since the three dimensions are quite different. For example:

\# Define the kernel kernel = C(1.0, (1e-3, 1e3)) \* Matern( nu=1.5, length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=\[(1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)\] )

\# Initialize the GPR

gpr = GaussianProcessRegressor(kernel=kernel, n\_restarts\_optimizer=5, alpha=alpha)

Watching the results at a specific location in time (fixed latitude and longitude, looking at the time series) of the predicted versus the real values, I think adding a periodic kernel in time may improve the results. This assumption makes sense as the parameters could exhibit time periodicity (e.g., wind speed).

I tried implementing this using an ExpSineSquared kernel, but it doesn't allow for anisotropy (I was thinking of setting it with very high bounds for periodicity in latitude and longitude so that it would effectively be neglected). However, the documentation states that the function does not support different length scales and periodicity for different dimensions.

Here is an example of what I tried:

\# Define the Matern kernel

matern\_3d = Matern( length\_scale=\[1.0, 1.0, 1.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), nu=1.5 )

\# Define the ExpSineSquared kernel

expsine\_3d = ExpSineSquared( length\_scale=\[1.0, 1.0, 1.0\], periodicity=\[1e6, 1e6, 24.0\], length\_scale\_bounds=((1e-3, 1e3), (1e-3, 1e3), (1e-3, 1e3)), periodicity\_bounds=((1e5, 1e8), (1e5, 1e8), (12.0, 48.0)) )

\# Combine the kernels

kernel = (C(1.0, (1e-3, 1e3)) \* matern\_3d) + (C(1.0, (1e-3, 1e3)) \* expsine\_3d)

However, this results in an error since ExpSineSquared does not support different length scales and periodicities across dimensions. Has anyone encountered this problem before? Do you know of another function or library that could allow this kind of anisotropic periodic kernel? Thanks in advance!"
machinelearning,[D] Have transformers won in Computer Vision?,179,1hzn0gg,https://www.reddit.com/r/MachineLearning/comments/1hzn0gg/d_have_transformers_won_in_computer_vision/,1736689650.0,"Hi,

Transformers have reigned supreme in Natural Language Processing applications, both written and spoken, since BERT and GPT-1 came out in 2018.

For Computer Vision, last I checked it was starting to gain momentum in 2020 with [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) but the sentiment then was ""Yeah transformers might be good for CV, for now I'll keep using my resnets""

Has this changed in 2025? Are Vision Transformers the preferred backbone for Computer Visions?

Put another way, if you were to start a new project from scratch to do image classification (medical diagnosis, etc), how would you approach it in terms of architecture and training objective?

I'm mainly an NLP guy so pardon my lack of exposure to CV problems in industry.  "
machinelearning,[R] Search-o1: Agentic Search-Enhanced Large Reasoning Models - Renmin University of China,33,1hzyjw1,https://search-o1.github.io/,1736720043.0,
machinelearning,"[R] optimizing looser bounds on train data, achieves better generalization",20,1hzychc,https://www.reddit.com/r/MachineLearning/comments/1hzychc/r_optimizing_looser_bounds_on_train_data_achieves/,1736719487.0,"I have encountered times that when optimizing with looser bounds, one can get better performance on test data. For example, in this paper:

[https://arxiv.org/pdf/2005.07186](https://arxiv.org/pdf/2005.07186)

authors state: ""It seems that, at least for misspecified models such as overparametrized neural networks, training a looser bound on the log-likelihood leads to improved predictive performance. We conjecture that this might simply be a case of ease of optimization allowing the model to explore more distinct modes throughout the training procedure.""

more details can be found below eq 14 in the appendix.

are there other problems where one has drawn a similar observation?

thanks!"
machinelearning,[P] Is it viable to use customer-declared information as proxy ML labels?,0,1i0d26d,https://www.reddit.com/r/MachineLearning/comments/1i0d26d/p_is_it_viable_to_use_customerdeclared/,1736772072.0,"**CONTEXT:**

Sort of a high-level hypothethical ML training data question: Let's say a company has adult customers and child customers. 90% of customers are adults, and 10% of them are children.\*

The problem is that whether a customer is an adult or child is declared by the customer, the company has no way of knowing the truth. Some children pretend to be adults, as it benefits them, but no adults pretend to be children. Thus the company wants to use ML to find the children pretending to be adults, using various other customer details as features.

**QUESTION:**

The question is, is it worth training a model with this proxy label of how they declared themselves, even though the training set will include children pretending to be adults? (Worth noting that we know that only about 1% of those declared as adults are actually children, ie. about 9% of children are pretending to be adults)

Obviously a MUCH better way to do this would be to have a labelled training set of confirmed adults and children, but there's no way of getting a labelled dataset, all we have is whether customers declared themselves as adults or children.

So what do we think? Is it a non-starter? Or might the 99% of true adults effectively drown-out the 1% of false adults, resulting in a viable model? Asuming the features and model type are otherwise apropriate.

Needless to say we're never going to get a great model, but we just need a model that will give us substantially higher than the 9% baseline, since the alternative is doing blind checks on small samples of customers. It feels wrong but I can't think of an alternative given the data at our disposal.

Would appreciate any thoughts, thanks

\*(Please ignore the fact that age is a continuous variable, the actual characteristic we're using is a binary variable)"
machinelearning,[D] Is a ViT with local window attention (SAM-style) not that much more efficient than a vanilla ViT with global attention in all layers? Especially at high resolution where global attention should be super expensive.,22,1hzupbd,https://www.reddit.com/r/MachineLearning/comments/1hzupbd/d_is_a_vit_with_local_window_attention_samstyle/,1736710224.0,"I was reading this blog post by Lucas Beyer: [https://lucasb.eyer.be/articles/vit\_cnn\_speed.html](https://lucasb.eyer.be/articles/vit_cnn_speed.html)

When he compares ViTB/16 and the SAM variant with mostly local attention (window size 14), it was a bit surprised that throughput improvements are slight (left) and that the SAM variant requires more peak memory.

Now this is inference only, so maybe during training the difference is larger, but I naively would have thought that local attention is much faster still, especially at high resolutions.

At 1024x1024, we should have 1024/16=64x64 patches - so the global attention operation should be extremely expensive? Am I missing something?

https://preview.redd.it/es7oj0ky6mce1.png?width=1425&format=png&auto=webp&s=5241198e5bb7129eae3d79e77f3a1dd136d64c2b

"
machinelearning,[D] At which floating point precision gradient descent training or inference breaks down,5,1hzy2ox,https://www.reddit.com/r/MachineLearning/comments/1hzy2ox/d_at_which_floating_point_precision_gradient/,1736718824.0,"We consider NNs as a ""differentiable"" model, i.e. assume that we use continuous differentiable functions. However, we use floating point representations which technically discrete. At some precision, the models start to break down. I.e. consider fp64 model. It might not work as well on fp16 precision, etc.


Could anyone point me to resources (papers) which investigate this, investigate failure modes, ways to work them around, etc.


P.S. This question is inspired by NVidia announcement, where they mentioned that Blackwell supports fp4 precision. I am now interested in how it is possible to do anything useful with such a low precision, and what is used to achieve it."
machinelearning,[P] Built a Snake game with a Diffusion model as the game engine. It runs in near real-time 🤖 It predicts next frame based on user input and current frames.,506,1hz1l2j,https://i.redd.it/bcwlu0jilece1.gif,1736618225.0,
machinelearning,[D] Cheaper alternative to modal.com?,5,1hzq0ac,https://www.reddit.com/r/MachineLearning/comments/1hzq0ac/d_cheaper_alternative_to_modalcom/,1736698271.0,"Are there any other good services that let you instantly spin up a docker image on an 8xH100 machine? Modal is twice the price per hour of lambda labs or voltage park, but I kind of need the quick up/down."
machinelearning,[R] FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers (https://arxiv.org/pdf/2411.14507v1),3,1hzsm1q,https://www.reddit.com/r/MachineLearning/comments/1hzsm1q/r_fusegpt_learnable_layers_fusion_of_generative/,1736704945.0,"Is this paper any good? I am having trouble grokking its essence, for instance what are blocks, group-level, etc. I was looking for a paper that talks about fusing multiple transformer blocks, but this paper doesn't seem to go into the technical implementation details."
machinelearning,[P] Llama3 Inference Engine - CUDA C,37,1hze3vs,https://github.com/abhisheknair10/Llama3.cu,1736653881.0,"Hey r/MachineLearning, recently I took inspiration from llama.cpp, ollama, and similar tools that enable inference of LLMs locally, and I just finished building a Llama inference engine for the 8B model in CUDA C.

As part of my explorative work in building optimized GPGPU software, I decided to build this from scratch. This project only makes use of the native CUDA runtime api and cuda_fp16. The inference takes place in fp16, so it requires around 17-18GB of VRAM (~16GB for model params and some more for intermediary caches). 

It doesn’t use cuBLAS or any similar libraries since I wanted to be exposed to the least amount of abstraction. Hence, it isn’t as optimized as a cuBLAS implementation or other inference engines like the ones that inspired the project.

## **A brief overview of the implementation** 

I used CUDA C. It reads a .safetensor file of the model that you can pull from HuggingFace. The actual kernels are fairly straightforward for normalizations, skip connections, RoPE, and activation functions (SiLU). 

For GEMM, I got as far as implementing tiled matrix multiplication with vectorized retrieval for each thread. The GEMM kernel is also written in such a way that the second matrix is not required to be pre-transposed while still achieving coalesced memory access to HBM.

There are some kernels like the one for RoPE and GEMM that use vectorized memory access. Parts of the SwiGLU feedforward computation takes place within a custom fused kernel.

Feel free to have a look at the project repo and try it out if you’re interested. If you like what you see, feel free to star the repo too!

I highly appreciate any feedback, good or constructive."
machinelearning,[N] I don't get LORA,48,1hz1xks,https://www.reddit.com/r/MachineLearning/comments/1hz1xks/n_i_dont_get_lora/,1736619107.0,"People keep giving me one line statements like decomposition of dW =A B, therefore vram and compute efficient, but I don't get this argument at all.

1. In order to compute dA and dB, don't you first need to compute dW then propagate them to dA and dB? At which point don't you need as much vram as required for computing dW? And more compute than back propagating the entire W?

2. During forward run: do you recompute the entire W with W= W' +A B after every step? Because how else do you compute the loss with the updated parameters?

Please no raging, I don't want to hear
1. This is too simple you should not ask
2. The question is unclear

Please just let me know what aspect is unclear instead.
Thanks"
machinelearning,[P] A hard algorithmic benchmark for future reasoning models,20,1hz4gdy,https://www.reddit.com/r/MachineLearning/comments/1hz4gdy/p_a_hard_algorithmic_benchmark_for_future/,1736625768.0,"Hi, I've been toying with a simple idea for developing a future-proof, dynamic, AI model benchmark. The idea is pretty simple. A hidden function transforms data, and the model only gets to see the before and after, and has to deduce the hidden logic. I've carefully curated several levels of slightly increasing difficulty, and I've been surprised to see most current models I can access (GTP, o1, Sonet, Gemini) suck at it.

For instance, the first puzzle simply does \^=0x55 to the bytes on the input buffers, yet most models struggle to see it or deduce it.

I've spin up a opensource MIT repo with a live demo, so others can give this idea a try or contribute. I appreciate any feedback. Thanks!"
machinelearning,[D] Which library is good for diffusion model research?,7,1hz541n,https://www.reddit.com/r/MachineLearning/comments/1hz541n/d_which_library_is_good_for_diffusion_model/,1736627543.0,"I wanted to play around with diffusion models and switch out different parts of the pipeline (such as samplers, models, data modalities etc or use custom ones). I had a look at some libraries such as modular_diffusion or diffusor, but they don't seem to be very mature yet or very high-level. What kind of libraries do you use to experiment with diffusion models in your research?"
machinelearning,[D] Thoughts on Google Paxml (aka Pax)?,8,1hz2dfp,https://www.reddit.com/r/MachineLearning/comments/1hz2dfp/d_thoughts_on_google_paxml_aka_pax/,1736620254.0,"I just discovered [Pax](https://github.com/google/paxml), a *framework to configure and run machine learning experiments on top of Jax*. Did you know about this? It could be a better solution than Pytorch for large-scale models."
machinelearning,[D] Do I require to Overclock my RTX 4090 for AI Training Tasks?,0,1hzpqgl,https://www.reddit.com/r/MachineLearning/comments/1hzpqgl/d_do_i_require_to_overclock_my_rtx_4090_for_ai/,1736697552.0,"Hello,
I mostly run AI training and experiments on my PC and these experiments sometimes last multiple days non-stop and this machine keeps running 24/7. 
Do you think overclocking is required for my use case to get better performance? I don't want to end up bricking the GPU or end up reducing its lifespan as well. Can OC affect that?
The reason Im asking this is because my GPU is ZOTAC GAMING GeForce RTX 4090 Trinity and it has 3 fans on it. Ive noticed that for all my AI experiments the fans never go above 30% and the GPU temperature is also around 50 - 55°C. Since the GPU can handle higher temperatures and also there is the possibility of the fan going above 30%, I feel like I can possibly get more juice from GPU? What do you recommend, will it be a good idea?"
machinelearning,"[D] Which, in your opinion, is better for cost-saving while maintaining quality?",0,1hznd9q,https://www.reddit.com/r/MachineLearning/comments/1hznd9q/d_which_in_your_opinion_is_better_for_costsaving/,1736690749.0,"I have a scenario where I need to feed PDFs of text data to a Generative AI model in order to summarize and fetch only information of interest from each PDF individually. Now, I was first thinking of using the OpenAI API (GPT-4o), but I was wondering if another solution may be cheaper while also maintaining the level of quality for the text comprehension and generation:

* Install a model locally on my machine to do this.
* Install a model on a cloud server, like an EC2 instance in AWS.
* Use a different GenAI offering, like Amazon Bedrock

I don't have experience with downloading a model and using it, as I've only used APIs of popular providers before. But I want to learn how it works and whether you believe these options are realistic."
machinelearning,[D] Why do we use RLFH instead of Gumbel softmax?,0,1hznbmr,https://www.reddit.com/r/MachineLearning/comments/1hznbmr/d_why_do_we_use_rlfh_instead_of_gumbel_softmax/,1736690615.0,"My question is fairly simple. RLHF is used to fine-tune LLMs because sampled tokens are not differentiable. Why don't we just use Gumbel softmax sampling to achieve differentiable sampling and directly optimize the LLM?

The whole RLHF feel like so much overhead and I do not see why it is necessary."
machinelearning,[D] Discrepancy in no. of slices in multimodal segmentation,0,1hzjlvy,https://www.reddit.com/r/MachineLearning/comments/1hzjlvy/d_discrepancy_in_no_of_slices_in_multimodal/,1736676187.0,"Hey I’m using DTI and conventional MRI scans for my segmentation task. DTI has 60 slices, MRI has 23 slices, the segmentation mask was produced based on MRI so it has 23 slices. Any advice how do I go about doing so? There’s a discrepancy in no. of slices"
machinelearning,[D] Finding optimal hyper parameter for neural network,4,1hz2ct5,https://www.reddit.com/r/MachineLearning/comments/1hz2ct5/d_finding_optimal_hyper_parameter_for_neural/,1736620209.0,"I have been trying to find optimal hyperparameter for LSTM model using gray wolf algorithm(GWO) and particle swarm optimizer(PSO). Its taking alot of time. Below is description for what I am doing.

I have a LSTM model wrapped in a objective function to be optimized. This function build model based on parameter passed to it, then it trains the model and find MSE on test data. This test data is returned based on which GWO optimizer will calculate fitness.

This process takes hours. Is there any other way to find optimum parameter?"
machinelearning,[Discussion] Unclear problem statement,0,1hzhrxb,https://www.reddit.com/r/MachineLearning/comments/1hzhrxb/discussion_unclear_problem_statement/,1736668030.0,"The following is a problem statement for a use case.  
  
""The nature of fraud is dynamic and ever-changing. Finding patterns and identifying anomalies are essential in this industry. Given a set of mobile device attributes (for example, brand, model) data, design a model to find patterns or anomalies in these data. 

Take into account that not all device attributes are readily available all the time and there is no historical data available.""

There is no dataset provided, I'll have to find it myself. I was thinking of obtaining the Kaggle mobile price dataset and do some basic anomaly checks (Z-score, IQR) + isolation forest to detect fraudulent postings. However, not sure what no historical data means? I interpreted it as having no time series information + unlabelled (to be safe). "
machinelearning,[R] Which Forecasting library should I be using for this task since all I've tried don't do what I need!,0,1hzbtak,https://www.reddit.com/r/MachineLearning/comments/1hzbtak/r_which_forecasting_library_should_i_be_using_for/,1736646378.0,"Hi all,

I'm trying to forecast a single column in my dataset by using multivariate inputs: Fuel % left in car depending on current fuel %, speed, radiator temperature. I need to train a model that can approximate the fuel consumption curve in real-time, therefore it has to predict on unseen data based on what it learnt, however the libraries I've tried don't do that, instead they just train on the previous data and predict the exact next n (fh). I don't need that, I don't want the next n steps of my training data, I want the next n steps of my testing data which is unseen. I built my own pytorch model and it works well, but I need to compare it against other methods to see how to improve the model.

I tried Facebook Prohpet, Nixtla, SKTime, Pytorch Forecasting, GluonTS, but they don't seem to do what I want and/or lack one of the requirements. I've read about TSAI, Darts, Kats, but I'm afraid that I'm wasting time that I might not have testing too many libraries only to find out that they don't do what I need.

Any recommendation that I can look into that can do what I need?

tl;dr

I need a library/model that can take multivariate input to predict a univariate output for the next n steps in real time (unseen data)."
machinelearning,"[Dataset][R] 19,762 Garbage Images for Building AI Recycling Solutions",108,1hyfaoc,https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/,1736543993.0,"Hi ML community!

I’m excited to share the **Garbage Classification V2 Dataset**, featuring **19,762 high-quality images** of garbage categorized into **10 distinct classes** (e.g., metal, plastic, clothes, and paper).

# Why this matters:

* Train AI models for **automated waste sorting and recycling**.
* Develop **waste segregation apps** or sustainability-focused tools.
* Create innovative **computer vision projects** for environmental impact.

🔗 **Dataset Link:** [Garbage Classification V2](https://www.kaggle.com/datasets/sumn2u/garbage-classification-v2/)

This dataset has been used in the research paper, *""Managing Household Waste Through Transfer Learning,""* proving its utility in real-world applications.

Looking forward to seeing how you can use it to promote sustainability!"
machinelearning,[D] Image segmentation with SAM,1,1hz7n71,https://www.reddit.com/r/MachineLearning/comments/1hz7n71/d_image_segmentation_with_sam/,1736634363.0,"Is there somewhere I can segment an image with SAM exactly the same way they do in their website by simply clicking on different parts of the image to add to the mask (or shift click to remove) and download the mask in the end?

I've tested a few labeling tools but I found none of them worked as well as the meta demo. The problem with the meta website is that I can't download the mask, I can just get a cut out of the image."
machinelearning,[D] Does softmax tend to result in unconstrained euclidean weight norms?,5,1hyxijp,https://www.reddit.com/r/MachineLearning/comments/1hyxijp/d_does_softmax_tend_to_result_in_unconstrained/,1736607201.0,"Bit of a silly question. While I was in the middle of analyzing neural network dynamics geometrically, I realized something about softmax. When using categorical cross entropy, it results in a lower loss value for pre-softmax vectors within the output layer that have a high positive magnitudes for the correct label-axis, and high negative magnitudes for the non-correct label-axes. I know that regularization techniques keeps weight updates bounded to a degree, but I can't help thinking that softmax + cross entropy is not really a good objective for classifiers, even if the argument that it results in a probability distribution as the output so it's ""more interpretable"".

  
Just me?"
machinelearning,[D] Where can I find Machine Learning Engineer/AI Engineer interview Experiences?,7,1hyskcn,https://www.reddit.com/r/MachineLearning/comments/1hyskcn/d_where_can_i_find_machine_learning_engineerai/,1736588560.0,"I need to go through some interview experiences of candidates other than glassdoor.
I want resources that tell me like there were so many rounds and what happened in each round.
Let me know if you have such resources."
machinelearning,How is the job market for machine learning and Al in Australia? [D],14,1hypeqa,https://www.reddit.com/r/MachineLearning/comments/1hypeqa/how_is_the_job_market_for_machine_learning_and_al/,1736574772.0,"
Hi all. I am a Researcher based in Australia and if possible I would like to hear your opinion regarding ML market.
I've found a post from 2yo ago, and want to have an updated point of view.
Thank you all in advance. "
bigdata,Just announced: Tableau Conference #TC25 Registration is Open! Who is going? ,1,1i1b9pe,https://www.linkedin.com/posts/tableau-software_tc25-datafam-activity-7284917234756435969-3nu-,1736875452.0,
bigdata,Hey friends! I just stumbled upon this awesome tool that gathers info on VC funded startups and helps you find contacts of key decision-makers. It’s a game changer for anyone looking to pitch services! Let me know if you're curious to give it a whirl!,0,1i0v37l,https://v.redd.it/5cddx66s8vce1,1736819780.0,
bigdata,Federated Modeling: When and Why to Adopt,3,1hxacjv,https://moderndata101.substack.com/p/federated-modeling-when-and-why,1736420337.0,
bigdata,I learned how big data fuels AI on platforms like Instagram and Pinterest,1,1hxdk90,https://www.reddit.com/r/bigdata/comments/1hxdk90/i_learned_how_big_data_fuels_ai_on_platforms_like/,1736431711.0,"I wrote an article about how **AI influences social media**, deciding what we see in our feeds, ads, and content. Key points:

* **Facebook and Instagram** use Meta AI to figure out what shows up in your feed based on what you like, comment on, or share.
* **TikTok’s Monolith AI** studies what you watch and interact with to fine-tune your For You Page.
* **LinkedIn** suggests jobs, articles, and connections that match your career goals.
* **YouTube** recommends videos and even picks when ads pop up during what you watch.
* **Pinterest’s PinSage AI** suggests pins and products based on your searches and saves.

It’s remarkable how much AI controls our online experience, but sometimes it can feel a little too spot-on.

If you want to tweak what you see:

* Check your privacy settings regularly to see what data is being used.
* Use tools like “Not Interested” to refine your feed.
* Be mindful of what you interact with—it directly affects future recommendations.

If you’re curious about how it all works, here is the full article: [https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/](https://aigptjournal.com/explore-ai/ai-guides/ai-in-social-media-platforms/)

Have you noticed how accurate your feeds are lately? Do you find it helpful, or is it over the top?"
bigdata,"Optimizing Retrieval Speeds for Fast, Real-Time Complex Queries",5,1hvjyig,https://www.reddit.com/r/bigdata/comments/1hvjyig/optimizing_retrieval_speeds_for_fast_realtime/,1736226116.0,"Dear big data geniuses:

  
I'm using snowflake to do complex muliti-hundred line queries with many joins and window functions. These queries can take up to 20 seconds. I need them to take <1 second. The queries are fully optimized on snowflake and cant be optimized further. What do you recommend?"
bigdata,How to create HIVE Table with multi character delimiter? (Hands On) ,2,1hurwxn,https://youtu.be/jgM3ds4_n4o,1736141542.0,
bigdata,"50+ Incredible Big Data Statistics for 2025: Facts, Market Size & Industry Growth",3,1hthw2p,https://bigdataanalyticsnews.com/big-data-statistics/,1736007688.0,
bigdata,25 Best Project Management software in 2025,0,1htc6wj,https://bigdataanalyticsnews.com/best-project-management-tools/,1735988681.0,
bigdata,About go get into Big Data ,8,1hsv9pr,https://i.redd.it/9k202a0j2uae1.jpeg,1735933831.0,"About to get into Big Data 

Hey there 

I’m 29 with background experience in farming, biology and nature with some skills related to tech and computers, looking forward to learn more about #BigData as I want to develop another career. 

What are your recommendations, tips, advices, etc.? 


p.s. Also my first time posting in Reddit, greetings from México🌮🌶️🇲🇽"
bigdata,"Hey folks! If you're in VC or a business analyst, you’ve got to check out this tool. It streams live data of VC-funded startups globally and gives you quick access to tons of company history (there's even a CSV or API option). Let me know if you want to give it a shot!",1,1hsyr6i,https://v.redd.it/5itajn3tsuae1,1735942713.0,
bigdata,[Poll] Has anyone used dbt's AI (dbt copilot) yet? What has your experience been?,2,1hs7pyr,/r/DataBuildTool/comments/1hs7pdf/has_anyone_used_dbts_ai_dbt_copilot_yet_what_has/,1735860170.0,
bigdata,guidance for finish and review my first mini-project,3,1hqgfr8,https://www.reddit.com/r/bigdata/comments/1hqgfr8/guidance_for_finish_and_review_my_first/,1735657345.0,"Hello guys , could anyone help me with reviewing and guide me thoughout my mini-project for big data ? ,this involves designing a (textual) information search engine and analyzing user reviews of your search engine.

here is the link : [https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0](https://www.kaggle.com/code/cherryblade29/notebook1e9ba773b0)"
bigdata,How automation and AI advanced data-driven reporting in 2024 [LinkedIn Post] ,2,1hq0ym3,https://www.linkedin.com/posts/rollstack_automating-data-driven-content-with-ai-activity-7279551358913953792-ax8p,1735601913.0,
bigdata,"Hey friends, if you're looking for a simple way to make some sales, you should consider selling to new startups that just landed venture capital! I found this awesome app that tracks real-time funding announcements, gathers verified emails of decision-makers, and even summarizes their buying hints w",0,1hpq5vk,https://v.redd.it/3c9m16qcc0ae1,1735573971.0,
bigdata,Hadoop vs. Spark: Which One Should Beginners Learn First?,6,1houh90,/r/BigDataEnginee/comments/1houfut/hadoop_vs_spark_which_one_should_beginners_learn/,1735472946.0,
bigdata,Welcome to r/BigDataEngineer: Let’s Build and Grow Together!,0,1hotwap,/r/BigDataEnginee/comments/1hotvvd/welcome_to_rbigdataengineer_lets_build_and_grow/,1735470480.0,
bigdata, Big data Hadoop and Spark Analytics Projects (End to End) ,24,1hkfzpa,https://www.reddit.com/r/bigdata/comments/1hkfzpa/big_data_hadoop_and_spark_analytics_projects_end/,1734927955.0,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1. [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)
2. [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)
3. [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)
4. [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)
5. [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)
6. [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)
7. [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)
8. [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1. [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)
2. [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)
3. [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)
4. [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)
5. [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials."
bigdata,"Don't make the CFO wait. Use Rollstack to automate recurring reports (QBRs, Annual Reports, MBRs, etc.,) ",0,1hkukvg,https://i.redd.it/w0tmr9h1an8e1.png,1734980002.0,
bigdata,Searching For Hive Alternatives,1,1hkkq2k,https://www.reddit.com/r/bigdata/comments/1hkkq2k/searching_for_hive_alternatives/,1734948441.0,"My current setup is Hive on Tez, running on YARN with data stored in HDFS.  
I feel like this setup is a bit outdated, and that the performance is not great. However I can't find alternatives.  
Every technology I found so far fails in one of the requirements that I'll mention.

I have the following requirements:

1. Be able to handle huge analytical batch jobs, with multiple heavy joins
2. Scalable (Petabytes)
3. Fault-tolerant, jobs must finish
4. On-premise

Would like to hear your suggestions!"
bigdata,Will Data Science be a big deal in 2025?,0,1hj6p76,https://www.reddit.com/r/bigdata/comments/1hj6p76/will_data_science_be_a_big_deal_in_2025/,1734774582.0,"# 1. Getting to know Data Science

# Explaining Data Science

Think of data science as a high-tech detective blending stats, math, and code skills to sniff out cool clues and crack tough puzzles in humongous data piles.

# Why Data Science Rocks Today

Nowadays, with all our lives so wrapped up in data, data science is pretty much a magic element. It's what makes your Netflix picks so spot on, forecasts trends, and helps companies make super-smart choices.

# 2. What's Hot in Data Science

# All About Big Data Analytics

Imagine big data as an all-you-can-eat info spread. Data scientists are like skilled foodies who know how to fill their plates picking out the tasty bits of knowledge that can spice up business plans and spark new ideas.

# Machine Learning and AI Uses

Self-driving automobiles and digital helpers are causing a revolution in our tech interactions, and data scientists are the wizards working magic to make it happen.

# Ways to Present Data

Data visualization turns snooze-fest tables into enthralling masterpieces. It allows a quick grasp of intricate data and shares knowledge with others super .

# 3. What Makes Data Science So In-Demand

# The Rise of Making Choices Based on Data

Since data's become the hot commodity, companies are super eager for data pros. They need these smart folks to transform basic digits into powerful wisdom to guide top-level choices and help their biz expand.

# AI and Automation Demand More Data Pros

The demand for data scientists to create and improve algorithms for AI and automation is soaring. These skills are becoming red-hot in the employment sphere.

# Meeting the Bar for Regulatory Stuff

In our super connected era where keeping data safe is huge, companies want data scientists to help them wade through the complex rules to make sure they play fair and keep data use on the up-and-up.

# 4. The Tough and Good Stuff in Data Science

# Keeping Data Safe and Sound

With data mishaps popping up in the news, data scientists have the tough job. They've got to dig out the good stuff from the data while making sure none of the secret info gets into the wrong hands. They're juggling keeping things fresh and new with making sure everything stays locked down tight.

# Lack of Data Science Experts

As more people want data experts than there are available, this creates a tough spot but also a huge chance for folks aiming to jump into this area offering great jobs and fat paychecks.

# Data Science Rocks Various Sectors

Whether it's in health or money stuff, data science is causing a stir across different work areas. It's leading cool things like making meds just for you spotting cons, and figuring out groups of buyers, proving just how much it can do and how cool it can be.

# 5. What Data Science Might Look Like in 2025

# What to Expect in the Data Science Work Scene

Heading into 2025, folks can expect the data science job scene to keep on climbing. With companies in all sorts of businesses getting how critical data-informed decisions are, there's gonna be a huge ask for data science whizzes. Anyone in data science is looking at some pretty sweet career moves and loads of chances to snag a job.

# Tech Upgrades Making Waves in What's Next

Tech upgrades are huge in deciding [what's next for data science](https://www.usdsi.org/data-science-insights/future-of-data-science-10-predictions-you-should-know). All the cool stuff like artificial intelligence learning machines, and big-time data studies will push forward new stuff for data scientists to do in 2025. Jumping on the tech bandwagon is super important to not fall behind in data science's fast-paced world.

# 6. Tech Stuff Changing the Data Scene

# Blending Blockchain with Crunching Numbers

Blockchain is about to make a big splash in the number-crunching game. It's gonna ramp up security and make sure everything is clear and trackable when it comes to moving digits around. Merging this tech with the brainy science of data could start a whole new game for keeping our online facts straight and real when everything is linked up.

# Making Sense of Internet of Things (IoT) Stats

Okay so all these Internet of Things gadgets are spitting out crazy amounts of info that's got some real golden nuggets hidden in there. By 2025, the brainiacs working with numbers will gotta dig in with some fancy figuring-out tricks to pull out the gems from this data gush. Getting a grip on this IoT number crunching is key for groups looking to smarten up their choices and spark some fresh ideas.

# 7. What You Gotta Have to Be a Data Scientist in 2025

# Know Your Coding and Gadget Game

Data scientists waiting for 2025 got to know their stuff with a bunch of coding languages and gadgets. You gotta be tight with Python, R, SQL, and TensorFlow. Being a wizard with these allows you to mess with big complex data, cook up some solid predictive stuff, and pull out the kind of know-how that makes businesses rock and roll.

# "
bigdata,"Build Real-Time Systems with NATS and Pathway, Scalable Alternatives to Apache Kafka and Flink",10,1hhsukf,https://www.reddit.com/r/bigdata/comments/1hhsukf/build_realtime_systems_with_nats_and_pathway/,1734614782.0,"Hey everyone! I wanted to share a tutorial created by a member of the Pathway community that explores using [NATS](https://docs.nats.io/) and [Pathway](https://pathway.com/) as an alternative to a Kafka + Flink setup.

The tutorial includes step-by-step instructions, sample code, and a real-world fleet monitoring example to show how you can simplify data pipelines while still handling large volumes of streaming data. It walks through setting up basic publishers and subscribers in Python with NATS, then integrates Pathway for real-time stream processing and alerting on anomalies.  
  
**App template link (with code and details):**  
[https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink](https://pathway.com/blog/build-real-time-systems-nats-pathway-alternative-kafka-flink) 

**Key Takeaways:**

* **Seamless Integration:** Pathway’s native NATS connectors allow direct ingestion from NATS subjects, reducing integration overhead.
* **High Performance & Low Latency:** NATS delivers messages quickly, while Pathway processes and analyzes data in real time, enabling near-instant alerts.
* **Scalability & Reliability:** With NATS clustering and Pathway’s distributed workloads, scaling is straightforward. Message acknowledgment and state recovery help maintain reliability.
* **Flexible Data Formats:** Pathway handles JSON, plaintext, and raw bytes, so you can choose the data format that suits your needs.
* **Lightweight & Efficient:** NATS’s simple pub/sub model is well-suited for asynchronous, cloud-native systems—without the added complexity of a Kafka cluster.
* **Advanced Analytics:** Pathway supports real-time machine learning, dynamic graph processing, and complex transformations, enabling a wide range of analytical use cases.

Would love to know what you think—any feedback or suggestions."
bigdata,MASTER DATA SCIENCE ACCELERATE YOUR FUTURE,2,1hhq92i,https://www.reddit.com/r/bigdata/comments/1hhq92i/master_data_science_accelerate_your_future/,1734605056.0,"https://preview.redd.it/iebxtn3dbs7e1.jpg?width=800&format=pjpg&auto=webp&s=36f9d0083def416dadc2e309f6e0544dab469776

    Organizations need data-driven leaders. With the USDSI® Certification, master data science skills that unlock insights, fuel decisions, and accelerate business growth. Become the data expert companies trust.
    "
bigdata,I built an end-to-end data pipeline tool in Go called Bruin ,6,1hhc8b5,https://www.reddit.com/r/bigdata/comments/1hhc8b5/i_built_an_endtoend_data_pipeline_tool_in_go/,1734557142.0,"Hi all, I have been pretty frustrated with how I had to bring together bunch of different tools together, so I built a CLI tool that brings together data ingestion, data transformation using SQL and Python and data quality in a single tool called Bruin:

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)

Bruin is written in Golang, and has quite a few features that makes it a daily driver:

* it can ingest data from many different sources using [ingestr](https://github.com/bruin-data/ingestr)
* it can run SQL & Python transformations with built-in materialization & Jinja templating
* it runs Python fully locally using the amazing [uv](https://github.com/astral-sh/uv), setting up isolated environments locally, mix and match Python versions even within the same pipeline
* it can run data quality checks against the data assets
* it has an open-source [VS Code extension](https://bruin-data.github.io/bruin/vscode-extension/overview.html) that can do things like syntax highlighting, lineage, and more.

We had a small pool of beta testers for quite some time and I am really excited to launch Bruin CLI to the rest of the world and get feedback from you all. I know it is not often to build data tooling in Go but I believe we found ourselves in a nice spot in terms of features, speed, and stability.

Looking forward to hearing your feedback!

[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)"
bigdata,The Art of Discoverability and Reverse Engineering User Happiness,2,1hg9u75,https://moderndata101.substack.com/p/the-art-of-discoverability-and-reverse,1734439533.0,
bigdata,String to number in case of having millions of unique values,1,1hgdqtn,https://www.reddit.com/r/bigdata/comments/1hgdqtn/string_to_number_in_case_of_having_millions_of/,1734451126.0,"Hello,  
I am currently working on preprocessing big data dataset for ML purposes. I am struggling with encoding strings as numbers. I have a dataset of multiple blockchain transactions and I have addresses of sender and receivers for these transactions. I use pyspark.

I've tried String Indexer but it throws out of memory errors due to number of unique values. How should I approach it? Is hasing with SHA256 and casting to big int good approach? Wouldn't big numbers influence ML methods too much? (i will try different methods ex. random forests, gan, some based on distance etc)"
bigdata,Data Science Projects for Beginners | Infographic,1,1hg8twp,https://www.reddit.com/r/bigdata/comments/1hg8twp/data_science_projects_for_beginners_infographic/,1734435744.0,"One way to excel above your competitors in the race for top data science jobs is by showcasing your practical experience and a strong portfolio to demonstrate your data science skills and knowledge practically. Check out our detailed infographic to learn about popular [data science projects](https://www.usdsi.org/data-science-insights/role-of-major-components-in-data-science-projects) for beginners that you can work on to apply your theoretical data science knowledge practically and build a strong portfolio. 

https://preview.redd.it/qn8ilmoxbe7e1.jpg?width=1080&format=pjpg&auto=webp&s=ff704d8e8a7fe6909dae083229e1c34771b95507

"
bigdata,Step-by-Step Tutorial: Setting Up Apache Spark with Docker (Beginner Friendly),1,1hfup8a,https://www.reddit.com/r/bigdata/comments/1hfup8a/stepbystep_tutorial_setting_up_apache_spark_with/,1734386073.0,"Hi everyone! I recently published a video tutorial on setting up Apache Spark using Docker. If you're new to Big Data or Data Engineering, this video will guide you through creating a local Spark environment.

📺 Watch it here: [https://www.youtube.com/watch?v=xnEXAD9kBeo](https://www.youtube.com/watch?v=xnEXAD9kBeo)

Feedback is welcome! Let me know if this helped or if you’d like me to cover more topics."
bigdata,Free Ungated Whitepaper: Personalized healthcare reporting with data and AI,2,1hfn1h0,https://www.rollstack.com/case-studies/personalized-healthcare-data-reporting-for-client-success,1734366719.0,
bigdata,Data-Driven Recruitment The WorkWolf Revolution,0,1hdzmhe,https://www.reddit.com/r/bigdata/comments/1hdzmhe/datadriven_recruitment_the_workwolf_revolution/,1734169453.0,"Discover how WorkWolf is transforming the recruitment game by reducing bias and enhancing efficiency with data-driven solutions. As the future of work becomes more data-centric, HR professionals must adapt to ensure ethical and fair hiring practices. [WorkWolf Revolution](https://www.usdsi.org/data-science-insights/data-driven-recruitment-using-workwolf-to-reduce-bias-and-increase-efficiency)

https://preview.redd.it/yvoj2h33cs6e1.jpg?width=1080&format=pjpg&auto=webp&s=58d3f24e9d25fbc390dab129366db0ebb290f53b

"
bigdata,30 Best IDE Software for Developers in 2025,0,1hdckux,https://bigdataanalyticsnews.com/top-ides-for-programmers/,1734097729.0,
bigdata,"DATA VISUALIZATION IN R: CHEATSHEET AHEAD OF 2025 | INFOGRAPHIC

",0,1hdbw7e,https://www.reddit.com/r/bigdata/comments/1hdbw7e/data_visualization_in_r_cheatsheet_ahead_of_2025/,1734095544.0,"Understanding data science has never been this convenient as it amalgamates with the R programming language. [Data science in R](https://www.usdsi.org/data-science-insights/data-visualization-in-r-cheatsheet-ahead-of-2025) is turning tables for deeper data-driven business insights to guide a better business landscape ahead. 

https://preview.redd.it/kehq19q88m6e1.jpg?width=1200&format=pjpg&auto=webp&s=545aa2e49a1d1c962d8e8067575aaffe1ed33ab6

"
bigdata,Data Science Roadmap 2025,4,1hch7ch,https://www.reddit.com/r/bigdata/comments/1hch7ch/data_science_roadmap_2025/,1733994686.0,"Explore the evolutionary journey of data science as it intertwines human intelligence with cutting-edge technology. This roadmap delves into essential skills, tools, and adaptations required to thrive in the ever-changing analytics landscape of 2025. [Data Science Roadmap 2025](https://www.usdsi.org/data-science-insights/data-science-roadmap-2025-a-darwinian-evolution-of-analytics)

https://preview.redd.it/06sw992fwd6e1.jpg?width=1920&format=pjpg&auto=webp&s=d82a7a1fd59ac6a5022323e7b4d717b0feacfa3a

"
bigdata,How Do You Do Data?,0,1hcai3m,https://www.reddit.com/r/bigdata/comments/1hcai3m/how_do_you_do_data/,1733969122.0,"Just curious about the types of infrastructure you folks use. Specifically, what kind of chips are you using to train/fine-tune/run your deep models?

I appreciate you filling out this  survey.

[https://forms.gle/uiAmfG9K7MpFvQtK7](https://forms.gle/uiAmfG9K7MpFvQtK7)"
bigdata,For those like me who like to have music on the background while working ,0,1hc5gm9,https://www.reddit.com/r/bigdata/comments/1hc5gm9/for_those_like_me_who_like_to_have_music_on_the/,1733954734.0,"I often need background music to help me increase my productivity while working. I created these playlists which I update regularly They help me stay calm, focused and productive. Perfect academia playlists! 



Ambient, chill & downtempo trip (a tasty mix of ambient, downtempo, IDM, trip-hop, electronica, jazz house music and more. Chill, hypnotic, trippy and atmospheric grooves for focus, relaxation, and deep listening) [https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg](https://open.spotify.com/playlist/7G5552u4lNldCrprVHzkMm?si=6fiOfJmeRi2CrnhNwHzyzg) 



Mental food (A bit of the same atmosphere as the previous one) [https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow](https://open.spotify.com/playlist/52bUff1hDnsN5UJpXyGLSC?si=37JEertEQkG9aba7xETmow) 



Something else (atmospheric, poetic, calm, soothing, cinematic and ambient soundscapes with a touch of mystery. Relaxing instrumental music for focus, relaxation, introspection, reading, writing, studying, meditation and mindfulness practice.) [https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS\_JvzkUYUw](https://open.spotify.com/playlist/0QMZwwUa1IMnMTV4Og0xAv?si=XEQqfz8OQaSDS_JvzkUYUw) 



Pure ambient (calming ambient music designed to enhance focus, relaxation, study, meditation, sleep, and mindfulness) [https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ](https://open.spotify.com/playlist/6NXv1wqHlUUV8qChdDNTuR?si=RE0d-iHuQd-5hGtboUq4OQ) 



Chill lofi day (mix of smooth lofi hip-hop beats, chillhop, jazzhop and soothing vibes. Chill background music for studying, working, reading or just unwinding) [https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ\_vPqdYQc-idTOg-kt5Vg](https://open.spotify.com/playlist/10MPEQeDufIYny6OML98QT?si=NZ_vPqdYQc-idTOg-kt5Vg) 


French Producers (dedicated to new independent French producers.  Several electronic genres covered but mostly chill) [https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ](https://open.spotify.com/playlist/5do4OeQjXogwVejCEcsvSj?si=4WN5523VRA6uaAvN5RDGLQ) 



Jrapzz (the latest in modern jazz with a mix of Nu-Jazz, Jazzhop, Acid Jazz, Jazz UK, Ambient Jazz, Jazztronica, Jazz House, Nu-Soul, Hip-Hop Jazz, rather chill) [https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA](https://open.spotify.com/playlist/3gBwgPNiEUHacWPS4BD2w8?si=pZ1LxONJSYqQRR483Q55tA) 



Cool stuff (chill indie pop & rock fresh finds, from emerging independent artists and few recognized talents) [https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA](https://open.spotify.com/playlist/2mgbWuWrYSVPrPNHbQMQec?si=FVMlFI5gTiWPkaJUWPUJtA) 



Enjoy! 


\- 


H-Music"
bigdata,Governance for AI Agents with Data Developer Platforms,2,1hbsznu,https://moderndata101.substack.com/p/governance-for-ai-agents-with-ddp,1733922232.0,
bigdata,Data Science Command the Future of Businesses in 2025?,2,1hbocvq,https://www.reddit.com/r/bigdata/comments/1hbocvq/data_science_command_the_future_of_businesses_in/,1733902639.0,"Data science has been transforming businesses for a long time now. But are these technologies capable of changing the future of the world? Download our comprehensive resource to understand the impact of data science on the world's future. To [download](https://www.usdsi.org/data-science-insights/resources/can-data-science-command-the-future-of-businesses-in-2025), click below.

https://preview.redd.it/x9f6awrqa66e1.jpg?width=1050&format=pjpg&auto=webp&s=0bf5a1720f7671c964936a13a2921a876a025fe1

"
bigdata,"Hey, I collected IMO the best product analytics tools for 2025",4,1hb0upu,https://www.reddit.com/r/bigdata/comments/1hb0upu/hey_i_collected_imo_the_best_product_analytics/,1733834425.0,"Helloo, I made a blogpost about the possible best product analytics tools (warehouse native and traditionals). Feel free to add any experience or comment. Thank youu

https://medium.com/@pambrus7/6-product-analytics-tool-for-2025-ab9766510551"
bigdata,2025 Guide to Architecting an Iceberg Lakehouse,2,1hb2w3b,https://medium.com/data-engineering-with-dremio/2025-guide-to-architecting-an-iceberg-lakehouse-9b19ed42c9de,1733840675.0,
bigdata,Has anyone tried this analytics automation tool yet? (Rollstack) What did you think? ,4,1haepvt,https://www.linkedin.com/posts/nathanbc_tableau-datafam-businessintelligence-activity-7271932754571718656-oOaf?utm_source=share&utm_medium=member_desktop,1733764394.0,
bigdata,Any good sources of Social Media/Search Engine Keyword Usage by Day?,2,1ha3ecw,https://www.reddit.com/r/bigdata/comments/1ha3ecw/any_good_sources_of_social_mediasearch_engine/,1733724850.0,"Hey there,

After exhaustively searching Google and trying to find APIs that would allow me to generate keyword search or post or comment frequency on any platform on a *daily* basis, I have been unable to find any providers of this type of data. Considering that this is kind of a niche request, I am dropping this inquiry here for the Data Science Gods of Reddit to assist.

Basically, I'm trying to create an ML model that can predict future increases/decreases in keyword usage (whether that be on Google Search or X posts; dosen't matter) on a daily basis. I've found plenty of monthly average keyword search providers but I cannot find any way to access more granulated, daily search totals for any platform. If you know of any sources for this kind of data, please drop them here... Or just tell me to give up if this is an impossible feat."
bigdata,Certified Lead Data Scientist  2025,0,1ha3bc9,https://www.reddit.com/r/bigdata/comments/1ha3bc9/certified_lead_data_scientist_2025/,1733724516.0,"Enhance your data science skills and knowledge to drive innovation, build efficient data science models, and manage data science projects effectively with the best data science certification from USDSI® for [CERTIFIED LEAD DATA SCIENTIST  - CLDS™](https://www.usdsi.org/data-science-certifications/certified-lead-data-scientist).

https://preview.redd.it/whm05s4pkr5e1.jpg?width=1080&format=pjpg&auto=webp&s=65f6ec8e5cbf2dba071249f7ca88e7b07da65971

"
bigdata,🚀 Quant Interview Prep - New Videos Added! 🚀,2,1h9lu8v,https://www.reddit.com/r/bigdata/comments/1h9lu8v/quant_interview_prep_new_videos_added/,1733673485.0,"To all aspiring Quants out there, I’ve restarted my journey of creating content around quantitative interview questions and brain teasers! These videos will help you get familiar with the types of questions typically asked in interviews for roles like quantitative analyst, data scientist, and more.📹

Check out my latest video here: [https://www.youtube.com/@prakarshduhoon1116](https://www.youtube.com/@prakarshduhoon1116)

Here is my LI: [https://www.linkedin.com/in/prakarshd/](https://www.linkedin.com/in/prakarshd/); I am ex Quant with 7 years of exp, working at top funds like Millennium and WorldQuant

If you find the content useful, feel free to like, share, and spread the word with your network. Together, we can make interview prep easier and more effective! Let's crush those interviews! 💪

[\#Quant](https://www.linkedin.com/feed/hashtag/?keywords=quant&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantInterviews](https://www.linkedin.com/feed/hashtag/?keywords=quantinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#InterviewPrep](https://www.linkedin.com/feed/hashtag/?keywords=interviewprep&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#TechInterviews](https://www.linkedin.com/feed/hashtag/?keywords=techinterviews&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#Finance](https://www.linkedin.com/feed/hashtag/?keywords=finance&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#BrainTeasers](https://www.linkedin.com/feed/hashtag/?keywords=brainteasers&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#QuantitativeAnalysis](https://www.linkedin.com/feed/hashtag/?keywords=quantitativeanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296) [\#CareerGrowth](https://www.linkedin.com/feed/hashtag/?keywords=careergrowth&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7268319352582455296)"
bigdata,Big data with GenAI coursework or prep path recommendations ,1,1h94wbm,https://www.reddit.com/r/bigdata/comments/1h94wbm/big_data_with_genai_coursework_or_prep_path/,1733613139.0,"Hey guys. I’m a data engineer and I am interested in learning GenAI. Just wondering how big data will work along GenAI and the way to learn the topics in GenAI 

Any recommendations which websites should I look at first or a project idea for Bigdata with GenAI "
bigdata,Certified Data Science Professional  2025,0,1h8tc2f,https://www.reddit.com/r/bigdata/comments/1h8tc2f/certified_data_science_professional_2025/,1733580855.0,"Certified data science professionals are in huge demand because of the rapid adoption of data science technologies. So, kickstart your data science journey by mastering the fundamentals and building a strong foundation with the best #beginner-level CERTIFIED DATA SCIENCE PROFESSIONAL CDSP™.

https://preview.redd.it/p9v7vz6qpf5e1.jpg?width=1080&format=pjpg&auto=webp&s=72198191ab9bb6f1068cfa14c652e3767ca58d80

"
bigdata,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),4,1h6x0jf,https://www.reddit.com/r/bigdata/comments/1h6x0jf/i_built_an_aipowered_website_builder_that_creates/,1733361485.0,"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  


Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website,](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]"
bigdata,I built an AI-powered website builder that creates custom websites in seconds (frustrated with WordPress/Squarespace templates),2,1h6x4jv,https://www.reddit.com/r/bigdata/comments/1h6x4jv/i_built_an_aipowered_website_builder_that_creates/,1733361797.0,"Hey folks! I'd like to show you the AI-powered website builder I developed, which I believe is super easy compared to others. Highly recommended for people who don't code and want a quick, neat website.  
About our website builder, Arco:  
\- You just need to tell it what kind of website you want or share your content - it creates a custom website for you in seconds  
\- If not satisfied, simply tell AI what to change (e.g., ""add a contact section"") - it will automatically adjust the design.  
\- No more struggling with rigid templates like WordPress/Squarespace where simple customizations become complicated  
  
Why I built this: I was frustrated with traditional website builders. For example, when I wanted to add text descriptions to images in a WordPress template, I found myself struggling with placement, sizing, and design complexities. That's when I realized AI could help create excellent initial designs that are fully customizable.

[Checkout Acor; Website](https://www.arcoer.com/create) FREE to use \[change to trackable url in the first sheet\]"
bigdata,Future of Data Science Technologies and Trends,1,1h6bm3x,https://www.reddit.com/r/bigdata/comments/1h6bm3x/future_of_data_science_technologies_and_trends/,1733301829.0,"This read caters to deciphering the [future of data science](https://www.usdsi.org/data-science-insights/the-future-of-data-science-emerging-technologies-and-trends). Make it a priority to understand these core nuances before diving in as a seasoned data scientist! Explore the  to know more.

https://preview.redd.it/83az58h8os4e1.jpg?width=1080&format=pjpg&auto=webp&s=ff2a6e5367de2ea4028036402e0f66613ca013d7

"
bigdata,"Rollstack Product Updates December 2024, AI-Powered Data Insights, Collections, and More",6,1h5uqly,/r/rollstack/comments/1h5uqaf/rollstack_product_updates_december_2024_aipowered/,1733251620.0,
bigdata,Amazon EKS Auto Mode: What It Is and How to Optimize Kubernetes Clusters,2,1h5lev5,https://scaleops.com/blog/how-scaleops-enhances-amazon-eks-auto-mode-for-kubernetes-cluster-management/,1733225677.0,
bigdata,10 Essential Conda Commands for Data Science - KDnuggets,3,1h5itgm,https://www.kdnuggets.com/10-essential-conda-commands-data-science,1733214217.0,
analytics,Monthly Career Advice and Job Openings,9,1hhw29e,https://www.reddit.com/r/analytics/comments/1hhw29e/monthly_career_advice_and_job_openings/,1734624027.0,"1. Have a question regarding interviewing, career advice, certifications?  Please include country, years of experience, vertical market, and size of business if applicable.
2. Share your current marketing openings in the comments below. Include description, location (city/state), requirements, if it's on-site or remote, and salary.

Check out the community sidebar for other resources and our Discord link"
analytics,Looking for community feedback,15,1dj1a5b,https://www.reddit.com/r/analytics/comments/1dj1a5b/looking_for_community_feedback/,1718744143.0,"Hey r/analytics community,

As this group continues to grow I want to make sure majority are finding it useful.

I'm looking for your ideas of where we can improve this group and what do you love about it, leave your comments below."
analytics,Frustrated as a Data Analyst: Are we just storytellers?,106,1i196lv,https://www.reddit.com/r/analytics/comments/1i196lv/frustrated_as_a_data_analyst_are_we_just/,1736870187.0,"I’ve worked in five different roles in the data field, and across most companies, I’ve noticed a common trend: data analysts are primarily tasked with producing dashboards or generating figures based on very specific business requests. However, when it comes to tackling broader, more open-ended questions, things seem to get more challenging—especially in companies where Python isn’t part of the toolkit.

In my current company, for example, we’re expected to find new insights regularly, but everything is done using SQL and Tableau. While these tools are fine for certain tasks, doing deeper data exploration with them can feel tedious and limiting. We’re also not encouraged to use statistical knowledge at all, since no one on the team, including our boss, has a statistical background. It feels like there’s no understanding or value placed on applying more advanced techniques. We just need to have exceptional data storytelling skills + put up some nice figures which confirm already known intuitions.

Honestly, I’m feeling a bit frustrated. I can’t help but wonder if this is common across the field or if it’s just the nature of certain industries or companies. Would things be different in a more tech-focused company or in a dedicated data science role?

What’s your experience with this? Is this a frequent issue in your work as well, or does it vary depending on the company or team? I’d love to hear your thoughts."
analytics,Predictive Analytics Cert?,4,1i1j7vr,https://www.reddit.com/r/analytics/comments/1i1j7vr/predictive_analytics_cert/,1736895947.0,I'm curious if I should get a certificate in Predictive Analytics. No one on my team or in my organization currently offers reporting like this and I would like to start. I manage a small team of analysts specializing in financial and operational reporting and analytics. We do most of our analytics in Tableau & excel but I'm trying to think ahead and there are plenty of use cases for predictive analytics. Any suggestions on who to get certified through? Has it been useful/successful at your organization? Thanks in advance!
analytics,YouTube channels for background noise?,4,1i1klda,https://www.reddit.com/r/analytics/comments/1i1klda/youtube_channels_for_background_noise/,1736899832.0,"So for IT it's easy to throw on any tech youtubers video for ambient noise relevant to the field and occasionally pick up some useful information. I understand it's easier to make content for IT, but  I'm wondering if there's anything similar for analytics that isn't just a python tutorial or a how to on landing your first job.

Thanks for any suggestions.

Also, if there's a better place to post this I'd be glad to move it there"
analytics,Where is the DS career headed?,3,1i1lu49,https://www.reddit.com/r/analytics/comments/1i1lu49/where_is_the_ds_career_headed/,1736903448.0,"Just saw the Rogan / Zuck podcast on how AI is changing most tech careers. I’m just now transitioning in a DS career, getting well versed with the ML algorithms and Gen AI concepts. For the more experienced folks in the field, how is the DS career specifically going to change in the coming years? How can we try to stay on top of all the changes coming in? 

PS: This might be more of a question for the r/datascience sub, but unable to post question there. "
analytics,How can I create a function using values from two different data sources in Looker Studio?,2,1i1njjf,https://www.reddit.com/r/analytics/comments/1i1njjf/how_can_i_create_a_function_using_values_from_two/,1736908679.0,"In my report, Data Source A is giving me the fields A, B and C. 

Data Source B is giving me the fields D, E and F. 

There's a formula behind each of these fields.

I want to create an additional field which would be pretty much (A-B) / D, but that is not possible because they come from two different data sources. 

If I select them and try to choose ""Blend data"", the option is greyed out saying ""You can't blend with an already blended chart"". So I'm currently lost if there's anyway to display this information to my client without manually calculating this.

Alternatively, is there any any to just use the fields as values, instead of replicating the massive formula that's behind each one of them?"
analytics,How do people progress from an Academic environment to real world? ,12,1i18boz,https://www.reddit.com/r/analytics/comments/1i18boz/how_do_people_progress_from_an_academic/,1736867902.0,"I recently graduated from an MS in Business Analytics program and had classes in Data Analytics, Stats, Machine Learning, R and Python. The courses covered things but some things were pretty basic. Like we covered SQL but we did not do queries involving multiple joins or CTEs or complex stuff. Rather simple individual queries on a chosen dataset, things like that. It feels like we did learn but did not go too far or deep like people do in industry or real jobs. We did not work with things like Qlik or do ETL. For Excel/Sheets, we had no class and just did some basics, while I have seen some jobs require proficiency. All in all, I feel like classes and class projects might not be enough. Or is this enough to get started? Because I have seen data roles are individual contributor roles where you are kind of on your own. How can an entry level person manage this straight out of college? Is it possible? What did people with experience do or what did your journey look like? "
analytics,New to SEO: I need some help with deciphering my GSC graph!,1,1i1osow,https://www.reddit.com/r/analytics/comments/1i1osow/new_to_seo_i_need_some_help_with_deciphering_my/,1736912731.0,"Does anyone have any experience with your page showing a sudden decrease in clicks and impressions? I optimised the blog on 26th December, which showed an increase for 1-2 days, but then hit a 0 on the third/fourth day. Position wise is one now."
analytics,New grad jobs,4,1i1efq3,https://www.reddit.com/r/analytics/comments/1i1efq3/new_grad_jobs/,1736883447.0,"Is January a bad time to look for jobs? Recently graduated in December but the issue I’m having is that there’s not that many jobs to begin with. LinkedIn is only showing about 20 - 30 jobs. Most of them are for senior roles too.

I’m not sure if I’m competitive enough for this job market tbh. I only have 1 internship utilizing sql, excel, and some data visualizations. The rest of my resume is some other unrelated job and a couple of projects on tableau public. "
analytics,Need help deciding which route to take for transition into DA,2,1i1akf5,https://www.reddit.com/r/analytics/comments/1i1akf5/need_help_deciding_which_route_to_take_for/,1736873736.0,"Hi everyone ! 

I bet this is a pretty much always asked question and sorry for asking it again but i would like some answers specific to my situation. 

First lemme say i live in France for some context, so things are a bit different here. 

I have 2 masters in engineering, one in Material Science and the other in Space Systems, from 2 highly recognized schools (+ i did my final year at Imperial College in the UK). 

I have worked 2 years as an R&D engineer in microelectronics, doing 40% of theorical physics and the other basically doing the job of a data analyst. The firm i was in had no data person whatsoever so i kinda became it and built a whole application in VBA to extract, transform, load, analyse and dashboard data coming from our devices tests. Did some python and Power BI dashboarding while i was there. 

 I am saying all this because i keep reading posts where ppl say that a degree is the most imporrtant thing in the field and a bootcamp in case you have the diploma will help but not as much. 

So i have a degree, in a related field, but we kinda did everything you do as a DA (or even DS). A lot of proba, stats, machine learning, math, python and such...

I quit my job a few months ago now and i'm lost between doing a bootcamp (and pay 5k+ for it) to learn more DA skills and have the certification or going the self taught route and build a learning path to be as close as the bootcamp's one, using DataCamp or Maven analytics resources.

On the one hand, self-teaching would save me a lot of money, and there’s a ton of free or affordable resources out there. On the other hand, bootcamps offer access to career coaching and industry networks, which could be invaluable for landing a job. A structured curriculum might also keep me on track and ensure I don’t miss any key concepts, plus they often provide real-world projects that would help me build a portfolio.

So i woul really need your advice here and what you think would be the best choice considering my background and situation. 

  
TL;DR: I’m an engineer with two master’s degrees and two years of data analysis related experience trying to decide between an expensive data science bootcamp and self-teaching. Looking for advice on which route might be better for breaking into data analytics

Thanks a lot ! "
analytics,How to get calls for Data analyst or product analyst when my current designation is BA?,3,1i171jw,https://www.reddit.com/r/analytics/comments/1i171jw/how_to_get_calls_for_data_analyst_or_product/,1736864311.0,"

Previously, I worked as a Data/BI Analyst, primarily focusing on dashboarding and reporting using Power BI (80%), SQL, and Excel. While I developed strong analytical and visualization skills, I sought more dynamic problem-solving roles and transitioned to my current position as a Business Analyst in a  company specializing in invoice processing automation. Here, my role is to gather and analyze requirements, creating detailed ETL mapping documents, and collaborating closely with development teams to implement automation solutions. Additionally, I do development work as well example: Writing etl packages in SQL.(Procedures, CTE, window functions and stuff)

Now, I am looking to leverage my analytical expertise, problem-solving mindset, and technical knowledge in roles such as Data Analyst or Product Analyst that offer strategic insights and data-driven decision-making responsibilities rather than traditional BI dashboarding tasks.  But I am just getting calls for BA roles. How should I modify my current job description to get calls for product analyst or data analyst roles? "
analytics,Drone Data Analysis Projects,2,1i19sg5,https://www.reddit.com/r/analytics/comments/1i19sg5/drone_data_analysis_projects/,1736871766.0,I recently got a DJI Tello drone and I am very passionate about drones. Would analyzing battery performance over time or doing flight data analysis be interesting projects? I was thinking I could use the SDK to get data from the drone and put it in the database. Then from there use SQL and maybe Looker Studio to manipulate the data and create a dashboard or some visualizations. Could these be interesting passions projects? Any recommendations or has anyone done something similar? 
analytics,Projects that got you A job,64,1i0l0i5,https://www.reddit.com/r/analytics/comments/1i0l0i5/projects_that_got_you_a_job/,1736793429.0,"If you don’t mind sharing, what project got you an entry level job? 

Background: I want to transition from teaching. I have a degree in math and computer science. I have completed Google Data Analytics on coursera. I currently have 2 personal projects completed. One is analyzing my finances using python to automate things. The other is analyzing student tests performance with excel. 

I want my 3rd project to be more business facing and impressive. Ive looked on Kaggle for data sets but the data seems basic. Like i can find average, increasing or decreasing trends, max and min but if i was a hiring manager i would not be that impressed. 


Tldr: 
I finished learning the basics and have 2 simple projects. I want to work on a project that would impress people but i am having a hard time finding interesting data sets. What project impressed your hiring manager enough to get you your first job? 

Thanks!"
analytics,Need interview preparation sources,2,1i15z8z,https://www.reddit.com/r/analytics/comments/1i15z8z/need_interview_preparation_sources/,1736861104.0,"For analyst interviews, can you suggest some good sources where I can practice questions on SQL, Python, Pandas, etc.?"
analytics,Can I use Leadsnavi as a lightweight alternative to GA for web analytics?,1,1i19n03,https://www.reddit.com/r/analytics/comments/1i19n03/can_i_use_leadsnavi_as_a_lightweight_alternative/,1736871377.0,"I have both GA and Leadsnavi on one of my client’s websites. We are using GA for analytics and Leadsnavi for identity resolution and lead generation. The web pages have gotten a little slow and I’m considering switching to a much lightweight analytics tool. I have tried MS clarity but there is not much difference there either.

Leadsnavi has analytics too but I’ve never used it for that, we just use it for identity resolution. I’m considering doing away with both GA and MS clarity and let Leadsnavi handle the analytics too.

Will it be enough or do I need to continue looking for alternative analytics tools?

Note: It was the client’s idea to add Leandsnavi for identity resolution and lead generation, my role is to set up the infrastructure, he uses the tools himself, that’s why I want to know if Leasnavi is good for analytics from a business point of view.

"
analytics,How Big is Your Team?,14,1i0v2kv,https://www.reddit.com/r/analytics/comments/1i0v2kv/how_big_is_your_team/,1736819726.0,"I’ve worked in analytics for a few years, starting off as a Sales Operations Analyst to now working as a Business Intelligence Analyst for a Fortune 50 company. 

Throughout the duration of my career, I’ve mostly worked on a team where I’m the only analyst and the only one responsible for data related projects and reporting. From the rhetoric I’ve seen on Reddit and having conversations with other analysts, there doesn’t seem to be many fully developed analytical teams within companies. 

Is this true for most businesses? Do most companies generally keep a small analytic team if not solely relying on one person? "
analytics,Am I overthinking my new position?,3,1i0y95h,https://www.reddit.com/r/analytics/comments/1i0y95h/am_i_overthinking_my_new_position/,1736829689.0,"I am in the ending stages of possibly landing a Marketing Analyst role with a Marketing company in my hometown (I currently live in the town I went to college in and want to move back home). I am currently an SQL programmer (pretty much an entry level data analyst). I have a degree in Computer Science and Engineering and a Minor in Math. 

I am personally concerned about this job being possibly a step back in my career or a major shake up. Both jobs have the same pay but because they have the job titled as ""Marketing Analyst"" in the title for the job and ""Marketing Specialist"" in the description, I am a bit weary about how I may be perceived in future endeavors. 

I would like to take this job because it's an ""in-person"" job in my hometown (where I am moving in 4 months regardless of this decision), somewhat similar but exciting as compared to my previous job, and I won't have to go remote for my current position but I am feeling anxious about the decision. 

Could you give your insight on my dilemma? Maybe this was a rant more than a question but I guess the question would be do you think my concerns are valid or am I overthinking as this is my first major career move?

Thanks in advance."
analytics,Is 74k too low for new grad?,0,1i1dxku,https://www.reddit.com/r/analytics/comments/1i1dxku/is_74k_too_low_for_new_grad/,1736882181.0,"I got an offer from a company that I've been interning for 2 years. The offer requires me to move to a State that I don't really like. The job is quite boring, but the pro is that I get to work remotely. Everyone at the company is quite chill and nice. The job is not too stressful and the company really values wlb. They also offer tuition reimbursement

The only thing I didn't feel happy about was the pay and the fact that I have to move to a different state. I don't know why I have to move, if they let me work remotely. I've been applying to other jobs and in the interview process with couple companies. Any advice what I should do moving forward?

I know the job market has been really difficult, so I'm grateful for my offer but I still want to know if there's anything else I can do."
analytics,Analytics communication and writing style,3,1i0u7q9,https://www.reddit.com/r/analytics/comments/1i0u7q9/analytics_communication_and_writing_style/,1736817199.0,"I've long struggled with writing style. I'm usually either too verbose or too concise. Rarely find a spot in the middle.

I've found some success with writing anything in a work/analytics doc as TLDRs, separating into bullet points, adding a table, markdown, having a consistent format like 2 sentence insight + chart + chart link, appendix section, using a general template for specific repeated projects. But in Slack or more detailed analysis docs, it's harder. The audience can be a large range, technical and nontechnical, executive and non-exec, sometimes only 3-5 people, sometimes 30-50.

Writing takes me a disproportionate amount of time. I'll spend 20 mins tweaking a Slack message and still edit it a min or two after I send it because I forgot this and that or I edited so much that I left an extra word in.
 
What to do? Any useful analytics writing guides or something that you can recommend?

I wish there was a leetcode for writing. I know I can use ChatGPT for suggestions but this is so niche and I want to be able to do this going forward with more care in the moment."
analytics,Can I work in analytics with Master’s in Computer Science and Bachelor’s in Business? ,4,1i0oyno,https://www.reddit.com/r/analytics/comments/1i0oyno/can_i_work_in_analytics_with_masters_in_computer/,1736803168.0,"Hey yall. I’m a 23 year-old with bachelor’s degree in Business Management. I recently got accepted to masters in CS in the UK( I actually applied to Data but they rejected it and offered me CS instead). I am quite interested in working in IT business analytics or data analytics. The thing is I don’t know if I can start working in those fields if I get masters degree in CS. I’m scared that it can cause me problems because it is completely different from my bachelor’s major. Can anyone give me advice? If I study CS what are my career prospects in business related field? Will it affect my career negatively?
Is it better to wait for the next year and apply to Data analytics at other universities? "
analytics,MIS/CIS or Data Science Degree,4,1i0jsim,https://www.reddit.com/r/analytics/comments/1i0jsim/miscis_or_data_science_degree/,1736790483.0,Hello everyone! I am currently finishing up my general studies in a community college and plan on transferring soon but not sure what to go for. I was planning on going for a data science major but started learning more about MIS/CIS degrees. I have to say I really like the versatility option of that but I most likely would still like to look for a job in the data science field when done. Would it be a waste to go for MIS/CIS degree? Is it a wiser choice since It would give me more options when i'm done? Another thing is I'm not sure if the data science program is just a cash grab from the school since it is fairly new. Anything helps!
analytics,I want to enroll in Analytics,1,1i0mmj3,https://www.reddit.com/r/analytics/comments/1i0mmj3/i_want_to_enroll_in_analytics/,1736797392.0,"Hello! I have an undergrad in clinical psych in India and want to enroll in georgia tech masters in analytics, online or offline both should be fine, what should I do to be proficient for this course and what requirements will I need to fulfill and would I be eligible? Would someone like me get in? I have decent knowledge of statistics and have dabbled into SPSS and a little bit into R. And what prospects would I be looking at after the completion of the course? Any advice would be very much appreciated. "
analytics,Need Resource for Speeding Up Power BI Data Refresh from SharePoint Excel Files,1,1i0l1yo,https://www.reddit.com/r/analytics/comments/1i0l1yo/need_resource_for_speeding_up_power_bi_data/,1736793531.0,"Hello,

I'm a Jr. Analyst working with Power BI and SharePoint, and my manager tasked me with fixing slow data refresh times for one of our reports. Currently, we're connecting to Excel files stored in a SharePoint folder using a basic web connection, which seems to be the bottleneck.

My manager requires that the refreshes be done in Power BI Desktop, and the reports to then be uploaded to the Power BI Service. Additionally, we rely on Power Automate flows to handle some parts of our workflow. These flows open up websites, download data, and upload it to SharePoint. However, this setup requires our laptops to remain running for the process to work, which isn't ideal for a nightly refresh scenario.

My manager suggested exploring the SharePoint API as a potential solution to improve refresh performance. They don't mind how it's done, as long as the data refresh speeds up significantly.

I've already looked at a few tutorials and articles, but I'm hoping someone with experience can point me to the best resources (videos, articles, or guides) to optimize this process. I'd appreciate help in cutting through bad advice and finding the most effective solution.

Thanks!"
analytics,Few questions ,2,1i0ku21,https://www.reddit.com/r/analytics/comments/1i0ku21/few_questions/,1736793002.0,"1.Is there any data analytics course or tutorial that is worth learning and that is free ?
2. Google data analysis course- how long does it take to finish the course- any experience?"
analytics,Transitioning from Accounting Specialist ,0,1i0jxzk,https://www.reddit.com/r/analytics/comments/1i0jxzk/transitioning_from_accounting_specialist/,1736790859.0,"Hello there,
I am transitioning from Accounting Specialist to Data Analyst. I have more than 8 years of experience in accounting and Bookkeeping. I am freelancer and have Top Rated Plus badge on Upwork. My hourly rate on Upwork is $8-10. I just got IBM Data Science Professional Certified from Coursera.
My question is:
-To be more competitive, what should I learn more and from where I should learn it? (I got Coursera certification through financial aid. So I have budget constraint.)
-What should I expect in my earning when I enter the market as financial data analyst?"
analytics,Entry salary expectations?,18,1i0059n,https://www.reddit.com/r/analytics/comments/1i0059n/entry_salary_expectations/,1736724334.0,"I know there’s been a few post regarding the same topic but everyone’s qualifications are different. I am entering my last semester and am graduating with a bachelor’s degree in business analytics. I’ve done 2 summer internships (about 7-8 months total) during my time in college. Some qualifications I have are mastering excel (who hasn’t at this point), good experience with power BI, JavaScript, python, tableau, and sql. So with that being said what are realistic salary expectations I can have for entry level jobs giving my qualifications?"
analytics,Quick question for everyone,0,1i0euyy,https://www.reddit.com/r/analytics/comments/1i0euyy/quick_question_for_everyone/,1736777638.0,How do you make sure your content stands out in a crowded niche?
analytics,Any good resources for case study? ,2,1hzx2fe,https://www.reddit.com/r/analytics/comments/1hzx2fe/any_good_resources_for_case_study/,1736716218.0,"Any thing that helps regarding a case study or a case interview - be it tutorials, example questions, tips, websites, YouTube channels etc. 

Thank you! "
analytics,Considering business analytics post grad - Biz Econ Major,1,1hzxb4s,https://www.reddit.com/r/analytics/comments/1hzxb4s/considering_business_analytics_post_grad_biz_econ/,1736716832.0,"I am a recent UCI grad with a Business Economics degree. I recently got laid off at my job as a business development associate. I am looking to move to a more analytical role and am trying to learn about data and business analytics. I have many questions on the topic and would appreciate any advice. 

1. Is a BA in Business Econ enough to start applying to business analyst roles  
2. I am currently taking a Google certificate for data analytics, but am finding that it's not teaching me the tools I need like SQL and Tableau. What recommendations are there for these certificates.   
3. I am considering applying to a master's in business analytics, but as the applications are due relatively soon I am unsure if this is a smart decision to rush through these right now. I would also like to consider a flex MBA/ MSBA as I would like to be working full-time   
4. As I am currently looking for new jobs, what is the best way to prioritize my time, and the best roles I could apply to? (can provide more info if necessary based on previous experience in more management roles)  
5. I am also considering project management. However, I do like the idea of being able to guide business decisions for a company (specifically the games industry). What does the day-to-day as a business analyst look like? I do prefer the idea of traveling to conferences and meeting with other people, but I hate traditional sales. I love the idea of working on a project, but don't want to be sitting without interaction all day, and also find coding to not be my favorite thing in the world. I enjoy a PM position but I feel I would not be as impactful in the overall decision-making for a company."
analytics,Just landed an internship interview at BMW! Any advice?,43,1hz9kpi,https://www.reddit.com/r/analytics/comments/1hz9kpi/just_landed_an_internship_interview_at_bmw_any/,1736639744.0,"Its in 2 days and I really want this internship, can you experts give me any advice?

Edit: its online btw "
analytics,how to transition into healthcare analytics,7,1hzb2mu,https://www.reddit.com/r/analytics/comments/1hzb2mu/how_to_transition_into_healthcare_analytics/,1736644074.0,"I graduated from Michigan this past May and started working at the Mayo Clinic as a clinical scientist. I seek to exit the lab and work more in Healthcare Analytics or anything adjacent to this position. Given my current position, can I get some pointers on how to be a good candidate? 

  
I am seeking to be proficient in Excel, SQL, among other things. Thanks!"
analytics,"Is ssms, SSRS and powerbi out of demand? ",6,1hz53m9,https://www.reddit.com/r/analytics/comments/1hz53m9/is_ssms_ssrs_and_powerbi_out_of_demand/,1736627510.0,"I'm in USA and have been working with the above 3 for 8 years in healthcare. I'm looking to make a move as the product will be sunsetting soon unfortunately in a year. I'm fully remote for 4 years and have been applying for the past 1 year. Haven't even received an interview with the techical person, which is very disappointing.

Should I pivot for remote opportunities to data engineering or maybe learn Tableau? 

 "
analytics,Is College Still Worth It?,41,1hyhkdj,https://www.reddit.com/r/analytics/comments/1hyhkdj/is_college_still_worth_it/,1736549995.0,"Hello,

I am a Sophomore in College and was just wondering which majors are useful in the current market. I am currently a Data Science Major, and I like it for the most part, but the tech job market is super competitive right now. I want to eventually get a job in analytics or something in big data, however, I've heard so many horror stories that I'm worried about going on about college and not being able to make it out with a job. Please let me know.

Thank you."
analytics,Excluding internal traffic,0,1hyy26u,https://www.reddit.com/r/analytics/comments/1hyy26u/excluding_internal_traffic/,1736608759.0,"I setup internal traffic filtering by the following method.

Data collection & modification > Data Streams  > Configuration > Define internal traffic

Using the IP address is in range (CIDR notation): 5.565.3.226/337 (not a real IP)

I have confirmed with the IT department the IP and hence know it is correct.

I then activated the Data Filter.

However, I am still seeing hits from the very small town the business is based. Hence I am pretty sure those are internal hits.

Is there a ""better"", more reliable way of filtering out internal traffic? "
analytics,Are you looking to crack into data domain.,0,1hz20tc,https://www.reddit.com/r/analytics/comments/1hz20tc/are_you_looking_to_crack_into_data_domain/,1736619342.0,"Hello everyone are you looking for data domain career. Lot of people are struggling and have no proper guidance and resources.Almost from 4 years in data domain and i can say that with right skills set you will definitely crack the data domain whether you are looking for data analyst, business analyst, financial analyst, product analyst. Some of the skills are common in this but yes enough to crack. Feel free to guide you and help you if you want any guidance."
analytics,Promotion Salary Negotiation,10,1hy7avi,https://www.reddit.com/r/analytics/comments/1hy7avi/promotion_salary_negotiation/,1736523935.0,"As the new year starts to warm up I’m looking at a promotion from senior to lead. This was earned mostly by leading a (small) team for a year to excellent business results (as documented in my annual review). Given that this jump is one that is more managerial in nature (hence the term ‘lead’) I am unsure of what to most reasonably expect in terms of salary increase as I’ve always been an IC. 

For those who are familiar with promotions at this level, in your experience, what has been the range? My gut says ~15% is where I’d be happy being but I absolutely do not want to leave money on the table. "
analytics,Zuck,0,1hyv7yo,https://www.reddit.com/r/analytics/comments/1hyv7yo/zuck/,1736599850.0,Did you guys hear that in meta ai will replace mid-level engineers sometime this year? How do you guys think it will impact analytics?
analytics,"Is it true that the field of analytics is over saturated? If so, what are other options or roles for one’s interested in analytics?",3,1hyc3oc,https://www.reddit.com/r/analytics/comments/1hyc3oc/is_it_true_that_the_field_of_analytics_is_over/,1736535947.0,"
In one of my previous posts someone commented that analytics is over-saturated. If that’s the case, what are other roles someone who is interested in analytics can look into ? I’m an MIS major at my undergrad college and my coding skills or skills necessary for analytics are below the bar for a tech/analytics role and I was wondering if analytics is actually over-saturated what are other roles I can look into ?"
analytics,Switching to Marketing Analyst from PPC,7,1hy3van,https://www.reddit.com/r/analytics/comments/1hy3van/switching_to_marketing_analyst_from_ppc/,1736514039.0,"Hi all, I went through previous similar questions and got some answers, but I am starting a new thread due to my somewhat specific situation. I work at a big e-commerce fashion company as a PPC/media activation manager. I have solid experience in digital marketing, having worked in media agencies before this job.

I need the change and don't see a future in PPC and media, to be honest, I would like to switch to Marketing analyst. I don't have any formal education from that field in maths and tech, my degree is in marketing and public relations. However, I always loved numbers and stats and I believe I can interpret data to real business conclusions and, let's say, real-life use.

My main question is - Do you think that's the right direction and more importantly how I should do it? The reason I think this is very doable is that my current company encourages career change and I could proactively reach to the Analytics team to have me in mind for that role when they need it (ofc when I am skilled enough) What do you think would be the best way education-wise (courses, specific tools, maybe go back to some math first) and how I could leverage a lot of marketing and business data I have access to to make some projects for myself and gain experience? Thank you!

"
analytics,Required work logging/tracking,10,1hxyy3j,https://www.reddit.com/r/analytics/comments/1hxyy3j/required_work_loggingtracking/,1736493236.0,"Role: data analyst (sql reporting, projects, validation, research, etc)

Recently required to start logging everything to the half hour. Example of one day below:

Meetings: 2.5 hours 
Project A: 3 hours 
Emails/issues: 1 hour 
Project B: 1.5 hours 


Does this seem a bit excessive? I get the need for resource planning but I haven’t heard of other teams doing this outside of an agile environment. Looking for ways to make this more efficient if I’m doing it everyday.

Any thoughts, experiences, or advice welcome."
analytics,Google Apprenticeship 2025 - Status Discussion Sub,0,1hy4jbu,/r/google/comments/1hy4d4f/google_apprenticeship_2025_status_discussion_sub/,1736516176.0,
analytics,Freelance/Practice Data Projects,11,1hxktf2,https://www.reddit.com/r/analytics/comments/1hxktf2/freelancepractice_data_projects/,1736450564.0,"Between jobs now and trying to keep my data skills sharp. I have tried working on a few sports and gaming data projects, but wondering if there is anything out there that has real life examples with prompts that can help me keep improving and keep my skill level up. Trying especially to stay up on excel and SQL as well as learn about PowerBI. Thanks for any help. "
analytics,Is it possible to transition to this career?,20,1hxd98p,https://www.reddit.com/r/analytics/comments/1hxd98p/is_it_possible_to_transition_to_this_career/,1736430803.0,"I graduated with a degree in Computer Science back in 2023. I have not found a job related to my degree. My internship was only a position as a QA Analyst which mostly involved testing software.

The problem is I'm not really passionate about CS. I have tried working on side projects but quickly lose interest/motivation in completing them. I have not really tried to find a job in CS hence why I have not held a position related to it since graduating. The job market for CS new grads is also really difficult where I live right now (not saying data analyst is any easier, I don't know).

Data Analyst has been something I've been interested in and I'm not sure how I can get my foot out the door. What should I do before applying for entry level positions to increase my chances? How long of a commitment do I need before I have decent chances at landing an entry level position?

I know the obvious answer is to go back to school and get a degree for it, but that isn't something I can do."
analytics,Resources to Learn APIs,58,1hwntvr,https://www.reddit.com/r/analytics/comments/1hwntvr/resources_to_learn_apis/,1736352858.0,"Hello Everyone,
I’ve been working as a data analyst for a little over a year now and have never needed to know how to use APIs until now. Does anyone have experience learning how? Any recommendations?"
analytics,is it overkill to be asked to provide 5 different references 3 mgrs 2 coworkers for a senior/mgr role at a non profit?,6,1hwydwt,https://www.reddit.com/r/analytics/comments/1hwydwt/is_it_overkill_to_be_asked_to_provide_5_different/,1736378948.0,No judgements or assumptions pls kindly stick to answering the question I just want to know if this is common. Thank you!
analytics,Live Gaussian Filter Line (Zero Lag/Loss Average) for Time Series Data,3,1hx1nqx,https://www.reddit.com/r/analytics/comments/1hx1nqx/live_gaussian_filter_line_zero_lagloss_average/,1736388019.0,"I have spent the past few months developing a formula (using python and linear regression models) for my time series data to generate a ""live"" Gaussian filter.  This way I can apply it to incoming data and have a smooth, zero lag, average for readability / further analysis. The best I have been able to accomplish so far is 94% correlation between my line and the original gaussian line...

I am looking for at least 96-98% for it to be useful in my case. There is still information to be extracted from the features I have derived for this calculation, since they are between 20-30% correlated with that last 6% error, but I am absolutely stumped and tired... 

Does anyone know where I can hire someone, who to hire, or where I could put out a prize, to come up with some kind of equation/function that is correlated with the error?"
analytics,Analytics Communities - Thoughts?,4,1hwtr8j,https://www.reddit.com/r/analytics/comments/1hwtr8j/analytics_communities_thoughts/,1736367379.0,"When you look for subreddits, LinkedIn Groups, Slack channels, etc. that you engage with outside of work/on your free time at work - what are you looking for?

Hoping to help members, mods & just really the whole analyst community grow. Lots of appetite to help each other and build relevance in our beloved profession, I thought it would be interesting to see what people have to say on this.

Some options to get the conversation started:

* Career Advice
* Educational Resources (certs, upskilling, etc.)
* Networking
* Fun/Memes
* Vetting About Boss/Executive Stupid Requests
* Showcasing Work/Portfolio Building"
analytics,"Is this survey biased, or am I overthinking?",1,1hx1dl5,https://www.reddit.com/r/analytics/comments/1hx1dl5/is_this_survey_biased_or_am_i_overthinking/,1736387225.0,"I'm reviewing survey data for one of our products. We have two versions: a digital version, and a printed version. Another team ran a survey because the print version is very, very expensive to publish, and we want to know if it's worth it. They gave me the results yesterday, and it seems there's a preference for print (82% want the print one to continue, 81% prefer print) -- until I asked **who** they surveyed. **They only emailed print users**!! Mind you, these people also receive the digital version (our lists double up), but isn't that iffy? Like, asking the people who use the thing if they want that thing to continue?? 

I think this is valuable data, but it needs more context (like offering a split survey wih one path for people who only use digital, and one for people who get both). Tell me why I'm wrong. "
analytics,I’m a statistics student looking for a internship ,10,1hwefma,https://www.reddit.com/r/analytics/comments/1hwefma/im_a_statistics_student_looking_for_a_internship/,1736321480.0,"What would you guys suggest I do to be ready or even a worthy candidate of finding a data science internship?

I am a junior at my college and I am 31 years old. 

Any suggestions would help. "
dataengineering,Monthly General Discussion - Jan 2025,14,1hr6zga,https://www.reddit.com/r/dataengineering/comments/1hr6zga/monthly_general_discussion_jan_2025/,1735750833.0,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)"
dataengineering,Quarterly Salary Discussion - Dec 2024,48,1h47qv8,https://www.reddit.com/r/dataengineering/comments/1h47qv8/quarterly_salary_discussion_dec_2024/,1733072430.0,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)"
dataengineering,dbt Labs acquires SDF Labs,115,1i17fyc,https://www.getdbt.com/blog/dbt-labs-announces-sdf-labs-acquisition,1736865450.0,What are your thoughts on this acquisition?
dataengineering,Python for Data Engineers: Key topics & techniques 👇,114,1i17bqf,https://i.redd.it/eh82tguozyce1.png,1736865119.0,
dataengineering,Fact table with 2 levels of grain,11,1i1j7ow,https://www.reddit.com/r/dataengineering/comments/1i1j7ow/fact_table_with_2_levels_of_grain/,1736895932.0,"I have a fact table called fact_bills that stores bill details of items purchased. Each row is an item for a specific bill. This works well for my current use case. 

I was tasked with adding a department dim to the fact table but it messes with the grain. Items can be billed to multiple departments. For example, a company buys 10 laptops but 5 are for engineering and 5 are for finance. There would be 1 row in fact_bill for the 10 laptops, and 2 rows in a different table-one for engineering and one for finance. If I add the department dim, then each bill item’s attributes are repeated for N departments. 

Some use cases include counting number of billed items. Some include department specific filtering. Obviously adding department dim complicates this. We could use count distinct, but I’m wondering if there is a better approach here?"
dataengineering,Who should own the data transformation/modeling layer (data platform team or the analytics team)?,22,1i187ob,https://www.reddit.com/r/dataengineering/comments/1i187ob/who_should_own_the_data_transformationmodeling/,1736867596.0,"I am a part of the data platform team and the team is responsible for creating and maintaining data pipelines (CDC pipelines + APIs which other applications use to publish events + some ad hoc data pipelines). The data (from all sources) is dumped into tables on data platform with almost no transformations i.e. there is a 1-1 mapping between the tables in the source database (in case of CDC pipelines) and the table on the data platform (e.g., a table on the data platform replicated from a MongoDB collection has the exact same schema plus some additional metadata columns injected by the data pipeline). The same goes for data coming from the event tables (there is minimal/no transformations). We make sure that the data is partitioned correctly, regular compaction is run on the data to avoid the small file problem etc. We don't deal with any business logic or analytical modeling and focus solely on the engineering. In an ideal scenario, the application teams should collaborate with the analysts (and the data platform probably) and think of metrics instrumentation in advance but most of the times, they do not have the bandwidth and the end result is that we have 100-200 tables created from tables on application DBs (MongoDB, MySQL etc). I have observed that in my company, analytics is an afterthought.

There is a team of analysts in my company who are responsible for producing dashboards, metrics on top of the data on the data platform and reporting numbers to stakeholders. The analysts are the end consumers of data platform, they use it the most and they write the queries mostly. For ad-hoc queries, there are a few BI tools that they use. To enable the analysts to schedule queries, the data platform has provided them with a platform where they can submit the queries, specify a schedule and their queries will be materialized to tables. As of now, they are responsible for gathering context on the business logic from the application teams and writing the queries.

A few problems arise from the above setup: -

1. Duplicate queries. Different analysts sometimes end up writing multiple queries to answer the same business questions. Although we have provided them with a tool to document the queries and their columns, it doesn't seem to be helping. There are about 300-400 scheduled queries running everyday on our system and the analysts don't take ownership of the web of inter-dependant tables they have created, hence there is no cleanup initiative from their end (which is understandable, as they don't always have the bandwidth to care about it, they are just focussed on getting the numbers).
2. Inefficient queries. Analysts (at least in our company) are unaware/have limited knowledge of how to write efficient queries (e.g., use partitions in queries), add filter in subqueries early on so that less data is scanned etc. As a result, they end up scanning 100s of GBs of data sometimes. Limited knowledge about how the underlying query engine works is one of the reasons for bad queries. Another reason is that we have not given them the option to create incremental tables, as a result, they end up scanning the same data (with an additional time delta) almost everyday (we are working on providing that option to the analysts). In my opinion, the problem of writing inefficient querying is solvable to some extent with good documentation on the best practices to query and sessions to educate them on how to write queries.

Our company is planning to improve the platform so that the analysts can get better feedback on the quality of their queries (data scanned, detecting early on if the query is bad), introducing true incremental queries etc so that the queries are more efficient. This solves 2 but doesn't solve for 1 above. We are also thinking about analyzing the scheduled queries on a regular basis to identify query patterns but we are a long way from actually implementing the solution.

In the midst of all this, I came across this article, [Databases in 2024: A Year in Review](https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html), which mentions the following: -

\> Most OLAP queries do not access that much data. Fivetran analyzed traces from Snowflake and Redshift and showed that the [median amount of data scanned by queries is only 100 MB](https://www.fivetran.com/blog/how-do-people-use-snowflake-and-redshift). Such a small amount of data means a single DuckDB instance is enough for most to handle most queries.

This made me realize that we are doing something wrong and led me to the conclusion that a transformation layer is missing from our company. To create that modeling layer, we need people who can efficiently populate the data models and also have business context. This will serve as a single source of truth plus the data models should (ideally) be easily queryable by the analysts (ideally less data scan, faster queries). But then the responsibility of data modeling will shift to the data platform and I think my company is not aligned with this idea (I am trying to bring up this point with the upper management and get their opinion on this).

So my question is who should shoulder the responsibility of data modeling? Should it be the data platform team or the analysts? How is it done in your company? Is the conclusion that we are missing a transformation layer correct or is there any other reason I am missing?

TL;DR: There are two teams in my company that interact with the data platform. The data platform team which brings data to the data platform and the analysts who form the majority of consumers of data platform. Currently, the analysts are responsible for data modeling (they schedule the queries whose results they require daily to produce reports/dashboards/metrics) but the queries are not efficient and there are duplicate data models. Should data platform own the transformation/modeling layer?

Thanks for reading."
dataengineering,Setting up a simple CDC pipeline using open source tooling,2,1i1oa6j,https://www.reddit.com/r/dataengineering/comments/1i1oa6j/setting_up_a_simple_cdc_pipeline_using_open/,1736911023.0,"Hello!

For a personal project, I am trying to set up a CDC (change data capture) pipeline to pull data from a MariaDB server located in one VPC to a PostgreSQL server in another VPC. My database is only 2 GB in size and a few minutes of latency is acceptable during replication. 

What open source tools/solutions do you recommend I use to set up such a pipeline that's resource light, maintainable, debuggable and will not result in data loss? 

I am not a data engineer, rather an analyst who can manage low code/config tools. Through Google search and Claude prompting, I have come across the following options and I need help evaluating these or finding out about other possible options:

1. Airbyte - Possibly the easiest but seems very resource intensive. Also needs docker desktop so I don't think I can run it in a small VM.

2. Debezium - Seems overly complicated for my use case, no?

3. Meltano - CLI/conflig based, seems lightweight but I couldn't find detailed instructions for setting up CDC.

  
Are there other options that I have not found and may be better suited to my use case?"
dataengineering,Fuzz Matching Bulk Wine Data,13,1i19kqh,https://www.reddit.com/r/dataengineering/comments/1i19kqh/fuzz_matching_bulk_wine_data/,1736871217.0,"Hi All,

My background is in Computer Science with a short stint as a data engineer, but I currently work at a wine storage provider/retailer. I am currently working on getting the inventory management on the retail side into the 21st century so that I can tie it into the warehouse management system on the storage side and start to generate better analytics for customer preferences. 

My current predicament is that our item naming and classification system is both inconsistent and also not inline with industry standard. The best system as of now is called LWIN which can be found here: https://www.liv-ex.com/wwd/lwin/. The LWIN ""database"" is open source and downloadable as a gigantic Excel file. My current plan is to pull LWIN into an actual database and to automatically pull and update the LWIN table nightly when we are closed.

This leads me into my main question, which is how can I best match up our existing item data to the items in the LWIN database so that I can then append things like inventory levels and internal pricing data to the items? The goal is to add functionality to our Shopify store to automatically pull the naming conventions from LWIN on item creation and automatically populate most of the data automatically with the only remaining fields being things that are unique to us, i.e. pricing, vintage, size. 

An example of an item in LWIN:


LWIN | DISPLAY_NAME | PRODUCER_TITLE | PRODUCER_NAME | WINE | COUNTRY | REGION | SUB_REGION | SITE | PARCEL | COLOUR | TYPE | SUB_TYPE | DESIGNATION | CLASSIFICATION
---|---|----|----|----|----|----|----|----|----|----|----|----|----|----|----
1077904 | Jean-Marc Pillot, Chassagne-Montrachet Premier Cru, Les Macherelles Blanc | NA | Jean-Marc Pillot | Blanc | France | Burgundy | Chassagne-Montrachet | Les Macherelles | NA | White | Wine | Still | AOP | Premier Cru


And then the same wine as we currently have it store in our inventory system:


id | descr | group | department | sub_department 
---|---|----|----|----
17113 | Chassagne-Montrachet, 1er Cru Macherelles, Jean-Marc Pillot | White Wine | France White Wine | Burgundy Saint-Aubin

As you can see we have a close enough match, but like the example I gave, many of the items have incorrect or missing sub_regions. 

How would I best go about finding a match where at least most of the process is automated? I am happy to manually confirm the match from a short list of items from LWIN, but I would love to not have to manually match 17,000+ items in their entirety.

Thanks in advance for any help, this is going to be an interesting project, I just don't want to waste a million years on it."
dataengineering,Rise of Data Engineers,89,1i0yvo3,https://www.reddit.com/r/dataengineering/comments/1i0yvo3/rise_of_data_engineers/,1736831927.0,"Is it just me or everyone is seeing the trend that companies have been asking data engineering+ml engineering for the position of data scientist?

So my last client was hiring 3 ml engineer+data engineer combination and then hired 1 lead data scientist. His idea was that Lead DS guy will be designing and provide the solutioning. While as DE+ML team will be implementing the entire project(coding part). He did it because this helped him implement projects faster as DE knew the data and they already knew ML part too.

So are you guys seeing that trend too?"
dataengineering,Do you guys think cloud networking should be part of a data engineers skill set?,11,1i19t2u,https://www.reddit.com/r/dataengineering/comments/1i19t2u/do_you_guys_think_cloud_networking_should_be_part/,1736871812.0,"There's usually been a separate team handling cloud infrastructure where I've worked, but it's always caused problems with connectivity in my pipelines involving white/black listing, firewalls, subnets, VPNs, etc. Do you all feel like networking is something data engineers need to have a strong knowledge of, or is it too much additional complexity or too much overhead and requires a dedicated position or team?"
dataengineering,Is freeCodeCamp good for learning Data Science/Engineering? Looking for additional free resources,5,1i1eche,https://www.reddit.com/r/dataengineering/comments/1i1eche/is_freecodecamp_good_for_learning_data/,1736883219.0,"I'm planning to start my data science journey through freeCodeCamp, specifically focusing on Data Analysis and Data Engineering. Would love to hear from people who've used their curriculum:

1. How was your experience with freeCodeCamp's data science track?
2. Did you land a job after completing it?
3. What supplementary resources did you use alongside i"
dataengineering,"Just finished building a job scraper using Selenium and mongoDB. It automatically scrapes job listings from Indeed at regular intervals and sends reports (e.g., how many new jobs are found) directly to Telegram.",2,1i1iogg,https://www.youtube.com/watch?v=9vKHdG9doUk,1736894506.0,
dataengineering,Would you guys quit over a full time RTO call?,75,1i0v8c1,https://www.reddit.com/r/dataengineering/comments/1i0v8c1/would_you_guys_quit_over_a_full_time_rto_call/,1736820193.0,"I started working for a new place recently. The agreement, which conveniently wasn’t in my offer letter, was that I’d get a schedule of 3days/2days in/out of office. Pending two months, I’d get upgraded to a 2/3 in/out schedule.

We also just recently migrated from CRM ABC to CRM XYZ, and it’s caused a lot of trouble. The dev team has been working long hours around the clock to put out those fires. The fires have yet to be extinguished after a few weeks. Not that there hasn’t been progress, just that there’s been a lot of fires. A fire gets put out, a new one pops up.

More recently, a nontechnical middle manager advised a director that the issue belongs with poor communication. Since then, the director called a full time RTO. He wants everyone in house to solve this lack-of-communication, “until further notice.”

Now, maybe some of you are wondering why this affects the data engineer? After all, I am not developing their products… I am doing BI related stuff to help the analysts work effectively with data. So why am I here? It’s because they want my help putting out the fires.

Part of me thinks that this could be a temporary, circumstantial issue—I shouldn’t let it get to me.

But there’s another part of me that thinks this is complete bullshit. There isn’t a project manager / scrum master with technical knowledge anywhere in the organization. Our products are manifestations of ideas passed onto developers and developers getting to work. No thorough planning, nobody connecting all the dots first, none of that. So, how the fuck is sticking your little fingers into my daily regime—saying I need to come in daily—supposed to solve that problem?

Communication issues don’t get solved by brute forcing a product managers limited ability to manage a project like a scrum master. Communication issues are solved by hiring someone who speaks the right language. I think it’s royally fucked up that the business fundamentally decided that rather than pay for a proper catalyst of business to technical communication, they’ll instead let their developers pay that cost with their livelihood. 

I know that, in business, you ought to best separate your emotional and logical responses. For example, if I don’t like this change, I’d best just find a new job and try hard not to burn any bridges on my way out. It’s just frustrating, and I guess I’m just venting. These guys are going to loose talent and it’s going to be a pain in the ass getting talent back, all because of the inability of upper management to adequately prepare a team with the resources it needs and instead allowing their shortsightedness to be compensated with my regime. Fuck that.

My wife carpools with colleagues whenever I need to go into the office. My kids stay longer at after school. I loose nearly two hours in commute. Nobody gives a shit about my wife, my kids, nor myself though. I guess it’s only my problem until I decide it isn’t anymore, and find a new job."
dataengineering,"Python vs Pyspark vs Snowpark
",23,1i1141c,https://www.reddit.com/r/dataengineering/comments/1i1141c/python_vs_pyspark_vs_snowpark/,1736841204.0,"Hello Experts,

Want to understand, how different are these technologies from each other? What basic level certifications should be targeted to get started?

Actually recently many team members moved to modern data engineering role where our organization uses snowflake and pyspark as key technology. Not having background of python but many of the folks have extensive coding skills in sql and plsql programming. Currently our organization wants to get certified in pyspark and snowflake. So want to understand which certification in pyspark should be attempted? 

Any documentation or books which will help to get started in quick time? Or it would be difficult for the folks to switch to these techstacks from pure sql/plsql background?

"
dataengineering,Struggling to Connect Trino to ADLS Gen2 — Need Guidance!,2,1i1fo6n,https://www.reddit.com/r/dataengineering/comments/1i1fo6n/struggling_to_connect_trino_to_adls_gen2_need/,1736886531.0,"Hi everyone,

I’m trying to set up Trino to connect with Azure Data Lake Storage Gen2 (ADLS Gen2) for querying Parquet files. While I’ve made some progress, I’m stuck on a few key points and would appreciate any advice from the community.

What I’ve Done So Far:

	1.	Installed Trino: Running Trino via Docker.

	2.	Configured Catalog: Created hive.properties for the Hive connector with the following config:

connector.name=hive
hive.metastore.uri=thrift://localhost:9083  # Using or trying to avoid Hive Metastore?

ADLS Gen2 Config
fs.native-azure.enabled=true
azure.auth-type=ACCESS_KEY
azure.access-key=sas token
azure.endpoint=core.windows.net


	3.	ADLS Path: Trying to query directly like:

SELECT * FROM hive.default.""abfs://my-container@storage-account.dfs.core.windows.net/path/to/parquet-file"";



Key Questions:
	1.	Do I Need a Hive Metastore?
	•	Can Trino query ADLS Gen2 files directly without a Hive Metastore?
	•	If a Hive Metastore is mandatory, is there a lightweight alternative, or do I need the full setup?
	2.	Connection Issues:
	•	With the above setup, I’m getting errors like “Could not read table schema” or connection failures to hive meta store
	•	Is my fs.native-azure.enabled=true config correct, or do I need additional settings?
	3.	Best Practices for ADLS Gen2:
	•	Should I consider using Iceberg or Delta Lake instead of Hive for managing metadata?
	•	If anyone has successfully queried ADLS Gen2 from Trino, could you share your configuration?"
dataengineering,Certification to make up for no internships,1,1i1i8b7,https://www.reddit.com/r/dataengineering/comments/1i1i8b7/certification_to_make_up_for_no_internships/,1736893347.0,"Hi everyone, I am a recent graduate. I completed my bachelors from a mid tier university this semester. Regrettably, I do not have any internships. I am trying to get some relevant certifications to make myself more marketable. I am also making projects after finishing certification for a certain tech stack. What can you recommend me to work on to land a data engineer job ? "
dataengineering,Completed DP-203,1,1i1i453,https://www.reddit.com/r/dataengineering/comments/1i1i453/completed_dp203/,1736893049.0,"I just passed the DP-203 exam for Microsoft certified Associate Data Engineer . I have studied for it for a month. I was absolutely new to cloud when I started it, watched videos of Tybul who explained all the services and concepts very clearly however later I found out those 54 1 hour long videos were not enough to pass the test. So I solved a lots of practice questions available online. I was easily able to pass the test with all the material
"
dataengineering,Is anyone using pyAirbyte?,1,1i1hygv,https://www.reddit.com/r/dataengineering/comments/1i1hygv/is_anyone_using_pyairbyte/,1736892637.0,PyAirbyte seems like the most useful tool to come out of the whole airbyte endeavor.  Unfortunately there's virtually no documentation and i can't figure out how to configure my sources and destinations.  Does anyone have experience with using PyAirbyte?  How did you figure out how to setup your connections/destinations?
dataengineering,FAANG Job Opportunity - Feels Weird?,44,1i0uaou,https://www.reddit.com/r/dataengineering/comments/1i0uaou/faang_job_opportunity_feels_weird/,1736817443.0,"Need some opinions on a situation I find myself in...

I'm a DE with about 3-4 years experience, mostly at a start-up where I was more of an ""analytics engineer"" by function, but held a Senior DE title. Back in September, I had started a new job as a DE at a different startup, much more technical place where I'd be doing true DE work. At that same time...I was offered an IC4 role at Meta. I was pretty shocked honestly, even more so when they pushed so aggressively to bring me onboard, as I don't think I'm all that well-versed in the DE space. I ended up turning them down, as the role I had just started was remote and moving to NYC was too daunting.

Last week, I was laid off from my job at the new start-up -- they said it came down to ""fit"". I had been trying so hard, but was struggling without any guidance, support, or standards. I was learning, but was not nearly as technical as they had thought I was, or I needed to be. 

I reached back out to Meta and, just 3 days later, they put that original offer *back on the table*, with their NYC, Menlo Park, and Seattle offices all possibilities. 

I want to accept *so badly*, even more so now that I am out of a job. But two things worry me:

* My last job made me feel so incompetent, despite having been very successful at previous stops before. Will Meta's culture crush me? I'm willing to do whatever it takes to learn, just need an environment where I can do so.
* I am a little concerned by how hard they pushed for me originally **and** how quickly they made that offer available again. I am worried that it speaks to making me expendable if they had to cut people. Moving to a big city only to feel vulnerable to a layoff...that's not a good feeling!

Am I overthinking this? Should I just simply trust that my experience and performance in the interviews/tests was good enough for them to want me? HELP!"
dataengineering,Using Node.js + Apryse to Convert DOCX to Web-Ready PDFs on the Server ,3,1i16ptk,https://javascript.plainenglish.io/using-node-js-apryse-to-convert-docx-to-web-ready-pdfs-on-the-server-c1767e1187bb,1736863349.0,
dataengineering,Kafka Transactions Explained (Twice!),0,1i1fgnu,https://www.warpstream.com/blog/kafka-transactions-explained-twice,1736886001.0,
dataengineering,Databricks - is there an alternative to using DESCRIBE HISTORY for finding the last time a table was optimised/vacuumed?,2,1i1941q,https://www.reddit.com/r/dataengineering/comments/1i1941q/databricks_is_there_an_alternative_to_using/,1736869998.0,"Hi all,

I've created a workflow that can scan over 6000+ tables and find out the last time a table had an optimise and vacuum ran on it but... it's super slow. I have tried to limit the number of rows and columns my DESCRIBE queries return and whilst it has helped it is still slow af.

From my understanding, DESCRIBE queries are slow and expensive, so I wondered if using the delta lake API would be faster?"
dataengineering,Trino - How to work with arrays of structs without unnesting?,3,1i15aov,https://www.reddit.com/r/dataengineering/comments/1i15aov/trino_how_to_work_with_arrays_of_structs_without/,1736858893.0,"Hi everyone, I work with Trino on a daily basis and I keep coming against a problem that results in what I consider to be less than optimal solutions: 

Tables containing arrays of structs, where I want to do some basic aggregation of specific members. 

As an example, a row contains a column with an array of payment line items for an order. each entry in the array is a struct of uniform format, and I want to aggregate the values from a particular member of the struct across the whole array.

A possible solution is of course to unnest the array and then I can handle the struct members as individual columns, but is there no way to directly address the array directly? 

Is there no way to do something like 

Select SUM(arrayCol.localPriceAmount) 

which outputs the total value of every array elements localPriceAmount member?"
dataengineering,How is the job market for data engineering?,3,1i1cbwp,https://www.reddit.com/r/dataengineering/comments/1i1cbwp/how_is_the_job_market_for_data_engineering/,1736878169.0,"So I have about 1 - 2 yoe of experience in software engineering. However, I haven't done anything at all computer science related and looking for a new field to go into. I have a good amount as in months that i can devote to learn something. The thing is there's no point in working hard for the market to be bad. Also, know that you can't predict the market so I'm just for your thoughts of how the market for data engineering is and what you think it be like in months/years."
dataengineering,Data engineering dev tool SaaS companies : Boom or Bust?,0,1i1bb5k,https://www.reddit.com/r/dataengineering/comments/1i1bb5k/data_engineering_dev_tool_saas_companies_boom_or/,1736875560.0,"With acquisitions of Datavolo, SDF labs & Upsolver does it mean that data engineering dev tool SaaS companies are falling apart and rest are doomed or the expected boom time for data is here?"
dataengineering,Need help on a weird thing,1,1i1b60g,https://www.reddit.com/r/dataengineering/comments/1i1b60g/need_help_on_a_weird_thing/,1736875197.0,"My background: currently working as a DE in a good project and in a good team. Here good team means really good, colleagues are nice to me, care about me, shield me in case I make mistakes and encourage me as well. Even clients are good towards me.


So the problem? All these people are far away from me and I always meet them in a virtual calls. And due to this I am having a hard time socialising as I am always stuck on screen with almost no time to interact with people beside me and if I am working from home.
I am now a days unable to sleep properly as it is just my work in my mind. I love my job but not to the point of breakdown. No proper sleep, no exercise, no socialising, unable to take leave due to fear that people will not work properly in my absence.


What should I do now? I want to switch but it will take time but until then what I should do?
Guide me please on this as I am kind of on verge of a breakdown."
dataengineering,GCP bucket to s3,1,1i19lz5,https://www.reddit.com/r/dataengineering/comments/1i19lz5/gcp_bucket_to_s3/,1736871300.0,"Hi all, 

I would need advice about transferring around 8TB of files from GCP to s3 bucket (potential ly I would need to change the format of the file) . The GCP is not under our ""control"" which means it is not ours. Is there some inexpensive solution or generally how to approach to this? Any information which could point me in the right direction would be great. Also any personal experiences i.e. what not to do would be welcomed! Thanks! "
dataengineering,Documenting the data world,1,1i19emm,https://www.reddit.com/r/dataengineering/comments/1i19emm/documenting_the_data_world/,1736870768.0,"So I searched and there is no real answer in the subreddit, that I can see at least. If you know a thread please send it :)

We are mostly doing analytics with our data but there are also some data processes that are not related to any analytics.

I want to document everything we have very well but i am struggling to find some resources for data world/platform/model documentation. Bits and paces here and there but nothing of real substance.

What I want to do is a top bottom level approach so that people who want to know what data is available to use and thats it. Then a bit more in depth of what the data is and who uses it, who is responsible, what values do this data bring to the organization, what are the cost of this pipeline(actual and man hours to maintain develop). Then a layer that describes the process more in detail such as a DFD (data flow diagram) that I found recently no idea if you have a better solution.

If anyone has any good resources or has been working in the data world for a while i would appreciate any tips. I am not that experienced so please be kind if some of my points do not make sense or whatever sounds obvious to you does not to me because of my experience. Thanks : )"
dataengineering,Database from scratch,67,1i0hj62,https://www.reddit.com/r/dataengineering/comments/1i0hj62/database_from_scratch/,1736784899.0,"Currently I am tasked with building a database for our company from scratch. 
Our data sources are different files (Excel,csv,excel binary) collect from different sources, so they in 100 different formats. Very unstructured. 

 1. Is there a way to automate this data cleaning? Python/data prep softwares failed me, because one of the columns (and very important one) is “Company Name”. Our very beautiful sources, aka, our sales team has 12 different versions of the same company, like ABC Company, A.B.C Company and ABCComp etc. How do I clean such a data? 

 2. After cleaning, what would be a good storage and format for storing database? Leaning towards no code options. Is red shift/snowflake good for a growing business. There will be a good flow of data, needed to be retrieved at least weekly for insights. 

 3. Is it better to Maintain as excel/csv in google drive? Management wants this, thought as a data scientist this is my last option. What are the pros and cons of this

"
dataengineering,Your production code contains ?,0,1i1e0eh,https://www.reddit.com/r/dataengineering/comments/1i1e0eh/your_production_code_contains/,1736882377.0,"Most of your production code contains the following library? Just a fun poll to understand the usage.

[View Poll](https://www.reddit.com/poll/1i1e0eh)"
dataengineering,Build a Database for Semi-Structured Data with Cloud Services,1,1i14xiw,https://www.reddit.com/r/dataengineering/comments/1i14xiw/build_a_database_for_semistructured_data_with/,1736857611.0,"Hello, data engineers! After introducing the rise of Shared Disk Architecture and Semi-structured Data Processing, we release the third blog to introduce the challenge we encountered in designing a database optimized for handling semi-structured data and our solution, [ScopeDB](https://www.scopedb.io/)

[https://flex-ninja.medium.com/build-a-database-for-semi-structured-data-with-cloud-services-4bce1b8f15f3](https://flex-ninja.medium.com/build-a-database-for-semi-structured-data-with-cloud-services-4bce1b8f15f3)

Takeaways:

* By leveraging object storage services (OSS) for user data and relational database services (RDS) for metadata, we can handle data in petabytes properly and achieve approximately *30 times* cost reduction compared to Shared Nothing Architecture solutions on the cloud.
* We define a dedicated internal format for variant data and optimize data analysis with a specific columnar data file format. As a result, searching among hundreds of millions of log records can be done in *one minute*, while if a time range filter in hours is specified, the query can be done in *seconds*.
* Our database can reflect the object schema and retrieve data from a variant value with its object path, allowing users to play with semi-structured data smoothly.

There are many more details and numbers we omit due to the space. Welcome to comment or drop an email to discuss :D"
dataengineering,Old person but early in DE career. Stick with comfortable but low growth job or take what is likely a huge stretch but ambition-aligned new role?,9,1i0t15s,https://www.reddit.com/r/dataengineering/comments/1i0t15s/old_person_but_early_in_de_career_stick_with/,1736813875.0,"What would you advise someone who has the opportunity to take a huge leap forward in terms of career development (first DE hire for a lead data engineer role responsible for migrating and modeling data, gradually building a team, and doing ML Ops for data science) but it would be a stretch and require a ton of learning while simultaneously having to deliver? When do you know it’s going to be too demanding and you risk performing poorly versus when you’re ok to take the leap since this isn’t a spectator sport and we often learn best on the job with all the trappings and complexity of a business, its people, and their data? 

My current gig is a dream, especially for someone new to data engineering. Low stress. Lots of encouragement. Learning a lot. Great team. No micromanaging. Work-life balance. Great benefits. Good comp. However, advancement seems rare. This new gig pays a good deal more, I’d advance (eventually leading a team and continuing to move up), and I get to satisfy my ambition of building something from the ground up that will scale and grow. Invariably, the pressure will be high and I’ll be working a good bit harder since, well, I’m pretty new at this. Oh, and I’m old, like, probably 15-20 years older than I imagine most folks on this sub. No kids. Just me, spouse, and I’d argue an enviable amount of free time. What would you do?"
dataengineering,I'm not sure if I'm allowed to web scrape,11,1i0qifq,https://www.reddit.com/r/dataengineering/comments/1i0qifq/im_not_sure_if_im_allowed_to_web_scrape/,1736807117.0,"Quick question!   
I'm working on a project that involves NBA2k attributes and tendencies.  
I was able to find CSV file of all current player attributes BUT couldn't find player tendencies.

ChatGPT gave me few options but I decided to go with web scraping this info from [https://2kdb.net/](https://2kdb.net/)  
This was my first time web scraping. I used Jupyter Lab. Took me about 2 hours but I got it done while learning in the process.

Am I allowed to extract information like this from this or any other website? I have no idea what are legal and ethical guidelines for web scraping. "
dataengineering,Data Management / Governance / Quality - Where do you go to stay up to date?,9,1i0qtzc,https://www.reddit.com/r/dataengineering/comments/1i0qtzc/data_management_governance_quality_where_do_you/,1736807945.0,"Hey all,
Essentially the name of the topic.

I follow Dataversity, but I'd like to hear more from you.

People you like or websites / newsletters Essentially focused mainly in Data Management / Governance / Quality

Thanks!"
dataengineering,How to configure Snowflake MySQL native connector,1,1i12rlw,https://www.reddit.com/r/dataengineering/comments/1i12rlw/how_to_configure_snowflake_mysql_native_connector/,1736848770.0,"Hello everyone

I’m excited to share my first article on Medium! In this article, I walk through the steps to configure the Snowflake MySQL native connector, including key setup details and best practices. I wanted to share these insights that might help others in similar roles.

Would love to hear your thoughts and feedback!

https://medium.com/@khaled.khouli175/how-to-configure-the-snowflake-mysql-connector-1b096f009c19"
dataengineering,"Integrating Medallion Architecture (yes, that) in a Data Lakehouse with a Semantic Layer Tool: where does the GOLD sit?",1,1i12i6w,https://www.reddit.com/r/dataengineering/comments/1i12i6w/integrating_medallion_architecture_yes_that_in_a/,1736847591.0,"At my company, we're implementing the Medallion Architecture (Bronze, Silver, Gold) within a Data Lakehouse framework and are looking to integrate a Semantic Layer tool (to support BI Team).

I believe that to fully leverage the Medallion Architecture, semantic modeling could begin even in the Silver Zone (IMHO) and definitely in the Gold Zone. In this scenario, should the Gold Zone be managed entirely within the Semantic Layer tool?

Looking for your **insights** or **experiences**: **approaches to utilize both the Medallion Architecture and a Semantic Layer?**

**P.S.**

I’ve noticed that the term ""Medallion Architecture"" is often met with skepticism. To clarify our approach: the Bronze layer handles raw data ingestion, the Silver layer focuses on key business data entities, and the Gold layer consists of datasets tailored for KPIs, which are joined, aggregated, and filtered. The Semantic Layer is centered on business KPIs, where it overlaps with the Gold Zone, but in addition manages derived metrics such as counts, averages, and sums, time context."
dataengineering,Getting the opportunity to eventually promote to Data Engineer,2,1i0z0bv,https://www.reddit.com/r/dataengineering/comments/1i0z0bv/getting_the_opportunity_to_eventually_promote_to/,1736832421.0,"Hey everyone,

I currently work as a Data Analyst / BI Analyst for a large non tech company. We recently got a new CIO and she wants to build a much more comprehensive data structure (which is much needed)

Long story short, she wants me to pursue some online certifications to become well versed in data engineering. She’s giving me a timeline of 6 months to a year. My homework assignment is to come up with a plan with courses and projects to gain these skills. Not too concerned with specific technologies, but rather learning the concepts and skills in some form. Obviously I would be in a more “junior” role, so I don’t need to necessarily come out guns blazing, but rather just have a good grasp on the fundamentals and be able to have something to show for it. 

All that to say is that I came seeking advice for someone in my position, good courses, certifications, etc.

Any help or advice would be great, thanks!"
dataengineering,Would you stage report-ready data directly into the gold lakehouse?,6,1i0p0yh,https://www.reddit.com/r/dataengineering/comments/1i0p0yh/would_you_stage_reportready_data_directly_into/,1736803327.0,"I have a system that we're bringing into our lakehouse architecture that is already in a format that we will report on directly. I'm stuck on whether to bring that data into the bronze lakehouse or the gold lakehouse based on standard best practice. Technically, this is gold-level data, but at the same time I would think it would be best to land all external data into the bronze lakehouse, then report off of that data. For the end user, I could see this as confusing since they're used to gold being where to pull clean data from. I'm curious, how would you go about this?"
dataengineering,Blog post: Data Masking with Azure Databricks,3,1i0r2pj,https://datanrg.blogspot.com/2025/01/data-masking-with-azure-databricks.html,1736808575.0,
dataengineering,Video walkthrough for setting up a GCP VM with the updated UI,5,1i0qrmz,https://i.redd.it/kpgkoxg79uce1.png,1736807782.0,
dataengineering,Am I Being Underpaid? C2H Data Warehouse Developer Position via Staffing Agency,4,1i0qpxg,https://www.reddit.com/r/dataengineering/comments/1i0qpxg/am_i_being_underpaid_c2h_data_warehouse_developer/,1736807658.0,"Hello everyone, hope you're all having a great day! 

I'm a new grad and recently received a contract-to-hire (C2H) offer through a staffing agency for a Data Warehouse Developer position at a company. 

I don't have previous work experience for this position but I am proficient in the programming languages they require so I got the offer. The agency initially offered me $30/hour and then called me and said they increased it to $35/hour the next day, explaining that they realized the initial offer was low. This includes standard holidays and 10 days of PTO. 

However, I found that this rate is still on the lower end of the market average for my role in Rochester, NY, which ranges from $34 to over $60 per hour (not sure if it's reliable). Given that this is a C2H position, shouldn't the rate be even higher than the average on the market? The job description indicates that the company is looking for someone with 3-5 years of experience, which I don't have. 

I'm concerned about how this might affect my perceived value. I'd love to become a direct hire with the company, but I'm uncertain how the staffing agency's interests might influence this transition, especially if they prefer to keep me as a contractor for revenue reasons... 

I'm also concerned about whether the company is fully aware of my actual pay rate, as I want to avoid any misunderstandings about my efforts and their value. (Imagine you are working with them as a $35/hour developer, but they believe they are paying for a $55/hour developer. How am I ever going to reach their expectations.)

I appreciate any advice on handling and understanding this situation. Thanks!"
dataengineering,Daily copy to Azure SQL,8,1i0kb85,https://www.reddit.com/r/dataengineering/comments/1i0kb85/daily_copy_to_azure_sql/,1736791752.0,"Hi, I have a SQL Server database containing 15 tables that need to be copied daily to an Azure SQL database (for various business reasons); this will be a mixture of incremental and full copy jobs.
I have established I can do this either using SSIS (Hosted on the Azure runtime of the Azure SQL instance, which I believe ADF needs to be enabled for) or using ADF and a copy data pipeline.
Can anyone comment on the price comparison for each of these transfer methods? It'll be about 100k new records every day.

Full disclosure, this is for work. But also for learning!"
dataengineering,The Future of Unified Observability: Integrating Data Observability with OpenTelemetry and eBPF,1,1i0z9y5,https://dsrnk.hashnode.dev/the-future-of-unified-observability-integrating-data-observability-with-opentelemetry-and-ebpf,1736833462.0,
dataengineering,Tips for junior engineer,3,1i0pd6j,https://www.reddit.com/r/dataengineering/comments/1i0pd6j/tips_for_junior_engineer/,1736804196.0,"Hi I hope everyone had a good start on the year. Some information about me, I graduated last year and have been working for a little over a year as a data platform engineer in aws. We have an ETL platform and are building it using IaC, I went from not knowing what aws was to be involved in some big projects which sometimes can feel overwhelming and a lot of pressure, but the upside is the learning curve. 

But I want to know if you guys have some tips that I can take with me in my career  "
dataengineering,"Is databricks workspace running on K8s? If yes, what kind of PV they use?",2,1i0tvnm,https://www.reddit.com/r/dataengineering/comments/1i0tvnm/is_databricks_workspace_running_on_k8s_if_yes/,1736816251.0,"Hi folks,

I'm trying to create a pipeline mimicking Databricks workspace. Basically just a simple Jupyterhub + Spark on K8s.

By documentation, Databricks use cloud object storage, when deploy a workspace. But Jupyterhub needs a file system as PV, not object storage.

How the hell did they do that? Did they convert the object storage to file system? Hope some experts can shed some light on this. Thanks in advance."
dataengineering,"Looking for the next step, B.S. in Data Sciences with 3+ years work experience, what now?",1,1i0ya7d,https://www.reddit.com/r/dataengineering/comments/1i0ya7d/looking_for_the_next_step_bs_in_data_sciences/,1736829791.0,"hey y'all! I just recently hit a career milestone and have achieved 3 years work experience, I accepted a job right out of college and ended up landing a position in philly as a junior data engineer. now with the experience collected, I wanted to explore opportunties to work elsewhere. im from the great lakes area and have been meaning to return, but struggling to find what/where to apply for in those areas or find stuff remotely.

What has been booming recently in the industry? Anything new or up and coming I should be out on the look for? I'd be happy working for any of these positions:

* Data Engineer
* Data Architect
* Data Integratior
* Data Automation Specialist
* Data Scientist
* SQL Developer
* Business Intelligence Developer
* Database Analyst
* Database Specialist
* DBA
* Data Warehouse administrator 
* IT Architect

& any other appliable position within the field of data science!

Thank you. I appreciate you for reading my post :)"
dataengineering,Data Warehousing Architecture for a Bank’s Data Infrastructure,16,1i0cpuy,https://www.reddit.com/r/dataengineering/comments/1i0cpuy/data_warehousing_architecture_for_a_banks_data/,1736770865.0,"I'm working on a data warehousing project for a bank, and I need advice on the best architecture for their needs. The bank handles large volumes of transactional and customer data, and we aim to support real-time analytics, improve reporting, and integrate multiple systems.

Specifically, I’m looking for input on:

1. Should we use a traditional star or snowflake schema, or is a modern cloud-native solution like Snowflake or BigQuery better?
2. Best practices for secure ETL processes in the banking sector?
3. Tools/architectures for handling real-time data, especially for fraud detection and customer behavior?
4. Should we use open-source tools like Kafka, PySpark, Postgres, etc., or stick with enterprise solutions?
5. How to ensure compliance and data security (GDPR, PCI DSS)?
6. Strategies for scaling the warehouse as data grows?

Any recommendations based on experience with financial institutions would be greatly appreciated."
dataengineering,What are my options once my dbt project grow beyond a couple hundred models,5,1i0mkm3,https://www.reddit.com/r/dataengineering/comments/1i0mkm3/what_are_my_options_once_my_dbt_project_grow/,1736797257.0,"So here is my situation. My project grew to the point (about 500 models) where the compile operation is taking a long time significantly impacting the development experience.

Is there anything I can do besides breaking up the project into smaller projects?

If so, is there anything I can do to make the process less painfull?"
dataengineering,What are my options once my dbt project grow beyond a couple hundred models,4,1i0mklp,https://www.reddit.com/r/dataengineering/comments/1i0mklp/what_are_my_options_once_my_dbt_project_grow/,1736797257.0,"So here is my situation. My project grew to the point (about 500 models) where the compile operation is taking a long time significantly impacting the development experience.

Is there anything I can do besides breaking up the project into smaller projects?

If so, is there anything I can do to make the process less painfull?"
dataengineering, Databricks or Informatica for AI pipelines?,3,1i0j47l,https://www.reddit.com/r/dataengineering/comments/1i0j47l/databricks_or_informatica_for_ai_pipelines/,1736788820.0,"Hey folks, I’m currently trying to get a better understanding of AI architecture and researching how to build efficient data pipelines for machine learning. While digging into different tools, I keep coming across Databricks and Informatica, but honestly, I’m struggling to fully grasp the differences and scope of each platform.

From what I’ve gathered so far:

\- Databricks: Seems great for big data and ML workflows, with unified analytics and strong support for data lakes.

\- Informatica: Appears to focus more on data integration, governance, and traditional ETL processes.

But here’s where I’m stuck:

When would you pick one over the other? 

And why is Informatica gaining more traction lately in the AI/ML space? Is it just marketing, or does it have an edge for things like compliance and data governance?

Would love to hear from anyone who’s worked with either (or both). "
